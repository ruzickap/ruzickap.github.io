<!doctype html><html lang="en" data-mode="dark"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Amazon EKS and Grafana stack" /><meta name="author" content="Petr Ruzicka" /><meta property="og:locale" content="en" /><meta name="description" content="Build secure Amazon EKS cluster with Grafana stack" /><meta property="og:description" content="Build secure Amazon EKS cluster with Grafana stack" /><link rel="canonical" href="https://ruzickap.github.io/posts/amazon-eks-grafana-stack/" /><meta property="og:url" content="https://ruzickap.github.io/posts/amazon-eks-grafana-stack/" /><meta property="og:site_name" content="Petr’s Blog" /><meta property="og:image" content="https://raw.githubusercontent.com/grafana/.github/12fb002302b5efad6251075f45ce5ac22db69a3f/LGTM_wallpaper_1920x1080.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2026-01-13T00:00:00+01:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://raw.githubusercontent.com/grafana/.github/12fb002302b5efad6251075f45ce5ac22db69a3f/LGTM_wallpaper_1920x1080.png" /><meta property="twitter:title" content="Amazon EKS and Grafana stack" /><meta name="twitter:site" content="@Ruzicka_Petr" /><meta name="twitter:creator" content="@Ruzicka_Petr" /><meta name="google-site-verification" content="4ugGShJoiB9veoPwFcMsMjzk-1BYbi5ozcSnovkBWtA" /><meta name="msvalidate.01" content="592AB158E3F9EF887F8DC8E69A2FF754" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Petr Ruzicka","url":"https://petr.ruzicka.dev"},"dateModified":"2026-02-17T23:50:29+01:00","datePublished":"2026-01-13T00:00:00+01:00","description":"Build secure Amazon EKS cluster with Grafana stack","headline":"Amazon EKS and Grafana stack","image":"https://raw.githubusercontent.com/grafana/.github/12fb002302b5efad6251075f45ce5ac22db69a3f/LGTM_wallpaper_1920x1080.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://ruzickap.github.io/posts/amazon-eks-grafana-stack/"},"url":"https://ruzickap.github.io/posts/amazon-eks-grafana-stack/"}</script><title>Amazon EKS and Grafana stack | Petr's Blog</title><link rel="icon" type="image/png" href="/assets/img/favicons/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/assets/img/favicons/favicon.svg"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.36.4/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.18/dayjs.min.js,npm/dayjs@1.11.18/locale/en.js,npm/dayjs@1.11.18/plugin/relativeTime.js,npm/dayjs@1.11.18/plugin/localizedFormat.js,npm/tocbot@4.36.4/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> document.addEventListener('DOMContentLoaded', () => { const pv = document.getElementById('pageviews'); if (pv !== null) { const uri = location.pathname.replace(/\/$/, ''); const url = `https://ruzickap-github-io.goatcounter.com/counter/${encodeURIComponent(uri)}.json`; fetch(url) .then((response) => response.json()) .then((data) => { const count = data.count.replace(/\D/g, ''); pv.innerText = new Intl.NumberFormat().format(count); }) .catch((error) => { pv.innerText = '1'; }); } }); </script> <script defer src="/app.min.js?baseurl=&register=true" ></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-64WRCWDZM8"></script> <script> document.addEventListener('DOMContentLoaded', () => { window.dataLayer = window.dataLayer || []; function gtag() { dataLayer.push(arguments); } gtag('js', new Date()); gtag('config', 'G-64WRCWDZM8'); }); </script> <script async src="https://gc.zgo.at/count.js" data-goatcounter="https://ruzickap-github-io.goatcounter.com/count" ></script> <script defer src="https://cloud.umami.is/script.js" data-website-id="380ea41a-7d5f-4423-8e13-03151bde195f" ></script> <script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"token": "ad8c64404e1e41dba184019564cf11bb"}' ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"><img src="https://0.gravatar.com/avatar/5484c0fd9f98e2ffd9212b158931bf4b?s=200" width="112" height="112" alt="avatar" onerror="this.style.display='none'"></a> <a class="site-title d-block" href="/">Petr's Blog</a><p class="site-subtitle fst-italic mb-0">DevOps • GitOps • K8s</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-project-diagram"></i> <span>PROJECTS</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <a href="https://github.com/ruzickap" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="javascript:location.href = 'mailto:' + ['petr.ruzicka','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="https://petr.ruzicka.dev" aria-label="homepage" target="_blank" rel="noopener noreferrer" > <i class="fas fa-home"></i> </a> <a href="https://mastodon.social/@petr_ruzicka" aria-label="mastodon" target="_blank" rel="noopener noreferrer me" > <i class="fab fa-mastodon"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Amazon EKS and Grafana stack</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>Amazon EKS and Grafana stack</h1><p class="post-desc fw-light mb-4">Build secure Amazon EKS cluster with Grafana stack</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1768258800" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Jan 13, 2026 </time> </span> <span> Updated <time data-ts="1771368629" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Feb 17, 2026 </time> </span><div class="mt-3 mb-3"> <a href="https://raw.githubusercontent.com/grafana/.github/12fb002302b5efad6251075f45ce5ac22db69a3f/LGTM_wallpaper_1920x1080.png" class="popup img-link preview-img shimmer"><img src="https://raw.githubusercontent.com/grafana/.github/12fb002302b5efad6251075f45ce5ac22db69a3f/LGTM_wallpaper_1920x1080.png" alt="Preview Image" width="1200" height="630" loading="lazy"></a></div><div class="d-flex justify-content-between"> <span> By <em> <a href="https://petr.ruzicka.dev">Petr Ruzicka</a> </em> </span><div> <span> <em id="pageviews"> <i class="fas fa-spinner fa-spin small"></i> </em> views </span> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="8008 words" > <em>44 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">Amazon EKS and Grafana stack</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">Amazon EKS and Grafana stack</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><p>I will outline the steps for setting up an <a href="https://aws.amazon.com/eks/">Amazon EKS</a> environment that prioritizes security, including the configuration of standard applications.</p><p>The Amazon EKS setup should align with the following criteria:</p><ul><li>Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic<li>Spot instances<li>Less expensive region - <code class="language-plaintext highlighter-rouge">us-east-1</code><li>Most price efficient EC2 instance type <code class="language-plaintext highlighter-rouge">t4g.medium</code> (2 x CPU, 4GB RAM) using <a href="https://aws.amazon.com/ec2/graviton/">AWS Graviton</a> based on ARM<li>Use <a href="https://github.com/bottlerocket-os/bottlerocket">Bottlerocket OS</a> for a minimal operating system, CPU, and memory footprint<li>Leverage <a href="https://aws.amazon.com/elasticloadbalancing/network-load-balancer/">Network Load Balancer (NLB)</a> for highly cost-effective and optimized load balancing<li><a href="https://karpenter.sh/">Karpenter</a> to enable automatic node scaling that matches the specific resource requirements of pods<li>The Amazon EKS control plane must be <a href="https://docs.aws.amazon.com/eks/latest/userguide/enable-kms.html">encrypted using KMS</a><li>Worker node <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html">EBS volumes must be encrypted</a><li><a href="https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html">EKS cluster logging</a> to <a href="https://aws.amazon.com/cloudwatch/">CloudWatch</a> must be configured<li><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">Network Policies</a> should be enabled where supported<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html">EKS Pod Identities</a> should be used to allow applications and pods to communicate with AWS APIs</ul><h2 id="build-amazon-eks"><span class="me-2">Build Amazon EKS</span><a href="#build-amazon-eks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="requirements"><span class="me-2">Requirements</span><a href="#requirements" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>You will need to configure the <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html">AWS CLI</a> and set up other necessary secrets and variables:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="c"># AWS Credentials</span>
<span class="nb">export </span><span class="nv">AWS_ACCESS_KEY_ID</span><span class="o">=</span><span class="s2">"xxxxxxxxxxxxxxxxxx"</span>
<span class="nb">export </span><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="o">=</span><span class="s2">"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"</span>
<span class="nb">export </span><span class="nv">AWS_SESSION_TOKEN</span><span class="o">=</span><span class="s2">"xxxxxxxx"</span>
<span class="nb">export </span><span class="nv">AWS_ROLE_TO_ASSUME</span><span class="o">=</span><span class="s2">"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle"</span>
<span class="nb">export </span><span class="nv">GOOGLE_CLIENT_ID</span><span class="o">=</span><span class="s2">"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com"</span>
<span class="nb">export </span><span class="nv">GOOGLE_CLIENT_SECRET</span><span class="o">=</span><span class="s2">"GOxxxxxxxxxxxxxxxtw"</span>
</pre></table></code></div></div><p>If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="c"># AWS Region</span>
<span class="nb">export </span><span class="nv">AWS_DEFAULT_REGION</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">:-</span><span class="nv">us</span><span class="p">-east-1</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Hostname / FQDN definitions</span>
<span class="nb">export </span><span class="nv">CLUSTER_FQDN</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">:-</span><span class="nv">k01</span><span class="p">.k8s.mylabs.dev</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Base Domain: k8s.mylabs.dev</span>
<span class="nb">export </span><span class="nv">BASE_DOMAIN</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="p">#*.</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Cluster Name: k01</span>
<span class="nb">export </span><span class="nv">CLUSTER_NAME</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="p">%%.*</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">export </span><span class="nv">MY_EMAIL</span><span class="o">=</span><span class="s2">"petr.ruzicka@gmail.com"</span>
<span class="nb">export </span><span class="nv">TMP_DIR</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">:-${</span><span class="nv">PWD</span><span class="k">}</span><span class="p">/tmp</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">export </span><span class="nv">KUBECONFIG</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">KUBECONFIG</span><span class="k">:-${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="p">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="p">/kubeconfig-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="p">.conf</span><span class="k">}</span><span class="s2">"</span>
<span class="c"># Tags used to tag the AWS resources</span>
<span class="nb">export </span><span class="nv">TAGS</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="k">:-</span><span class="nv">Owner</span><span class="p">=</span><span class="k">${</span><span class="nv">MY_EMAIL</span><span class="k">}</span><span class="p">,Environment=dev,Cluster=</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}}</span><span class="s2">"</span>
<span class="nb">export </span><span class="nv">AWS_PARTITION</span><span class="o">=</span><span class="s2">"aws"</span>
<span class="nv">AWS_ACCOUNT_ID</span><span class="o">=</span><span class="si">$(</span>aws sts get-caller-identity <span class="nt">--query</span> <span class="s2">"Account"</span> <span class="nt">--output</span> text<span class="si">)</span> <span class="o">&amp;&amp;</span> <span class="nb">export </span>AWS_ACCOUNT_ID
<span class="nb">mkdir</span> <span class="nt">-pv</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><p>Confirm that all essential variables have been properly configured:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre>: <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_ACCESS_KEY_ID</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>
: <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>
: <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_SECRET_ACCESS_KEY</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>
: <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_ROLE_TO_ASSUME</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>
: <span class="s2">"</span><span class="k">${</span><span class="nv">GOOGLE_CLIENT_ID</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>
: <span class="s2">"</span><span class="k">${</span><span class="nv">GOOGLE_CLIENT_SECRET</span><span class="p">?</span><span class="k">}</span><span class="s2">"</span>

<span class="nb">echo</span> <span class="nt">-e</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MY_EMAIL</span><span class="k">}</span><span class="s2"> | </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2"> | </span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2"> | </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="se">\n</span><span class="k">${</span><span class="nv">TAGS</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><p>Install the required tools:</p><blockquote class="prompt-tip"><p>You can bypass these procedures if you already have all the essential software installed.</p></blockquote><ul><li><a href="https://aws.amazon.com/cli/">AWS CLI</a><li><a href="https://eksctl.io/">eksctl</a><li><a href="https://github.com/kubernetes/kubectl">kubectl</a><li><a href="https://github.com/helm/helm">helm</a></ul><h3 id="configure-aws-route-53-domain-delegation"><span class="me-2">Configure AWS Route 53 Domain delegation</span><a href="#configure-aws-route-53-domain-delegation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote class="prompt-info"><p>The DNS delegation tasks should be executed as a one-time operation.</p></blockquote><p>Create a DNS zone for the EKS clusters:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">CLOUDFLARE_EMAIL</span><span class="o">=</span><span class="s2">"petr.ruzicka@gmail.com"</span>
<span class="nb">export </span><span class="nv">CLOUDFLARE_API_KEY</span><span class="o">=</span><span class="s2">"1xxxxxxxxx0"</span>

aws route53 create-hosted-zone <span class="nt">--output</span> json <span class="se">\</span>
  <span class="nt">--name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--caller-reference</span> <span class="s2">"</span><span class="si">$(</span><span class="nb">date</span><span class="si">)</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--hosted-zone-config</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">Comment</span><span class="se">\"</span><span class="s2">: </span><span class="se">\"</span><span class="s2">Created by petr.ruzicka@gmail.com</span><span class="se">\"</span><span class="s2">, </span><span class="se">\"</span><span class="s2">PrivateZone</span><span class="se">\"</span><span class="s2">: false}"</span> | jq
</pre></table></code></div></div><p><a href="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedzones-k8s.mylabs.dev-1.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedzones-k8s.mylabs.dev-1.avif" alt="Route53 k8s.mylabs.dev zone" loading="lazy"></a> <em>Route53 k8s.mylabs.dev zone</em></p><p>Utilize your domain registrar to update the nameservers for your zone (e.g., <code class="language-plaintext highlighter-rouge">mylabs.dev</code>) to point to Amazon Route 53 nameservers. Here’s how to discover the required Route 53 nameservers:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="nv">NEW_ZONE_ID</span><span class="o">=</span><span class="si">$(</span>aws route53 list-hosted-zones <span class="nt">--query</span> <span class="s2">"HostedZones[?Name==</span><span class="se">\`</span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2">.</span><span class="se">\`</span><span class="s2">].Id"</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="nv">NEW_ZONE_NS</span><span class="o">=</span><span class="si">$(</span>aws route53 get-hosted-zone <span class="nt">--output</span> json <span class="nt">--id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NEW_ZONE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s2">"DelegationSet.NameServers"</span><span class="si">)</span>
<span class="nv">NEW_ZONE_NS1</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NEW_ZONE_NS</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-r</span> <span class="s2">".[0]"</span><span class="si">)</span>
<span class="nv">NEW_ZONE_NS2</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NEW_ZONE_NS</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-r</span> <span class="s2">".[1]"</span><span class="si">)</span>
</pre></table></code></div></div><p>Establish the NS record in <code class="language-plaintext highlighter-rouge">k8s.mylabs.dev</code> (your <code class="language-plaintext highlighter-rouge">BASE_DOMAIN</code>) for proper zone delegation. This operation’s specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>ansible <span class="nt">-m</span> cloudflare_dns <span class="nt">-c</span> <span class="nb">local</span> <span class="nt">-i</span> <span class="s2">"localhost,"</span> localhost <span class="nt">-a</span> <span class="s2">"zone=mylabs.dev record=</span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2"> type=NS value=</span><span class="k">${</span><span class="nv">NEW_ZONE_NS1</span><span class="k">}</span><span class="s2"> solo=true proxied=no account_email=</span><span class="k">${</span><span class="nv">CLOUDFLARE_EMAIL</span><span class="k">}</span><span class="s2"> account_api_token=</span><span class="k">${</span><span class="nv">CLOUDFLARE_API_KEY</span><span class="k">}</span><span class="s2">"</span>
ansible <span class="nt">-m</span> cloudflare_dns <span class="nt">-c</span> <span class="nb">local</span> <span class="nt">-i</span> <span class="s2">"localhost,"</span> localhost <span class="nt">-a</span> <span class="s2">"zone=mylabs.dev record=</span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2"> type=NS value=</span><span class="k">${</span><span class="nv">NEW_ZONE_NS2</span><span class="k">}</span><span class="s2"> solo=false proxied=no account_email=</span><span class="k">${</span><span class="nv">CLOUDFLARE_EMAIL</span><span class="k">}</span><span class="s2"> account_api_token=</span><span class="k">${</span><span class="nv">CLOUDFLARE_API_KEY</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><div class="language-console highlighter-rouge"><div class="code-header"> <span data-label-text="Console"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
</pre><td class="rouge-code"><pre><span class="gp">localhost | CHANGED =&gt;</span><span class="w"> </span><span class="o">{</span>
<span class="go">    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": true,
    "result": {
        "record": {
            "content": "ns-885.awsdns-46.net",
            "created_on": "2020-11-13T06:25:32.18642Z",
            "id": "dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb",
            "locked": false,
            "meta": {
                "auto_added": false,
                "managed_by_apps": false,
                "managed_by_argo_tunnel": false,
                "source": "primary"
            },
            "modified_on": "2020-11-13T06:25:32.18642Z",
            "name": "k8s.mylabs.dev",
            "proxiable": false,
            "proxied": false,
            "ttl": 1,
            "type": "NS",
            "zone_id": "2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe",
            "zone_name": "mylabs.dev"
        }
    }
}
</span><span class="gp">localhost | CHANGED =&gt;</span><span class="w"> </span><span class="o">{</span>
<span class="go">    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": true,
    "result": {
        "record": {
            "content": "ns-1692.awsdns-19.co.uk",
            "created_on": "2020-11-13T06:25:37.605605Z",
            "id": "9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb",
            "locked": false,
            "meta": {
                "auto_added": false,
                "managed_by_apps": false,
                "managed_by_argo_tunnel": false,
                "source": "primary"
            },
            "modified_on": "2020-11-13T06:25:37.605605Z",
            "name": "k8s.mylabs.dev",
            "proxiable": false,
            "proxied": false,
            "ttl": 1,
            "type": "NS",
            "zone_id": "2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe",
            "zone_name": "mylabs.dev"
        }
    }
}
</span></pre></table></code></div></div><p><a href="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/cloudflare-mylabs-dev-dns-records.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/cloudflare-mylabs-dev-dns-records.avif" alt="CloudFlare mylabs.dev zone" loading="lazy"></a> <em>CloudFlare mylabs.dev zone</em></p><h3 id="create-the-service-linked-role"><span class="me-2">Create the service-linked role</span><a href="#create-the-service-linked-role" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><blockquote class="prompt-info"><p>Creating the service-linked role for Spot Instances is a one-time operation.</p></blockquote><p>Create the <code class="language-plaintext highlighter-rouge">AWSServiceRoleForEC2Spot</code> role to use Spot Instances in the Amazon EKS cluster:</p><div class="language-shell highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>aws iam create-service-linked-role <span class="nt">--aws-service-name</span> spot.amazonaws.com
</pre></table></code></div></div><p>Details: <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-requests.html">Work with Spot Instances</a></p><h3 id="create-route53-zone-and-kms-key-infrastructure"><span class="me-2">Create Route53 zone and KMS key infrastructure</span><a href="#create-route53-zone-and-kms-key-infrastructure" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Generate a CloudFormation template that defines an <a href="https://aws.amazon.com/route53/">Amazon Route 53</a> zone and an <a href="https://aws.amazon.com/kms/">AWS Key Management Service (KMS)</a> key.</p><p>Add the new domain <code class="language-plaintext highlighter-rouge">CLUSTER_FQDN</code> to Route 53, and set up DNS delegation from the <code class="language-plaintext highlighter-rouge">BASE_DOMAIN</code>.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
</pre><td class="rouge-code"><pre><span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/aws-cf-route53-kms.yml"</span> <span class="o">&lt;&lt;</span> <span class="sh">\</span><span class="no">EOF</span><span class="sh">
AWSTemplateFormatVersion: 2010-09-09
Description: Route53 entries and KMS key

Parameters:
  BaseDomain:
    Description: "Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev"
    Type: String
  ClusterFQDN:
    Description: "Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev"
    Type: String
  ClusterName:
    Description: "Cluster Name - Ex: k01"
    Type: String
Resources:
  HostedZone:
    Type: AWS::Route53::HostedZone
    Properties:
      Name: !Ref ClusterFQDN
  RecordSet:
    Type: AWS::Route53::RecordSet
    Properties:
      HostedZoneName: !Sub "</span><span class="k">${</span><span class="nv">BaseDomain</span><span class="k">}</span><span class="sh">."
      Name: !Ref ClusterFQDN
      Type: NS
      TTL: 60
      ResourceRecords: !GetAtt HostedZone.NameServers
  KMSAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub "alias/eks-</span><span class="k">${</span><span class="nv">ClusterName</span><span class="k">}</span><span class="sh">"
      TargetKeyId: !Ref KMSKey
  KMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: !Sub "KMS key for </span><span class="k">${</span><span class="nv">ClusterName</span><span class="k">}</span><span class="sh"> Amazon EKS"
      EnableKeyRotation: true
      PendingWindowInDays: 7
      KeyPolicy:
        Version: "2012-10-17"
        Id: !Sub "eks-key-policy-</span><span class="k">${</span><span class="nv">ClusterName</span><span class="k">}</span><span class="sh">"
        Statement:
          - Sid: Allow direct access to key metadata to the account
            Effect: Allow
            Principal:
              AWS:
                - !Sub "arn:</span><span class="k">${</span><span class="nv">AWS</span>::Partition<span class="k">}</span><span class="sh">:iam::</span><span class="k">${</span><span class="nv">AWS</span>::AccountId<span class="k">}</span><span class="sh">:root"
            Action:
              - kms:*
            Resource: "*"
          - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS
            Effect: Allow
            Principal:
              AWS: "*"
            Action:
              - kms:Encrypt
              - kms:Decrypt
              - kms:ReEncrypt*
              - kms:GenerateDataKey*
              - kms:CreateGrant
              - kms:DescribeKey
            Resource: "*"
            Condition:
              StringEquals:
                kms:ViaService: !Sub "ec2.</span><span class="k">${</span><span class="nv">AWS</span>::Region<span class="k">}</span><span class="sh">.amazonaws.com"
                kms:CallerAccount: !Sub "</span><span class="k">${</span><span class="nv">AWS</span>::AccountId<span class="k">}</span><span class="sh">"
  S3AccessPolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub "eksctl-</span><span class="k">${</span><span class="nv">ClusterName</span><span class="k">}</span><span class="sh">-s3-access-policy"
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - s3:AbortMultipartUpload
              - s3:DeleteObject
              - s3:GetObject
              - s3:ListMultipartUploadParts
              - s3:ListObjects
              - s3:PutObject
              - s3:PutObjectTagging
            Resource: !Sub "arn:aws:s3:::</span><span class="k">${</span><span class="nv">ClusterFQDN</span><span class="k">}</span><span class="sh">/*"
          - Effect: Allow
            Action:
              - s3:ListBucket
            Resource: !Sub "arn:aws:s3:::</span><span class="k">${</span><span class="nv">ClusterFQDN</span><span class="k">}</span><span class="sh">"
Outputs:
  KMSKeyArn:
    Description: The ARN of the created KMS Key to encrypt EKS related services
    Value: !GetAtt KMSKey.Arn
    Export:
      Name:
        Fn::Sub: "</span><span class="k">${</span><span class="nv">AWS</span>::StackName<span class="k">}</span><span class="sh">-KMSKeyArn"
  KMSKeyId:
    Description: The ID of the created KMS Key to encrypt EKS related services
    Value: !Ref KMSKey
    Export:
      Name:
        Fn::Sub: "</span><span class="k">${</span><span class="nv">AWS</span>::StackName<span class="k">}</span><span class="sh">-KMSKeyId"
  S3AccessPolicyArn:
    Description: IAM policy ARN for S3 access by EKS workloads
    Value: !Ref S3AccessPolicy
    Export:
      Name:
        Fn::Sub: "</span><span class="k">${</span><span class="nv">AWS</span>::StackName<span class="k">}</span><span class="sh">-S3AccessPolicy"
</span><span class="no">EOF

</span><span class="c"># shellcheck disable=SC2001</span>
<span class="nb">eval </span>aws cloudformation deploy <span class="nt">--capabilities</span> CAPABILITY_NAMED_IAM <span class="se">\</span>
  <span class="nt">--parameter-overrides</span> <span class="s2">"BaseDomain=</span><span class="k">${</span><span class="nv">BASE_DOMAIN</span><span class="k">}</span><span class="s2"> ClusterFQDN=</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2"> ClusterName=</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-route53-kms"</span> <span class="nt">--template-file</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/aws-cf-route53-kms.yml"</span> <span class="nt">--tags</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="p">//,/ </span><span class="k">}</span><span class="s2">"</span>

<span class="nv">AWS_CLOUDFORMATION_DETAILS</span><span class="o">=</span><span class="si">$(</span>aws cloudformation describe-stacks <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-route53-kms"</span> <span class="nt">--query</span> <span class="s2">"Stacks[0].Outputs[? OutputKey==</span><span class="se">\`</span><span class="s2">KMSKeyArn</span><span class="se">\`</span><span class="s2"> || OutputKey==</span><span class="se">\`</span><span class="s2">KMSKeyId</span><span class="se">\`</span><span class="s2"> || OutputKey==</span><span class="se">\`</span><span class="s2">S3AccessPolicyArn</span><span class="se">\`</span><span class="s2">].{OutputKey:OutputKey,OutputValue:OutputValue}"</span><span class="si">)</span>
<span class="nv">AWS_KMS_KEY_ARN</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLOUDFORMATION_DETAILS</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-r</span> <span class="s2">".[] | select(.OutputKey==</span><span class="se">\"</span><span class="s2">KMSKeyArn</span><span class="se">\"</span><span class="s2">) .OutputValue"</span><span class="si">)</span>
<span class="nv">AWS_KMS_KEY_ID</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLOUDFORMATION_DETAILS</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-r</span> <span class="s2">".[] | select(.OutputKey==</span><span class="se">\"</span><span class="s2">KMSKeyId</span><span class="se">\"</span><span class="s2">) .OutputValue"</span><span class="si">)</span>
<span class="nv">AWS_S3_ACCESS_POLICY_ARN</span><span class="o">=</span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLOUDFORMATION_DETAILS</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-r</span> <span class="s2">".[] | select(.OutputKey==</span><span class="se">\"</span><span class="s2">S3AccessPolicyArn</span><span class="se">\"</span><span class="s2">) .OutputValue"</span><span class="si">)</span>
</pre></table></code></div></div><p>After running the CloudFormation stack, you should see the following Route53 zones:</p><p><a href="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedzones-k01.k8s.mylabs.dev.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedzones-k01.k8s.mylabs.dev.avif" alt="Route53 k01.k8s.mylabs.dev zone" loading="lazy"></a> <em>Route53 k01.k8s.mylabs.dev zone</em></p><p><a href="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedones-k8s.mylabs.dev-2.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2022/2022-11-27-cheapest-amazon-eks/route53-hostedones-k8s.mylabs.dev-2.avif" alt="Route53 k8s.mylabs.dev zone" loading="lazy"></a> <em>Route53 k8s.mylabs.dev zone</em></p><p>You should also see the following KMS key:</p><p><a href="/assets/img/posts/2023/2023-08-03-cilium-amazon-eks/kms-key.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2023/2023-08-03-cilium-amazon-eks/kms-key.avif" alt="KMS key" loading="lazy"></a> <em>KMS key</em></p><h3 id="create-karpenter-infrastructure"><span class="me-2">Create Karpenter infrastructure</span><a href="#create-karpenter-infrastructure" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Use CloudFormation to set up the infrastructure needed by the EKS cluster. See <a href="https://karpenter.sh/docs/reference/cloudformation/">CloudFormation</a> for a complete description of what <code class="language-plaintext highlighter-rouge">cloudformation.yaml</code> does for Karpenter.</p><p><a href="https://raw.githubusercontent.com/aws/karpenter/efa141bc7276db421980bf6e6483d9856929c1e9/website/static/banner.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/aws/karpenter/efa141bc7276db421980bf6e6483d9856929c1e9/website/static/banner.png" alt="Karpenter" width="400" loading="lazy"></a></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/aws/karpenter-provider-aws/refs/heads/main/website/content/en/v1.8/getting-started/getting-started-with-karpenter/cloudformation.yaml <span class="o">&gt;</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/cloudformation-karpenter.yml"</span>
<span class="nb">eval </span>aws cloudformation deploy <span class="se">\</span>
  <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-karpenter"</span> <span class="se">\</span>
  <span class="nt">--template-file</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/cloudformation-karpenter.yml"</span> <span class="se">\</span>
  <span class="nt">--capabilities</span> CAPABILITY_NAMED_IAM <span class="se">\</span>
  <span class="nt">--parameter-overrides</span> <span class="s2">"ClusterName=</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--tags</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="p">//,/ </span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><h3 id="create-amazon-eks"><span class="me-2">Create Amazon EKS</span><a href="#create-amazon-eks" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>I will use <a href="https://eksctl.io/">eksctl</a> to create the <a href="https://aws.amazon.com/eks/">Amazon EKS</a> cluster.</p><p><a href="https://raw.githubusercontent.com/weaveworks/eksctl/2b1ec6223c4e7cb8103c08162e6de8ced47376f9/userdocs/src/img/eksctl.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/weaveworks/eksctl/2b1ec6223c4e7cb8103c08162e6de8ced47376f9/userdocs/src/img/eksctl.png" alt="eksctl" width="700" loading="lazy"></a></p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
</pre><td class="rouge-code"><pre><span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
  region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
  tags:
    karpenter.sh/discovery: </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
    </span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s2">"s/,/</span><span class="se">\\</span><span class="s2">n    /g; s/=/: /g"</span><span class="si">)</span><span class="sh">
availabilityZones:
  - </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">a
  - </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">b
accessConfig:
  accessEntries:
    - principalARN: arn:</span><span class="k">${</span><span class="nv">AWS_PARTITION</span><span class="k">}</span><span class="sh">:iam::</span><span class="k">${</span><span class="nv">AWS_ACCOUNT_ID</span><span class="k">}</span><span class="sh">:role/admin
      accessPolicies:
        - policyARN: arn:</span><span class="k">${</span><span class="nv">AWS_PARTITION</span><span class="k">}</span><span class="sh">:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy
          accessScope:
            type: cluster
iam:
  withOIDC: true
  podIdentityAssociations:
    - namespace: aws-load-balancer-controller
      serviceAccountName: aws-load-balancer-controller
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-aws-load-balancer-controller
      wellKnownPolicies:
        awsLoadBalancerController: true
    - namespace: cert-manager
      serviceAccountName: cert-manager
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-cert-manager
      wellKnownPolicies:
        certManager: true
    - namespace: external-dns
      serviceAccountName: external-dns
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-external-dns
      wellKnownPolicies:
        externalDNS: true
    - namespace: karpenter
      serviceAccountName: karpenter
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-karpenter
      permissionPolicyARNs:
        - arn:</span><span class="k">${</span><span class="nv">AWS_PARTITION</span><span class="k">}</span><span class="sh">:iam::</span><span class="k">${</span><span class="nv">AWS_ACCOUNT_ID</span><span class="k">}</span><span class="sh">:policy/KarpenterControllerPolicy-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
    - namespace: loki
      serviceAccountName: loki
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-loki
      permissionPolicyARNs:
        - </span><span class="k">${</span><span class="nv">AWS_S3_ACCESS_POLICY_ARN</span><span class="k">}</span><span class="sh">
    - namespace: mimir
      serviceAccountName: mimir
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-mimir
      permissionPolicyARNs:
        - </span><span class="k">${</span><span class="nv">AWS_S3_ACCESS_POLICY_ARN</span><span class="k">}</span><span class="sh">
    - namespace: tempo
      serviceAccountName: tempo
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-tempo
      permissionPolicyARNs:
        - </span><span class="k">${</span><span class="nv">AWS_S3_ACCESS_POLICY_ARN</span><span class="k">}</span><span class="sh">
    - namespace: velero
      serviceAccountName: velero
      roleName: eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-velero
      permissionPolicyARNs:
        - </span><span class="k">${</span><span class="nv">AWS_S3_ACCESS_POLICY_ARN</span><span class="k">}</span><span class="sh">
      permissionPolicy:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action: [
              "ec2:DescribeVolumes",
              "ec2:DescribeSnapshots",
              "ec2:CreateTags",
              "ec2:CreateSnapshot",
              "ec2:DeleteSnapshots"
            ]
            Resource:
              - "*"
iamIdentityMappings:
  - arn: "arn:</span><span class="k">${</span><span class="nv">AWS_PARTITION</span><span class="k">}</span><span class="sh">:iam::</span><span class="k">${</span><span class="nv">AWS_ACCOUNT_ID</span><span class="k">}</span><span class="sh">:role/KarpenterNodeRole-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
    username: system:node:
    groups:
      - system:bootstrappers
      - system:nodes
addons:
  - name: coredns
  - name: eks-pod-identity-agent
  - name: kube-proxy
  - name: snapshot-controller
  - name: aws-ebs-csi-driver
    configurationValues: |-
      defaultStorageClass:
        enabled: true
      controller:
        extraVolumeTags:
          </span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s2">"s/,/</span><span class="se">\\</span><span class="s2">n          /g; s/=/: /g"</span><span class="si">)</span><span class="sh">
        loggingFormat: json
  - name: vpc-cni
    configurationValues: |-
      enableNetworkPolicy: "true"
      env:
        ENABLE_PREFIX_DELEGATION: "true"
managedNodeGroups:
  - name: mng01-ng
    amiFamily: Bottlerocket
    instanceType: t4g.medium
    desiredCapacity: 2
    availabilityZones:
      - </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">a
    minSize: 2
    maxSize: 3
    volumeSize: 20
    volumeEncrypted: true
    volumeKmsKeyID: </span><span class="k">${</span><span class="nv">AWS_KMS_KEY_ID</span><span class="k">}</span><span class="sh">
    privateNetworking: true
    nodeRepairConfig:
      enabled: true
    bottlerocket:
      settings:
        kubernetes:
          seccomp-default: true
secretsEncryption:
  keyARN: </span><span class="k">${</span><span class="nv">AWS_KMS_KEY_ARN</span><span class="k">}</span><span class="sh">
cloudWatch:
  clusterLogging:
    logRetentionInDays: 1
    enableTypes:
      - all
</span><span class="no">EOF
</span>eksctl create cluster <span class="nt">--config-file</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">.yml"</span> <span class="nt">--kubeconfig</span> <span class="s2">"</span><span class="k">${</span><span class="nv">KUBECONFIG</span><span class="k">}</span><span class="s2">"</span> <span class="o">||</span> eksctl utils write-kubeconfig <span class="nt">--cluster</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--kubeconfig</span> <span class="s2">"</span><span class="k">${</span><span class="nv">KUBECONFIG</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><p>Enhance the security posture of the EKS cluster by addressing the following concerns:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="nv">AWS_VPC_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-vpcs <span class="nt">--filters</span> <span class="s2">"Name=tag:alpha.eksctl.io/cluster-name,Values=</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s1">'Vpcs[*].VpcId'</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="nv">AWS_SECURITY_GROUP_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-security-groups <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="k">${</span><span class="nv">AWS_VPC_ID</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"Name=group-name,Values=default"</span> <span class="nt">--query</span> <span class="s1">'SecurityGroups[*].GroupId'</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="nv">AWS_NACL_ID</span><span class="o">=</span><span class="si">$(</span>aws ec2 describe-network-acls <span class="nt">--filters</span> <span class="s2">"Name=vpc-id,Values=</span><span class="k">${</span><span class="nv">AWS_VPC_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s1">'NetworkAcls[*].NetworkAclId'</span> <span class="nt">--output</span> text<span class="si">)</span>
</pre></table></code></div></div><ul><li><p>The default security group should have no rules configured:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>aws ec2 revoke-security-group-egress <span class="nt">--group-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_SECURITY_GROUP_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--protocol</span> all <span class="nt">--port</span> all <span class="nt">--cidr</span> 0.0.0.0/0 | jq <span class="o">||</span> <span class="nb">true
</span>aws ec2 revoke-security-group-ingress <span class="nt">--group-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_SECURITY_GROUP_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--protocol</span> all <span class="nt">--port</span> all <span class="nt">--source-group</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_SECURITY_GROUP_ID</span><span class="k">}</span><span class="s2">"</span> | jq <span class="o">||</span> <span class="nb">true</span>
</pre></table></code></div></div><li><p>The VPC should have Route 53 DNS resolver with logging enabled:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="nv">AWS_CLUSTER_LOG_GROUP_ARN</span><span class="o">=</span><span class="si">$(</span>aws logs describe-log-groups <span class="nt">--query</span> <span class="s2">"logGroups[?logGroupName=='/aws/eks/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">/cluster'].arn"</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="nv">AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID</span><span class="o">=</span><span class="si">$(</span>aws route53resolver create-resolver-query-log-config <span class="se">\</span>
  <span class="nt">--name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-vpc-dns-logs"</span> <span class="se">\</span>
  <span class="nt">--destination-arn</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLUSTER_LOG_GROUP_ARN</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--creator-request-id</span> <span class="s2">"</span><span class="si">$(</span>uuidgen<span class="si">)</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s1">'ResolverQueryLogConfig.Id'</span> <span class="nt">--output</span> text<span class="si">)</span>

aws route53resolver associate-resolver-query-log-config <span class="se">\</span>
  <span class="nt">--resolver-query-log-config-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--resource-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_VPC_ID</span><span class="k">}</span><span class="s2">"</span>
</pre></table></code></div></div><li><p>Remove overly permissive NACL rules to follow the principle of least privilege:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="c"># Delete the overly permissive inbound rule</span>
aws ec2 delete-network-acl-entry <span class="se">\</span>
  <span class="nt">--network-acl-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_NACL_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--rule-number</span> 100 <span class="se">\</span>
  <span class="nt">--ingress</span>

<span class="c"># Create restrictive inbound TCP rules</span>
<span class="nv">NACL_RULES</span><span class="o">=(</span>
  <span class="s2">"100 443 443 0.0.0.0/0"</span>
  <span class="s2">"110 80 80 0.0.0.0/0"</span>
  <span class="s2">"120 1024 65535 0.0.0.0/0"</span>
<span class="o">)</span>

<span class="k">for </span>RULE <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">NACL_RULES</span><span class="p">[@]</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">do
  </span><span class="nb">read</span> <span class="nt">-r</span> RULE_NUM PORT_FROM PORT_TO CIDR <span class="o">&lt;&lt;&lt;</span> <span class="s2">"</span><span class="k">${</span><span class="nv">RULE</span><span class="k">}</span><span class="s2">"</span>
  aws ec2 create-network-acl-entry <span class="se">\</span>
    <span class="nt">--network-acl-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_NACL_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--rule-number</span> <span class="s2">"</span><span class="k">${</span><span class="nv">RULE_NUM</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--protocol</span> <span class="s2">"tcp"</span> <span class="se">\</span>
    <span class="nt">--port-range</span> <span class="s2">"From=</span><span class="k">${</span><span class="nv">PORT_FROM</span><span class="k">}</span><span class="s2">,To=</span><span class="k">${</span><span class="nv">PORT_TO</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--cidr-block</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CIDR</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
    <span class="nt">--rule-action</span> allow <span class="se">\</span>
    <span class="nt">--ingress</span>
<span class="k">done</span>

<span class="c"># Allow all traffic from VPC CIDR</span>
aws ec2 create-network-acl-entry <span class="se">\</span>
  <span class="nt">--network-acl-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_NACL_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
  <span class="nt">--rule-number</span> 130 <span class="se">\</span>
  <span class="nt">--protocol</span> <span class="s2">"all"</span> <span class="se">\</span>
  <span class="nt">--cidr-block</span> <span class="s2">"192.168.0.0/16"</span> <span class="se">\</span>
  <span class="nt">--rule-action</span> allow <span class="se">\</span>
  <span class="nt">--ingress</span>
</pre></table></code></div></div></ul><h3 id="prometheus-operator-crds"><span class="me-2">Prometheus Operator CRDs</span><a href="#prometheus-operator-crds" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-operator-crds">Prometheus Operator CRDs</a> provides the Custom Resource Definitions (CRDs) that define the Prometheus operator resources. These CRDs are required before installing ServiceMonitor resources.</p><p>Install the <code class="language-plaintext highlighter-rouge">prometheus-operator-crds</code> <a href="https://github.com/prometheus-community/helm-charts/tree/prometheus-operator-crds-23.0.0/charts/prometheus-operator-crds">Helm chart</a> to set up the necessary CRDs:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>helm <span class="nb">install </span>prometheus-operator-crds oci://ghcr.io/prometheus-community/charts/prometheus-operator-crds
</pre></table></code></div></div><h3 id="aws-load-balancer-controller"><span class="me-2">AWS Load Balancer Controller</span><a href="#aws-load-balancer-controller" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/">AWS Load Balancer Controller</a> is a controller that manages Elastic Load Balancers for a Kubernetes cluster.</p><p><a href="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/05071ecd0f2c240c7e6b815c0fdf731df799005a/docs/assets/images/aws_load_balancer_icon.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/05071ecd0f2c240c7e6b815c0fdf731df799005a/docs/assets/images/aws_load_balancer_icon.svg" alt="AWS Load Balancer Controller" width="150" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">aws-load-balancer-controller</code> <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/tree/main/helm/aws-load-balancer-controller">Helm chart</a> and modify its <a href="https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/v2.16.0/helm/aws-load-balancer-controller/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=aws-load-balancer-controller registryUrl=https://aws.github.io/eks-charts</span>
<span class="nv">AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"1.17.1"</span>

helm repo add <span class="nt">--force-update</span> eks https://aws.github.io/eks-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-aws-load-balancer-controller.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
serviceAccount:
  name: aws-load-balancer-controller
clusterName: </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
serviceMonitor:
  enabled: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> aws-load-balancer-controller <span class="nt">--create-namespace</span> <span class="nt">--wait</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-aws-load-balancer-controller.yml"</span> aws-load-balancer-controller eks/aws-load-balancer-controller
</pre></table></code></div></div><h3 id="pod-scheduling-priorityclasses"><span class="me-2">Pod Scheduling PriorityClasses</span><a href="#pod-scheduling-priorityclasses" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Configure <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/">PriorityClasses</a> to control the scheduling priority of pods in your cluster. PriorityClasses allow you to influence which pods are scheduled or evicted first when resources are constrained. These classes help ensure that critical workloads receive scheduling priority over less important workloads.</p><p>Create custom PriorityClass resources to define priority levels for different workload types:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/k8s-scheduling-priorityclass.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: critical-priority
value: 100001000
globalDefault: false
description: "This priority class should be used for critical workloads only"
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 100000000
globalDefault: false
description: "This priority class should be used for high priority workloads"
</span><span class="no">EOF
</span></pre></table></code></div></div><h3 id="add-storage-classes-and-volume-snapshots"><span class="me-2">Add Storage Classes and Volume Snapshots</span><a href="#add-storage-classes-and-volume-snapshots" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Configure persistent storage for your EKS cluster by setting up GP3 storage classes and volume snapshot capabilities. This ensures encrypted, expandable storage with proper backup functionality.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre><span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/k8s-storage-snapshot-storageclass-volumesnapshotclass.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
  encrypted: "true"
  kmsKeyId: </span><span class="k">${</span><span class="nv">AWS_KMS_KEY_ARN</span><span class="k">}</span><span class="sh">
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: WaitForFirstConsumer
---
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: ebs-vsc
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: "true"
driver: ebs.csi.aws.com
deletionPolicy: Delete
</span><span class="no">EOF
</span></pre></table></code></div></div><p>Delete the <code class="language-plaintext highlighter-rouge">gp2</code> StorageClass, as <code class="language-plaintext highlighter-rouge">gp3</code> will be used instead:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl delete storageclass gp2 <span class="o">||</span> <span class="nb">true</span>
</pre></table></code></div></div><h3 id="karpenter"><span class="me-2">Karpenter</span><a href="#karpenter" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://karpenter.sh/">Karpenter</a> is a Kubernetes node autoscaler built for flexibility, performance, and simplicity.</p><p><a href="https://raw.githubusercontent.com/aws/karpenter-provider-aws/41b115a0b85677641e387635496176c4cc30d4c6/website/static/full_logo.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/aws/karpenter-provider-aws/41b115a0b85677641e387635496176c4cc30d4c6/website/static/full_logo.svg" alt="Karpenter" width="500" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">karpenter</code> <a href="https://github.com/aws/karpenter-provider-aws/tree/main/charts/karpenter">Helm chart</a> and customize its <a href="https://github.com/aws/karpenter-provider-aws/blob/v1.8.2/charts/karpenter/values.yaml">default values</a> to fit your environment and storage backend:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=github-tags depName=aws/karpenter-provider-aws</span>
<span class="nv">KARPENTER_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"1.8.4"</span>

<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-karpenter.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
serviceMonitor:
  enabled: true
settings:
  clusterName: </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
  eksControlPlane: true
  interruptionQueue: </span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">
  featureGates:
    spotToSpotConsolidation: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">KARPENTER_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> karpenter <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-karpenter.yml"</span> karpenter oci://public.ecr.aws/karpenter/karpenter
</pre></table></code></div></div><p>Configure <a href="https://karpenter.sh/">Karpenter</a> by applying the following provisioner definition:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
</pre><td class="rouge-code"><pre><span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/k8s-karpenter-nodepool.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh"> | kubectl apply -f -
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: default
spec:
  amiFamily: Bottlerocket
  amiSelectorTerms:
    - alias: bottlerocket@latest
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: "</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: "</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
  role: "KarpenterNodeRole-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
  tags:
    Name: "</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">-karpenter"
    </span><span class="si">$(</span><span class="nb">echo</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TAGS</span><span class="k">}</span><span class="s2">"</span> | <span class="nb">sed</span> <span class="s2">"s/,/</span><span class="se">\\</span><span class="s2">n    /g; s/=/: /g"</span><span class="si">)</span><span class="sh">
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 2Gi
        volumeType: gp3
        encrypted: true
        kmsKeyID: </span><span class="k">${</span><span class="nv">AWS_KMS_KEY_ARN</span><span class="k">}</span><span class="sh">
    - deviceName: /dev/xvdb
      ebs:
        volumeSize: 20Gi
        volumeType: gp3
        encrypted: true
        kmsKeyID: </span><span class="k">${</span><span class="nv">AWS_KMS_KEY_ARN</span><span class="k">}</span><span class="sh">
---
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      requirements:
        # keep-sorted start
        - key: "karpenter.k8s.aws/instance-memory"
          operator: Gt
          values: ["4095"]
        - key: "topology.kubernetes.io/zone"
          operator: In
          values: ["</span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">a"]
        - key: karpenter.k8s.aws/instance-family
          operator: In
          values: ["t4g", "t3a"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: kubernetes.io/arch
          operator: In
          values: ["arm64", "amd64"]
        # keep-sorted end
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default
</span><span class="no">EOF
</span></pre></table></code></div></div><h3 id="cert-manager"><span class="me-2">cert-manager</span><a href="#cert-manager" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://cert-manager.io/">cert-manager</a> adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates.</p><p><a href="https://raw.githubusercontent.com/cert-manager/cert-manager/7f15787f0f146149d656b6877a6fbf4394fe9965/logo/logo.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/cert-manager/cert-manager/7f15787f0f146149d656b6877a6fbf4394fe9965/logo/logo.svg" alt="cert-manager" width="150" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">cert-manager</code> <a href="https://artifacthub.io/packages/helm/cert-manager/cert-manager">Helm chart</a> and modify its <a href="https://github.com/cert-manager/cert-manager/blob/v1.19.1/deploy/charts/cert-manager/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io extractVersion=^(?&lt;version&gt;.+)$</span>
<span class="nv">CERT_MANAGER_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"v1.19.1"</span>

helm repo add <span class="nt">--force-update</span> jetstack https://charts.jetstack.io
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-cert-manager.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
global:
  priorityClassName: high-priority
crds:
  enabled: true
extraArgs:
  - --enable-certificate-owner-ref=true
serviceAccount:
  name: cert-manager
enableCertificateOwnerRef: true
webhook:
  replicaCount: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/instance: cert-manager
            app.kubernetes.io/component: webhook
        topologyKey: kubernetes.io/hostname
prometheus:
  servicemonitor:
    enabled: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CERT_MANAGER_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> cert-manager <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-cert-manager.yml"</span> cert-manager jetstack/cert-manager
</pre></table></code></div></div><h3 id="velero"><span class="me-2">Velero</span><a href="#velero" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Velero is an open-source tool for backing up and restoring Kubernetes cluster resources and persistent volumes. It enables disaster recovery, data migration, and scheduled backups by integrating with cloud storage providers such as AWS S3.</p><p><a href="https://raw.githubusercontent.com/vmware-tanzu/velero/c663ce15ab468b21a19336dcc38acf3280853361/site/static/img/Velero.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/vmware-tanzu/velero/c663ce15ab468b21a19336dcc38acf3280853361/site/static/img/Velero.svg" alt="velero" width="400" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">velero</code> <a href="https://artifacthub.io/packages/helm/vmware-tanzu/velero">Helm chart</a> and modify its <a href="https://github.com/vmware-tanzu/helm-charts/blob/velero-11.2.0/charts/velero/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts</span>
<span class="nv">VELERO_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"11.3.2"</span>

helm repo add <span class="nt">--force-update</span> vmware-tanzu https://vmware-tanzu.github.io/helm-charts
<span class="nb">cat</span> <span class="o">&gt;</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-velero.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
initContainers:
  - name: velero-plugin-for-aws
    # renovate: datasource=github-tags depName=vmware-tanzu/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$
    image: velero/velero-plugin-for-aws:v1.13.1
    volumeMounts:
      - mountPath: /target
        name: plugins
priorityClassName: high-priority
metrics:
  serviceMonitor:
    enabled: true
  prometheusRule:
    enabled: true
    spec:
      - alert: VeleroBackupPartialFailures
        annotations:
          message: Velero backup {{ </span><span class="se">\$</span><span class="sh">labels.schedule }} has {{ </span><span class="se">\$</span><span class="sh">value | humanizePercentage }} partially failed backups.
        expr: velero_backup_partial_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} &gt; 0.25
        for: 15m
        labels:
          severity: warning
      - alert: VeleroBackupFailures
        annotations:
          message: Velero backup {{ </span><span class="se">\$</span><span class="sh">labels.schedule }} has {{ </span><span class="se">\$</span><span class="sh">value | humanizePercentage }} failed backups.
        expr: velero_backup_failure_total{schedule!=""} / velero_backup_attempt_total{schedule!=""} &gt; 0.25
        for: 15m
        labels:
          severity: warning
      - alert: VeleroBackupSnapshotFailures
        annotations:
          message: Velero backup {{ </span><span class="se">\$</span><span class="sh">labels.schedule }} has {{ </span><span class="se">\$</span><span class="sh">value | humanizePercentage }} failed snapshot backups.
        expr: increase(velero_volume_snapshot_failure_total{schedule!=""}[1h]) &gt; 0
        for: 15m
        labels:
          severity: warning
      - alert: VeleroRestorePartialFailures
        annotations:
          message: Velero restore {{ </span><span class="se">\$</span><span class="sh">labels.schedule }} has {{ </span><span class="se">\$</span><span class="sh">value | humanizePercentage }} partially failed restores.
        expr: increase(velero_restore_partial_failure_total{schedule!=""}[1h]) &gt; 0
        for: 15m
        labels:
          severity: warning
      - alert: VeleroRestoreFailures
        annotations:
          message: Velero restore {{ </span><span class="se">\$</span><span class="sh">labels.schedule }} has {{ </span><span class="se">\$</span><span class="sh">value | humanizePercentage }} failed restores.
        expr: increase(velero_restore_failure_total{schedule!=""}[1h]) &gt; 0
        for: 15m
        labels:
          severity: warning
configuration:
  backupStorageLocation:
    - name:
      provider: aws
      bucket: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      prefix: velero
      config:
        region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
  volumeSnapshotLocation:
    - name:
      provider: aws
      config:
        region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
serviceAccount:
  server:
    name: velero
credentials:
  useSecret: false
# Create scheduled backup to periodically backup the let's encrypt production resources in the "cert-manager" namespace:
schedules:
  monthly-backup-cert-manager-production:
    labels:
      letsencrypt: production
    schedule: "@monthly"
    template:
      ttl: 2160h
      includedNamespaces:
        - cert-manager
      includedResources:
        - certificates.cert-manager.io
        - secrets
      labelSelector:
        matchLabels:
          letsencrypt: production
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">VELERO_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> velero <span class="nt">--create-namespace</span> <span class="nt">--wait</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-velero.yml"</span> velero vmware-tanzu/velero
</pre></table></code></div></div><h4 id="restore-cert-manager-objects"><span class="me-2">Restore cert-manager objects</span><a href="#restore-cert-manager-objects" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The following steps will guide you through restoring a Let’s Encrypt production certificate, previously backed up by Velero to S3, onto a new cluster.</p><p>Initiate the restore process for the cert-manager objects.</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">while</span> <span class="o">[</span> <span class="nt">-z</span> <span class="s2">"</span><span class="si">$(</span>kubectl <span class="nt">-n</span> velero get backupstoragelocations default <span class="nt">-o</span> <span class="nv">jsonpath</span><span class="o">=</span><span class="s1">'{.status.lastSyncedTime}'</span><span class="si">)</span><span class="s2">"</span> <span class="o">]</span><span class="p">;</span> <span class="k">do </span><span class="nb">sleep </span>5<span class="p">;</span> <span class="k">done
</span>velero restore create <span class="nt">--from-schedule</span> velero-monthly-backup-cert-manager-production <span class="nt">--labels</span> <span class="nv">letsencrypt</span><span class="o">=</span>production <span class="nt">--wait</span> <span class="nt">--existing-resource-policy</span><span class="o">=</span>update
</pre></table></code></div></div><p>View details about the restore process:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>velero restore describe <span class="nt">--selector</span> <span class="nv">letsencrypt</span><span class="o">=</span>production <span class="nt">--details</span>
</pre></table></code></div></div><div class="language-console highlighter-rouge"><div class="code-header"> <span data-label-text="Console"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
</pre><td class="rouge-code"><pre><span class="go">Name:         velero-monthly-backup-cert-manager-production-20251030075321
Namespace:    velero
Labels:       letsencrypt=production
</span><span class="gp">Annotations:  &lt;none&gt;</span><span class="w">
</span><span class="go">
Phase:                       Completed
Total items to be restored:  3
Items restored:              3

Started:    2025-10-30 07:53:22 +0100 CET
Completed:  2025-10-30 07:53:24 +0100 CET

Backup:  velero-monthly-backup-cert-manager-production-20250921155028

Namespaces:
  Included:  all namespaces found in the backup
</span><span class="gp">  Excluded:  &lt;none&gt;</span><span class="w">
</span><span class="go">
Resources:
  Included:        *
  Excluded:        nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io
  Cluster-scoped:  auto

</span><span class="gp">Namespace mappings:  &lt;none&gt;</span><span class="w">
</span><span class="go">
</span><span class="gp">Label selector:  &lt;none&gt;</span><span class="w">
</span><span class="go">
</span><span class="gp">Or label selector:  &lt;none&gt;</span><span class="w">
</span><span class="go">
Restore PVs:  auto

</span><span class="gp">CSI Snapshot Restores: &lt;none included&gt;</span><span class="w">
</span><span class="go">
Existing Resource Policy:   update
ItemOperationTimeout:       4h0m0s

Preserve Service NodePorts:  auto

Uploader config:


HooksAttempted:   0
HooksFailed:      0

Resource List:
  cert-manager.io/v1/Certificate:
    - cert-manager/ingress-cert-production(created)
  v1/Secret:
    - cert-manager/ingress-cert-production(created)
    - cert-manager/letsencrypt-production-dns(created)
</span></pre></table></code></div></div><p>Verify that the certificate was restored properly:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>kubectl describe certificates <span class="nt">-n</span> cert-manager ingress-cert-production
</pre></table></code></div></div><div class="language-console highlighter-rouge"><div class="code-header"> <span data-label-text="Console"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
</pre><td class="rouge-code"><pre><span class="go">Name:         ingress-cert-production
Namespace:    cert-manager
Labels:       letsencrypt=production
              velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250921155028
              velero.io/restore-name=velero-monthly-backup-cert-manager-production-20251030075321
</span><span class="gp">Annotations:  &lt;none&gt;</span><span class="w">
</span><span class="go">API Version:  cert-manager.io/v1
Kind:         Certificate
Metadata:
  Creation Timestamp:  2025-10-30T06:53:23Z
  Generation:          1
  Resource Version:    5521
  UID:                 33422558-3105-4936-87d8-468befb5dc2b
Spec:
  Common Name:  *.k01.k8s.mylabs.dev
  Dns Names:
    *.k01.k8s.mylabs.dev
    k01.k8s.mylabs.dev
  Issuer Ref:
    Group:      cert-manager.io
    Kind:       ClusterIssuer
    Name:       letsencrypt-production-dns
  Secret Name:  ingress-cert-production
  Secret Template:
    Labels:
      Letsencrypt:  production
Status:
  Conditions:
    Last Transition Time:  2025-10-30T06:53:23Z
    Message:               Certificate is up to date and has not expired
    Observed Generation:   1
    Reason:                Ready
    Status:                True
    Type:                  Ready
  Not After:               2025-12-20T10:53:07Z
  Not Before:              2025-09-21T10:53:08Z
  Renewal Time:            2025-11-20T10:53:07Z
</span><span class="gp">Events:                    &lt;none&gt;</span><span class="w">
</span></pre></table></code></div></div><h3 id="externaldns"><span class="me-2">ExternalDNS</span><a href="#externaldns" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://github.com/kubernetes-sigs/external-dns">ExternalDNS</a> synchronizes exposed Kubernetes Services and Ingresses with DNS providers.</p><p><a href="https://raw.githubusercontent.com/kubernetes-sigs/external-dns/afe3b09f45a241750ec3ddceef59ceaf84c096d0/docs/img/external-dns.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/kubernetes-sigs/external-dns/afe3b09f45a241750ec3ddceef59ceaf84c096d0/docs/img/external-dns.png" alt="ExternalDNS" width="200" loading="lazy"></a></p><p>ExternalDNS will manage the DNS records. Install the <code class="language-plaintext highlighter-rouge">external-dns</code> <a href="https://artifacthub.io/packages/helm/external-dns/external-dns">Helm chart</a> and modify its <a href="https://github.com/kubernetes-sigs/external-dns/blob/external-dns-helm-chart-1.19.0/charts/external-dns/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/</span>
<span class="nv">EXTERNAL_DNS_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"1.20.0"</span>

helm repo add <span class="nt">--force-update</span> external-dns https://kubernetes-sigs.github.io/external-dns/
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-external-dns.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
serviceAccount:
  name: external-dns
priorityClassName: high-priority
serviceMonitor:
  enabled: true
interval: 20s
policy: sync
domainFilters:
  - </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">EXTERNAL_DNS_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> external-dns <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-external-dns.yml"</span> external-dns external-dns/external-dns
</pre></table></code></div></div><h3 id="ingress-nginx-controller"><span class="me-2">Ingress NGINX Controller</span><a href="#ingress-nginx-controller" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx</a> is an Ingress controller for Kubernetes that uses <a href="https://www.nginx.org/">nginx</a> as a reverse proxy and load balancer.</p><p>Install the <code class="language-plaintext highlighter-rouge">ingress-nginx</code> <a href="https://artifacthub.io/packages/helm/ingress-nginx/ingress-nginx">Helm chart</a> and modify its <a href="https://github.com/kubernetes/ingress-nginx/blob/helm-chart-4.14.1/charts/ingress-nginx/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx</span>
<span class="nv">INGRESS_NGINX_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"4.14.1"</span>

helm repo add <span class="nt">--force-update</span> ingress-nginx https://kubernetes.github.io/ingress-nginx
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-ingress-nginx.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
controller:
  config:
    annotations-risk-level: Critical
    use-proxy-protocol: true
  allowSnippetAnnotations: true
  ingressClassResource:
    default: true
  extraArgs:
    default-ssl-certificate: cert-manager/ingress-cert-production
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/instance: ingress-nginx
            app.kubernetes.io/component: controller
        topologyKey: kubernetes.io/hostname
  replicaCount: 2
  service:
    annotations:
      # https://www.qovery.com/blog/our-migration-from-kubernetes-built-in-nlb-to-alb-controller/
      # https://www.youtube.com/watch?v=xwiRjimKW9c
      service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: </span><span class="k">${</span><span class="nv">TAGS</span><span class="p">//\</span><span class="s1">'/}
      service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME}
      service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
      service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing
      service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true
      service.beta.kubernetes.io/aws-load-balancer-type: external
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true
    prometheusRule:
      enabled: true
      rules:
        - alert: NGINXConfigFailed
          expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0
          for: 1s
          labels:
            severity: critical
          annotations:
            description: bad ingress config - nginx config test failed
            summary: uninstall the latest ingress changes to allow config reloads to resume
        - alert: NGINXCertificateExpiry
          expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!="_"}) by (host) - time()) &lt; 604800
          for: 1s
          labels:
            severity: critical
          annotations:
            description: ssl certificate(s) will expire in less then a week
            summary: renew expiring certificates to avoid downtime
        - alert: NGINXTooMany500s
          expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"5.+"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5
          for: 1m
          labels:
            severity: warning
          annotations:
            description: Too many 5XXs
            summary: More than 5% of all requests returned 5XX, this requires your attention
        - alert: NGINXTooMany400s
          expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"4.+"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5
          for: 1m
          labels:
            severity: warning
          annotations:
            description: Too many 4XXs
            summary: More than 5% of all requests returned 4XX, this requires your attention
  priorityClassName: critical-priority
EOF
helm upgrade --install --version "${INGRESS_NGINX_HELM_CHART_VERSION}" --namespace ingress-nginx --create-namespace --wait --values "${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml" ingress-nginx ingress-nginx/ingress-nginx
</span></pre></table></code></div></div><h3 id="mimir"><span class="me-2">Mimir</span><a href="#mimir" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://grafana.com/oss/mimir/">Grafana Mimir</a> is an open source, horizontally scalable, multi-tenant time series database for Prometheus metrics, designed for high availability and cost efficiency. It enables you to centralize metrics from multiple clusters or environments, and integrates seamlessly with <a href="https://grafana.com/">Grafana</a> dashboards for visualization and alerting.</p><p><a href="https://raw.githubusercontent.com/grafana/mimir/38563275a149baaf659e566990fe66a13db9e3c6/docs/sources/mimir/mimir-logo.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/grafana/mimir/38563275a149baaf659e566990fe66a13db9e3c6/docs/sources/mimir/mimir-logo.png" alt="Grafana Mimir" width="400" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">mimir-distributed</code> <a href="https://github.com/grafana/mimir/tree/main/operations/helm/charts/mimir-distributed">Helm chart</a> and customize its <a href="https://github.com/grafana/mimir/blob/mimir-distributed-6.0.0/operations/helm/charts/mimir-distributed/values.yaml">default values</a> to fit your environment and storage backend:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=mimir-distributed registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">MIMIR_DISTRIBUTED_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"6.1.0-weekly.376"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-mimir-distributed.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
serviceAccount:
  name: mimir
mimir:
  structuredConfig:
    multitenancy_enabled: false
    limits:
      compactor_blocks_retention_period: 30d
      # "err":"server returned HTTP status 400 Bad Request: received a series whose number of labels exceeds the limit (actual: 31, limit: 30) ... (err-mimir-max-label-names-per-series). To adjust the related per-tenant limit, configure -validation.max-label-names-per-series, or contact your service administrator
      max_label_names_per_series: 50
      # Default is 150000
      max_global_series_per_user: 300000
    common:
      # https://grafana.com/docs/mimir/v2.17.x/configure/configuration-parameters/
      storage:
        backend: s3
        s3:
          endpoint: s3.</span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">.amazonaws.com
          region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
          storage_class: ONEZONE_IA
    alertmanager_storage:
      s3:
        bucket_name: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      storage_prefix: mimiralertmanager
    blocks_storage:
      s3:
        bucket_name: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      storage_prefix: mimirblocks
    ruler_storage:
      s3:
        bucket_name: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      storage_prefix: mimirruler
alertmanager:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: alertmanager
        topologyKey: kubernetes.io/hostname
distributor:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: distributor
        topologyKey: kubernetes.io/hostname
ingester:
  zoneAwareReplication:
    enabled: false
  replicas: 2
  priorityClassName: high-priority
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ingester
        topologyKey: kubernetes.io/hostname
overrides_exporter:
  priorityClassName: high-priority
ruler:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ruler
        topologyKey: kubernetes.io/hostname
ruler_querier:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ruler-querier
        topologyKey: kubernetes.io/hostname
ruler_query_frontend:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ruler-query-frontend
        topologyKey: kubernetes.io/hostname
ruler_query_scheduler:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: ruler-query-scheduler
        topologyKey: kubernetes.io/hostname
querier:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: querier
        topologyKey: kubernetes.io/hostname
query_frontend:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: query-frontend
        topologyKey: kubernetes.io/hostname
query_scheduler:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: query-scheduler
        topologyKey: kubernetes.io/hostname
store_gateway:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: store-gateway
        topologyKey: kubernetes.io/hostname
compactor:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: compactor
        topologyKey: kubernetes.io/hostname
# https://github.com/grafana/helm-charts/blob/main/charts/rollout-operator/values.yaml
rollout_operator:
  serviceMonitor:
    enabled: true
  priorityClassName: high-priority
minio:
  enabled: false
kafka:
  # 3 replicas required for Raft quorum (tolerates 1 node failure)
  replicas: 3
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: kafka
        topologyKey: kubernetes.io/hostname
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    gethomepage.dev/enabled: "true"
    gethomepage.dev/description: Grafana Mimir provides horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus
    gethomepage.dev/group: Apps
    gethomepage.dev/icon: https://raw.githubusercontent.com/grafana/mimir/843897414dba909dfd44f5b93dad35a8a6694d06/images/logo.png
    gethomepage.dev/name: Mimir
    nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/auth
    nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/start?rd=</span><span class="se">\$</span><span class="sh">scheme://</span><span class="se">\$</span><span class="sh">host</span><span class="se">\$</span><span class="sh">request_uri
  hosts:
    - mimir.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  tls:
    - hosts:
        - mimir.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
gateway:
  priorityClassName: high-priority
  replicas: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/component: gateway
        topologyKey: kubernetes.io/hostname
metaMonitoring:
  serviceMonitor:
    enabled: true
  prometheusRule:
    enabled: true
    mimirAlerts: true
    mimirRules: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MIMIR_DISTRIBUTED_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> mimir <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-mimir-distributed.yml"</span> mimir grafana/mimir-distributed
</pre></table></code></div></div><h3 id="loki"><span class="me-2">Loki</span><a href="#loki" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://grafana.com/oss/loki/">Grafana Loki</a> is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost-effective and easy to operate, as it does not index the contents of the logs, but rather a set of labels for each log stream.</p><p><a href="https://raw.githubusercontent.com/grafana/loki/5a8bc848dbe453ce27576d2058755a90f79d07b6/docs/sources/logo_and_name.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/grafana/loki/5a8bc848dbe453ce27576d2058755a90f79d07b6/docs/sources/logo_and_name.png" alt="Grafana Loki" width="300" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">loki</code> <a href="https://github.com/grafana/loki/tree/helm-loki-6.42.0/production/helm/loki">Helm chart</a> and customize its <a href="https://github.com/grafana/loki/blob/helm-loki-6.49.0/production/helm/loki/values.yaml">default values</a> to fit your environment and storage requirements:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=loki registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">LOKI_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"6.49.0"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-loki.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
global:
  priorityClassName: high-priority
deploymentMode: SingleBinary
loki:
  auth_enabled: false
  commonConfig:
    replication_factor: 2
  storage:
    bucketNames:
      chunks: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      ruler: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
      admin: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
    s3:
      region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
  schemaConfig:
    configs:
      - from: 2024-04-01
        store: tsdb
        object_store: s3
        schema: v13
        index:
          prefix: loki_index_
          period: 24h
  storage_config:
    aws:
      region: </span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">
      bucketnames: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  limits_config:
    retention_period: 1w
  # Log retention in Loki is achieved through the Compactor (https://grafana.com/docs/loki/v3.5.x/get-started/components/#compactor)
  compactor:
    delete_request_store: s3
    retention_enabled: true
lokiCanary:
  kind: Deployment
gateway:
  replicas: 2
singleBinary:
  replicas: 2
  priorityClassName: high-priority
  persistence:
    size: 5Gi
write:
  replicas: 0
read:
  replicas: 0
backend:
  replicas: 0
ruler:
  priorityClassName: high-priority
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">LOKI_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> loki <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-loki.yml"</span> loki grafana/loki
</pre></table></code></div></div><h3 id="tempo"><span class="me-2">Tempo</span><a href="#tempo" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://grafana.com/oss/tempo/">Grafana Tempo</a> is an open source, easy-to-use, and high-scale distributed tracing backend. It is designed to be cost-effective and simple to operate, as it only requires object storage to operate its backend and does not index the trace data.</p><p><a href="https://raw.githubusercontent.com/grafana/tempo/8dd75d18773d77149de8588f9dccbd680a03b00e/docs/sources/tempo/logo_and_name.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/grafana/tempo/8dd75d18773d77149de8588f9dccbd680a03b00e/docs/sources/tempo/logo_and_name.png" alt="Grafana Tempo" width="500" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">tempo</code> <a href="https://github.com/grafana/helm-charts/tree/main/charts/tempo">Helm chart</a> and customize its <a href="https://github.com/grafana/helm-charts/blob/tempo-distributed-1.61.3/charts/tempo/values.yaml">default values</a> to fit your environment and storage requirements:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=tempo registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">TEMPO_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"1.24.3"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-tempo.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
replicas: 2
tempo:
  # https://youtu.be/PmE9mgYaoQA?t=817
  metricsGenerator:
    enabled: true
    remoteWriteUrl: http://mimir-gateway.mimir.svc.cluster.local/api/v1/push
  storage:
    trace:
      backend: s3
      s3:
        bucket: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
        endpoint: s3.</span><span class="k">${</span><span class="nv">AWS_DEFAULT_REGION</span><span class="k">}</span><span class="sh">.amazonaws.com
serviceMonitor:
  enabled: true
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: tempo
        topologyKey: kubernetes.io/hostname
priorityClassName: high-priority
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TEMPO_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> tempo <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-tempo.yml"</span> tempo grafana/tempo
</pre></table></code></div></div><h3 id="pyroscope"><span class="me-2">Pyroscope</span><a href="#pyroscope" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://github.com/grafana/pyroscope">Grafana Pyroscope</a> is a Continuous Profiling Platform.</p><p><a href="https://raw.githubusercontent.com/grafana/pyroscope/d3818254b7c70a43104effcfd300ff885035ac50/images/logo.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/grafana/pyroscope/d3818254b7c70a43104effcfd300ff885035ac50/images/logo.png" alt="Grafana Pyroscope" width="100" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">pyroscope</code> <a href="https://github.com/grafana/pyroscope/tree/main/operations/pyroscope/helm/pyroscope">Helm chart</a> and customize its <a href="https://github.com/grafana/pyroscope/blob/v1.16.0/operations/pyroscope/helm/pyroscope/values.yaml">default values</a> to fit your environment and storage requirements:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=pyroscope registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">PYROSCOPE_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"1.17.0"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-pyroscope.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
pyroscope:
  replicaCount: 2
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/instance: pyroscope
          topologyKey: kubernetes.io/hostname
  priorityClassName: high-priority
ingress:
  enabled: true
  className: nginx
  annotations:
    gethomepage.dev/enabled: "true"
    gethomepage.dev/description: Continuous Profiling Platform
    gethomepage.dev/group: Observability
    gethomepage.dev/icon: https://raw.githubusercontent.com/grafana/pyroscope/d3818254b7c70a43104effcfd300ff885035ac50/images/logo.png
    gethomepage.dev/name: Pyroscope
    gethomepage.dev/pod-selector: app.kubernetes.io/instance=pyroscope
    nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/auth
    nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/start?rd=</span><span class="se">\$</span><span class="sh">scheme://</span><span class="se">\$</span><span class="sh">host</span><span class="se">\$</span><span class="sh">request_uri
  hosts:
    - pyroscope.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  tls:
    - hosts:
        - pyroscope.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
serviceMonitor:
  enabled: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">PYROSCOPE_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> pyroscope <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-pyroscope.yml"</span> pyroscope grafana/pyroscope
</pre></table></code></div></div><h3 id="grafana-kubernetes-monitoring-helm-chart"><span class="me-2">Grafana Kubernetes Monitoring Helm chart</span><a href="#grafana-kubernetes-monitoring-helm-chart" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The <a href="https://github.com/grafana/k8s-monitoring-helm/">Grafana Kubernetes Monitoring Helm chart</a> offers a complete solution for configuring infrastructure, zero-code instrumentation, and gathering telemetry.</p><p>For additional configuration options, refer to the <a href="https://grafana.com/docs/grafana-cloud/monitor-infrastructure/kubernetes-monitoring/configuration/helm-chart-config/helm-chart/">Helm chart documentation</a>.</p><p>Install the <code class="language-plaintext highlighter-rouge">k8s-monitoring</code> <a href="https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring">Helm chart</a> and customize its <a href="https://github.com/grafana/k8s-monitoring-helm/blob/v2.1.4/charts/k8s-monitoring/values.yaml">default values</a> to fit your environment and storage requirements:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=k8s-monitoring registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">K8S_MONITORING_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"3.7.1"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-k8s-monitoring.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
# Cluster identification used in metrics labels
cluster:
  name: "</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
# Backend destinations where telemetry data will be sent
destinations:
  # Metrics destination - sends to Mimir for long-term storage
  - name: prometheus
    type: prometheus
    url: http://mimir-gateway.mimir.svc.cluster.local/api/v1/push
  # Logs destination - sends to Loki for log aggregation
  - name: loki
    type: loki
    url: http://loki-gateway.loki.svc.cluster.local/loki/api/v1/push
  # Traces destination - sends to Tempo via OTLP protocol
  - name: otlpgateway
    type: otlp
    url: http://tempo.tempo.svc.cluster.local:4317
    tls:
      insecure: true
      insecureSkipVerify: true
  # Profiling destination - sends to Pyroscope for continuous profiling
  - name: pyroscope
    type: pyroscope
    url: http://pyroscope.pyroscope.svc.cluster.local:4040
    tls:
      insecure_skip_verify: true
# Collect K8s cluster-level metrics (nodes, pods, deployments, etc.)
clusterMetrics:
  enabled: true
  # Scrape metrics from the Kubernetes API server (Kubernetes / System / API Server)
  apiServer:
    enabled: true
  # Disable the default allowlist to allow Node Exporter Full dashboard to work properly - all node-exporter metrics will now be collected (https://github.com/grafana/k8s-monitoring-helm/issues/1296)
  node-exporter:
    metricsTuning:
      useDefaultAllowList: false
  windows-exporter:
    enabled: false
    deploy: false
  # Disable the default allowlist for kube-state-metrics to enable all namespace metrics required by "Kubernetes / Views / Global" dashboard (https://github.com/dotdc/grafana-dashboards-kubernetes/issues/176)
  kube-state-metrics:
    metricsTuning:
      useDefaultAllowList: false
# Collect Kubernetes events (pod scheduling, failures, etc.)
clusterEvents:
  enabled: true
# Collect logs from node-level services (kubelet, containerd)
nodeLogs:
  enabled: true
# Collect logs from all pods in the cluster
podLogs:
  enabled: true
# Enable application-level observability (traces and spans)
applicationObservability:
  enabled: true
  # Configure OTLP receivers for ingesting traces from applications
  receivers:
    otlp:
      grpc:
        enabled: true
      http:
        enabled: true
# Automatic instrumentation for supported languages (Java, Python, etc.)
autoInstrumentation:
  enabled: true
# Discover and scrape metrics from pods with Prometheus annotations - https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring/docs/examples/features/annotation-autodiscovery/prom-annotations
annotationAutodiscovery:
  enabled: true
  annotations:
    scrape: prometheus.io/scrape
    metricsPath: prometheus.io/path
    metricsPortNumber: prometheus.io/port
    metricsScheme: prometheus.io/scheme
# Support for ServiceMonitor and PodMonitor CRDs from Prometheus Operator
# https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring/charts/feature-prometheus-operator-objects
prometheusOperatorObjects:
  enabled: true
# Enable continuous profiling data collection
profiling:
  enabled: true
# Alloy collector for scraping and forwarding metrics
alloy-metrics:
  enabled: true
# Single-instance Alloy for cluster-wide tasks (kube-state-metrics), the Kubernetes Cluster events feature requires the use of the alloy-singleton collector.
alloy-singleton:
  enabled: true
# Alloy DaemonSet for collecting logs from each node
alloy-logs:
  enabled: true
# Alloy deployment for receiving OTLP data from applications
alloy-receiver:
  enabled: true
# Alloy for collecting profiling data via eBPF
alloy-profiles:
  enabled: true
# Common settings for all Alloy collector instances
collectorCommon:
  alloy:
    # Ensure collectors are scheduled even under resource pressure
    priorityClassName: system-node-critical
    controller:
      priorityClassName: system-node-critical
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">K8S_MONITORING_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> k8s-monitoring <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-k8s-monitoring.yml"</span> k8s-monitoring grafana/k8s-monitoring
</pre></table></code></div></div><h3 id="grafana"><span class="me-2">Grafana</span><a href="#grafana" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p><a href="https://github.com/grafana/grafana">Grafana</a> is an open-source analytics and monitoring platform that allows you to query, visualize, alert on, and understand your metrics, logs, and traces. It provides a powerful and flexible way to create dashboards and visualizations for monitoring your Kubernetes cluster and applications.</p><p><a href="https://raw.githubusercontent.com/grafana/grafana/cdca1518d2d2ee5d725517a8d8206b0cfa3656d0/public/img/grafana_text_logo_light.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/grafana/grafana/cdca1518d2d2ee5d725517a8d8206b0cfa3656d0/public/img/grafana_text_logo_light.svg" alt="Grafana" width="400" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">grafana</code> <a href="https://github.com/grafana/helm-charts/tree/main/charts/grafana">Helm chart</a> and modify its <a href="https://github.com/grafana/helm-charts/blob/grafana-10.4.0/charts/grafana/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=grafana registryUrl=https://grafana.github.io/helm-charts</span>
<span class="nv">GRAFANA_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"10.5.5"</span>

helm repo add <span class="nt">--force-update</span> grafana https://grafana.github.io/helm-charts
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-grafana.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
serviceMonitor:
  enabled: true
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    gethomepage.dev/description: Visualization Platform
    gethomepage.dev/enabled: "true"
    gethomepage.dev/group: Observability
    gethomepage.dev/icon: grafana.svg
    gethomepage.dev/name: Grafana
    gethomepage.dev/app: grafana
    gethomepage.dev/pod-selector: "app.kubernetes.io/name=grafana"
    nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/auth
    nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/start?rd=</span><span class="se">\$</span><span class="sh">scheme://</span><span class="se">\$</span><span class="sh">host</span><span class="se">\$</span><span class="sh">request_uri
    nginx.ingress.kubernetes.io/configuration-snippet: |
      auth_request_set </span><span class="se">\$</span><span class="sh">email </span><span class="se">\$</span><span class="sh">upstream_http_x_auth_request_email;
      proxy_set_header X-Email </span><span class="se">\$</span><span class="sh">email;
  path: /
  pathType: Prefix
  hosts:
    - grafana.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  tls:
    - hosts:
        - grafana.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
datasources:
  datasources.yaml:
    apiVersion: 1
    datasources:
      - name: Mimir
        type: prometheus
        url: http://mimir-gateway.mimir.svc.cluster.local/prometheus
        isDefault: true
        jsonData:
          prometheusType: Mimir
          prometheusVersion: 2.9.1
          # Scrape interval must match Prometheus/Mimir config for accurate rate() calculations (CPU usage in Node Exporter Full dashboard)
          timeInterval: "1m"
      - name: Loki
        type: loki
        url: http://loki-gateway.loki.svc.cluster.local/
        access: proxy
      - name: Tempo
        type: tempo
        url: http://tempo.tempo.svc.cluster.local:3200
        access: proxy
      - name: Pyroscope
        type: grafana-pyroscope-datasource
        url: http://pyroscope.pyroscope.svc.cluster.local:4040
      - name: Alertmanager
        type: alertmanager
        url: http://mimir-alertmanager.mimir.svc.cluster.local:8080
        access: proxy
        editable: true
        jsonData:
          implementation: mimir
notifiers:
  notifiers.yaml:
    notifiers:
    - name: email-notifier
      type: email
      uid: email1
      org_id: 1
      is_default: true
      settings:
        addresses: </span><span class="k">${</span><span class="nv">MY_EMAIL</span><span class="k">}</span><span class="sh">
dashboardProviders:
  dashboardproviders.yaml:
    apiVersion: 1
    providers:
      - name: "default"
        orgId: 1
        folder: ""
        type: file
        disableDeletion: false
        editable: false
        options:
          path: /var/lib/grafana/dashboards/default
dashboards:
  default:
    # keep-sorted start numeric=yes
    1860-node-exporter-full:
      # renovate: depName="Node Exporter Full"
      gnetId: 1860
      revision: 42
      datasource: Mimir
    9614-nginx-ingress-controller:
      # renovate: depName="NGINX Ingress controller"
      gnetId: 9614
      revision: 1
      datasource: Mimir
    # https://github.com/DevOps-Nirvana/Grafana-Dashboards
    14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana:
      # renovate: depName="Kubernetes Nginx Ingress Prometheus NextGen"
      gnetId: 14314
      revision: 2
      datasource: Mimir
    23969-external-dns:
      # renovate: depName="External DNS"
      gnetId: 23969
      revision: 1
      datasource: Mimir
    15757-kubernetes-views-global:
      # renovate: depName="Kubernetes / Views / Global"
      gnetId: 15757
      revision: 43
      datasource: Mimir
    15758-kubernetes-views-namespaces:
      # renovate: depName="Kubernetes / Views / Namespaces"
      gnetId: 15758
      revision: 44
      datasource: Mimir
    15759-kubernetes-views-nodes:
      # renovate: depName="Kubernetes / Views / Nodes"
      gnetId: 15759
      revision: 40
      datasource: Mimir
    # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes
    15760-kubernetes-views-pods:
      # renovate: depName="Kubernetes / Views / Pods"
      gnetId: 15760
      revision: 37
      datasource: Mimir
    15761-kubernetes-system-api-server:
      # renovate: depName="Kubernetes / System / API Server"
      gnetId: 15761
      revision: 20
      datasource: Mimir
    15762-kubernetes-system-coredns:
      # renovate: depName="Kubernetes / System / CoreDNS"
      gnetId: 15762
      revision: 22
      datasource: Mimir
    16006-mimir-alertmanager-resources:
      # renovate: depName="Mimir / Alertmanager resources"
      gnetId: 16006
      revision: 17
      datasource: Mimir
    16007-mimir-alertmanager:
      # renovate: depName="Mimir / Alertmanager"
      gnetId: 16007
      revision: 17
      datasource: Mimir
    16008-mimir-compactor-resources:
      # renovate: depName="Mimir / Compactor resources"
      gnetId: 16008
      revision: 17
      datasource: Mimir
    16009-mimir-compactor:
      # renovate: depName="Mimir / Compactor"
      gnetId: 16009
      revision: 17
      datasource: Mimir
    16010-mimir-config:
      # renovate: depName="Mimir / Config"
      gnetId: 16010
      revision: 17
      datasource: Mimir
    16011-mimir-object-store:
      # renovate: depName="Mimir / Object Store"
      gnetId: 16011
      revision: 17
      datasource: Mimir
    16012-mimir-overrides:
      # renovate: depName="Mimir / Overrides"
      gnetId: 16012
      revision: 17
      datasource: Mimir
    16013-mimir-queries:
      # renovate: depName="Mimir / Queries"
      gnetId: 16013
      revision: 17
      datasource: Mimir
    16014-mimir-reads-networking:
      # renovate: depName="Mimir / Reads networking"
      gnetId: 16014
      revision: 17
      datasource: Mimir
    16015-mimir-reads-resources:
      # renovate: depName="Mimir / Reads resources"
      gnetId: 16015
      revision: 17
      datasource: Mimir
    16016-mimir-reads:
      # renovate: depName="Mimir / Reads"
      gnetId: 16016
      revision: 17
      datasource: Mimir
    16017-mimir-rollout-progress:
      # renovate: depName="Mimir / Rollout progress"
      gnetId: 16017
      revision: 17
      datasource: Mimir
    16018-mimir-ruler:
      # renovate: depName="Mimir / Ruler"
      gnetId: 16018
      revision: 17
      datasource: Mimir
    16019-mimir-scaling:
      # renovate: depName="Mimir / Scaling"
      gnetId: 16019
      revision: 17
      datasource: Mimir
    16020-mimir-slow-queries:
      # renovate: depName="Mimir / Slow queries"
      gnetId: 16020
      revision: 17
      datasource: Mimir
    16021-mimir-tenants:
      # renovate: depName="Mimir / Tenants"
      gnetId: 16021
      revision: 17
      datasource: Mimir
    16022-mimir-top-tenants:
      # renovate: depName="Mimir / Top tenants"
      gnetId: 16022
      revision: 16
      datasource: Mimir
    16023-mimir-writes-networking:
      # renovate: depName="Mimir / Writes networking"
      gnetId: 16023
      revision: 16
      datasource: Mimir
    16024-mimir-writes-resources:
      # renovate: depName="Mimir / Writes resources"
      gnetId: 16024
      revision: 17
      datasource: Mimir
    16026-mimir-writes:
      # renovate: depName="Mimir / Writes"
      gnetId: 16026
      revision: 17
      datasource: Mimir
    17605-mimir-overview-networking:
      # renovate: depName="Mimir / Overview networking"
      gnetId: 17605
      revision: 13
      datasource: Mimir
    17606-mimir-overview-resources:
      # renovate: depName="Mimir / Overview resources"
      gnetId: 17606
      revision: 13
      datasource: Mimir
    17607-mimir-overview:
      # renovate: depName="Mimir / Overview"
      gnetId: 17607
      revision: 13
      datasource: Mimir
    17608-mimir-remote-ruler-reads:
      # renovate: depName="Mimir / Remote ruler reads"
      gnetId: 17608
      revision: 13
      datasource: Mimir
    17609-mimir-remote-ruler-reads-resources:
      # renovate: depName="Mimir / Remote ruler reads resources"
      gnetId: 17609
      revision: 13
      datasource: Mimir
    19923-beyla-red-metrics:
      # renovate: depName="Beyla RED Metrics"
      gnetId: 19923
      revision: 3
      datasource: Mimir
    20842-cert-manager-kubernetes:
      # renovate: depName="Cert-manager-Kubernetes"
      gnetId: 20842
      revision: 3
      datasource: Mimir
    22184-cert-manager2:
      # renovate: depName="cert-manager2"
      gnetId: 22184
      revision: 3
      datasource: Mimir
    22171-kubernetes-autoscaling-karpenter-overview:
      # renovate: depName="Kubernetes / Autoscaling / Karpenter / Overview"
      gnetId: 22171
      revision: 3
      datasource: Mimir
    22172-kubernetes-autoscaling-karpenter-activity:
      # renovate: depName="Kubernetes / Autoscaling / Karpenter / Activity"
      gnetId: 22172
      revision: 3
      datasource: Mimir
    22173-kubernetes-autoscaling-karpenter-performance:
      # renovate: depName="Kubernetes / Autoscaling / Karpenter / Performance"
      gnetId: 22173
      revision: 3
      datasource: Mimir
    23471-karpenter-cluster-cost-estimate:
      # renovate: depName="Karpenter Cluster Cost Estimate"
      gnetId: 23471
      revision: 1
      datasource: Mimir
    23838-velero-overview:
      # renovate: depName="Velero Overview"
      gnetId: 23838
      revision: 1
      datasource: Mimir
    # keep-sorted end
grafana.ini:
  analytics:
    check_for_updates: false
  auth.basic:
    enabled: false
  auth.proxy:
    enabled: true
    header_name: X-Email
    header_property: email
  smtp:
    enabled: true
    host: mailpit-smtp.mailpit.svc.cluster.local:25
    from_address: grafana@</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  users:
    auto_assign_org_role: Admin
networkPolicy:
  enabled: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">GRAFANA_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> grafana <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-grafana.yml"</span> grafana grafana/grafana
</pre></table></code></div></div><h3 id="mailpit"><span class="me-2">Mailpit</span><a href="#mailpit" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Mailpit will be used to receive email alerts from Prometheus.</p><p><a href="https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg" alt="mailpit" width="150" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">mailpit</code> <a href="https://artifacthub.io/packages/helm/jouve/mailpit">Helm chart</a> and modify its <a href="https://github.com/jouve/charts/blob/mailpit-0.31.0/charts/mailpit/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/</span>
<span class="nv">MAILPIT_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"0.31.0"</span>

helm repo add <span class="nt">--force-update</span> jouve https://jouve.github.io/charts/
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-mailpit.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    gethomepage.dev/enabled: "true"
    gethomepage.dev/description: An email and SMTP testing tool with API for developers
    gethomepage.dev/group: Apps
    gethomepage.dev/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg
    gethomepage.dev/name: Mailpit
    nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/auth
    nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/start?rd=</span><span class="se">\$</span><span class="sh">scheme://</span><span class="se">\$</span><span class="sh">host</span><span class="se">\$</span><span class="sh">request_uri
  hostname: mailpit.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">MAILPIT_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> mailpit <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-mailpit.yml"</span> mailpit jouve/mailpit
</pre></table></code></div></div><p>Screenshot:</p><p><a href="/assets/img/posts/2024/2024-05-03-secure-cheap-amazon-eks-with-pod-identities/mailpit.avif" class="popup img-link shimmer"><img src="/assets/img/posts/2024/2024-05-03-secure-cheap-amazon-eks-with-pod-identities/mailpit.avif" alt="Mailpit" width="700" loading="lazy"></a></p><h3 id="oauth2-proxy"><span class="me-2">OAuth2 Proxy</span><a href="#oauth2-proxy" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Use <a href="https://oauth2-proxy.github.io/oauth2-proxy/">OAuth2 Proxy</a> to protect application endpoints with Google Authentication.</p><p><a href="https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_horizontal.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_horizontal.svg" alt="OAuth2 Proxy" width="300" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">oauth2-proxy</code> <a href="https://artifacthub.io/packages/helm/oauth2-proxy/oauth2-proxy">Helm chart</a> and modify its <a href="https://github.com/oauth2-proxy/manifests/blob/oauth2-proxy-9.0.0/helm/oauth2-proxy/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests</span>
<span class="nv">OAUTH2_PROXY_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"10.0.0"</span>

<span class="nb">set</span> +x
<span class="nv">COOKIE_SECRET</span><span class="o">=</span><span class="s2">"</span><span class="si">$(</span>openssl rand <span class="nt">-base64</span> 32 | <span class="nb">head</span> <span class="nt">-c</span> 32 | <span class="nb">base64</span><span class="si">)</span><span class="s2">"</span>
<span class="nb">echo</span> <span class="s2">"::add-mask::</span><span class="k">${</span><span class="nv">COOKIE_SECRET</span><span class="k">}</span><span class="s2">"</span>
<span class="nb">set</span> <span class="nt">-x</span>

helm repo add <span class="nt">--force-update</span> oauth2-proxy https://oauth2-proxy.github.io/manifests
<span class="nb">tee</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-oauth2-proxy.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
config:
  clientID: </span><span class="k">${</span><span class="nv">GOOGLE_CLIENT_ID</span><span class="k">}</span><span class="sh">
  clientSecret: </span><span class="k">${</span><span class="nv">GOOGLE_CLIENT_SECRET</span><span class="k">}</span><span class="sh">
  cookieSecret: </span><span class="k">${</span><span class="nv">COOKIE_SECRET</span><span class="k">}</span><span class="sh">
  configFile: |-
    cookie_domains = ".</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">"
    set_authorization_header = "true"
    set_xauthrequest = "true"
    upstreams = [ "file:///dev/null" ]
    whitelist_domains = ".</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">"
authenticatedEmailsFile:
  enabled: true
  restricted_access: |-
    </span><span class="k">${</span><span class="nv">MY_EMAIL</span><span class="k">}</span><span class="sh">
ingress:
  enabled: true
  ingressClassName: nginx
  annotations:
    gethomepage.dev/enabled: "true"
    gethomepage.dev/description: A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers
    gethomepage.dev/group: Cluster Management
    gethomepage.dev/icon: https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_icon.svg
    gethomepage.dev/name: OAuth2-Proxy
  hosts:
    - oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  tls:
    - hosts:
        - oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
priorityClassName: critical-priority
affinity:
  podAntiAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
    - labelSelector:
        matchLabels:
          app.kubernetes.io/component: authentication-proxy
          app.kubernetes.io/instance: oauth2-proxy
      topologyKey: kubernetes.io/hostname
replicaCount: 2
metrics:
  servicemonitor:
    enabled: true
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">OAUTH2_PROXY_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> oauth2-proxy <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-oauth2-proxy.yml"</span> oauth2-proxy oauth2-proxy/oauth2-proxy
</pre></table></code></div></div><h3 id="homepage"><span class="me-2">Homepage</span><a href="#homepage" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Install <a href="https://gethomepage.dev/">Homepage</a> to provide a nice dashboard.</p><p><a href="https://raw.githubusercontent.com/gethomepage/homepage/e56dccc7f17144a53b97a315c2e4f622fa07e58d/images/banner_light%402x.png" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/gethomepage/homepage/e56dccc7f17144a53b97a315c2e4f622fa07e58d/images/banner_light%402x.png" alt="Homepage" width="400" loading="lazy"></a></p><p>Install the <code class="language-plaintext highlighter-rouge">homepage</code> <a href="https://github.com/jameswynn/helm-charts/tree/homepage-2.1.0/charts/homepage">Helm chart</a> and modify its <a href="https://github.com/jameswynn/helm-charts/blob/homepage-2.1.0/charts/homepage/values.yaml">default values</a>:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
</pre><td class="rouge-code"><pre><span class="c"># renovate: datasource=helm depName=homepage registryUrl=http://jameswynn.github.io/helm-charts</span>
<span class="nv">HOMEPAGE_HELM_CHART_VERSION</span><span class="o">=</span><span class="s2">"2.1.0"</span>

helm repo add <span class="nt">--force-update</span> jameswynn http://jameswynn.github.io/helm-charts
<span class="nb">cat</span> <span class="o">&gt;</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-homepage.yml"</span> <span class="o">&lt;&lt;</span> <span class="no">EOF</span><span class="sh">
enableRbac: true
serviceAccount:
  create: true
ingress:
  main:
    enabled: true
    annotations:
      gethomepage.dev/enabled: "true"
      gethomepage.dev/name: Homepage
      gethomepage.dev/description: A modern, secure, highly customizable application dashboard
      gethomepage.dev/group: Apps
      gethomepage.dev/icon: homepage.png
      nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/auth
      nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">/oauth2/start?rd=</span><span class="se">\$</span><span class="sh">scheme://</span><span class="se">\$</span><span class="sh">host</span><span class="se">\$</span><span class="sh">request_uri
    ingressClassName: nginx
    hosts:
      - host: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
        paths:
          - path: /
            pathType: Prefix
    tls:
      - hosts:
          - </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
config:
  bookmarks:
  services:
  widgets:
    - logo:
        icon: kubernetes.svg
    - kubernetes:
        cluster:
          show: true
          cpu: true
          memory: true
          showLabel: true
          label: "</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="sh">"
        nodes:
          show: true
          cpu: true
          memory: true
          showLabel: true
  kubernetes:
    mode: cluster
  settings:
    hideVersion: true
    title: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
    favicon: https://raw.githubusercontent.com/homarr-labs/dashboard-icons/38631ad11695467d7a9e432d5fdec7a39a31e75f/svg/kubernetes.svg
    layout:
      Apps:
        icon: mdi-apps
      Observability:
        icon: mdi-chart-bell-curve-cumulative
      Cluster Management:
        icon: mdi-tools
env:
  - name: HOMEPAGE_ALLOWED_HOSTS
    value: </span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="sh">
  - name: LOG_TARGETS
    value: stdout
</span><span class="no">EOF
</span>helm upgrade <span class="nt">--install</span> <span class="nt">--version</span> <span class="s2">"</span><span class="k">${</span><span class="nv">HOMEPAGE_HELM_CHART_VERSION</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--namespace</span> homepage <span class="nt">--create-namespace</span> <span class="nt">--values</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">/helm_values-homepage.yml"</span> homepage jameswynn/homepage
</pre></table></code></div></div><h2 id="clean-up"><span class="me-2">Clean-up</span><a href="#clean-up" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p><a href="https://raw.githubusercontent.com/cubanpit/cleanupdate/7aaccaa36ab4888a0847b267ed24d079dfed7863/icons/cleanupdate.svg" class="popup img-link shimmer"><img src="https://raw.githubusercontent.com/cubanpit/cleanupdate/7aaccaa36ab4888a0847b267ed24d079dfed7863/icons/cleanupdate.svg" alt="Clean-up" width="150" loading="lazy"></a></p><p>Back up the certificate before deleting the cluster (in case it was renewed):</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="o">[[</span> <span class="s2">"</span><span class="si">$(</span>kubectl get <span class="nt">--raw</span> /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | <span class="nb">awk</span> <span class="s1">'/certmanager_http_acme_client_request_count.*acme-v02\.api.*finalize/ { print $2 }'</span><span class="si">)</span><span class="s2">"</span> <span class="nt">-gt</span> 0 <span class="o">]]</span><span class="p">;</span> <span class="k">then
  </span>velero backup create <span class="nt">--labels</span> <span class="nv">letsencrypt</span><span class="o">=</span>production <span class="nt">--ttl</span> 2160h <span class="nt">--from-schedule</span> velero-monthly-backup-cert-manager-production
<span class="k">fi</span>
</pre></table></code></div></div><p>Stop Karpenter from launching additional nodes:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>helm uninstall <span class="nt">-n</span> karpenter karpenter <span class="o">||</span> <span class="nb">true
</span>helm uninstall <span class="nt">-n</span> ingress-nginx ingress-nginx <span class="o">||</span> <span class="nb">true</span>
</pre></table></code></div></div><p>Remove any remaining EC2 instances provisioned by Karpenter (if they still exist):</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">for </span>EC2 <span class="k">in</span> <span class="si">$(</span>aws ec2 describe-instances <span class="nt">--filters</span> <span class="s2">"Name=tag:kubernetes.io/cluster/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">,Values=owned"</span> <span class="s2">"Name=tag:karpenter.sh/nodepool,Values=*"</span> <span class="nv">Name</span><span class="o">=</span>instance-state-name,Values<span class="o">=</span>running <span class="nt">--query</span> <span class="s2">"Reservations[].Instances[].InstanceId"</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nb">echo</span> <span class="s2">"🗑️  Removing Karpenter EC2: </span><span class="k">${</span><span class="nv">EC2</span><span class="k">}</span><span class="s2">"</span>
  aws ec2 terminate-instances <span class="nt">--instance-ids</span> <span class="s2">"</span><span class="k">${</span><span class="nv">EC2</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</pre></table></code></div></div><p>Disassociate a Route 53 Resolver query log configuration from an Amazon VPC:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">for </span>RESOLVER_QUERY_LOG_CONFIGS_ID <span class="k">in</span> <span class="si">$(</span>aws route53resolver list-resolver-query-log-configs <span class="nt">--query</span> <span class="s2">"ResolverQueryLogConfigs[?contains(DestinationArn, '/aws/eks/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">/cluster')].Id"</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nv">RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID</span><span class="o">=</span><span class="si">$(</span>aws route53resolver list-resolver-query-log-config-associations <span class="nt">--filters</span> <span class="s2">"Name=ResolverQueryLogConfigId,Values=</span><span class="k">${</span><span class="nv">RESOLVER_QUERY_LOG_CONFIGS_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s1">'ResolverQueryLogConfigAssociations[].ResourceId'</span> <span class="nt">--output</span> text<span class="si">)</span>
  <span class="k">if</span> <span class="o">[[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID</span><span class="k">}</span><span class="s2">"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
    </span>aws route53resolver disassociate-resolver-query-log-config <span class="nt">--resolver-query-log-config-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">RESOLVER_QUERY_LOG_CONFIGS_ID</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--resource-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID</span><span class="k">}</span><span class="s2">"</span>
    <span class="nb">sleep </span>5
  <span class="k">fi
done</span>
</pre></table></code></div></div><p>Clean up AWS Route 53 Resolver query log configurations:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">for </span>AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID <span class="k">in</span> <span class="si">$(</span>aws route53resolver list-resolver-query-log-configs <span class="nt">--query</span> <span class="s2">"ResolverQueryLogConfigs[?Name=='</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-vpc-dns-logs'].Id"</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">do
  </span>aws route53resolver delete-resolver-query-log-config <span class="nt">--resolver-query-log-config-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</pre></table></code></div></div><p>Remove the EKS cluster and its created components:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">if </span>eksctl get cluster <span class="nt">--name</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span><span class="p">;</span> <span class="k">then
  </span>eksctl delete cluster <span class="nt">--name</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--force</span>
<span class="k">fi</span>
</pre></table></code></div></div><p>Remove the Route 53 DNS records from the DNS Zone:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="nv">CLUSTER_FQDN_ZONE_ID</span><span class="o">=</span><span class="si">$(</span>aws route53 list-hosted-zones <span class="nt">--query</span> <span class="s2">"HostedZones[?Name==</span><span class="se">\`</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">.</span><span class="se">\`</span><span class="s2">].Id"</span> <span class="nt">--output</span> text<span class="si">)</span>
<span class="k">if</span> <span class="o">[[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN_ZONE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
  </span>aws route53 list-resource-record-sets <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN_ZONE_ID</span><span class="k">}</span><span class="s2">"</span> | jq <span class="nt">-c</span> <span class="s1">'.ResourceRecordSets[] | select (.Type != "SOA" and .Type != "NS")'</span> |
    <span class="k">while </span><span class="nb">read</span> <span class="nt">-r</span> RESOURCERECORDSET<span class="p">;</span> <span class="k">do
      </span>aws route53 change-resource-record-sets <span class="se">\</span>
        <span class="nt">--hosted-zone-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_FQDN_ZONE_ID</span><span class="k">}</span><span class="s2">"</span> <span class="se">\</span>
        <span class="nt">--change-batch</span> <span class="s1">'{"Changes":[{"Action":"DELETE","ResourceRecordSet": '</span><span class="s2">"</span><span class="k">${</span><span class="nv">RESOURCERECORDSET</span><span class="k">}</span><span class="s2">"</span><span class="s1">' }]}'</span> <span class="se">\</span>
        <span class="nt">--output</span> text <span class="nt">--query</span> <span class="s1">'ChangeInfo.Id'</span>
    <span class="k">done
fi</span>
</pre></table></code></div></div><p>Delete Instance profile which belongs to Karpenter role:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">if </span><span class="nv">AWS_INSTANCE_PROFILES_FOR_ROLE</span><span class="o">=</span><span class="si">$(</span>aws iam list-instance-profiles-for-role <span class="nt">--role-name</span> <span class="s2">"KarpenterNodeRole-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--query</span> <span class="s1">'InstanceProfiles[].{Name:InstanceProfileName}'</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">then
  if</span> <span class="o">[[</span> <span class="nt">-n</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_INSTANCE_PROFILES_FOR_ROLE</span><span class="k">}</span><span class="s2">"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
    </span>aws iam remove-role-from-instance-profile <span class="nt">--instance-profile-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_INSTANCE_PROFILES_FOR_ROLE</span><span class="k">}</span><span class="s2">"</span> <span class="nt">--role-name</span> <span class="s2">"KarpenterNodeRole-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span>
    aws iam delete-instance-profile <span class="nt">--instance-profile-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">AWS_INSTANCE_PROFILES_FOR_ROLE</span><span class="k">}</span><span class="s2">"</span>
  <span class="k">fi
fi</span>
</pre></table></code></div></div><p>Remove the CloudFormation stack:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre>aws cloudformation delete-stack <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-route53-kms"</span>
aws cloudformation delete-stack <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-karpenter"</span>
aws cloudformation <span class="nb">wait </span>stack-delete-complete <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-route53-kms"</span>
aws cloudformation <span class="nb">wait </span>stack-delete-complete <span class="nt">--stack-name</span> <span class="s2">"</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-karpenter"</span>
aws cloudformation <span class="nb">wait </span>stack-delete-complete <span class="nt">--stack-name</span> <span class="s2">"eksctl-</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-cluster"</span>
</pre></table></code></div></div><p>Remove volumes and snapshots related to the cluster (as a precaution):</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">for </span>VOLUME <span class="k">in</span> <span class="si">$(</span>aws ec2 describe-volumes <span class="nt">--filter</span> <span class="s2">"Name=tag:KubernetesCluster,Values=</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">"</span> <span class="s2">"Name=tag:kubernetes.io/cluster/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">,Values=owned"</span> <span class="nt">--query</span> <span class="s1">'Volumes[].VolumeId'</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nb">echo</span> <span class="s2">"Removing Volume: </span><span class="k">${</span><span class="nv">VOLUME</span><span class="k">}</span><span class="s2">"</span>
  aws ec2 delete-volume <span class="nt">--volume-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">VOLUME</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>

<span class="c"># Remove EBS snapshots associated with the cluster</span>
<span class="k">for </span>SNAPSHOT <span class="k">in</span> <span class="si">$(</span>aws ec2 describe-snapshots <span class="nt">--owner-ids</span> self <span class="nt">--filter</span> <span class="s2">"Name=tag:Name,Values=</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">-dynamic-snapshot*"</span> <span class="s2">"Name=tag:kubernetes.io/cluster/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">,Values=owned"</span> <span class="nt">--query</span> <span class="s1">'Snapshots[].SnapshotId'</span> <span class="nt">--output</span> text<span class="si">)</span><span class="p">;</span> <span class="k">do
  </span><span class="nb">echo</span> <span class="s2">"Removing Snapshot: </span><span class="k">${</span><span class="nv">SNAPSHOT</span><span class="k">}</span><span class="s2">"</span>
  aws ec2 delete-snapshot <span class="nt">--snapshot-id</span> <span class="s2">"</span><span class="k">${</span><span class="nv">SNAPSHOT</span><span class="k">}</span><span class="s2">"</span>
<span class="k">done</span>
</pre></table></code></div></div><p>Remove the CloudWatch log group:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="o">[[</span> <span class="s2">"</span><span class="si">$(</span>aws logs describe-log-groups <span class="nt">--query</span> <span class="s2">"logGroups[?logGroupName==</span><span class="se">\`</span><span class="s2">/aws/eks/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">/cluster</span><span class="se">\`</span><span class="s2">] | [0].logGroupName"</span> <span class="nt">--output</span> text<span class="si">)</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"/aws/eks/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">/cluster"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
  </span>aws logs delete-log-group <span class="nt">--log-group-name</span> <span class="s2">"/aws/eks/</span><span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span><span class="s2">/cluster"</span>
<span class="k">fi</span>
</pre></table></code></div></div><p>Remove the <code class="language-plaintext highlighter-rouge">${TMP_DIR}/${CLUSTER_FQDN}</code> directory:</p><div class="language-sh highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">if</span> <span class="o">[[</span> <span class="nt">-d</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
  for </span>FILE <span class="k">in</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">"</span>/<span class="o">{</span>kubeconfig-<span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span>.conf,<span class="o">{</span>aws-cf-route53-kms,cloudformation-karpenter,eksctl-<span class="k">${</span><span class="nv">CLUSTER_NAME</span><span class="k">}</span>,helm_values-<span class="o">{</span>aws-load-balancer-controller,cert-manager,external-dns,grafana,homepage,ingress-nginx,k8s-monitoring,karpenter,loki,mailpit,mimir-distributed,oauth2-proxy,pyroscope,tempo,velero<span class="o">}</span>,k8s-<span class="o">{</span>karpenter-nodepool,scheduling-priorityclass,storage-snapshot-storageclass-volumesnapshotclass<span class="o">}}</span>.yml<span class="o">}</span><span class="p">;</span> <span class="k">do
    if</span> <span class="o">[[</span> <span class="nt">-f</span> <span class="s2">"</span><span class="k">${</span><span class="nv">FILE</span><span class="k">}</span><span class="s2">"</span> <span class="o">]]</span><span class="p">;</span> <span class="k">then
      </span><span class="nb">rm</span> <span class="nt">-v</span> <span class="s2">"</span><span class="k">${</span><span class="nv">FILE</span><span class="k">}</span><span class="s2">"</span>
    <span class="k">else
      </span><span class="nb">echo</span> <span class="s2">"❌ File not found: </span><span class="k">${</span><span class="nv">FILE</span><span class="k">}</span><span class="s2">"</span>
    <span class="k">fi
  done
  </span><span class="nb">rmdir</span> <span class="s2">"</span><span class="k">${</span><span class="nv">TMP_DIR</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">CLUSTER_FQDN</span><span class="k">}</span><span class="s2">"</span>
<span class="k">fi</span>
</pre></table></code></div></div><p>Enjoy … 😉</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw me-1"></i> <a href="/categories/kubernetes/">Kubernetes</a>, <a href="/categories/cloud/">Cloud</a>, <a href="/categories/security/">Security</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/amazon-eks/" class="post-tag no-text-decoration" >amazon-eks</a> <a href="/tags/kubernetes/" class="post-tag no-text-decoration" >kubernetes</a> <a href="/tags/security/" class="post-tag no-text-decoration" >security</a> <a href="/tags/eksctl/" class="post-tag no-text-decoration" >eksctl</a> <a href="/tags/cert-manager/" class="post-tag no-text-decoration" >cert-manager</a> <a href="/tags/external-dns/" class="post-tag no-text-decoration" >external-dns</a> <a href="/tags/prometheus/" class="post-tag no-text-decoration" >prometheus</a> <a href="/tags/grafana/" class="post-tag no-text-decoration" >grafana</a> <a href="/tags/sso/" class="post-tag no-text-decoration" >sso</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Amazon%20EKS%20and%20Grafana%20stack%20-%20Petr's%20Blog&url=https%3A%2F%2Fruzickap.github.io%2Fposts%2Famazon-eks-grafana-stack%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Amazon%20EKS%20and%20Grafana%20stack%20-%20Petr's%20Blog&u=https%3A%2F%2Fruzickap.github.io%2Fposts%2Famazon-eks-grafana-stack%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fruzickap.github.io%2Fposts%2Famazon-eks-grafana-stack%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Linkedin" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <script defer type="module" src="https://cdn.jsdelivr.net/npm/@justinribeiro/share-to-mastodon/+esm"></script> <button class="btn text-start" data-bs-toggle="tooltip" data-bs-placement="top" title="Mastodon" aria-label="Mastodon"> <share-to-mastodon class="share-mastodon" message="Amazon%20EKS%20and%20Grafana%20stack%20-%20Petr's%20Blog" url="https%3A%2F%2Fruzickap.github.io%2Fposts%2Famazon-eks-grafana-stack%2F"customInstanceList="[{&quot;label&quot;:&quot;mastodon.social&quot;,&quot;link&quot;:&quot;https://mastodon.social/&quot;},{&quot;label&quot;:&quot;mastodon.online&quot;,&quot;link&quot;:&quot;https://mastodon.online/&quot;},{&quot;label&quot;:&quot;fosstodon.org&quot;,&quot;link&quot;:&quot;https://fosstodon.org/&quot;},{&quot;label&quot;:&quot;photog.social&quot;,&quot;link&quot;:&quot;https://photog.social/&quot;}]" > <i class="fa-fw fa-brands fa-mastodon"></i> </share-to-mastodon> </button> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/installtion-f5-bigip-virtual-edition-to-rhel7/">Installation F5 BIGIP Virtual Edition to RHEL7</a><li class="text-truncate lh-lg"> <a href="/posts/detect-a-hacker-attacks-eks-vm/">Detect the hacker attacks on Amazon EKS and EC2 instances</a><li class="text-truncate lh-lg"> <a href="/posts/secure-cheap-amazon-eks-auto/">Build secure and cheap Amazon EKS Auto Mode</a><li class="text-truncate lh-lg"> <a href="/posts/eks-auto-cert-manager-velero/">Amazon EKS Auto Mode with cert-manager and Velero</a><li class="text-truncate lh-lg"> <a href="/posts/mcp-servers-k8s/">MCP Servers running on Kubernetes</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag btn btn-outline-primary" href="/tags/amazon-eks/">amazon-eks</a> <a class="post-tag btn btn-outline-primary" href="/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/tags/cert-manager/">cert-manager</a> <a class="post-tag btn btn-outline-primary" href="/tags/eksctl/">eksctl</a> <a class="post-tag btn btn-outline-primary" href="/tags/bash/">bash</a> <a class="post-tag btn btn-outline-primary" href="/tags/pxe/">pxe</a> <a class="post-tag btn btn-outline-primary" href="/tags/wifi/">wifi</a> <a class="post-tag btn btn-outline-primary" href="/tags/prometheus/">prometheus</a> <a class="post-tag btn btn-outline-primary" href="/tags/automation/">automation</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/secure-cheap-amazon-eks-auto/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1734130800" data-df="ll" > Dec 14, 2024 </time><h4 class="pt-0 my-2">Build secure and cheap Amazon EKS Auto Mode</h4><div class="text-muted"><p>Build "cheap and secure" Amazon EKS Auto Mode with network policies, cluster encryption and logging</p></div></div></a></article><article class="col"> <a href="/posts/secure-cheap-amazon-eks-with-pod-identities/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1714687200" data-df="ll" > May 3, 2024 </time><h4 class="pt-0 my-2">Build secure and cheap Amazon EKS with Pod Identities</h4><div class="text-muted"><p>Build "cheap and secure" Amazon EKS with Pod Identities, network policies, cluster encryption and logging</p></div></div></a></article><article class="col"> <a href="/posts/secure-cheap-amazon-eks/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1695592800" data-df="ll" > Sep 25, 2023 </time><h4 class="pt-0 my-2">Build secure and cheap Amazon EKS</h4><div class="text-muted"><p>Build "cheap and secure" Amazon EKS with network policies, cluster encryption and logging</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/ollama-k8s-exploitation/" class="btn btn-outline-primary" aria-label="Older" ><p>Exploiting RCE Vulnerabilities in Ollama on Kubernetes</p></a> <a href="/posts/bootstrap-eks-auto-mode-bootstrap-ack-kro-velero/" class="btn btn-outline-primary" aria-label="Newer" ><p>Bootstrap EKS Auto Mode Cluster with ACK and kro using Kind</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2026</time> <a href="https://twitter.com/Ruzicka_Petr">Petr Ruzicka</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.4.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/kubernetes/">kubernetes</a> <a class="post-tag btn btn-outline-primary" href="/tags/amazon-eks/">amazon-eks</a> <a class="post-tag btn btn-outline-primary" href="/tags/security/">security</a> <a class="post-tag btn btn-outline-primary" href="/tags/cert-manager/">cert-manager</a> <a class="post-tag btn btn-outline-primary" href="/tags/eksctl/">eksctl</a> <a class="post-tag btn btn-outline-primary" href="/tags/bash/">bash</a> <a class="post-tag btn btn-outline-primary" href="/tags/pxe/">pxe</a> <a class="post-tag btn btn-outline-primary" href="/tags/wifi/">wifi</a> <a class="post-tag btn btn-outline-primary" href="/tags/prometheus/">prometheus</a> <a class="post-tag btn btn-outline-primary" href="/tags/automation/">automation</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> (function () { const themeMapper = Theme.getThemeMapper('light', 'dark_dimmed'); const initTheme = themeMapper[Theme.visualState]; let lang = 'en';if (lang.length > 2 && !lang.startsWith('zh')) { lang = lang.slice(0, 2); } let giscusAttributes = { src: 'https://giscus.app/client.js', 'data-repo': 'ruzickap/ruzickap.github.io', 'data-repo-id': 'R_kgDOGkp4nQ', 'data-category': 'General', 'data-category-id': 'DIC_kwDOGkp4nc4CSdu1', 'data-mapping': 'pathname', 'data-strict' : '0', 'data-reactions-enabled': '1', 'data-emit-metadata': '0', 'data-theme': initTheme, 'data-input-position': 'bottom', 'data-lang': lang, 'data-loading': 'lazy', crossorigin: 'anonymous', async: '' }; let giscusNode = document.createElement('script'); Object.entries(giscusAttributes).forEach(([key, value]) => giscusNode.setAttribute(key, value) ); const $footer = document.querySelector('footer'); $footer.insertAdjacentElement("beforebegin", giscusNode); addEventListener('message', (event) => { if (event.source === window && event.data && event.data.id === Theme.ID) { const newTheme = themeMapper[Theme.visualState]; const message = { setConfig: { theme: newTheme } }; const giscus = document.getElementsByClassName('giscus-frame')[0].contentWindow; giscus.postMessage({ giscus: message }, 'https://giscus.app'); } }); })(); </script> <script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
